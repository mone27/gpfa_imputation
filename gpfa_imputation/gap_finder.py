# AUTOGENERATED! DO NOT EDIT! File to edit: ../lib_nbs/20_gap_finder.ipynb.

# %% auto 0
__all__ = ['find_gap', 'find_all_gaps', 'scan_fluxnet_csv', 'get_site_info', 'find_gaps_fluxnet_archive', 'download_fluxnet',
           'download_and_find_gaps']

# %% ../lib_nbs/20_gap_finder.ipynb 2
import polars as pl
import zipfile
from pathlib import Path
import requests
import re
from tqdm.auto import tqdm

# %% ../lib_nbs/20_gap_finder.ipynb 24
def find_gap(df, col_name):
    return df.select(
        [col_name, "TIMESTAMP_END"]
    ).with_column(
        pl.first().cumcount().alias("row_num")
    ).filter(
        pl.col(col_name) != 0
    ).with_columns([
        (pl.col("row_num") - pl.col("row_num").shift() ).alias("before"),
        (pl.col("row_num").shift(-1) - pl.col("row_num")).alias("after"),
    ]).filter(
        (pl.col("before") != 1) | (pl.col("after") != 1)
    ).with_column(
        (pl.when((pl.col("before") != 1) & (pl.col("after") != 1))
        .then(1)
        .otherwise(pl.col("row_num").shift(-1) - pl.col("row_num") + 1)
        .alias("gap_len"))
    ).filter(
        pl.col("before") != 1
    ).select(
        ["TIMESTAMP_END", "gap_len", pl.lit(col_name).alias("variable")]
    )

# %% ../lib_nbs/20_gap_finder.ipynb 27
def find_all_gaps(df):
    return pl.concat(
        [find_gap(df, col_name) for col_name in df.select(pl.col("^.*_QC$")).columns]
    )

# %% ../lib_nbs/20_gap_finder.ipynb 30
def scan_fluxnet_csv(f):
    
    # col names may be different between the stations, so read them from the csv before parsing the whole file
    col_names = pl.read_csv(f, n_rows=1).columns
        
    types = {
        **{
            # pl.Uint8 should be enough for a QC flag, but some columns are floats in the csv ...
            col_name: pl.Float32 if col_name.endswith("_QC") else pl.Float64 for col_name in col_names
        },
        "TIMESTAMP_START": pl.Int64,
        "TIMESTAMP_END": pl.Int64
    } 
    
    return pl.scan_csv(f, null_values=["-9999", "-9999.99"], dtypes=types)
    

# %% ../lib_nbs/20_gap_finder.ipynb 31
def get_site_info(df, site):
    return df.select([
        pl.col("TIMESTAMP_END").first().alias("start"),
        pl.col("TIMESTAMP_END").last().alias("end"),
        pl.lit(site).alias("site")
    ]).collect()

# %% ../lib_nbs/20_gap_finder.ipynb 33
def _get_site_url(url): return re.search(r"[A-Z]{2}-[A-z0-9]{3}", url).group()
    
def find_gaps_fluxnet_archive(path_zip, # zip file path that uses fluxnet 
                  out_dir,
                  tmp_dir,
                  delete_file = True
                         ):
    try:
        fname = path_zip.stem.replace("FULLSET", "FULLSET_HH") 
        out_name = out_dir / f"GAPS_stat_{fname}.parquet"
        f = zipfile.ZipFile(path_zip).extract(fname + ".csv", path=tmp_dir)
    except KeyError:
        fname = path_zip.stem.replace("FULLSET", "FULLSET_HR") # some sites are naed differently
        out_name = out_dir / f"GAPS_stat_{fname}.parquet"
        f = zipfile.ZipFile(path_zip).extract(fname + ".csv", path=tmp_dir)
    
    df = scan_fluxnet_csv(f)
    
    site = _get_site_url(fname)
    site_info = get_site_info(df, site)
    
    
    find_all_gaps(df).collect().write_parquet(out_name)
    
    if delete_file: Path(f).unlink()
    
    return fname, site_info

# %% ../lib_nbs/20_gap_finder.ipynb 43
def download_fluxnet(url, download_dir):
    
    
    file_name = download_dir / re.search(r"([^/]*)\?", url).group()[:-1] 
    
    if file_name.exists(): return file_name
    
    n_iter = int(requests.head(url).headers['Content-Length']) / 1024
    r = requests.get(url, allow_redirects=True, stream=True)
    n_iter = int(r.headers['Content-Length'])
    with open(file_name, 'wb') as file:
        with tqdm(total=n_iter, unit_divisor=1024, unit_scale=True, unit='B') as pbar:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:
                    file.write(chunk)
                    pbar.set_postfix(site=file_name.name[:10], refresh=False)
                    pbar.update(1024) # one chunck
    
    return file_name

# %% ../lib_nbs/20_gap_finder.ipynb 46
def download_and_find_gaps(urls, download_dir, out_dir, tmp_dir):
    site_infos = []
    for url in tqdm(urls):
        file_zip = download_fluxnet(url, download_dir)
        file, site_info = find_gaps_fluxnet_archive(file_zip, out_dir, tmp_dir)
        site_infos.append(site_info)
        print(file)
        
    return pl.concat(site_infos)
