# AUTOGENERATED! DO NOT EDIT! File to edit: ../lib_nbs/10_SimpleGP_Imputation.ipynb.

# %% auto 0
__all__ = ['SimpleGP', 'SimpleGPLearner', 'SimpleGPImputationExplorer']

# %% ../lib_nbs/10_SimpleGP_Imputation.ipynb 2
from .data_preparation import *
from .gpfa.learner import NormParam
from .results import ImputationResult

from fastcore.foundation import patch 
import gpytorch
import torch
from torch import Tensor
from tqdm.auto import tqdm

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# %% ../lib_nbs/10_SimpleGP_Imputation.ipynb 4
class SimpleGP(gpytorch.models.ExactGP):
    "Exact GP implemnetation using GPyTorch"
    def __init__(self, train_x, train_y, likelihood):
        super().__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())

    def forward(self, x, **params):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x, **params)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# %% ../lib_nbs/10_SimpleGP_Imputation.ipynb 9
@patch
def get_info(self: SimpleGP,
             var_names = None # Optional variable names for better printing
            ) -> dict[str, pd.DataFrame]:
    "Model info for a GPFA with a RBFKernel"
    out = {}

    out["lengthscale"] = pd.DataFrame({'lengthscale': [self.covar_module.base_kernel.lengthscale.item()]})
    out["outputscale"] = pd.DataFrame({'outputscale': [self.covar_module.outputscale.item()]})
    out["likelihood"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.item()]})

    return out

# %% ../lib_nbs/10_SimpleGP_Imputation.ipynb 12
class SimpleGPLearner():
    "Learner for a simple GP process. It handles only 1 dimensional time series"
    def __init__(self,
                 X: Tensor, # (n_obs) Univariate time series
                 T: Tensor = None, # (n_obs) Vector of time of observations.
                 # If none each observation are considered to be at the same distance
                ):
        self.prepare_X(X)
        if T is None: self.default_time(X)
        else: self.T = T
        self.T = self.T.to(X.device) # to support GPUs
        
        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()
        self.model = SimpleGP(self.T, self.X, self.likelihood)
                
    def prepare_X(self, X):
        self.norm = Standardizer(X)
        self.X = self.norm.normalize(X)
        
    def default_time(self, X):
        self.T = torch.arange(X.shape[0])
        
    
    def train(self, n_iter=100, lr=0.1):
        # need to enable training mode
        self.model.train()
        self.likelihood.train()
        
        # Use the adam optimizer
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr) 
        
        # create new losses
        if not hasattr(self, 'losses'):
            self.losses = torch.zeros(n_iter)
            loss_offset = 0
        # append to previous losses
        else:
            loss_offset = self.losses.shape[0]
            self.losses = torch.concat([self.losses, torch.zeros(n_iter)])
            
        # "Loss" for GPs - the marginal log likelihood
        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)
        for i in tqdm(range(n_iter)):
            # Zero gradients from previous iteration
            optimizer.zero_grad()
            # Output from model
            output = self.model(self.T)
            # Calc loss and backprop gradients
            loss = -mll(output, self.X)
            self.losses[i + loss_offset] = loss.detach()
            loss.backward()
            self.printer(i)

            optimizer.step()
        
        
    def printer(self, i):
        pass
    
    @torch.no_grad() # don't calc gradients on predictions
    def predict_raw(self, T):
        self.model.eval()
        self.likelihood.eval()
        return self.likelihood(self.model(T))

    @torch.no_grad() # needed because raw output still has gradients attached
    def prediction_from_raw(self, raw_mean, raw_std):
        """ Takes a raw prediction and produces and final prediction, by reshaping and reversing normalization"""
        pred_mean = self.norm.reverse_normalize(raw_mean)
        pred_std = self.norm.reverse_normalize_std(raw_std)

        #remove pytorch gradients
        return NormParam(pred_mean.detach(), pred_std.detach())

    def predict(self, T):
        pred_raw = self.predict_raw(T)
        return self.prediction_from_raw(pred_raw.mean, pred_raw.stddev)

# %% ../lib_nbs/10_SimpleGP_Imputation.ipynb 19
class SimpleGPImputationExplorer:
    def __init__(
        self,
        data: pd.DataFrame , #observed data with missing data as NA
        cuda = False, # Use GPU?
    ):
        self.data = data
        
        device = 'cuda' if cuda else 'cpu'
        
        self.T = torch.arange(0, len(data), dtype=torch.float32, device=device) # time is encoded with a increase of 1
        
        # Training data
        self.train_idx = ~self.data.isna().any(axis=1)
        self.train_data = torch.tensor(self.data[self.train_idx].to_numpy().astype(np.float32), device=device)
        self.train_T = self.T[self.train_idx]
        
        self.learners = []
        
        for i in range(self.data.shape[1]):
            self.make_var_learner(i)
        
                       
        # if cuda: self.learner.cuda()
        
    def make_var_learner(self, i):
        # get only the ith column from the data
        x = self.train_data[:,i]
        learner = SimpleGPLearner(x, self.train_T)
        self.learners.append(learner)

        
    def fit(self):
        "Fit learners to training data"
        
        for learner in self.learners:
            learner.train()
            
        return self

    def predict(self):
        
        # return always tidy df
        
        preds = []
        
        for learner, feature_name in zip(self.learners, self.data.columns):
            pred = learner.predict(self.T)
            pred_mean = pd.DataFrame(pred.mean.cpu(), columns = ["mean"]).assign(time = self.T.cpu())
            pred_std = pd.DataFrame(pred.std.cpu(), columns = ["std"]).assign(time = self.T.cpu())
            
            pred = pd.merge(pred_mean, pred_std, on='time').assign(variable = feature_name)
            preds.append(pred)
                    
        
        return pd.concat(preds)
    
    def fit_predict(self):
        self.fit()
        return self.predict()

    
    def __repr__(self):
        return f"""Simple GP Imputation Explorer:
    N obs: {self.data.shape[0]}
    N features {self.data.shape[1]} ({', '.join(self.data.columns)})
    N missing observations {self.data.isna().to_numpy().flatten().sum()}"""

    def __str__(self):
        return self.__repr__()

# %% ../lib_nbs/10_SimpleGP_Imputation.ipynb 32
@patch
def model_info(self: SimpleGPImputationExplorer):
    "Combine parameters of different kernels into one output" 
    infos = [learner.model.get_info() for learner in self.learners]
    out = {}
    for key in infos[0].keys():
        values = pd.concat([info[key] for info in infos])
        values.insert(0, "variable", self.data.columns)
        out[key] = values 
    return out

# %% ../lib_nbs/10_SimpleGP_Imputation.ipynb 34
@patch
def to_result(self: SimpleGPImputationExplorer, data_complete, units=None):
    return ImputationResult(self.predict(), data_complete, self.model_info(), units=units)
