# AUTOGENERATED! DO NOT EDIT! File to edit: ../../lib_nbs/kalman/00_filter.ipynb.

# %% auto 0
__all__ = ['unsqueeze_first', 'unsqueeze_last', 'KalmanFilterBase', 'KalmanFilter', 'unsqueeze_iter']

# %% ../../lib_nbs/kalman/00_filter.ipynb 3
from fastcore.test import *
from fastcore.basics import *
from ..utils import *
from ..gaussian import *
from ..data_preparation import MeteoDataTest
from typing import *
from functools import partial

import numpy as np
import pandas as pd
import torch
from torch import Tensor
from torch.distributions import MultivariateNormal

# %% ../../lib_nbs/kalman/00_filter.ipynb 8
def _add_batch_dim(x):
    """make x 3 dimensional by adding empty dims in the correct place"""
    if x.dim() == 1: return x.unsqueeze(0).unsqueeze(-1)
    elif x.dim() == 2: return x.unsqueeze(0)
    else: return x

# %% ../../lib_nbs/kalman/00_filter.ipynb 11
class KalmanFilterBase(torch.nn.Module):
    """Base class for handling Kalman Filter implementation in PyTorch"""
    
    params_constr = {
        #name constraint
        'A':  None        ,
        'b':  None        ,
        'Q':  DiagPosDef(),
        'B':  None        ,
        'H':  None        ,
        'd':  None        ,
        'R':  DiagPosDef(),
        'm0': None       ,
        'P0': PosDef()   ,
        }
    
    def __init__(self,
            A: Tensor,                               # [n_dim_state,n_dim_state] $A$, state transition matrix 
            H: Tensor,                               # [n_dim_obs, n_dim_state] $H$, observation matrix
            B: Tensor,                               # [n_dim_state, n_dim_contr] $B$ control matrix
            Q: Tensor,                               # [n_dim_state, n_dim_state] $Q$, state trans covariance matrix
            R: Tensor,                               # [n_dim_obs, n_dim_obs] $R$, observations covariance matrix
            b: Tensor,                               # [n_dim_state] $b$, state transition offset
            d: Tensor,                               # [n_dim_obs] $d$, observations offset
            m0: Tensor,                              # [n_dim_state] $m_0$
            P0: Tensor,                              # [n_dim_state, n_dim_state] $P_0$
            
            n_dim_state: int = None,                 # Number of dimensions for state - default infered from parameters
            n_dim_obs: int = None,                   # Number of dimensions for observations - default  infered from parameters
            n_dim_contr: int = None,                 # Number of dimensions for control - default infered from parameters
                 
            var_names: Iterable[str]|None = None,    # Names of variables for printing 
            contr_names: Iterable[str]|None = None,  # Names of control variables for printing
            
            cov_checker: CheckPosDef = CheckPosDef(),# Check covariance at every step
            use_conditional: bool = True,            # Use conditional distribution for gaps that don't have all variables missing
            use_control: bool = True,                # Use the control in the filter
            use_smooth: bool = True,                 # Use smoother for predictions (otherwise is filter only)
                ):
        
        super().__init__()
        store_attr("var_names, contr_names, use_conditional, use_control, use_smooth, cov_checker")
        
        self._check_params(A, H, B, Q, R, b, d, m0, P0, n_dim_state, n_dim_obs, n_dim_contr)
        self._init_params(A=A, H=H, B=B, Q=Q, R=R, b=b, d=d, m0=m0, P0=P0)

    
    def _check_params(self, A, H, B, Q, R, b, d, m0, P0, n_dim_state, n_dim_obs, n_dim_contr):
        """Checks that the parameters are dimensions are consistent and sets n_dim"""
        self.n_dim_state = determine_dimensionality(
            [(A, array2d, -2),
             (b, array1d, -1),
             (Q, array2d, -2),
             (m0, array1d, -1),
             (P0, array2d, -2),
             (H, array2d, -1)],
            n_dim_state
        )
        self.n_dim_obs = determine_dimensionality(
            [(H, array2d, -2),
             (d, array1d, -1),
             (R, array2d, -2)],
            n_dim_obs
        )
        
        self.n_dim_contr = determine_dimensionality([(B, array2d, -1)], n_dim_contr)
        
        
    def _init_params(self, **params):
        for name, value in params.items():
            value = _add_batch_dim(value)
            if (constraint := self.params_constr[name]) is not None:
                name, value = self._init_constraint(name, value, constraint)
            self._init_param(name, value, train=True)    
    
    def _init_param(self, param_name, value, train):
        self.register_parameter(param_name, torch.nn.Parameter(value, requires_grad=train))
    
    ### === Constraints utils
    def _init_constraint(self, param_name, value, constraint):
        name = f"{param_name}_raw"
        value = constraint.inverse_transform(value)
        setattr(self, param_name + "_constraint", constraint)
        return name, value
    
    def _get_constraint(self, param_name):
        """get the original value"""
        constraint = getattr(self, param_name + "_constraint")
        raw_value = getattr(self, f"{param_name}_raw")
        return constraint.transform(raw_value)

    def _set_constraint(self, value, param_name, train=True):
            """set the transformed value"""
            constraint = getattr(self, param_name + "_constraint")
            raw_value = constraint.inverse_transform(value)
            self._init_param(f"{param_name}_raw", raw_value, train)
            
               
    @property
    def Q_C(self):
        "Cholesky factor of Q"
        return torch.diag_embed(self.Q_raw, dim1=-2, dim2=-1)
    @property
    def Q(self): return self._get_constraint('Q')
    @Q.setter
    def Q(self, value): self._set_constraint(value, 'Q')

    @property
    def R_C(self):
        "Cholesky factor of R"
        return torch.diag_embed(self.R_raw, dim1=-2, dim2=-1)
    @property
    def R(self): return self._get_constraint('R')
    @R.setter
    def R(self, value): self._set_constraint(value, 'R')
    
    @property
    def P0_C(self):
        "Cholesky factor of P0"
        return self.P0_raw
    @property
    def P0(self): return self._get_constraint('P0')
    @P0.setter
    def P0(self, value): self._set_constraint(value, 'P0')


    ### === Utility Func    
    def _parse_obs(self, obs, mask, control):
        """maybe get mask from `nan`"""
        # if mask is None: mask = ~torch.isnan(obs)
        return _add_batch_dim(obs), _add_batch_dim(mask), _add_batch_dim(control)
    def __repr__(self):
        return f"""Kalman Filter
        N dim obs: {self.n_dim_obs},
        N dim state: {self.n_dim_state},
        N dim contr: {self.n_dim_contr}"""

# %% ../../lib_nbs/kalman/00_filter.ipynb 15
@patch(cls_method=True)
def init_random(cls: KalmanFilterBase, n_dim_obs, n_dim_state, n_dim_contr, dtype=torch.float64, **kwargs):
    """kalman filter with random parameters"""
    return cls(
        A  = torch.rand(n_dim_state, n_dim_state, dtype=dtype),
        b  = torch.rand(n_dim_state, dtype=dtype),        
        Q  = to_diagposdef(torch.rand(n_dim_state, n_dim_state, dtype=dtype)),        
        B  = torch.rand(n_dim_state, n_dim_contr, dtype=dtype),
        H  = torch.rand(n_dim_obs, n_dim_state, dtype=dtype),
        d  = torch.rand(n_dim_obs, dtype=dtype),          
        R  = to_diagposdef(torch.rand(n_dim_obs, n_dim_obs, dtype=dtype)),            
        m0 = torch.rand(n_dim_state, dtype=dtype),        
        P0 = to_posdef(torch.rand(n_dim_state, n_dim_state, dtype=dtype)),
        **kwargs) 
        

# %% ../../lib_nbs/kalman/00_filter.ipynb 25
@patch
def get_info(self: KalmanFilterBase):
    out = {}
    var_names = ifnone(self.var_names, [f"y_{i}" for i in range(self.n_dim_obs)])
    latent_names = [f"x_{i}" for i in range(self.n_dim_state)]
    contr_names = ifnone(self.contr_names, [f"c_{i}" for i in range(self.n_dim_contr)])
    out['trans matrix $A$'] = array2df(self.A[0] , latent_names, latent_names, 'state')
    out['trans cov (Q)']    = array2df(self.Q[0] , latent_names, latent_names, 'state')
    out['trans off']        = array2df(self.b[0] , latent_names, ['offset'],   'state')
    out['obs matrix (H)']   = array2df(self.H[0] , var_names,    latent_names, 'variable')
    out['obs cov (R)']      = array2df(self.R[0] , var_names,    var_names,    'variable')
    out['obs off']          = array2df(self.d[0] , var_names,    ['offset'],   'variable')
    out['contr matrix (B)'] = array2df(self.B[0] , latent_names, contr_names,  'state')
    out['init state mean']  = array2df(self.m0[0], latent_names, ['mean'],     'state')
    out['init state cov']   = array2df(self.P0[0], latent_names, latent_names, 'state')

    return out

# %% ../../lib_nbs/kalman/00_filter.ipynb 29
@patch
def _repr_html_(self: KalmanFilter):
    title = f"Kalman Filter ({self.n_dim_obs} obs, {self.n_dim_state} state, {self.n_dim_contr} contr)"
    return row_dfs(self.get_info(), title , hide_idx=True)

# %% ../../lib_nbs/kalman/00_filter.ipynb 32
def get_test_data(n_obs = 10, n_dim_obs=3, n_dim_contr = 3, p_missing=.3, bs=2, dtype=torch.float32, device='cpu'):
    data = torch.rand(bs, n_obs, n_dim_obs, dtype=dtype, device=device)
    mask = torch.rand(bs, n_obs, n_dim_obs, device=device) > p_missing
    control = torch.rand(bs, n_obs, n_dim_contr, dtype=dtype, device=device)
    data[~mask] = torch.nan # ensure that the missing data cannot be used
    return data, mask, control

# %% ../../lib_nbs/kalman/00_filter.ipynb 35
class KalmanFilter(KalmanFilterBase):
    pass

# %% ../../lib_nbs/kalman/00_filter.ipynb 43
def _filter_predict_cov_stand(A, Q, P_pr):
    """Standard - Kalman Filter predict covariance"""
    return A @ P_pr @ A.mT + Q

# %% ../../lib_nbs/kalman/00_filter.ipynb 46
def _filter_predict_mean(
    A,      # transition matrix
    B,      # control matrix
    b,      # transition offset
    m_pr,   # Mean previous time step $m_{t-1}$
    control, # control variable
):
    return A @ m_pr + B @ control + b

# %% ../../lib_nbs/kalman/00_filter.ipynb 47
def _filter_predict(A,
                    Q,
                    b,
                    B, #[n_dim_state, n_dim_contr]
                    m_pr,
                    P_pr,
                    control, #[n_batches, n_dim_contr]
                    ):
    """Calculate the state at time `t` given the state at time `t-1`"""
    m_m = _filter_predict_mean(A, B, b, m_pr, control)
    P_m = _filter_predict_cov_stand(A, Q, P_pr)
    return (m_m, P_m)

# %% ../../lib_nbs/kalman/00_filter.ipynb 48
def _filter_predict_mean(
    A,      # transition matrix
    B,      # control matrix
    b,      # transition offset
    m_pr,   # Mean previous time step $m_{t-1}$
    control, # control variable
):
    return A @ m_pr + B @ control + b

# %% ../../lib_nbs/kalman/00_filter.ipynb 49
def unsqueeze_iter(*args, dim): return list(map(partial(torch.unsqueeze, dim=dim), args))
unsqueeze_first = partial(unsqueeze_iter, dim=0)
unsqueeze_last = partial(unsqueeze_iter, dim=-1)

# %% ../../lib_nbs/kalman/00_filter.ipynb 50
def _filter_predict(A,
                    Q,
                    b,
                    B, #[n_dim_state, n_dim_contr]
                    m_pr,
                    P_pr,
                    control, #[n_batches, n_dim_contr]
                    ):
    """Calculate the state at time `t` given the state at time `t-1`"""
    m_m = _filter_predict_mean(A, B, b, m_pr, control)
    P_m = _filter_predict_cov_stand(A, Q, P_pr)
    return (m_m, P_m)

# %% ../../lib_nbs/kalman/00_filter.ipynb 108
def _filter_correct_batch(
                    H,
                    R,
                    d,
                    m_m,
                    P_m,
                    obs, # [n_obs]
                    mask, # [n_obs_np, n_obs] mask to obtain non missing obs from obs
                    cov_checker=CheckPosDef()):
    """Update state at time `t` given observations at time `t` assuming that all observations have the same mask"""
    
    if (~mask).all(): return (m_m, P_m)

    m_H, m_d, m_obs, m_R = H[mask], d[mask], obs[:, mask], R[mask][:,mask]
    
    # extra dim needed to have batched matmul working between matrices and means
    (m_H,), (m_d, m_obs) = unsqueeze_first(m_H), unsqueeze_last(m_d, m_obs) 
    
    pred_obs_mean = m_H @ m_m + m_d
    pred_R = m_H @ P_m @ m_H.mT + m_R
    kalman_gain = P_m @ m_H.mT @ torch.inverse(pred_R) # torch.cholesky_inverse(torch.linalg.cholesky(pred_R))

    corr_state_mean = m_m + kalman_gain @ (m_obs - pred_obs_mean) #select with the mask instead of multipling so that support nan in the dataset
    corr_state_cov = P_m - kalman_gain @ m_H @ P_m

    cov_checker.check(P_m, caller='filter_correct')
    return (corr_state_mean, corr_state_cov)

# %% ../../lib_nbs/kalman/00_filter.ipynb 112
def _filter_correct(H,
                    R,
                    d,
                    m_m,
                    P_m,
                    obs,
                    mask,
                    cov_checker=CheckPosDef()) -> ListMNormal:
    """Update state at time `t` given observations at time `t`"""

    corr_state_mean, corr_state_cov = torch.empty_like(m_m), torch.empty_like(P_m)
    
    # find the unique values of the mask and make a sub-batches with it
    mask_values, indices = torch.unique(mask, return_inverse=True, dim=0)  
    for i, mask_v in enumerate(mask_values):
        idx_select = indices == i 
        corr_state_mean[idx_select], corr_state_cov[idx_select] = _filter_correct_batch(
            H, R, d,
            m_m[idx_select], P_m[idx_select],
            obs[idx_select], mask_v,
            cov_checker
        
        )
        assert all(mask[idx_select][0] == mask_v)
    
    return ListMNormal(corr_state_mean, corr_state_cov)

# %% ../../lib_nbs/kalman/00_filter.ipynb 120
def _times2batch(x):
    """Permutes `x` so that the first dimension is the number of batches and not the times"""
    return x.permute(1,0,-2,-1)

# %% ../../lib_nbs/kalman/00_filter.ipynb 121
def _filter(A, H, B,
            Q, R,
            b, d,
            m0, P0,
            obs, mask, control,
            cov_checker=CheckPosDef()
           ) ->Tuple[List, List, List, List]: # m_ms, P_ms, ms, Ps
    """Filter observations using kalman filter """
    n_timesteps = obs.shape[-2]
    bs = obs.shape[0]
    # lists are mutable so need to copy them
    m_ms, P_ms, ms, Ps = [[None for _ in range(n_timesteps)].copy() for _ in range(4)] 

    for t in range(n_timesteps):
        if t == 0:
            m_ms[t], P_ms[t] = torch.stack([m0]*bs).unsqueeze(-1), torch.stack([P0]*bs)
        else:
            m_ms[t], P_ms[t] = _filter_predict(A, Q, b, B,
                                                                      ms[t - 1], Ps[t - 1], control[:,t,:],
                                                                      cov_checker.add_args(t=t))

        ms[t], Ps[t] = _filter_correct(H, R, d,
                                                                     m_ms[t], P_ms[t],
                                                                     obs[:,t,:], mask[:,t,:],
                                                                     cov_checker.add_args(t=t))
    
    ret = list(maps(torch.stack, _times2batch, (m_ms, P_ms, ms, Ps,)))
    return ret

# %% ../../lib_nbs/kalman/00_filter.ipynb 128
@patch
def _filter_all(self: KalmanFilter, obs, mask, control
               ) ->Tuple[List, List, List, List]: # m_ms, P_ms, ms, Ps
    """ wrapper around `_filter`"""
    obs, mask = self._parse_obs(obs, mask)
    return _filter(
            self.A, self.H,
            self.B if self.use_control else torch.zeros_like(self.B),
            self.Q, self.R,
            self.b, self.d,
            self.m0, self.P0,
            obs, mask, control,
            self.cov_checker
        )

# %% ../../lib_nbs/kalman/00_filter.ipynb 133
@patch
def filter(self: KalmanFilter,
          obs: Tensor, # [n_timesteps, n_dim_obs] obs for times [0...n_timesteps-1]
          mask: Tensor,  # [n_timesteps, n_dim_obs] obs for times [0...n_timesteps-1]
          control: Tensor, # [n_timesteps, n_dim_contr] control for times [1...n_timesteps-1]
          ) -> ListMNormal: # Filtered state
    """Filter observation"""
    _, _, ms, Ps = self._filter_all(obs, mask, control)
    return ListMNormal(ms.squeeze(-1), Ps)

# %% ../../lib_nbs/kalman/00_filter.ipynb 138
def _smooth_update(A,                # [n_dim_state, n_dim_state]
                   filt_state: MNormal,         # [n_dim_state] filtered state at time `t`
                   pred_state: MNormal,         # [n_dim_state] state before filtering at time `t + 1` (= using the observation until time t)
                   next_smoothed_state: Normal, # [n_dim_state] smoothed state at time  `t+1`
                   cov_checker = CheckPosDef()
                   ) -> MNormal:                # mean and cov of smoothed state at time `t`
    """Correct a pred state with a Kalman Smoother update"""
    kalman_smoothing_gain = filt_state.cov @ A.unsqueeze(0).mT @ torch.inverse(pred_state.cov) # torch.cholesky_inverse(torch.linalg.cholesky(pred_state.cov))

    m_p = filt_state.mean + kalman_smoothing_gain @ (next_smoothed_state.mean - pred_state.mean)
    P_p = filt_state.cov + kalman_smoothing_gain @ (next_smoothed_state.cov - pred_state.cov) @ kalman_smoothing_gain.mT

    cov_checker.check(P_p, caller='smooth_update')
    
    return MNormal(m_p, P_p)

# %% ../../lib_nbs/kalman/00_filter.ipynb 143
def _smooth(A, # `[n_dim_state, n_dim_state]`
            filt_state: ListMNormal, # `[n_timesteps, n_dim_state]`
                # `ms[t]` is the state estimate for time t given obs from times `[0...t]`
            pred_state: ListMNormal, # `[n_timesteps, n_dim_state]`
                # `m_ms[t]` is the state estimate for time t given obs from times `[0...t-1]`
            cov_checker = CheckPosDef()
           ) -> ListMNormal: # `[n_timesteps, n_dim_state]` Smoothed state 
    """Apply the Kalman Smoother"""
    x = pred_state.mean # sample for getting tensor properties
    bs, n_timesteps, n_dim_state = x.shape[0], x.shape[1], x.shape[2]

    smoothed_state = ListMNormal(torch.zeros((bs, n_timesteps,n_dim_state,1),             dtype=x.dtype, device=x.device), 
                                 torch.zeros((bs, n_timesteps, n_dim_state,n_dim_state), dtype=x.dtype, device=x.device))
    # For the last timestep cannot use the smoother
    smoothed_state.mean[:,-1,] = filt_state.mean[:,-1]
    smoothed_state.cov[:,-1] = filt_state.cov[:,-1]

    for t in reversed(range(n_timesteps - 1)):
        (smoothed_state.mean[:,t], smoothed_state.cov[:,t]) = (
            _smooth_update(
                A,
                filt_state[:,t],
                pred_state[:,t + 1],
                smoothed_state[:,t+1],
            )
        )
    return smoothed_state

# %% ../../lib_nbs/kalman/00_filter.ipynb 149
@patch
def smooth(self: KalmanFilter,
           obs: Tensor,
           mask: Tensor,
           control: Tensor
          ) -> ListMNormal: # `[n_timesteps, n_dim_state]` smoothed state
        
    """Kalman Filter Smoothing"""

    (m_ms, P_ms, ms, Ps) = self._filter_all(obs, mask, control)

    smoothed_state = _smooth(self.A,
                   ListMNormal(ms, Ps), ListMNormal(m_ms, P_ms),
                   self.cov_checker)
    smoothed_state.mean.squeeze_(-1)
    return smoothed_state

# %% ../../lib_nbs/kalman/00_filter.ipynb 161
@patch
def _obs_from_state(self: KalmanFilter, state: ListMNormal):

    mean = self.H @ state.mean.unsqueeze(-1) + self.d.unsqueeze(-1)
    cov = self.H @ state.cov @ self.H.mT + self.R
    
    for c in cov: # this is batched and for all timestamps
        self.cov_checker.check(c, caller='predict')
    
    return ListMNormal(mean.squeeze(-1), cov)

# %% ../../lib_nbs/kalman/00_filter.ipynb 166
@patch
def predict(self: KalmanFilter, obs, mask, control, smooth=True):
    """Predicted observations at all times """
    state = self.smooth(obs, mask, control) if smooth else self.filter(obs, mask, control)
    obs, mask = self._parse_obs(obs, mask)
    
    pred_obs = self._obs_from_state(state)
    pred_mean, pred_std = pred_obs.mean, cov2std(pred_obs.cov)
    
    if self.use_conditional:
        # conditional predictions are slow, do only if some obs are missing 
        cond_mask = torch.logical_xor(mask.all(-1), mask.any(-1))

        # this cannot be batched so returns a list
        cond_preds = cond_gaussian_batched(
            pred_obs[cond_mask], obs[cond_mask], mask[cond_mask])
    
        for i, c_pred in enumerate(cond_preds):
            m = ~mask[cond_mask][i]
            pred_mean[cond_mask][i][m] = c_pred.mean
            pred_std [cond_mask][i][m] = cov2std(c_pred.cov)
    
    return ListNormal(pred_mean, pred_std)

# %% ../../lib_nbs/kalman/00_filter.ipynb 192
def _filter_predict_cov_stand(A, # transition covariance $A_t$
                        Q_C, # Cholesky Factor of transition covariance $Q_t$
                        P_pr_C # Cholesky Factor of previous state covariance $P_{t-1}$
                       ):
    """Numerical stable Kalman filter predict for covariance"""
    W = torch.concat([A @ P_pr_C, Q_C.expand_as(P_pr_C)], dim=-1)
    return torch.linalg.qr(W.mT).R.mT 

# %% ../../lib_nbs/kalman/00_filter.ipynb 201
@patch(cls_method=True)
def init_simple(cls: KalmanFilter,
                n_dim, # n_dim_obs and n_dim_state
                dtype=torch.float64):
    """Simplest version of kalman filter parameters"""
    return cls(
        A =     torch.eye(n_dim, dtype=dtype),
        b =        torch.zeros(n_dim, dtype=dtype),        
        Q =        torch.eye(n_dim, dtype=dtype),        
        H =       torch.eye(n_dim, dtype=dtype),
        d =          torch.zeros(n_dim, dtype=dtype),          
        R =          torch.eye(n_dim, dtype=dtype),            
        B =     torch.eye(n_dim, dtype=dtype),
        m0 =  torch.zeros(n_dim, dtype=dtype),        
        P0 =   torch.eye(n_dim, dtype=dtype),
    )

# %% ../../lib_nbs/kalman/00_filter.ipynb 206
from torch import hstack, eye, vstack, ones, zeros, tensor
from functools import partial
from sklearn.decomposition import PCA

# %% ../../lib_nbs/kalman/00_filter.ipynb 207
def set_dtype(*args, dtype=torch.float64):
    return [partial(arg, dtype=dtype) for arg in args] 

eye, ones, zeros, tensor = set_dtype(eye, ones, zeros, tensor)

# %% ../../lib_nbs/kalman/00_filter.ipynb 208
# @delegates(KalmanFilter)
@patch(cls_method=True)
def init_local_slope_pca(cls: KalmanFilter,
                n_dim_obs, # n_dim_obs and n_dim_contr
                n_dim_state: int, # n_dim_state
                df_pca: pd.DataFrame|None = None, # dataframe for PCA init, None no PCA init
                **kwargs
            ):
    """Local Slope + PCA init"""
    if df_pca is not None:
        comp = PCA(n_dim_state).fit(df_pca).components_
        H = tensor(comp.T) # transform state -> obs
        B = tensor(comp) # transform obs -> state
    else:
        H, B = eye(n_dim_obs), eye(n_dim_obs)
        
    return cls(
        A =     vstack([hstack([eye(n_dim_state),                eye(n_dim_state)]),
                                   hstack([zeros(n_dim_state, n_dim_state), eye(n_dim_state)])]),
        b =        zeros(n_dim_state * 2),        
        Q =        eye(n_dim_state * 2)*.1,        
        H =       hstack([H, zeros(n_dim_obs, n_dim_state)]),
        d =          zeros(n_dim_obs),          
        R =          eye(n_dim_obs)*.01,            
        B =     vstack([hstack([-B,                  B]),
                                   hstack([ zeros(n_dim_state,n_dim_obs), zeros(n_dim_state, n_dim_obs)])]),
        m0 =  zeros(n_dim_state * 2),        
        P0 =   eye(n_dim_state * 2) * 3,
        **kwargs
    ) 
