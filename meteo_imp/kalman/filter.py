# AUTOGENERATED! DO NOT EDIT! File to edit: ../../lib_nbs/kalman/00_filter.ipynb.

# %% auto 0
__all__ = ['to_posdef', 'is_symmetric', 'symmetric_upto', 'is_posdef', 'is_posdef_eigv', 'PosDef', 'KalmanFilter']

# %% ../../lib_nbs/kalman/00_filter.ipynb 3
from fastcore.test import *
from fastcore.basics import *
from ..utils import *
from ..gaussian import *
from ..data_preparation import MeteoDataTest
from typing import *

import numpy as np
import pandas as pd
import torch
from torch import Tensor
from torch.distributions import MultivariateNormal

# %% ../../lib_nbs/kalman/00_filter.ipynb 6
def array1d(X):
    """Returns at least 1-d array with data from X"""
    return torch.atleast_1d(torch.as_tensor(X))

def array2d(X):
    """Returns at least 2-d array with data from X"""
    return torch.atleast_2d(torch.as_tensor(X))


# %% ../../lib_nbs/kalman/00_filter.ipynb 7
def _determine_dimensionality(variables, default):
    """Derive the dimensionality of the state space

    Parameters
    ----------
    variables : list of ({None, array}, conversion function, index)
        variables, functions to convert them to arrays, and indices in those
        arrays to derive dimensionality from.
        
    Returns
    -------
    dim : int
        dimensionality of state space as derived from variables or default.
    """
    # gather possible values based on the variables
    candidates = []
    for (v, converter, idx) in variables:
        if v is not None:
            v = converter(v)
            candidates.append(v.shape[idx])
    
     # also use the manually specified default
    if default is not None:
        candidates.append(default)
    
    # ensure consistency of all derived values
    if len(candidates) == 0:
        return 1
    else:
        if not torch.all(torch.tensor(candidates) == candidates[0]):
            raise ValueError(
                "The shape of all " +
                "parameters is not consistent.  " +
                "Please re-check their values."
            )
        return candidates[0]


def _last_dims(X: Tensor, t: int, ndims: int=2):
    """Extract the final dimensions of `X`

    Extract the final `ndim` dimensions at index `t` if `X` has >= `ndim` + 1
    dimensions, otherwise return `X`.

    Parameters
    ----------
    X : Tensor with at least dimension `ndims`
    t : int
        index to use for the `ndims` + 1th dimension
    ndims : int, optional
        number of dimensions in the array desired

    Returns
    -------
    Y : array with dimension `ndims`
        the final `ndims` dimensions indexed by `t`
    """
    if len(X.shape) == ndims + 1:
        return X[t]
    elif len(X.shape) == ndims:
        return X
    else:
        raise ValueError(("X only has %d dimensions when %d" +
                " or more are required") % (len(X.shape), ndims))

# %% ../../lib_nbs/kalman/00_filter.ipynb 8
def _stack_detach(l: Collection[Tensor]):
    return torch.stack(list(map(lambda x: x.detach(), l)))

# %% ../../lib_nbs/kalman/00_filter.ipynb 13
def is_symmetric(value, atol=1e-5):
    return torch.isclose(value, value.mT, atol=atol).all().item()

# %% ../../lib_nbs/kalman/00_filter.ipynb 15
def symmetric_upto(value, start=-8):
    for exp in torch.arange(start, 3):
        if is_symmetric(value, atol=10**exp):
            return exp.item()
    return exp.item()

# %% ../../lib_nbs/kalman/00_filter.ipynb 19
def is_posdef(cov):
    return torch.distributions.constraints.positive_definite.check(cov).item()

# %% ../../lib_nbs/kalman/00_filter.ipynb 23
def is_posdef_eigv(cov):
    eigv = torch.linalg.eigvalsh(cov)
    if (eigv < 0).any():
        return False, eigv
    return True, eigv

# %% ../../lib_nbs/kalman/00_filter.ipynb 27
class PosDef():
    """ Positive Definite Constraint for PyTorch parameters"""
    def transform(self,
                  raw # square matrix
                 ):
        """transform any matrix into a positive definite one"""
        C = torch.tril(raw)
        return C @ C.mT
    
    def inverse_transform(self,
                          value # a positive definite matrix
                         ):
        """tranform positive definite matrix into a matrix that can be back_transformed using `transform`"""
        return torch.linalg.cholesky(value)

to_posdef = PosDef().transform

# %% ../../lib_nbs/kalman/00_filter.ipynb 39
from warnings import warn

# %% ../../lib_nbs/kalman/00_filter.ipynb 51
class KalmanFilter(torch.nn.Module):
    """Kalman Filter and Smoother using PyTorch"""
    def __init__(self,
            trans_matrix: Tensor,    # [n_dim_state,n_dim_state] $A$, state transition matrix 
            obs_matrix: Tensor,      # [n_dim_obs, n_dim_state] $H$, observation matrix
            trans_cov: Tensor,       # [n_dim_state, n_dim_state] $Q$, state trans covariance matrix
            obs_cov: Tensor,         # [n_dim_obs, n_dim_obs] $R$, observations covariance matrix
            trans_off: Tensor,       # [n_dim_state] $b$, state transition offset
            obs_off: Tensor,         # [n_dim_obs] $d$, observations offset
            init_state_mean: Tensor, # [n_dim_state] $\mu_0"$
            init_state_cov: Tensor,  # [n_dim_state, n_dim_state] $\Sigma_0$
            n_dim_state: int = None, # Number of dimensions for state - defaults to 1 if cannot be infered from parameters
            n_dim_obs: int = None,   # Number of dimensions for observations - defaults to 1 if cannot be infered from parameters
            cov_checker: CheckPosDef = CheckPosDef()
                ):
        
        super().__init__()
        # check parameters are consistent
        self.n_dim_state = _determine_dimensionality(
            [(trans_matrix, array2d, -2),
             (trans_off, array1d, -1),
             (trans_cov, array2d, -2),
             (init_state_mean, array1d, -1),
             (init_state_cov, array2d, -2),
             (obs_matrix, array2d, -1)],
            n_dim_state
        )
        self.n_dim_obs = _determine_dimensionality(
            [(obs_matrix, array2d, -2),
             (obs_off, array1d, -1),
             (obs_cov, array2d, -2)],
            n_dim_obs
        )
        
        params = {
        #name               value             constraint
        'trans_matrix':     [trans_matrix,    None    ],
        'trans_off':        [trans_off,       None    ],
        'trans_cov':        [trans_cov,       PosDef()],
        'obs_matrix':       [obs_matrix,      None    ],
        'obs_off':          [obs_off,         None    ],
        'obs_cov':          [obs_cov,         PosDef()],
        'init_state_mean':  [init_state_mean, None    ],
        'init_state_cov':   [init_state_cov,  PosDef()],
        }
        self._init_params(params)
        
        self.cov_checker = cov_checker
        
    def _init_params(self, params):
        for name, (value, constraint) in params.items():
            if constraint is not None:
                name, value = self._init_constraint(name, value, constraint)
            self._init_param(name, value, train=True)    
    
    def _init_param(self, param_name, value, train):
        self.register_parameter(param_name, torch.nn.Parameter(value, requires_grad=train))
    
    ### === Constraints utils
    def _init_constraint(self, param_name, value, constraint):
        name = param_name + "_raw"
        value = constraint.inverse_transform(value)
        setattr(self, param_name + "_constraint", constraint)
        return name, value
    
    def _get_constraint(self, param_name):
        constraint = getattr(self, param_name + "_constraint")
        raw_value = getattr(self, param_name + "_raw")
        return constraint.transform(raw_value)
    
    def _set_constraint(self, param_name, value, train=True):
        constraint = getattr(self, param_name + "_constraint")
        raw_value = constraint.inverse_transform(value)
        self._init_param(param_name + "_raw", raw_value, train)
    
    ### === Convenience functions to get and set parameters that have a constraint
    @property
    def trans_cov(self): return self._get_constraint('trans_cov')
    @trans_cov.setter
    def trans_cov(self, value): return self._set_constraint('trans_cov', value)

    @property
    def obs_cov(self): return self._get_constraint('obs_cov')
    @obs_cov.setter
    def obs_cov(self, value): return self._set_constraint('obs_cov', value)
    
    @property
    def init_state_cov(self): return self._get_constraint('init_state_cov')
    @init_state_cov.setter
    def init_state_cov(self, value): return self._set_constraint('init_state_cov', value)
    
    
    ### === Utility Func    
    def _parse_obs(self, obs, mask=None):
        """get mask from `nan`"""
        if mask is None: mask = ~torch.isnan(obs)
        return obs, mask
    
    def __repr__(self):
        return f"""Kalman Filter
        N dim obs: {self.n_dim_obs}, N dim state: {self.n_dim_state}"""

# %% ../../lib_nbs/kalman/00_filter.ipynb 71
from datetime import datetime
def _filter_predict(trans_matrix,
                    trans_cov,
                    trans_off,
                    curr_state_mean,
                    curr_state_cov,
                    control_matrix=0,
                    control=0,
                    cov_checker=CheckPosDef()):
    r"""Calculate the state at time `t+1` given the state at time `t`"""
    pred_state_mean = trans_matrix.unsqueeze(0) @ curr_state_mean + trans_off.unsqueeze(-1)
    pred_state_cov =  trans_matrix.unsqueeze(0) @ curr_state_cov @ trans_matrix.unsqueeze(0).mT + trans_cov.unsqueeze(0)

    cov_checker.check(pred_state_cov, caller='filter_predict')
    return (pred_state_mean, pred_state_cov)

# %% ../../lib_nbs/kalman/00_filter.ipynb 78
def _filter_correct(obs_matrix,
                    obs_cov,
                    obs_off,
                    pred_state_mean,
                    pred_state_cov,
                    obs,
                    mask,
                    cov_checker=CheckPosDef()):
    """Update state at time `t` given observations at time `t`"""
    
    pred_obs_mean = obs_matrix.unsqueeze(0) @ pred_state_mean + obs_off.unsqueeze(-1) # extra dim needed to hae batched matmul working
    pred_obs_cov = obs_matrix.unsqueeze(0) @ pred_state_cov @ obs_matrix.unsqueeze(0).mT + obs_cov

    kalman_gain = pred_state_cov @ obs_matrix.T @ torch.cholesky_inverse(torch.linalg.cholesky(pred_obs_cov))

    corr_state_mean = pred_state_mean + kalman_gain @ (obs.unsqueeze(-1) - pred_obs_mean)
    corr_state_cov = pred_state_cov - kalman_gain @ obs_matrix @ pred_state_cov
    
    mask = mask.all(-1) # if any observation is missing need to discard the whole vector
    corr_state_mean[~mask] = pred_state_mean[~mask]
    corr_state_cov[~mask] = pred_state_cov[~mask]

    cov_checker.check(pred_state_cov, caller='filter_correct')
    return (kalman_gain, corr_state_mean, corr_state_cov)

# %% ../../lib_nbs/kalman/00_filter.ipynb 87
def _times2batc(x):
    """Permutes `x` so that the first dimension is the number of batches and not the times"""
    return x.permute(1,0,-2,-1)

# %% ../../lib_nbs/kalman/00_filter.ipynb 88
def _filter(trans_matrix, obs_matrix,
            trans_cov, obs_cov,
            trans_off, obs_off,
            init_state_mean, init_state_cov,
            obs, mask,
            cov_checker=CheckPosDef()
           ) ->Tuple[List, List, List, List]: # pred_state_means, pred_state_covs, filt_state_means, filt_state_covs
    """Filter observations using kalman filter """
    n_timesteps = obs.shape[-2]
    bs = obs.shape[0]
    # lists are mutable so need to copy them
    pred_state_means, pred_state_covs, filt_state_means, filt_state_covs = [[None for _ in range(n_timesteps)].copy() for _ in range(4)] 

    for t in range(n_timesteps):
        if t == 0:
            pred_state_means[t], pred_state_covs[t] = torch.stack([init_state_mean]*bs).unsqueeze(-1), torch.stack([init_state_cov]*bs)
        else:
            pred_state_means[t], pred_state_covs[t] = _filter_predict(trans_matrix, trans_cov, trans_offset,
                                                                      filt_state_means[t - 1], filt_state_covs[t - 1],
                                                                      cov_checker.add_args(t=t))

        _, filt_state_means[t], filt_state_covs[t] = _filter_correct(obs_matrix, obs_cov, obs_off,
                                                                     pred_state_means[t], pred_state_covs[t],
                                                                     obs[:,t,:], mask[:,t,:],
                                                                     cov_checker.add_args(t=t))
    
    ret = maps(torch.stack, _times2batch, (pred_state_means, pred_state_covs, filt_state_means, filt_state_covs,))
    return ret

# %% ../../lib_nbs/kalman/00_filter.ipynb 95
@patch
def _filter_all(self: KalmanFilter, obs, mask=None, cov_checker=CheckPosDef()
               ) ->Tuple[List, List, List, List]: # pred_state_means, pred_state_covs, filt_state_means, filt_state_covs
    """ wrapper around `_filter`"""
    obs, mask = self._parse_obs(obs, mask)
    return _filter(
            self.trans_matrix, self.obs_matrix,
            self.trans_cov, self.obs_cov,
            self.trans_off, self.obs_off,
            self.init_state_mean, self.init_state_cov,
            obs, mask,
            cov_checker
        )

# %% ../../lib_nbs/kalman/00_filter.ipynb 104
def _smooth_update(trans_matrix,                # [n_dim_state, n_dim_state]
                   filt_state: MNormal,         # [n_dim_state] filtered state at time `t`
                   pred_state: MNormal,         # [n_dim_state] state before filtering at time `t + 1` (= using the observation until time t)
                   next_smoothed_state: Normal, # [n_dim_state] smoothed state at time  `t+1`
                   cov_checker = CheckPosDef()
                   ) -> MNormal:                # mean and cov of smoothed state at time `t`
    """Correct a pred state with a Kalman Smoother update"""
    kalman_smoothing_gain = filt_state.cov @ trans_matrix.unsqueeze(0).mT @ torch.cholesky_inverse(torch.linalg.cholesky(pred_state.cov))

    smoothed_state_mean = filt_state.mean + kalman_smoothing_gain @ (next_smoothed_state.mean - pred_state.mean)
    smoothed_state_cov = filt_state.cov + kalman_smoothing_gain @ (next_smoothed_state.cov - pred_state.cov) @ kalman_smoothing_gain.mT

    cov_checker.check(smoothed_state_cov, caller='smooth_update')
    
    return MNormal(smoothed_state_mean, smoothed_state_cov)

# %% ../../lib_nbs/kalman/00_filter.ipynb 108
def _smooth(trans_matrices, # `[n_dim_state, n_dim_state]`
            filt_state: ListMNormal, # `[n_timesteps, n_dim_state]`
                # `filt_state_means[t]` is the state estimate for time t given obs from times `[0...t]`
            pred_state: ListMNormal, # `[n_timesteps, n_dim_state]`
                # `pred_state_means[t]` is the state estimate for time t given obs from times `[0...t-1]`
            cov_checker = CheckPosDef()
           ) -> ListMNormal: # `[n_timesteps, n_dim_state]` Smoothed state 
    """Apply the Kalman Smoother"""
    x = pred_state.mean[0] # sample for getting tensor properties
    n_timesteps, bs, n_dim_state = len(pred_state.mean), x.shape[0], x.shape[1]

    smoothed_state = ListMNormal(torch.zeros((bs, n_timesteps,n_dim_state,1 ),          dtype=x.dtype, device=x.device), 
                                 torch.zeros((bs, n_timesteps,n_dim_state,n_dim_state), dtype=x.dtype, device=x.device))
    # For the last timestep cannot use the smoother
    smoothed_state.mean[:,-1,:,:] = filt_state.mean[-1]
    smoothed_state.cov[:,-1,:,:] = filt_state.cov[-1]

    for t in reversed(range(n_timesteps - 1)):
        trans_matrix = _last_dims(trans_matrices, t)
        (smoothed_state.mean[t], smoothed_state.cov[t]) = (
            _smooth_update(
                trans_matrix,
                filt_state.get_nth(t),
                pred_state.get_nth(t + 1),
                smoothed_state.get_nth(t+1),
                check_args = {'t': t, **check_args} if check_args is not None else None
            )
        )
    return smoothed_state

# %% ../../lib_nbs/kalman/00_filter.ipynb 118
@patch
def smooth(self: KalmanFilter,
           obs: Tensor, # dataset
           mask = None,
           check_args=None
          ) -> ListMNormal: # `[n_timesteps, n_dim_state]` smoothed hidden state distributions for times `[0...n_timesteps-1]`
        
    """Kalman Filter Smoothing"""

    (pred_state_means, pred_state_covs, filt_state_means, filt_state_covs) = self._filter_all(obs, mask, check_args)

    return _smooth(
            self.trans_matrices,
            ListMNormal(filt_state_means, filt_state_covs),
            ListMNormal(pred_state_means, pred_state_covs),
            check_args
        )

  

# %% ../../lib_nbs/kalman/00_filter.ipynb 129
def _get_cond_pred(pred: ListMNormal,
                  obs,
                  mask
                  ) -> ListNormal:
    """Conditional prediction given observations and transforms covariances into std deviations"""
    
    obs = obs[mask] # select only actually observed values
    pred_cond = conditional_guassian(pred.mean, pred.cov, obs, mask)
    
    mean = pred.mean.clone()
    mean[~mask] = pred_cond.mean
    
    std = torch.diagonal(pred.cov.clone(), dim1=-2, dim2=-1)
    std[~mask] = torch.diagonal(pred_cond.cov, dim1=-2, dim2=-1)
    
    return ListMNormal(mean, std)

# %% ../../lib_nbs/kalman/00_filter.ipynb 138
@patch
def _obs_from_state(self: KalmanFilter, state_mean, state_cov, check_args=None):

    mean = self.obs_matrices @ state_mean
    cov = self.obs_matrices @ state_cov @ self.obs_matrices.mT + self.obs_cov
    
    if check_args is not None: check_posdef(cov, 'predict',  **check_args)
    
    return ListMNormal(mean, cov)

@patch
def predict(self: KalmanFilter, obs, mask=None, smooth=True, check_args=None):
    """Predicted observations at all times """
    state = self.smooth(obs, mask, check_args) if smooth else self.filter(obs, mask, check_args)
    obs, mask = self._parse_obs(obs, mask)
    
    means = torch.empty_like(obs)
    stds = torch.empty_like(obs)
                             
    for t in range(obs.shape[0]):
        mean, std = self._obs_from_state(
            state.mean[t],
            state.cov[t],
            {'t': t, **check_args} if check_args is not None else None
        )
        
        means[t], stds[t] = _get_cond_pred(ListNormal(mean, std), obs[t], mask[t])
    
    return ListNormal(means, stds)

# %% ../../lib_nbs/kalman/00_filter.ipynb 152
@patch
def filter_loglikelihood(self: KalmanFilter, obs, mask=None):
    "Compute log likelihood using only filter step"
    # Those are the means and covs before the updating step,
    # otherwise the model would have already seen the observation that we are predicting 
    pred_state_mean, pred_state_cov, _, _ = self._filter_all(obs, mask)
    obs, obs_mask = self._parse_obs(obs, mask)

    max_t = obs.shape[0]
    lls = torch.zeros(max_t)
    for t in range(max_t):
        if obs_mask[t].all():
            pred_obs_mean, pred_obs_cov = self._obs_from_state(pred_state_mean[t], pred_state_cov[t])
            ll = MultivariateNormal(pred_obs_mean, pred_obs_cov, validate_args=False).log_prob(obs[t])
            lls[t] = ll

    return lls.sum()

# %% ../../lib_nbs/kalman/00_filter.ipynb 164
@patch
def loglikelihood(self: KalmanFilter,
                  obs_train: Tensor, # [n_timesteps, n_dim_obs] Observations use for the filter (can containt missing data)
                  times: Tensor, # [n_pred_timesteps] time at which to calculate the log likelihood
                  obs_test: Tensor, # [n_pred_timesteps, n_dim_obs] observed data to compute log likelihood
                  mask: Tensor=None, # [n_timesteps, n_dim_obs]
                 ) -> Tensor: # scalar that is sum of log likelihoods for all `times`
    "Log likelihood only for the `obs_test` at giben times"
    means, stds = self.predict(obs_train, mask=mask)
    lls = torch.zeros(len(times))
    for t in range(len(times)):
        lls[t] = MultivariateNormal(means[t], torch.diag(stds[t]), validate_args=False).log_prob(obs_test[t:t+1])
    return lls.sum() 
        

# %% ../../lib_nbs/kalman/00_filter.ipynb 172
@patch
def get_info(self: KalmanFilter, var_names=None):
    out = {}
    if var_names is not None: self.var_names = var_names 
    latent_names = [f"z_{i}" for i in range(self.trans_matrices.shape[0])]
    out['A'] = array2df(self.trans_matrices, latent_names, latent_names, 'latent')
    out['H'] = array2df(self.obs_matrices,        var_names,    latent_names, 'variable')
    out['R'] = array2df(self.obs_cov,             var_names,    var_names,     'variable')
    out['Q'] = array2df(self.trans_cov,      latent_names, latent_names, 'latent')
    return out
