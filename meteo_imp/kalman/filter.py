# AUTOGENERATED! DO NOT EDIT! File to edit: ../../lib_nbs/10_Kalman_Filter.ipynb.

# %% auto 0
__all__ = ['filter_classes', 'KalmanFilterBase', 'KalmanFilter', 'KalmanFilterSR', 'cat_2d', 'tensor_info', 'replacing_ctx',
           'with_settings', 'buffer_pred_single', 'buffer_pred']

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 3
from fastcore.test import *
from fastcore.basics import *
from ..utils import *
from ..gaussian import *
from typing import *
from functools import partial

import numpy as np
import pandas as pd
import torch
from torch import Tensor
from torch.distributions import MultivariateNormal

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 8
def _add_batch_dim(x):
    """make x 3 dimensional by adding empty dims in the correct place"""
    if x.dim() == 1: return x.unsqueeze(0).unsqueeze(-1)
    elif x.dim() == 2: return x.unsqueeze(0)
    else: return x

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 9
def _add_batch_dims_iter(*xs):
    """vectorize `add_batch_dim`"""
    return [_add_batch_dim(x) for x in xs]

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 12
def _check_same_size(
    os: Sequence[tuple[Tensor, int]], # sequences of tensors and the dimension to check
    size=None, # Optional size of the common dimension
)-> int: # size of common dimension
    """Check that all args have the same size at the given dimension, raise `ValueError` if not """
    size = ifnone(size, os[0][0].shape[os[0][1]])
    if not all([size == x.shape[dim] for x, dim in os]):
        raise ValueError("All parameters must have the same size at the given dimension")
    return size

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 14
class KalmanFilterBase(torch.nn.Module):
    """Base class for handling Kalman Filter implementation in PyTorch"""
    
    params_constr = {
        #name constraint
        'A':  None        ,
        'b':  None        ,
        'Q':  PosDef(),
        'B':  None        ,
        'H':  None        ,
        'd':  None        ,
        'R':  PosDef(),
        'm0': None       ,
        'P0': PosDef()   ,
        }
    
    def __init__(self,
            A: Tensor,                             # [n_dim_state,n_dim_state] $A$, state transition matrix 
            H: Tensor,                             # [n_dim_obs, n_dim_state] $H$, observation matrix
            B: Tensor,                             # [n_dim_state, n_dim_contr] $B$ control matrix
            Q: Tensor,                             # [n_dim_state, n_dim_state] $Q$, state trans covariance matrix
            R: Tensor,                             # [n_dim_obs, n_dim_obs] $R$, observations covariance matrix
            b: Tensor,                             # [n_dim_state] $b$, state transition offset
            d: Tensor,                             # [n_dim_obs] $d$, observations offset
            m0: Tensor,                            # [n_dim_state] $m_0$
            P0: Tensor,                            # [n_dim_state, n_dim_state] $P_0$
    
            n_dim_state: int = None,               # Number of dimensions for state - default infered from parameters
            n_dim_obs: int = None,                 # Number of dimensions for observations - default  infered from parameters
            n_dim_contr: int = None,               # Number of dimensions for control - default infered from parameters
            
            var_names: Iterable[str]|None = None,  # Names of variables for printing 
            contr_names: Iterable[str]|None = None,# Names of control variables for printing
    
            cov_checker: CheckPosDef|None = None,  # Check covariance at every step
            use_conditional: bool = False,         # Use conditional distribution for gaps that don't have all variables missing
            use_control: bool = True,              # Use the control in the filter
            use_smooth: bool = True,               # Use smoother for predictions (otherwise is filter only)
            pred_only_gap: bool = False,           # it True predictions are only for the gap
            pred_std: bool = False,                # return only stds and not covariances
                ):
        
        super().__init__()
        store_attr("var_names, contr_names, use_conditional, use_control, use_smooth, cov_checker, pred_only_gap, pred_std")
        
        A, H, B, Q, R, b, d, m0, P0 = _add_batch_dims_iter(A, H, B, Q, R, b, d, m0, P0)
        
        self._check_params(A, H, B, Q, R, b, d, m0, P0, n_dim_state, n_dim_obs, n_dim_contr)
        self._init_params(A=A, H=H, B=B, Q=Q, R=R, b=b, d=d, m0=m0, P0=P0)

    
    def _check_params(self, A, H, B, Q, R, b, d, m0, P0, n_dim_state, n_dim_obs, n_dim_contr):
        """Checks that the parameters are dimensions are consistent and sets n_dim"""
        self.n_dim_state = _check_same_size(
            [(A,  -2),
             (b,  -2),
             (Q,  -2),
             (m0, -2),
             (P0, -2),
             (H,  -1)],
            n_dim_state
        )
        self.n_dim_obs = _check_same_size(
            [(H, -2),
             (d, -2),
             (R, -2)],
            n_dim_obs
        )
        
        self.n_dim_contr = _check_same_size([(B, -1)], n_dim_contr)
        
        
    def _init_params(self, **params):
        for name, value in params.items():
            if (constraint := self.params_constr[name]) is not None:
                name, value = self._init_constraint(name, value, constraint)
            self._init_param(name, value, train=True)    
    
    def _init_param(self, param_name, value, train):
        self.register_parameter(param_name, torch.nn.Parameter(value, requires_grad=train))
    
    ### === Constraints utils
    def _init_constraint(self, param_name, value, constraint):
        name = f"{param_name}_raw"
        value = constraint.inverse_transform(value)
        setattr(self, param_name + "_constraint", constraint)
        return name, value
    
    def _get_constraint(self, param_name):
        """get the original value"""
        constraint = getattr(self, param_name + "_constraint")
        raw_value = getattr(self, f"{param_name}_raw")
        return constraint.transform(raw_value)
    
    def _get_constraint_cho_fact(self, param_name):
        """get the original value"""
        constraint = getattr(self, param_name + "_constraint")
        raw_value = getattr(self, f"{param_name}_raw")
        return constraint.transform_cho_factor(raw_value)

    def _set_constraint(self, value, param_name, train=True):
            """set the transformed value"""
            constraint = getattr(self, param_name + "_constraint")
            raw_value = constraint.inverse_transform(value)
            self._init_param(f"{param_name}_raw", raw_value, train)
            
               
    @property
    def Q_C(self): return self._get_constraint_cho_fact('Q')
    @property
    def Q(self): return self._get_constraint('Q')
    @Q.setter
    def Q(self, value): self._set_constraint(value, 'Q')

    @property
    def R_C(self): return self._get_constraint_cho_fact('R')
    @property
    def R(self): return self._get_constraint('R')
    @R.setter
    def R(self, value): self._set_constraint(value, 'R')
    
    @property
    def P0_C(self): return self._get_constraint_cho_fact('P0')
    @property
    def P0(self): return self._get_constraint('P0')
    @P0.setter
    def P0(self, value): self._set_constraint(value, 'P0')


    ### === Utility Func    
    def _parse_obs(self, obs, mask, control):
        """maybe get mask from `nan`"""
        # if mask is None: mask = ~torch.isnan(obs)
        return _add_batch_dim(obs).unsqueeze(-1), _add_batch_dim(mask), _add_batch_dim(control).unsqueeze(-1)
    def __repr__(self):
        return f"""Kalman Filter
        N dim obs: {self.n_dim_obs},
        N dim state: {self.n_dim_state},
        N dim contr: {self.n_dim_contr}"""

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 18
class KalmanFilter(KalmanFilterBase):
    pass

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 19
class KalmanFilterSR(KalmanFilterBase):
    pass

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 20
filter_classes = [KalmanFilterBase, KalmanFilter, KalmanFilterSR]

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 22
#| include: false
@patch_to(filter_classes, cls_method=True)
def init_random(cls,
                n_dim_obs,
                n_dim_state,
                n_dim_contr,
                dtype=torch.float64,
                seed:int|None = 27,
                **kwargs):
    """kalman filter with random parameters"""
    if seed is not None: torch.manual_seed(seed)
    return cls(
        A  = torch.rand(n_dim_state, n_dim_state, dtype=dtype),
        b  = torch.rand(n_dim_state, dtype=dtype),        
        Q  = to_posdef(torch.rand(n_dim_state, n_dim_state, dtype=dtype)),        
        B  = torch.rand(n_dim_state, n_dim_contr, dtype=dtype),
        H  = torch.rand(n_dim_obs, n_dim_state, dtype=dtype),
        d  = torch.rand(n_dim_obs, dtype=dtype),          
        R  = to_posdef(torch.rand(n_dim_obs, n_dim_obs, dtype=dtype)),            
        m0 = torch.rand(n_dim_state, dtype=dtype),        
        P0 = to_posdef(torch.rand(n_dim_state, n_dim_state, dtype=dtype)),
        **kwargs) 
        

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 35
@patch(cls_method=True)
def init_from(cls: KalmanFilter|KalmanFilterBase|KalmanFilterSR, o: filter_classes # Other filter
             ):
    """Initialize Filter by copying all parameters from another one"""
    return cls(o.A, o.H, o.B, o.Q, o.R, o.b, o.d, o.m0, o.P0,
               o.n_dim_state, o.n_dim_obs, o.n_dim_contr,
               o.var_names, o.contr_names, o.cov_checker,
               o.use_conditional, o.use_control, o.use_smooth, o.pred_only_gap, o.pred_std)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 38
@patch
def get_info(self: KalmanFilterBase):
    out = {}
    var_names = ifnone(self.var_names, [f"y_{i}" for i in range(self.n_dim_obs)])
    latent_names = [f"x_{i}" for i in range(self.n_dim_state)]
    contr_names = ifnone(self.contr_names, [f"c_{i}" for i in range(self.n_dim_contr)])
    out['$A$'] = array2df(self.A[0] , latent_names, latent_names, 'state')
    out['$Q$']    = array2df(self.Q[0] , latent_names, latent_names, 'state')
    out['$b$']        = array2df(self.b[0] , latent_names, ['offset'],   'state')
    out['$H$']   = array2df(self.H[0] , var_names,    latent_names, 'variable')
    out['$R$']      = array2df(self.R[0] , var_names,    var_names,    'variable')
    out['$d$']          = array2df(self.d[0] , var_names,    ['offset'],   'variable')
    out['$B$'] = array2df(self.B[0] , latent_names, contr_names,  'state')
    out['$m_0$']  = array2df(self.m0[0], latent_names, ['mean'],     'state')
    out['$P_0$']   = array2df(self.P0[0], latent_names, latent_names, 'state')

    return out

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 39
@patch
def _repr_html_(self: filter_classes):
    title = f"Kalman Filter ({self.n_dim_obs} obs, {self.n_dim_state} state, {self.n_dim_contr} contr)"
    return row_dfs(self.get_info(), title , hide_idx=True)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 42
def get_test_data(n_obs = 10, n_dim_obs=3, n_dim_contr = 3, p_missing=.3, gap_len=None, bs=2, dtype=torch.float64, device='cpu'):
    data = torch.rand(bs, n_obs, n_dim_obs, dtype=dtype, device=device)
    mask = torch.rand(bs, n_obs, n_dim_obs, device=device)
    if gap_len is not None:
        mask[:, n_obs//2-gap_len//2,n_obs//2+gap_len//2, :] = False
    else:
        mask = mask > p_missing
    control = torch.rand(bs, n_obs, n_dim_contr, dtype=dtype, device=device)
    data[~mask] = torch.nan # ensure that the missing data cannot be used
    return data, mask, control

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 53
def _filter_predict_cov_stand(A, Q, P_pr):
    """Standard - Kalman Filter predict covariance"""
    return A @ P_pr @ A.mT + Q

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 56
def _filter_predict_mean(
    A,      # transition matrix
    B,      # control matrix
    b,      # transition offset
    m_pr,   # Mean previous time step $m_{t-1}$
    control, # control variable
):
    return A @ m_pr + B @ control + b

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 58
def _filter_predict_mean(
    A,      # transition matrix
    B,      # control matrix
    b,      # transition offset
    m_pr,   # Mean previous time step $m_{t-1}$
    control, # control variable
):
    return A @ m_pr + B @ control + b

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 59
def _filter_predict(A,
                    Q,
                    b,
                    B, #[n_dim_state, n_dim_contr]
                    m_pr,
                    P_pr,
                    control, #[n_batches, n_dim_contr]
                    ):
    """Calculate the state at time `t` given the state at time `t-1`"""
    m_m = _filter_predict_mean(A, B, b, m_pr, control)
    P_m = _filter_predict_cov_stand(A, Q, P_pr)
    return (m_m, P_m)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 73
def _filter_update_k_gain(H, R,P_m):
    "kalman gain for filter update"
    S = H @ P_m @ H.mT + R
    S_C = torch.linalg.cholesky(S)
    return torch.cholesky_solve(H @ P_m.mT, S_C).mT

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 77
def _filter_update_cov(H, K, P_m):
    return (eye_like(P_m) - K @ H) @ P_m

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 81
def _filter_update_mean(H, d, K, m_m, y):
    z = H @ m_m + d
    return m_m + K @ (y - z)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 83
def _filter_update(
    H, # [1, n_dim_obs, n_dim_state]
    d, # [1, n_dim_obs, 1]
    R, # [1, n_dim_obs, n_dim_obs]
    m_m, # [n_batches, n_dim_state, 1]
    P_m, # [n_batches, n_dim_state, n_dim_state]
    obs # # [n_batches, n_dim_obs, 1]
) -> Tuple: # Filtered state (mean, covariance) [n_batches, n_dim_state]
    "Filter update state at `t` with obs at `t`"
    K = _filter_update_k_gain(H, R, P_m)
    m = _filter_update_mean(H, d, K, m_m, obs)
    P = _filter_update_cov(H, K, P_m)
    return m, P

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 98
def _filter_update_mask(
        H, # [1, n_dim_obs, n_dim_state]
        d, # [1, n_dim_obs, 1]
        R, # [1, n_dim_obs, n_dim_obs]
        m_m, # [n_batches, n_dim_state, 1]
        P_m, # [n_batches, n_dim_state, n_dim_state]
        obs, # [n_batches, n_dim_obs, 1] observations
        mask # [n_dim_obs] mask must be the same across batches
                       ):
    """Update state at time `t` given observations at time `t` assuming that all observations have the same mask"""
    if (~mask).all(): return (m_m, P_m) # all data is missing
    H_m, d_m, R_m, obs_m, = H[:, mask,:], d[:, mask,:], R[:, mask,:][:, :,mask], obs[:, mask] # _m for masked
    return _filter_update(H_m, d_m, R_m, m_m, P_m, obs_m)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 103
def _filter_update_mask_batch(
        H, # [1, n_dim_obs, n_dim_state]
        d, # [1, n_dim_obs, 1]
        R, # [1, n_dim_obs, n_dim_obs]
        m_m, # [n_batches, n_dim_state, 1]
        P_m, # [n_batches, n_dim_state, n_dim_state]
        obs, # [n_batches, n_dim_obs, 1] observations
        mask # [n_batches, n_dim_obs] mask must be the same across batches
                       ):
    """Support batches with different masks when update state at time `t` given observations at time `t`"""
    
    ms, Ps= torch.empty_like(m_m), torch.empty_like(P_m)
    
    # find the unique values of the mask and make a sub-batches with it
    mask_values, indices = torch.unique(mask, return_inverse=True, dim=0)  
    for i, mask_v in enumerate(mask_values):
        idx_select = indices == i 
        m, P = _filter_update_mask(
            H, d, R,
            m_m[idx_select], P_m[idx_select],
            obs[idx_select],
            mask_v,
        )
        ms[idx_select], Ps[idx_select] = m, P
    
    return ms, Ps

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 108
def _times2batch(x):
    """Permutes `x` so that the first dimension is the number of batches and not the times"""
    return x.permute(1,0,-2,-1)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 109
@patch
def _filter_all(self: KalmanFilter,
            obs: Tensor, # `([n_batches], n_obs, [self.n_dim_obs])` where `n_batches` and `n_dim_obs` dimensions can be omitted if 1
            mask: Tensor, # `([n_batches], n_obs, [self.n_dim_obs])` where `n_batches` and `n_dim_obs` dimensions can be omitted if 1
            control: Tensor, # ([n_batches], n_obs, [self.n_dim_contr]) 
            
           ) ->Tuple[ListMNormal, ListMNormal]: # (Filtered state, predicted state) with shape (n_batches, n_obs, self.n_dim_state)
    """Filter observations using kalman filter """
    obs, mask, control = self._parse_obs(obs, mask, control)
    n_obs = obs.shape[1]
    bs = obs.shape[0]
    # lists are mutable so need to copy them
    m_ms, P_ms, ms, Ps = [[None for _ in range(n_obs)].copy() for _ in range(4)] 

    for t in range(n_obs):
        # --- Predict
        if t == 0:
            m_ms[t], P_ms[t] = self.m0.expand(bs, -1, -1), self.P0.expand(bs, -1, -1)
        else:
            m_ms[t], P_ms[t] = _filter_predict(self.A, self.Q, self.b,
                                               self.B if self.use_control else torch.zeros_like(self.B), # maybe disable control
                                               ms[t - 1], Ps[t - 1], control[:,t,:])
        
        # --- Update
        ms[t], Ps[t] = _filter_update_mask_batch(self.H, self.d, self.R, m_ms[t], P_ms[t], obs[:,t,:], mask[:,t,:])
        
        if self.cov_checker is not None:
            self.cov_checker.check(P_ms[t], t=t, name="filter_predict")
            self.cov_checker.check(Ps[t], t=t, name="filter_update")
    
    m_ms, P_ms, ms, Ps = list(maps(torch.stack, _times2batch, (m_ms, P_ms, ms, Ps,))) # reorder dimensions and convert to tensor
    return ListMNormal(ms, Ps), ListMNormal(m_ms, P_ms) 

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 117
@patch
def filter(self: KalmanFilter,
            obs: Tensor, # `([n_batches], n_obs, [self.n_dim_obs])` where `n_batches` and `n_dim_obs` dimensions can be omitted if 1
            mask: Tensor, # `([n_batches], n_obs, [self.n_dim_obs])` where `n_batches` and `n_dim_obs` dimensions can be omitted if 1
            control: Tensor, # ([n_batches], n_obs, [self.n_dim_contr])
          ) -> ListMNormal: # Filtered state (n_batches, n_obs, self.n_dim_state)
    """Filter observation"""
    filt_state, _ = self._filter_all(obs, mask, control)
    return filt_state

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 122
def _smooth_gain(A, P, P_m):
    S_C = torch.linalg.cholesky(P_m)
    return torch.cholesky_solve(A @ P, S_C).mT

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 125
def _smooth_mean(K_p,                # [n_dim_state, n_dim_state]
               m,         # [n_dim_state] filtered state at time `t`
               m_m,         # [n_dim_state] state before filtering at time `t + 1` (= using the observation until time t)
               next_m_p): # [n_dim_state] smoothed state at time  `t+1`
    return m + K_p @ (next_m_p - m_m)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 127
def _smooth_cov(K_p, P, P_m, next_P_p):
    P_p = P + K_p @ (next_P_p - P_m) @ K_p.mT 
    return (P_p + P_p.mT) / 2 # force symmetric to improve num stability 

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 129
def _smooth_update(A,                # [n_dim_state, n_dim_state]
                   filt_state: MNormal,         # [n_dim_state] filtered state at time `t`
                   pred_state: MNormal,         # [n_dim_state] state before filtering at time `t + 1` (= using the observation until time t)
                   next_smoothed_state: Normal, # [n_dim_state] smoothed state at time  `t+1`
                   ) -> MNormal:                # mean and cov of smoothed state at time `t`
    """Correct a pred state with a Kalman Smoother update"""
    smooth_gain = _smooth_gain(A, filt_state.cov, pred_state.cov)

    m_p = _smooth_mean(smooth_gain, filt_state.mean, pred_state.mean, next_smoothed_state.mean)
    P_p = _smooth_cov(smooth_gain,  filt_state.cov, pred_state.cov, next_smoothed_state.cov)
    
    return MNormal(m_p, P_p)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 133
def _smooth(A, # `[n_dim_state, n_dim_state]`
            filt_state: ListMNormal, # `[n_timesteps, n_dim_state]`
                # `ms[t]` is the state estimate for time t given obs from times `[0...t]`
            pred_state: ListMNormal, # `[n_timesteps, n_dim_state]`
                # `m_ms[t]` is the state estimate for time t given obs from times `[0...t-1]`
            cov_checker = None
           ) -> ListMNormal: # `[n_timesteps, n_dim_state]` Smoothed state 
    """Apply the Kalman Smoother"""
    n_obs = pred_state.mean.shape[1]

    smoothed_state = ListMNormal(torch.zeros_like(filt_state.mean), torch.zeros_like(filt_state.cov))
    # For the last timestep cannot use the smoother
    smoothed_state.mean[:,-1] = filt_state.mean[:,-1]
    smoothed_state.cov[:,-1] = filt_state.cov[:,-1]

    for t in reversed(range(n_obs - 1)):
        (smoothed_state.mean[:,t], smoothed_state.cov[:,t]) = (
            _smooth_update(
                A,
                filt_state[:,t],
                pred_state[:,t + 1],
                smoothed_state[:,t+1],
            )
        )
        if cov_checker is not None:
            cov_checker.check(smoothed_state.cov[:,t], name="smooth", t=t)
    return smoothed_state

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 138
@patch
def smooth(self: KalmanFilter,
           obs: Tensor,
           mask: Tensor,
           control: Tensor
          ) -> ListMNormal: # `[n_timesteps, n_dim_state]` smoothed state
        
    """Kalman Filter Smoothing"""

    filt_state, pred_state = self._filter_all(obs, mask, control)

    smoothed_state = _smooth(self.A,
                   filt_state, pred_state,
                   self.cov_checker)
    return smoothed_state

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 151
@patch
def _obs_from_state(self: KalmanFilter, state: ListMNormal):

    mean = self.H @ state.mean + self.d
    cov = self.H @ state.cov @ self.H.mT + self.R
    
    if self.cov_checker is not None:
        for c in cov: # this is batched and for all timestamps
            self.cov_checker.check(c, caller='predict')
    
    return ListMNormal(mean.squeeze(-1), cov)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 156
@patch
def predict(self: KalmanFilter, obs, mask, control, smooth=True):
    """Predicted observations at all times """
    if self.use_conditional and self.pred_only_gap:
        raise ValueError("Kalman Filter predict cannot have conditional predictions and all predictions at the same time")
    
    state = self.smooth(obs, mask, control) if smooth else self.filter(obs, mask, control)
    obs, mask, control = self._parse_obs(obs, mask, control)
    
    pred_obs = self._obs_from_state(state)
    pred_mean, pred_std = pred_obs.mean, cov2std(pred_obs.cov)
    
    if self.use_conditional:
        # conditional predictions are slow, do only if some obs are missing 
        mask = mask.squeeze(0)
        cond_mask = torch.logical_xor(mask.all(-1), mask.any(-1))
        # there may be no conditional prediction to do
        if cond_mask.any():
            # this cannot be batched so returns a list
            cond_preds = cond_gaussian_batched(pred_obs[cond_mask], obs[cond_mask].squeeze(-1), mask[cond_mask])

            for i, c_pred in enumerate(cond_preds):
                m = ~mask[cond_mask][i]
                pred_mean[cond_mask][i][m] = c_pred.mean
                pred_std [cond_mask][i][m] = cov2std(c_pred.cov)
    
    return ListNormal(pred_mean, pred_std)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 163
@patch
def predict(self: KalmanFilter, obs, mask, control, smooth=True):
    """Predicted observations at all times """
    state = self.smooth(obs, mask, control) if smooth else self.filter(obs, mask, control)
    obs, mask, control = self._parse_obs(obs, mask, control)
    
    pred_obs = self._obs_from_state(state)
    pred_mean, pred_std = pred_obs.mean, cov2std(pred_obs.cov)
    
    if self.use_conditional:
        # conditional predictions are slow, do only if some obs are missing 
        mask = mask.squeeze(0)
        cond_mask = torch.logical_xor(mask.all(-1), mask.any(-1))
        # this cannot be batched so returns a list
        cond_preds = cond_gaussian_batched(
            pred_obs[cond_mask], obs[cond_mask].squeeze(-1), mask[cond_mask])
        
        for i, c_pred in enumerate(cond_preds):
            m = ~mask[cond_mask][i]
            pred_mean[cond_mask][i][m] = c_pred.mean
            pred_std [cond_mask][i][m] = cov2std(c_pred.cov)
    
    return ListNormal(pred_mean, pred_std)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 187
def _filter_predict_cov_SR(A, # transition covariance $A_t$
                        Q_C, # Cholesky Factor of transition covariance $Q_t$
                        P_pr_C # Cholesky Factor of previous state covariance $P_{t-1}$
                       ):
    """Numerical stable Kalman filter predict for covariance"""
    W = torch.concat([A @ P_pr_C, Q_C.expand_as(P_pr_C)], dim=-1)
    return torch.linalg.qr(W.mT).R.mT 

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 192
def _filter_predict_SR(A, Q_C, b, B, m_pr, P_pr_C,control) -> Tuple: # predicted state
    """Calculate the state at time `t` given the state at time `t-1`"""
    m_m = _filter_predict_mean(A, B, b, m_pr, control)
    P_m_C = _filter_predict_cov_SR(A, Q_C, P_pr_C)
    return (m_m, P_m_C)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 197
def cat_2d(x): # matrix as list of list of Tensor
    return torch.cat([torch.cat(row, dim=-1) for row in x], dim=-2)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 218
def tensor_info(x): return {'dtype': x.dtype, 'device': x.device}

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 219
def _filter_update_cov_SR(
    H,
    R_C,
    P_m_C
) -> Tuple: # (P_C, S_C) Square roots of filtered covariance and S
    """Covariance measurement update for Square root Filter"""
    bs,n, k = P_m_C.shape[0], R_C.shape[-1], H.shape[-1] # batch size, dim observations, dim_state
    zeros = partial(torch.zeros, **tensor_info(H))
    
    M = cat_2d([[R_C.expand(bs,-1,-1),   H @ P_m_C], 
                [zeros(bs, k, n),        P_m_C    ]])

    V = torch.linalg.qr(M.mT).R.mT

    P_C = V[:, n:, n:]
    S_C = V[:, :n, :n]
    return P_C, S_C

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 225
def _filter_update_k_gain_SR(
    H,
    P_m_C, # square root of $P^-$
    S_C # square root of S = (HPH^T +R)
):
    """kalman gain for filter update for SR filter"""
    return torch.cholesky_solve(H @ P_m_C @ P_m_C.mT, S_C).mT

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 234
def _filter_update_SR(
    H, # [1, n_dim_obs, n_dim_state]
    d, # [1, n_dim_obs, 1]
    R_C, # [1, n_dim_obs, n_dim_obs]
    m_m, # [n_batches, n_dim_state, 1]
    P_m_C, # [n_batches, n_dim_state, n_dim_state] square root predicted covariance
    obs # # [n_batches, n_dim_obs, 1]
) -> Tuple: # Filtered state (mean, chol_covariance) [n_batches, n_dim_state]
    "Filter update state at `t` with obs at `t`"
    P_C, S_C = _filter_update_cov_SR(H, R_C, P_m_C)
    K = _filter_update_k_gain_SR(H, P_m_C, S_C)
    m = _filter_update_mean(H, d, K, m_m, obs)
    return m, P_C

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 248
def _filter_update_mask_SR(
        H, # [1, n_dim_obs, n_dim_state]
        d, # [1, n_dim_obs, 1]
        R, # [1, n_dim_obs, n_dim_obs]
        m_m, # [n_batches, n_dim_state, 1]
        P_m_C, # [n_batches, n_dim_state, n_dim_state]
        obs, # [n_batches, n_dim_obs, 1] observations
        mask # [n_dim_obs] mask must be the same across batches
                       ):
    """SR Filter Update state at time `t` given observations at time `t` assuming that all observations have the same mask"""
    if (~mask).all(): return (m_m, P_m_C) # all data is missing
    H_m, d_m, R_m, obs_m, = H[:, mask,:], d[:, mask,:], R[:, mask,:][:, :,mask], obs[:, mask] # _m for masked
    R_C_m = torch.linalg.cholesky(R_m)
    return _filter_update_SR(H_m, d_m, R_C_m, m_m, P_m_C, obs_m)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 259
def _filter_update_mask_batch_SR(
        H, # [1, n_dim_obs, n_dim_state]
        d, # [1, n_dim_obs, 1]
        R, # [1, n_dim_obs, n_dim_obs]
        m_m, # [n_batches, n_dim_state, 1]
        P_m_C, # [n_batches, n_dim_state, n_dim_state]
        obs, # [n_batches, n_dim_obs, 1] observations
        mask # [n_batches, n_dim_obs] mask must be the same across batches
                       ):
    """Support batches with different masks when update state at time `t` given observations at time `t`"""
    
    ms, P_Cs= torch.empty_like(m_m), torch.empty_like(P_m_C)
    
    # find the unique values of the mask and make a sub-batches with it
    mask_values, indices = torch.unique(mask, return_inverse=True, dim=0)  
    for i, mask_v in enumerate(mask_values):
        idx_select = indices == i 
        m, P_C = _filter_update_mask_SR(
            H, d, R,
            m_m[idx_select], P_m_C[idx_select],
            obs[idx_select],
            mask_v,
        )
        ms[idx_select], P_Cs[idx_select] = m, P_C
    
    return ms, P_Cs

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 264
@patch
def _filter_all(self: KalmanFilterSR,
            obs: Tensor, # `([n_batches], n_obs, [self.n_dim_obs])` where `n_batches` and `n_dim_obs` dimensions can be omitted if 1
            mask: Tensor, # `([n_batches], n_obs, [self.n_dim_obs])` where `n_batches` and `n_dim_obs` dimensions can be omitted if 1
            control: Tensor, # ([n_batches], n_obs, [self.n_dim_contr]) 
            
           ) ->Tuple[ListMNormal, ListMNormal]: # (Filtered state, predicted state) with shape (n_batches, n_obs, self.n_dim_state)
    """Filter observations using kalman filter """
    obs, mask, control = self._parse_obs(obs, mask, control)
    bs, n_obs = obs.shape[0], obs.shape[1]
    # lists are mutable so need to copy them
    m_ms, P_m_Cs, ms, P_Cs = [[None for _ in range(n_obs)].copy() for _ in range(4)] 

    for t in range(n_obs):
        # Predict
        if t == 0:
            m_ms[t], P_m_Cs[t] = self.m0.expand(bs, -1, -1), self.P0_C.expand(bs, -1, -1)
        else:
            m_ms[t], P_m_Cs[t] = _filter_predict_SR(self.A, self.Q_C, self.b,
                                                    self.B if self.use_control else torch.zeros_like(self.B),
                                                    ms[t - 1], P_Cs[t - 1], control[:,t,:])
        
        # Update
        ms[t], P_Cs[t] = _filter_update_mask_batch_SR(self.H, self.d, self.R, m_ms[t], P_m_Cs[t], obs[:,t,:], mask[:,t,:])
        
        if self.cov_checker is not None:
            self.cov_checker.check(P_m_Cs[t] @ P_m_Cs[t].mT, t=t, name="filter_predict", type="SR")
            self.cov_checker.check(P_Cs[t] @ P_Cs[t].mT, t=t, name="filter_update", type="SR")
    
    m_ms, P_m_Cs, ms, P_Cs = list(maps(torch.stack, _times2batch, (m_ms, P_m_Cs, ms, P_Cs,))) # reorder dimensions and convert to tensor
    return ListMNormal(ms, P_Cs), ListMNormal(m_ms, P_m_Cs) 

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 272
@patch
def filter(self: KalmanFilterSR,
            obs: Tensor, # `([n_batches], n_obs, [self.n_dim_obs])` where `n_batches` and `n_dim_obs` dimensions can be omitted if 1
            mask: Tensor, # `([n_batches], n_obs, [self.n_dim_obs])` where `n_batches` and `n_dim_obs` dimensions can be omitted if 1
            control: Tensor, # ([n_batches], n_obs, [self.n_dim_contr])
          ) -> ListMNormal: # Filtered state (n_batches, n_obs, self.n_dim_state)
    """Filter observation"""
    filt_state, _ = self._filter_all(obs, mask, control)
    return filt_state

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 279
def _smooth_gain_SR(A, P_C, P_m_C):
    return torch.cholesky_solve(A @ P_C @ P_C.mT, P_m_C).mT

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 285
def _smooth_update_SR(A,                # [n_dim_state, n_dim_state]
                   filt_stateSR: MNormal,         # [n_dim_state] filtered state at time `t`
                   pred_stateSR: MNormal,         # [n_dim_state] state before filtering at time `t + 1` (= using the observation until time t)
                   next_smoothed_state: Normal, # [n_dim_state] smoothed state at time  `t+1`
                   ) -> MNormal:                # mean and cov of smoothed state at time `t`
    """Correct a pred state with a Kalman Smoother update"""
    # for now use standard smoother
    smooth_gain = _smooth_gain_SR(A, filt_stateSR.cov, pred_stateSR.cov)
    
    # Convert to full covariance matrix only the filter output
    filt_state_cov, pred_state_cov = map(lambda x: x @ x.mT, [filt_stateSR.cov, pred_stateSR.cov])
    
    m_p = _smooth_mean(smooth_gain, filt_stateSR.mean, pred_stateSR.mean, next_smoothed_state.mean)
    P_p = _smooth_cov(smooth_gain,  filt_state_cov, pred_state_cov, next_smoothed_state.cov)
    
    return MNormal(m_p, P_p)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 294
def _smooth_SR(A, # `[n_dim_state, n_dim_state]`
            filt_stateSR: ListMNormal, # `[n_timesteps, n_dim_state]`
                # `ms[t]` is the state estimate for time t given obs from times `[0...t]`
            pred_stateSR: ListMNormal, # `[n_timesteps, n_dim_state]`
                # `m_ms[t]` is the state estimate for time t given obs from times `[0...t-1]`
            until=0, # iteration where to stop the smoother
            cov_checker = None
           ) -> ListMNormal: # `[n_timesteps, n_dim_state]` Smoothed state 
    """Apply the Kalman Smoother"""
    n_obs = pred_stateSR.mean.shape[1]

    smoothed_state = ListMNormal(torch.zeros_like(filt_stateSR.mean), torch.zeros_like(filt_stateSR.cov))
    
    # For the last timestep cannot use the smoother
    smoothed_state.mean[:,-1] = filt_stateSR.mean[:,-1]
    smoothed_state.cov[:,-1] = filt_stateSR.cov[:,-1] @ filt_stateSR.cov[:,-1].mT
    
    for t in reversed(range(until, n_obs - 1)):
        (smoothed_state.mean[:,t], smoothed_state.cov[:,t]) = (
            _smooth_update_SR(
                A,
                filt_stateSR[:,t],
                pred_stateSR[:,t + 1],
                smoothed_state[:,t+1],
            )
        )
        if cov_checker is not None:
            cov_checker.check(smoothed_state.cov[:,t], name="smooth", t=t)
    return smoothed_state

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 304
@patch
def smooth(self: KalmanFilterSR,
           obs: Tensor,
           mask: Tensor,
           control: Tensor
          ) -> ListMNormal: # `[n_timesteps, n_dim_state]` smoothed state
        
    """Kalman Filter Smoothing"""

    filt_stateSR, pred_stateSR = self._filter_all(obs, mask, control)
    # run smoother until there is a gap
    # if self.pred_only_gap:
    #     gap_idx = torch.argwhere((~mask).any(-1).any(0))
    #     # no data to predict       
    #     if gap_idx.numel() == 0: return ListMNormal(torch.zeros(0), torch.zeros(0))
    #     until = gap_idx.min()
    # else:
    #     until = 0
    until = 0
        
    smoothed_state = _smooth_SR(self.A,
                   filt_stateSR, pred_stateSR,
                    until=until,
                   cov_checker = self.cov_checker)
    return smoothed_state

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 315
@patch
def _obs_from_state(self: KalmanFilterSR, state: ListMNormal):
    
    mean = self.H @ state.mean + self.d
    
    if (self.use_sr_pred if hasattr(self, 'use_sr_pred') else False):
        HP = self.H @ state.cov
        W = torch.cat([HP, self.R_C.expand(*HP.shape[:-2], -1, -1)], dim=-1)
        cov = torch.linalg.qr(W.mT).R.mT
    else: # actually compute the covariance matrix
        cov = self.H @ state.cov @ self.H.mT + self.R
    
    if self.cov_checker is not None:
        for c in cov: # this is batched and for all timestamps
            self.cov_checker.check(c, caller='predict')
    
    return ListMNormal(mean, cov)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 323
def _masked2batch(x: Tensor, # (`n_time_missing` for every `batch`, n, [n])
                  mask: Tensor, # (`n_batch`, `n_times`, `n`)
                 ) -> list[list[Tensor]]: 
    """transform a flattened masked prediction, into a prediction with a batch shape and select only predictions where the mask is false"""
    batches = []
    n_prev = 0
    gap_mask = ~mask.all(-1)
    for i, n in enumerate(gap_mask.sum(-1)):
        batch = x[n_prev:n_prev+n]
        mask_batch = mask[i][gap_mask[i]]
        assert  (mask_batch == mask[gap_mask][n_prev:n_prev+n]).all() # sanity check that the function is working
        times = []
        for t_pred, t_mask in zip(batch, mask_batch):
            times.append(t_pred[~t_mask] if t_pred.dim() == 1 else t_pred[~t_mask, :][:,~t_mask])
        batches.append(times)
        n_prev += n
    return batches

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 330
@patch
def predict(self: KalmanFilterSR, obs, mask, control, smooth=True):
    """Predicted observations at all times """
    if self.use_conditional and not self.pred_only_gap:
        raise ValueError("Kalman Filter predict cannot have conditional predictions and all predictions at the same time")
        
    state = self.smooth(obs, mask, control) if smooth else self.filter(obs, mask, control)
    self.use_sr_pred = not smooth
    if not smooth:
        state = ListMNormal(state.mean, state.cov @ state.cov.mT) # convert to actual covariance
    
    if self.pred_only_gap:
        gap_mask = ~mask.all(-1)
        # this destroy batches! so need to do some magic after
        state = state[gap_mask]
    pred_obs = self._obs_from_state(state)
    pred_obs.mean.squeeze_(-1)
    pred_mean, pred_cov = pred_obs.mean, pred_obs.cov
    if self.use_sr_pred:
        pred_cov = pred_cov @ pred_cov.mT
    # pred_std = cov2std(pred_cov)
    
    if self.use_conditional:
        obs, mask, control = self._parse_obs(obs, mask, control)
        # conditional predictions are slow, do only if some obs are missing 
        # cond_mask = torch.logical_xor(mask.all(-1), mask.any(-1))
        cond_mask = mask[gap_mask]
        if cond_mask.any():
            # this cannot be batched so returns a list
            cond_preds = cond_gaussian_batched(
                pred_obs, obs[gap_mask].squeeze(-1), cond_mask)

            for i, c_pred in enumerate(cond_preds):
                m = ~cond_mask[i]
                pred_mean[i][m] = c_pred.mean
                pred_cov[i][m,:][:,m]= c_pred.cov
    
    if self.pred_only_gap:
        pred_mean = _masked2batch(pred_mean, mask)
        pred_cov =  _masked2batch(pred_cov, mask)
    return ListMNormal(pred_mean, pred_cov) if not self.pred_std else ListNormal(pred_mean, cov2std(pred_cov))

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 332
from fastai.learner import replacing_yield
from fastcore.xtras import ContextManagers
from contextlib import contextmanager

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 333
@contextmanager
def replacing_ctx(*args): return replacing_yield(*args)

def with_settings(k, **kwargs):
    return ContextManagers([replacing_ctx(k, attr, v) for attr,v in kwargs.items()])

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 340
def buffer_pred_single(preds: list[Tensor],
                masks: Tensor) -> Tensor:
    """For predictions are for gaps only add buffer of `Nan` so they have same shape of targets"""
    all_pred = torch.empty(masks.shape, dtype=preds[0][0].dtype).fill_(torch.nan)
    i_p = 0
    for i, (mask) in enumerate(masks.cpu()):
        if not mask.all():
            all_pred[i][~mask] = preds[i_p].detach().cpu()
            i_p += 1
    assert i_p == len(preds)
    return all_pred

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 341
def buffer_pred(preds: list[list[Tensor]],
                masks: Tensor) -> Tensor:
    """For predictions are for gaps only add buffer of `Nan` so they have same shape of targets"""
    return torch.stack([buffer_pred_single(pred, mask) for pred, mask in zip(preds, masks)])

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 372
@patch
def _predict_filter(self: KalmanFilterSR, data, mask, control):
    """Predict every obsevation using only the filter step"""
    # use the predicted state not the filtered state!
    self.use_sr_pred = True
    filt_state, pred_state = self._filter_all(data, mask, control)
    mean, cov = self._obs_from_state(pred_state)
            
    return ListNormal(mean.squeeze(-1), cov @ cov.mT) # convert to actual covariances

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 416
@patch(cls_method=True)
def init_simple(cls: KalmanFilter|KalmanFilterSR,
                n_dim, # n_dim_obs and n_dim_state
                dtype=torch.float64):
    """Simplest version of kalman filter parameters"""
    return cls(
        A =     torch.eye(n_dim, dtype=dtype),
        b =        torch.zeros(n_dim, dtype=dtype),        
        Q =        torch.eye(n_dim, dtype=dtype),        
        H =       torch.eye(n_dim, dtype=dtype),
        d =          torch.zeros(n_dim, dtype=dtype),          
        R =          torch.eye(n_dim, dtype=dtype),            
        B =     torch.eye(n_dim, dtype=dtype),
        m0 =  torch.zeros(n_dim, dtype=dtype),        
        P0 =   torch.eye(n_dim, dtype=dtype),
    )

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 421
from torch import hstack, eye, vstack, ones, zeros, tensor
from functools import partial
from sklearn.decomposition import PCA

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 422
def set_dtype(*args, dtype=torch.float64):
    return [partial(arg, dtype=dtype) for arg in args] 

eye, ones, zeros, tensor = set_dtype(eye, ones, zeros, tensor)

# %% ../../lib_nbs/10_Kalman_Filter.ipynb 423
@patch(cls_method=True)
def init_local_slope_pca(cls: KalmanFilter|KalmanFilterSR,
                n_dim_obs, # n_dim_obs and n_dim_contr
                n_dim_state: int, # n_dim_state
                n_dim_contr:int, #n dim control
                df_pca: pd.DataFrame|None = None, # dataframe for PCA init, None no PCA init,
                pca_contr:int = False,
                **kwargs
            ):
    """Local Slope + PCA init"""
    
    if df_pca is not None:
        comp = PCA(n_dim_state).fit(df_pca).components_
        H = tensor(comp.T) # transform state -> obs 
        if pca_contr:
            if n_dim_obs != n_dim_contr:
                raise ValueError("n dim obs and n dim contr must be the same for pca of control")
            else:
                B = torch.tensor(comp)
        else:
            B = eye(n_dim_contr)
    else:
        H, B = eye(n_dim_obs), eye(n_dim_contr)
        
    return cls(
        A =     vstack([hstack([eye(n_dim_state),                eye(n_dim_state)]),
                                   hstack([zeros(n_dim_state, n_dim_state), eye(n_dim_state)])]),
        b =        zeros(n_dim_state * 2),        
        Q =        eye(n_dim_state * 2)*.1,        
        H =       hstack([H, zeros(n_dim_obs, n_dim_state)]),
        d =          zeros(n_dim_obs),          
        R =          eye(n_dim_obs)*.01,            
        B =     vstack([hstack([-B,                  B]),
                        hstack([ zeros(2 * n_dim_state-n_dim_contr,n_dim_contr), zeros(2 * n_dim_state-n_dim_contr, n_dim_contr)])]),
        m0 =  zeros(n_dim_state * 2),        
        P0 =   eye(n_dim_state * 2) * 3,
        **kwargs
    ) 
