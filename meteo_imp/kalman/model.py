# AUTOGENERATED! DO NOT EDIT! File to edit: ../../lib_nbs/kalman/01_Kalman_Learner.ipynb.

# %% auto 0
__all__ = ['KalmanModel']

# %% ../../lib_nbs/kalman/01_Kalman_Learner.ipynb 5
import pandas as pd
from fastcore.basics import store_attr

import numpy as np
from fastcore.meta import delegates
from fastcore.basics import patch

from ..results import ImputationResult, _plot_error_bar
from ..utils import *
from .filter import KalmanFilter

from torch import Tensor
import torch
from tqdm.auto import tqdm
import altair as alt

# %% ../../lib_nbs/kalman/01_Kalman_Learner.ipynb 6
class KalmanModel():
    """Kalman Model wtih max likelihood and gradient descend to optimize paramters and support for missing observations"""
    _var_names = None
    def __init__(self,
                 data: Tensor, # array of observations containg `NaN` for missing obs
                 **kwargs):
        self.data = data
        self.n_obs = data.shape[0]
        self.n_features = data.shape[1]
        
        kwargs = {**kwargs,
               'n_dim_state': self.n_features,
                'n_dim_obs': self.n_features
               }
        
        self.filter = KalmanFilter(**kwargs)
    
    def train(self,  
              times: Tensor|None, # for loss function times
              obs_test: Tensor|None, # for loss function observations
              n_iter: int=20,
              lr=0.1,
             ):
        "fit parameters by maximizing log likelihood using gradient descent"
        self.filter.train() # set training model for model
        
        if times is None and obs_test is None:
            times = torch.arange(self.n_obs)
            obs_test = self.data
        
        # Use the adam optimizer
        optimizer = torch.optim.Adam(self.filter.parameters(), lr=lr) 
        
        if not hasattr(self, 'losses'):
            self.losses = torch.zeros(n_iter)
            self.model_infos = [None for _ in range(n_iter)]
            offset = 0
        else:
            self.losses = torch.concat([self.losses, torch.zeros(n_iter)])
            self.model_infos.extend([None for _ in range(n_iter)])
            offset = self.losses.shape[0]
        
        for i in tqdm(range(n_iter)):
            optimizer.zero_grad()
            # Output from model
            # Calc loss and backprop gradients
            loss = -self.filter.loglikelihood(self.data, times, obs_test)
            self.losses[i + offset] = loss.detach()
            loss.backward()
            
            self.model_infos[i + offset] = self.filter.get_info(self.var_names)

            optimizer.step()
        
        
        # mean, cov = self.filter.smooth(self.data) 
        # self.state = ListNormal(mean, cov)
        return self
    
    @torch.no_grad()
    def predict(self, t):
        return self.filter.predict(self.data, t).detach()
    
    @property
    def latent_names(self):
        return [f"z_{i}" for i in range(self.filter.transition_matrices.shape[0])]
    @property
    def var_names(self):
        return self._var_names if self._var_names is not None else [f"x_{i}" for i in range(self.n_features)]
    
    @var_names.setter
    def var_names(self, var_names):
        self._var_names = var_names
    

# %% ../../lib_nbs/kalman/01_Kalman_Learner.ipynb 18
def _cov2std(x):
    "convert cov of array of covariances to array of stddev"
    return torch.diagonal(torch.sqrt(x), dim1=1, dim2=2)

# %% ../../lib_nbs/kalman/01_Kalman_Learner.ipynb 19
@patch
def plot_state(self: KalmanModel, n_cols = 2, bind_interaction = True, properties={}):

    s_mean, s_cov = self.state
    s_std = _cov2std(s_cov)
    
    time = torch.arange(self.n_obs)
    
    mean = array2df(s_mean, time, self.latent_names, 'time').melt('time', value_name='mean')
    std = array2df(s_std, time, self.latent_names, 'time').melt('time', value_name='std' )
    
    state = pd.merge(mean, std, on=['time', 'variable'])
    
    plot_list = [alt.hconcat() for _ in range(0, len(self.latent_names), n_cols)]
    selection_scale = alt.selection_interval(bind="scales", encodings=['x']) if bind_interaction else None
    for idx, variable in enumerate(self.latent_names):
        data = state[state.variable == variable]
        plot_list[idx // n_cols] |= _plot_error_bar(data,
                                                   variable,
                                                   y_label = variable,
                                                   sel = selection_scale, properties=properties)
    
    plot = alt.vconcat(*plot_list)
    
    return plot


# %% ../../lib_nbs/kalman/01_Kalman_Learner.ipynb 22
@patch
def plot_loss(self: KalmanModel, size={'width': 250, 'height': 120}):
    
    sel = alt.selection_interval(bind="scales", encodings=['x'])
    
    plt_losses = alt.Chart(
        pd.DataFrame({'loss': self.losses, 'n_iter': range(self.losses.shape[0])})
    ).mark_line().encode(
        x = 'n_iter',
        y = alt.Y('loss', scale=alt.Scale(zero=False))
    ).properties(title="loss", **size).add_selection(sel)
    
    return plt_losses
