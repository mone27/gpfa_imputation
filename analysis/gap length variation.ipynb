{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7bac9c-cfcf-4cef-9269-148fc197666d",
   "metadata": {},
   "source": [
    "# Gap Length Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f31fd4-7b8b-4ace-9bec-647cbd793c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852e189-a8bd-4674-a22e-53c4fc7451f0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c12fbc-2617-4439-aeab-67cbf805ffad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'meteo_imp.gpfa.data_preparation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmeteo_imp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpfa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimputation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmeteo_imp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpfa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_preparation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmeteo_imp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresults\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmeteo_imp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresults\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _display_as_row \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'meteo_imp.gpfa.data_preparation'"
     ]
    }
   ],
   "source": [
    "from meteo_imp.gpfa.imputation import *\n",
    "from meteo_imp.data_preparation import *\n",
    "from meteo_imp.results import *\n",
    "from meteo_imp.results import _display_as_row \n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pyprojroot import here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from meteo_imp.utils import cache_disk\n",
    "\n",
    "from itertools import combinations, repeat, zip_longest\n",
    "\n",
    "from ipywidgets import interact\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68f080-2641-4bc4-80ea-04588bdbc160",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "take the first 200 rows from the Hainich dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd34722-9117-4d1b-a0f0-2368dd244cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\n",
    "hai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782fb96-3157-4089-a9a6-8118118bc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_vars = {\n",
    "    \"TA_F\": \"TA\",\n",
    "    \"SW_IN_F\": \"SW_IN\",\n",
    "    #\"LW_IN_F\": \"LW_IN\",\n",
    "    \"VPD_F\": \"VPD\",\n",
    "    #\"PA\": \"PA\"\n",
    "}\n",
    "\n",
    "units = {\n",
    "    'TA': 'Â°C',\n",
    "    'SW_IN': 'W m-2',\n",
    "    # 'LW_IN': 'W m-2',\n",
    "    'VPD': 'hPa'\n",
    "}\n",
    "\n",
    "hai = (hai_raw\n",
    "       .rename(columns=meteo_vars)\n",
    "       .set_index(\"TIMESTAMP_END\")\n",
    "       .loc[:, meteo_vars.values()])\n",
    "hai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6276b1b5-548d-4f2d-8e2a-a191763bb75c",
   "metadata": {},
   "source": [
    "###  Functions\n",
    "\n",
    "makes here all the slow computations and cache them on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e49bb3-f55a-4ede-b2f6-e045d716bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 200\n",
    "n_latent = 1\n",
    "total_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206117c2-8406-40ab-9111-405ba1b3d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = here() / \"analysis/trained_models\"\n",
    "\n",
    "model_path = model_save_dir / f\"GPFA_l_{n_latent}_train_{total_iter}_1ker_{n_obs}_obs.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c93ab-5a74-4cb1-a9da-992b1e0c165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GPFADataTest(hai[:n_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32573b-7239-4d8b-bad6-a2a92c1ab4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired from https://datagy.io/python-combinations-of-a-list/\n",
    "def all_comb(l):\n",
    "    list_combinations = []\n",
    "    for n in range(1, len(l) + 1):\n",
    "        list_combinations += list(combinations(l, n))\n",
    "    return list_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b2423-3b86-42c8-83a7-fe436ae09029",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comb(meteo_vars.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eacfeb-5e1f-4a57-a808-2dfbd7e0ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_result_pretrained(gap_len, n_latent, var_sel, gap_start=None):\n",
    "    data = GPFADataTest(hai[:n_obs]).add_gap(gap_len, var_sel, gap_start)\n",
    "    imp = GPFAImputationExplorer(data.data, latent_dims = n_latent)\n",
    "    model_path = model_save_dir / f\"GPFA_l_{n_latent}_train_{total_iter}_1ker_{n_obs}_obs.pickle\"\n",
    "    imp.learner.load(model_path)\n",
    "    return imp.to_result(data.data_compl_tidy, units=units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58776777-2c0f-4685-84a3-f84072278f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_result_pretrained(10, 1, ['TA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655bb81-0866-4727-8374-e82ef2dde0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# @cache_disk(here() / \".cache/diff_gaps\")\n",
    "# def diff_gaps(gap_start=30):\n",
    "#     return {n_lat:\n",
    "#             {var_sel:{ \n",
    "#                 gap_len: to_result_pretrained(GPFADataTest(hai[:n_obs]).add_gap(gap_len, ['TA'], gap_start), n_lat)\n",
    "#                 for gap_len in [2, 4, 5, 7, 10 , 15, 20, 30, 50, 100]\n",
    "#                 }\n",
    "#                 for var_sel in all_comb(meteo_vars.values())}\n",
    "#             for n_lat in range(1,4)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa184d-0663-4e10-b1ee-ec9a07e134ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps = [2, 5, 7, 10, 20, 30, 50, 100]\n",
    "gap_starts = [0, 30, 60, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abee14-c14e-465a-838e-8ab729db2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = here() / \".cache/diff_gap_partial\"\n",
    "# path_base.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163dc22-b4ec-4904-bc77-86081829643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_var_sel(args, path_base=path_base):\n",
    "    var_sel, n_lat = args # limitations in python map...\n",
    "    f_name = path_base / f\"{'-'.join(var_sel)}__l_{n_lat}.pickle\"\n",
    "    if f_name.exists(): return\n",
    "    out = {}\n",
    "    for gap_len in gaps:\n",
    "        out[gap_len] = {}\n",
    "        for gap_start in gap_starts:\n",
    "            out[gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start) \n",
    "    with open(f_name, \"wb\") as f:\n",
    "        pickle.dump(out, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c59d78a-b784-4e79-867b-348b08eba698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# # this is going to run on the process\n",
    "# # @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\n",
    "# def process_n_lat(n_lat):\n",
    "#     out = {}\n",
    "#     for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n",
    "#         out[var_sel] = {}\n",
    "#         for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n",
    "#             out[var_sel][gap_len] = {}\n",
    "#             for gap_start in tqdm(gap_starts, desc=f\"gap len: {gap_len}\"):\n",
    "#                 out[var_sel][gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fa5b0-2bd2-4bd1-a808-37beafe77bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\n",
    "def compute_diff_gaps(gap_start=30):\n",
    "    for n_lat in tqdm(range(1,4)):\n",
    "        with Pool(processes=4) as pool:\n",
    "            list(pool.imap(process_var_sel, zip(all_comb(meteo_vars.values()), repeat(n_lat,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee66f85-9e2d-477a-8c42-02f8c67ed683",
   "metadata": {},
   "source": [
    "### Compute\n",
    "\n",
    "this is memory intensive! (maybe there is a leak to fix somewhere ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6bb002-068f-4964-bd9b-12a46efefe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_diff_gaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57da6fb-ab0b-4ddc-9def-98a7490b5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\n",
    "# def diff_gaps(gap_start=30):\n",
    "#     with Pool(processes=4) as pool:\n",
    "#         out = pool.map(res, range(1,4)\n",
    "#         for n_lat in tqdm(range(1,4)):\n",
    "#             out[n_lat] = {}\n",
    "#             for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n",
    "#                 out[n_lat][var_sel] = {}\n",
    "#                 for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n",
    "#                     out[n_lat][var_sel][gap_len] = {}\n",
    "#                     for gap_start in tqdm(gap_starts, desc=f\"gap len: {gap_len}\"):\n",
    "#                         out[n_lat][var_sel][gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n",
    "#     return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e76cea-e1b8-43f7-ac05-d3075011ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# @cache_disk(here() / \".cache/diff_gaps\")\n",
    "# def diff_gaps(gap_start=30):\n",
    "#     with Pool(processes=4) as pool:\n",
    "#         out = {}  \n",
    "#         for n_lat in tqdm(range(1,4)):\n",
    "#             out[n_lat] = {}\n",
    "#             for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n",
    "#                 out[n_lat][var_sel] = {}\n",
    "#                 for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n",
    "#                     out[n_lat][var_sel][gap_len] = {}\n",
    "#                     f = lambda gap_start: to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n",
    "#                     results = pool.map(f, gap_starts)\n",
    "#                     for gap_start, res in zip(gap_starts, results):\n",
    "#                         out[n_lat][var_sel][gap_len][gap_start] = res\n",
    "#     return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f9a6c-3c43-4a9d-9c03-4726760268b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# diff_gaps_res = diff_gaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12921c67-d9d5-41c6-b3fd-9b2f5dc871da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads computations from disk\n",
    "def load_diff_gaps():\n",
    "    out = {}\n",
    "    for n_lat in tqdm(range(1,4)):\n",
    "        out[n_lat] = {}\n",
    "        for var_sel in all_comb(meteo_vars.values()):\n",
    "            f_name = path_base / f\"{'-'.join(var_sel)}__l_{n_lat}.pickle\"\n",
    "            with open(f_name, \"rb\") as f:\n",
    "                out[n_lat][var_sel] = pickle.load(f)  \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34b3d4-e048-4d48-81f1-56f4be706390",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_gaps_res = load_diff_gaps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099facc7-e738-4005-95e1-d1d0879837c7",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70c128-2e08-43eb-a5d7-9d60196ad9c9",
   "metadata": {},
   "source": [
    "What I am doing here:\n",
    "\n",
    "- take a dataset with 200 obs and 3 variables\n",
    "- [distribution and correlation between vars](var_distribution.ipynb)\n",
    "- fit the kernel parameters using gradient descend on whole dataset and save trained model [notebook](Train multiple latent.ipynb)\n",
    "- create a dataset with all combinations of gap_len, gap_start, n latents and variable missing\n",
    "- predict the model for all 200 Obs also when there are no gaps!\n",
    "- **Note: in case there is a gap in not all variable, the variable with the gap have the (correct) prediction conditioned on the other variables, but the variables with no gap have the base model prediction (which is often bad), which should not be considered**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1117006-6b22-4d49-a712-6ece339199c4",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "What we can see from this result:\n",
    "\n",
    "##### Latent\n",
    "\n",
    "- 1 latent the Lambda is almost 1 for `TA`, 0 for `SW_IN` and .4 for `VPD`. Hence is good for `TA`, horrible for `SW_IN and somehow okayish for `VPD`\n",
    "- 2 latent2: good for `TA` and `SW_IN`, still limited for `VPD`\n",
    "- 3 latents: quite good fit for all 3 models\n",
    "\n",
    "**comments**\n",
    "\n",
    "- correlation between `SW_IN` and the others variables is pretty low\n",
    "- therefore with 1-2 latents the model cannot model accurately more then 1 variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059fb4a-ed1d-4ab9-9e49-ac2a19b44f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "hai.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268442b-a7d8-4517-9e8c-051c30c22b34",
   "metadata": {},
   "source": [
    "##### Gap Len\n",
    "\n",
    "\n",
    "- when gaps are short <~10 the model works kind of well, but there are issues in some locations (eg: TA, len: 10 start: 60)\n",
    "\n",
    "**comments**\n",
    "\n",
    "the lengthscale of the kernel is quite small (3 latents):\n",
    "\n",
    "- 5.2 z0\n",
    "- 1.8 z1\n",
    "- 4.0 z2\n",
    "\n",
    "so for longer gaps (in only one var) the main driver for the predictions are observations from the other variables, otherwise the models predictions are contastant as there is no way to use more information (eg. gap_len: 50, gap_start: 30, gaps in all vars)\n",
    "\n",
    "**notes**\n",
    "- when SW_IN and VPD are close to 0 the gap filling is not that great also for shorter gaps (eg. `SW_IN`, len: 7, start: 30, n_lat: 3)\n",
    "\n",
    "\n",
    "##### Gaps multiple variables\n",
    "\n",
    "the interesting aspect is when there is a long gap, but in only 1-2 variables\n",
    "\n",
    "- for gaps only in `TA` with len up to 50 the models manages to follow the variations in the measurements, but with an error\n",
    "    - this is pretty similar if there are gaps also in `SW_IN`, but not if there are gaps in `VPD`\n",
    "    - with gap len over 100 it get way worse\n",
    "- - for gaps only in `VP` with len up to 50 the models overall manages to follow the variations in the measurements, but with a considerable error (measurements are still in error bar) and the models has a lot of variations which are not present in the data\n",
    "- the predictions for `SW_IN` are bad (underestimates a lot the values) during the day for long gaps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424d073-0ff3-40d7-a918-aad4b0771f7f",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63690c48-73f6-4c03-a15b-d3a2ef7a8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "@interact(n_lat = range(1,4), TA=True, SW_IN=True, VPD=True, gap_len=gaps, gap_start=gap_starts)\n",
    "def show_diff_gaps_res(n_lat, TA, SW_IN, VPD, gap_len, gap_start):\n",
    "    var_sel = []\n",
    "    if TA: var_sel.append('TA')\n",
    "    if SW_IN: var_sel.append('SW_IN')\n",
    "    if VPD: var_sel.append('VPD')\n",
    "    var_sel = (*var_sel,)\n",
    "    diff_gaps_res[n_lat][var_sel][gap_len][gap_start].units = units\n",
    "    diff_gaps_res[n_lat][var_sel][gap_len][gap_start].display_results()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a95b7-ee25-400c-8f66-c3748c21a786",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7473f2a-daf6-469b-a6bf-b1996500832f",
   "metadata": {},
   "source": [
    "### Improvements GPFA\n",
    "\n",
    "- [more kernels](Additional latent kernel.ipynb) -> can have different timescales. However with 150 obs both kernels have the same timescale, should use more data but then there are computation issues (with 1500 it would take more then 20hours to do the training)\n",
    "- [log transform](Log transform - Multi latent - Imputation GPFA - Hainich.ipynb)\n",
    "- more variables\n",
    "\n",
    "#### Technical\n",
    "\n",
    "- model performance:\n",
    "at the moment it takes ~8 minutes to train with 200 obs and ~20 seconds for inference\n",
    "    - profile current model\n",
    "    - use SparseGP\n",
    "    - CUDA support\n",
    "- [parameters init](Init_parameters_effect.ipynb)\n",
    "- [learning rate and stability of parameters over training](Train multiple latent.ipynb)\n",
    "- variable transformation:\n",
    "    - all vars are now normalized (0 mean, 1 std)\n",
    "    - time is enconded as integer increasing at steps of 1. Maybe not a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca6c37-8a11-4a3c-a8f2-78b9d2aa4114",
   "metadata": {},
   "source": [
    "### Next step\n",
    "\n",
    "- use ERA5-Land (world-wide dataset with complete meteo vars, but a coarse spatial-temporal scale)\n",
    "- compare performance with state of art models\n",
    "- model where relation between variables changes over time\n",
    "- understand gap distribution in real world:\n",
    "    - average gap len (tentative results are: a lot of short gaps(<10) and some pretty long gap (>10.0000)\n",
    "    - correlation between variable gaps\n",
    "    - site distributions of gaps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f8497-868f-4c8b-aa1f-636c8a858bf6",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a28a5-0498-4906-a7b4-be6fe564a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time r = to_result_pretrained(12, 3, ['SW_IN', 'TA'], gap_start=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ec741-731f-40ec-8e31-a1990da5b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca70f7-0870-4d3e-9bb3-d5588fc422b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GPFADataTest(hai[:n_obs]).add_gap(50, ['TA'], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458a8dd-ae1d-4639-9c2c-009527a17952",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = GPFAImputationExplorer(data.data, latent_dims = n_latent)\n",
    "imp.learner.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d49f2-5523-4402-bc0e-2eb7f05a9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pretrained(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
