[
  {
    "objectID": "results/model_training.html",
    "href": "results/model_training.html",
    "title": "Training Kalman Filter for Results - 21 Feb",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom meteo_imp.kalman.training import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\nfrom meteo_imp.gaussian import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\nfrom fastcore.foundation import patch\nfrom pathlib import Path, PosixPath\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image, HTML\n\nfrom tqdm.auto import tqdm\nfrom fastcore.basics import *\nshow_metrics = False\nreset_seed()\nhai = pd.read_parquet(hai_big_path)\nhai_era = pd.read_parquet(hai_era_big_path)\nbase = here(\"analysis/results/trained_models\")\nbase.mkdir(exist_ok=True)\n@patch\ndef add_end(self: PosixPath, end): return self.parent / (self.name + end)\ndef train_or_load(model, dls, lr, n, path, keep=True):\n    save_models = SaveModelsBatch(times_epoch=5)\n    csv_logger = CSVLogger(path.add_end(\"log.csv\"))\n    learn = Learner(dls, model, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models, csv_logger], metrics=rmse_gap) \n    items = random.choices(dls.valid.items, k=4) \n    if path.add_end(\".pickle\").exists() and keep:\n        learn.model = torch.load(path.add_end(\".pickle\"))\n        display(csv_logger.read_log())\n        plot = Image(filename=path.add_end(\"_loss_plot.png\"))\n        display(plot)\n    else:\n        learn.fit(lr, n)\n        \n        torch.save(learn.model, path.add_end(\".pickle\"))\n        learn.recorder.plot_loss()\n        plt.savefig(path.add_end(\"_loss_plot.png\"))\n    return learn, items\ndef metric_valid(learn, dls=None):\n    nrmse = []\n    losses = []\n    dls = ifnone(dls, learn.dls.valid)\n    for input, target in tqdm(dls, leave=False):\n        pred = learn.model(input)\n        nrmse.append(learn.metrics[0](pred, target))\n        losses.append(learn.loss_func(pred, target).item())\n    metric = pd.DataFrame({'loss': losses, 'rmse': nrmse})\n    return metric.agg(['mean', 'std'])\nhai.columns\n\nIndex(['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], dtype='object')"
  },
  {
    "objectID": "results/model_training.html#generic-model-gap-len-3-336-gap-1-random",
    "href": "results/model_training.html#generic-model-gap-len-3-336-gap-1-random",
    "title": "Training Kalman Filter for Results - 21 Feb",
    "section": "Generic model | gap len 3-336 | gap 1 random",
    "text": "Generic model | gap len 3-336 | gap 1 random\n\ndls_A1v = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns), n_var=1),\n    block_len=100+336,\n    gap_len=gen_gap_len(6, 336),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5).cpu()\n\n\nlen(hai)\n\n227952\n\n\n\nlen(dls_A1v.train)*20, len(dls_A1v.valid)*20\n\n(2080, 520)\n\n\n\nmodel_A1v = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca = None, \n    pred_only_gap=True)\n\n\nmodel_A1v.B.shape\n\ntorch.Size([1, 18, 14])\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 3, 1e-3, base / \"1_gap_varying_6-336_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n196.765350\n163.493486\n0.579074\n48:33\n\n\n1\n1\n138.298704\n123.299909\n0.490741\n48:14\n\n\n2\n2\n113.640141\n116.746793\n0.488059\n39:00\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 1, 1e-4, base / \"1_gap_varying_6-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n94.869328\n112.046392\n0.471249\n43:59\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 1, 1e-6, base / \"1_gap_varying_6-336_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n95.239438\n104.268073\n0.467282\n40:19"
  },
  {
    "objectID": "results/model_training.html#var-gap---varying-336---no-control",
    "href": "results/model_training.html#var-gap---varying-336---no-control",
    "title": "Training Kalman Filter for Results - 21 Feb",
    "section": "1 var gap - varying 336 - No Control",
    "text": "1 var gap - varying 336 - No Control\n\nmodel_A1v_nc = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca=None,\n    pred_only_gap=True,\n    use_control=False\n)\n\n\nlearn_A1v_nc, items_A1v_nc = train_or_load(model_A1v_nc, dls_A1v, 3, 1e-3, base / \"1_gap_varying_336_no_control_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n219.686355\n178.506325\n0.658579\n37:14\n\n\n1\n1\n176.039201\n160.979378\n0.583213\n37:00\n\n\n2\n2\n166.012525\n158.206468\n0.574111\n36:47"
  },
  {
    "objectID": "results/model_training.html#short-gaps",
    "href": "results/model_training.html#short-gaps",
    "title": "Training Kalman Filter for Results - 21 Feb",
    "section": "Short gaps",
    "text": "Short gaps\n\nAll variables - 30 all\n\ndls_Aa = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = list(hai.columns),\n    block_len=120,\n    gap_len=gen_gap_len(6, 30),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5\n).cpu()\n\n\ndls_Aa = imp_dataloader(hai, hai_era, var_sel = list(hai.columns), block_len=120, gap_len=gen_gap_len(6,30), bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa = learn_A1v.model.copy()\n\n\nif show_metrics: display(metric_valid(learn_A1v, dls=dls_Aa.valid))\n\n\ndls_A1v30 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns), n_var=1),\n    block_len=100+30,\n    gap_len=gen_gap_len(6, 30),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5).cpu()\n\n\nif show_metrics: display(metric_valid(learn_A1v, dls=dls_A1v30.valid))\n\n\nlearn_Aa, items_Aa = train_or_load(model_Aa, dls_Aa, 3, 3e-4, base / \"All_gap_all_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n24.688308\n21.049544\n0.365108\n28:05\n\n\n1\n1\n-7.512621\n-4.152000\n0.342344\n27:26\n\n\n2\n2\n-18.230698\n-19.744404\n0.327594\n26:15\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v30, items_A1v30 = train_or_load(learn_A1v.model.copy(), dls_A1v30, 3, 3e-4, base / \"1_gap_varying_tuned_6-30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n4.763339\n3.134267\n0.276432\n15:02\n\n\n1\n1\n2.390429\n1.721823\n0.267772\n16:12\n\n\n2\n2\n0.780169\n0.607745\n0.255524\n16:05\n\n\n\n\n\n\n\n\n\n\nso this is not working …\n\n\nVarying number of variables missing | short gaps 6-30\n\ndls_Vv30 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns)),\n    block_len=100+30,\n    gap_len=gen_gap_len(6, 30),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=20).cpu()\n\n\nlearn_Vv30, items_Vv30 = train_or_load(learn_Aa.model, dls_Vv30, 3, 5e-4, base / \"all_varying_gap_varying_len_6-30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-5.794561\n-4.508800\n0.213908\n1:00:55\n\n\n1\n1\n-3.717697\n-5.165062\n0.205841\n1:00:26\n\n\n2\n2\n-1.928287\n-6.012112\n0.202048\n1:00:21\n\n\n\n\n\n\n\n\n\n\n\nlearn_Vv30, items_Vv30 = train_or_load(learn_Aa.model, dls_Vv30, 1, 1e-5, base / \"all_varying_gap_varying_len_6-30_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.061014\n-6.663726\n0.192194\n57:55\n\n\n\n\n\n\n\n\n\n\n\nlearn_Vv30, items_Vv30 = train_or_load(learn_Aa.model, dls_Vv30, 1, 1e-5, base / \"all_varying_gap_varying_len_6-30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.417934\n-6.799482\n0.19076\n1:00:23\n\n\n\n\n\n\n\n\n\n\n\n\nRandom parameters\n\nmodel_Vv_rand = KalmanFilterSR.init_random(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=2*len(hai.columns),\n    n_dim_contr = 2*len(hai_era.columns),\n    seed=27,\n    pred_only_gap=True)\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(model_Vv_rand, dls_Vv30, 3, 1e-3, base / \"rand_all_varying_gap_varying_len_6-30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n13.508053\n14.374478\n0.448185\n1:00:35\n\n\n1\n9.766153\n11.389963\n0.395332\n58:51\n\n\n2\n6.503961\n6.754238\n0.305433\n54:16\n\n\n\n\n\n\n\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(learn_Vv_rand.model, dls_Vv30, 3, 1e-4, base / \"rand_all_varying_gap_varying_len_6-30_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n5.101230\n5.345336\n0.285398\n1:02:15\n\n\n1\n4.838514\n4.907970\n0.281667\n1:02:44\n\n\n2\n4.571287\n4.295109\n0.275344\n1:04:47\n\n\n\n\n\n\n\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(learn_Vv_rand.model, dls_Vv30, 3, 1e-5, base / \"rand_all_varying_gap_varying_len_6-30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n4.075190\n4.282635\n0.275272\n2:36:25\n\n\n1\n5.134852\n4.244826\n0.274858\n1:17:58\n\n\n2\n3.923739\n4.185355\n0.274057\n1:11:31\n\n\n\n\n\n\n\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(learn_Vv_rand.model, dls_Vv30, 3, 1e-5, base / \"rand_all_varying_gap_varying_len_6-30_v4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n4.100049\n4.142156\n0.273928\n56:48\n\n\n1\n3.712223\n4.112651\n0.273566\n56:59\n\n\n2\n3.942678\n4.073334\n0.273349\n57:03\n\n\n\n\n\n\n\n\n\nmodel_Vv_rand\n\n\nKalman Filter (9 obs, 18 state, 14 contr)  $A$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\nx_0\n0.8775\n0.2675\n0.0937\n0.6706\n0.1638\n0.9272\n0.2620\n0.4967\n0.2630\n0.1175\n0.1694\n0.2100\n0.4890\n0.0564\n0.4760\n0.7606\n0.7759\n0.5243\n\n\nx_1\n0.3714\n0.0426\n0.2343\n0.9991\n0.1775\n0.6319\n0.6734\n0.7937\n0.6468\n0.5825\n0.4599\n0.7960\n0.9038\n0.9735\n0.6428\n0.3725\n0.2052\n0.0507\n\n\nx_2\n0.4448\n0.5775\n0.7237\n0.5927\n0.3217\n0.6441\n0.2801\n0.9132\n0.0329\n0.4856\n0.9927\n0.5895\n0.2611\n0.9413\n0.1371\n0.8726\n0.5590\n0.8451\n\n\nx_3\n0.1253\n0.9434\n0.0462\n0.2360\n0.0239\n0.8950\n0.7419\n0.9471\n0.6690\n0.1554\n0.0821\n0.7309\n0.7764\n0.9769\n0.0196\n0.0384\n0.4294\n0.3438\n\n\nx_4\n0.5494\n0.8238\n0.9845\n0.6826\n0.9001\n0.3022\n0.7509\n0.0926\n0.0328\n0.4798\n0.5335\n0.0434\n0.3530\n0.4157\n0.7495\n0.1716\n0.1980\n0.4298\n\n\nx_5\n0.9201\n0.6883\n0.5342\n0.7847\n0.3137\n0.1778\n0.5838\n0.9799\n0.3611\n0.3155\n0.7475\n0.5450\n0.5641\n0.2493\n0.8323\n0.9723\n0.1883\n0.3605\n\n\nx_6\n0.5344\n0.3443\n0.7696\n0.3410\n0.7553\n0.3177\n0.0315\n0.5209\n0.6514\n0.3131\n0.4510\n0.3550\n0.4790\n0.0676\n0.3606\n0.7299\n0.6713\n0.3134\n\n\nx_7\n0.7460\n0.1291\n0.4653\n0.5693\n0.9906\n0.8288\n0.9039\n0.5240\n0.6277\n0.3574\n0.0076\n0.6530\n0.8667\n0.9368\n0.8667\n0.6749\n0.3526\n0.6618\n\n\nx_8\n0.0837\n0.7188\n0.7247\n0.3211\n0.4898\n0.9030\n0.0358\n0.1662\n0.7741\n0.7937\n0.7183\n0.5141\n0.4918\n0.2773\n0.6901\n0.8565\n0.3723\n0.3410\n\n\nx_9\n0.4035\n0.0591\n0.6836\n0.8306\n0.4312\n0.0210\n0.0032\n0.9010\n0.6741\n0.3875\n0.3683\n0.5337\n0.0706\n0.8516\n0.7304\n0.8507\n0.6829\n0.6900\n\n\nx_10\n0.1059\n0.0500\n0.5736\n0.9595\n0.8101\n0.7397\n0.5282\n0.1294\n0.2746\n0.5556\n0.6463\n0.0023\n0.1761\n0.3391\n0.3346\n0.4655\n0.8172\n0.4176\n\n\nx_11\n0.1349\n0.0519\n0.1180\n0.9767\n0.1679\n0.8635\n0.3753\n0.9760\n0.2125\n0.8049\n0.2124\n0.6794\n0.0037\n0.9711\n0.5679\n0.9474\n0.8593\n0.6385\n\n\nx_12\n0.8770\n0.0469\n0.1582\n0.6694\n0.5670\n0.9794\n0.6498\n0.3257\n0.8462\n0.7727\n0.3213\n0.7318\n0.3665\n0.9550\n0.7188\n0.2660\n0.5867\n0.1134\n\n\nx_13\n0.7401\n0.1982\n0.4165\n0.3814\n0.5263\n0.6516\n0.9604\n0.8996\n0.8318\n0.7448\n0.6912\n0.5938\n0.0929\n0.5298\n0.2637\n0.8722\n0.5430\n0.2217\n\n\nx_14\n0.3495\n0.3756\n0.1251\n0.4052\n0.0638\n0.0588\n0.4379\n0.4891\n0.2796\n0.0740\n0.2123\n0.1370\n0.4477\n0.3628\n0.9125\n0.4047\n0.8130\n0.2332\n\n\nx_15\n0.8424\n0.0816\n0.8791\n0.3892\n0.2923\n0.8603\n0.1172\n0.6212\n0.6087\n0.6072\n0.8778\n0.6758\n0.5495\n0.8240\n0.7461\n0.1555\n0.2950\n0.0365\n\n\nx_16\n0.8060\n0.8602\n0.9453\n0.7811\n0.5495\n0.5861\n0.8480\n0.1940\n0.9206\n0.5589\n0.2148\n0.1828\n0.0636\n0.2885\n0.9426\n0.6787\n0.0080\n0.7527\n\n\nx_17\n0.5032\n0.5585\n0.0789\n0.0409\n0.3918\n0.2908\n0.3802\n0.0407\n0.6447\n0.3241\n0.8544\n0.4245\n0.3987\n0.4367\n0.3384\n0.2285\n0.7890\n0.9094\n\n\n\n\n$Q$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\nx_0\n1.4985\n0.1823\n0.8819\n1.1215\n0.8124\n1.0154\n0.4127\n0.4993\n0.3792\n1.1249\n0.3349\n0.5480\n1.1208\n1.0406\n0.0373\n0.0713\n0.0563\n0.0038\n\n\nx_1\n0.1823\n0.7474\n0.3388\n0.8779\n0.3284\n0.4377\n0.5683\n0.6806\n0.1517\n0.6860\n0.1334\n0.2869\n0.3903\n0.5594\n0.2687\n0.7078\n0.5301\n0.5990\n\n\nx_2\n0.8819\n0.3388\n1.5250\n1.3832\n1.4418\n0.7184\n0.4702\n1.2013\n1.1584\n1.3029\n1.0110\n1.0116\n1.4669\n1.5483\n0.6885\n0.7897\n1.0060\n0.8858\n\n\nx_3\n1.1215\n0.8779\n1.3832\n2.4360\n1.6422\n1.2901\n1.1545\n1.7087\n1.4626\n2.4015\n1.3411\n1.4348\n2.0194\n2.1839\n1.2870\n1.3528\n1.5829\n1.6857\n\n\nx_4\n0.8124\n0.3284\n1.4418\n1.6422\n3.2440\n1.8293\n0.6635\n2.2540\n1.7788\n2.2643\n1.5406\n1.9412\n2.5939\n2.3073\n2.1935\n1.1262\n1.5039\n1.7599\n\n\nx_5\n1.0154\n0.4377\n0.7184\n1.2901\n1.8293\n2.6367\n0.6717\n2.0377\n1.2394\n1.6564\n1.4530\n1.5842\n2.3638\n2.1887\n2.0828\n1.2967\n0.6836\n0.8444\n\n\nx_6\n0.4127\n0.5683\n0.4702\n1.1545\n0.6635\n0.6717\n1.8814\n1.4915\n0.6485\n1.3643\n0.6293\n1.5192\n1.1450\n1.7599\n1.6733\n1.4465\n0.7419\n1.1889\n\n\nx_7\n0.4993\n0.6806\n1.2013\n1.7087\n2.2540\n2.0377\n1.4915\n3.7419\n2.0936\n2.3554\n2.8167\n2.9808\n2.6624\n3.4940\n3.4288\n2.7951\n1.6056\n2.6631\n\n\nx_8\n0.3792\n0.1517\n1.1584\n1.4626\n1.7788\n1.2394\n0.6485\n2.0936\n3.2420\n2.6115\n2.9707\n2.0584\n3.2186\n2.4394\n2.7694\n2.0308\n2.6026\n2.2404\n\n\nx_9\n1.1249\n0.6860\n1.3029\n2.4015\n2.2643\n1.6564\n1.3643\n2.3554\n2.6115\n4.7867\n3.5267\n2.4691\n3.7434\n3.6431\n3.0501\n2.1870\n3.3714\n2.4697\n\n\nx_10\n0.3349\n0.1334\n1.0110\n1.3411\n1.5406\n1.4530\n0.6293\n2.8167\n2.9707\n3.5267\n4.8661\n2.7361\n3.9109\n3.7590\n4.1276\n3.2131\n3.3042\n2.8676\n\n\nx_11\n0.5480\n0.2869\n1.0116\n1.4348\n1.9412\n1.5842\n1.5192\n2.9808\n2.0584\n2.4691\n2.7361\n4.5057\n3.4122\n4.2709\n3.9642\n3.3446\n2.1979\n2.7754\n\n\nx_12\n1.1208\n0.3903\n1.4669\n2.0194\n2.5939\n2.3638\n1.1450\n2.6624\n3.2186\n3.7434\n3.9109\n3.4122\n6.2732\n5.0000\n5.0583\n4.1123\n3.9812\n3.5458\n\n\nx_13\n1.0406\n0.5594\n1.5483\n2.1839\n2.3073\n2.1887\n1.7599\n3.4940\n2.4394\n3.6431\n3.7590\n4.2709\n5.0000\n6.2369\n5.2005\n4.6903\n3.3753\n4.1217\n\n\nx_14\n0.0373\n0.2687\n0.6885\n1.2870\n2.1935\n2.0828\n1.6733\n3.4288\n2.7694\n3.0501\n4.1276\n3.9642\n5.0583\n5.2005\n7.8159\n5.5318\n4.1568\n5.4245\n\n\nx_15\n0.0713\n0.7078\n0.7897\n1.3528\n1.1262\n1.2967\n1.4465\n2.7951\n2.0308\n2.1870\n3.2131\n3.3446\n4.1123\n4.6903\n5.5318\n5.8308\n3.6118\n5.0658\n\n\nx_16\n0.0563\n0.5301\n1.0060\n1.5829\n1.5039\n0.6836\n0.7419\n1.6056\n2.6026\n3.3714\n3.3042\n2.1979\n3.9812\n3.3753\n4.1568\n3.6118\n5.2990\n4.3170\n\n\nx_17\n0.0038\n0.5990\n0.8858\n1.6857\n1.7599\n0.8444\n1.1889\n2.6631\n2.2404\n2.4697\n2.8676\n2.7754\n3.5458\n4.1217\n5.4245\n5.0658\n4.3170\n7.1195\n\n\n\n\n\n$b$\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.5371\n\n\nx_1\n0.6015\n\n\nx_2\n0.3190\n\n\nx_3\n0.9543\n\n\nx_4\n0.5112\n\n\nx_5\n0.0341\n\n\nx_6\n0.9601\n\n\nx_7\n0.1604\n\n\nx_8\n0.4499\n\n\nx_9\n0.8575\n\n\nx_10\n0.2647\n\n\nx_11\n0.4293\n\n\nx_12\n0.9210\n\n\nx_13\n0.5512\n\n\nx_14\n0.0890\n\n\nx_15\n0.4351\n\n\nx_16\n0.3804\n\n\nx_17\n0.4879\n\n\n\n\n\n$H$\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\ny_0\n0.5241\n0.2182\n0.7958\n0.7816\n0.3235\n0.8518\n0.4334\n0.7567\n0.5235\n0.2247\n0.2498\n0.6324\n0.0037\n0.8468\n0.7664\n0.0362\n0.2519\n0.5872\n\n\ny_1\n0.4556\n0.2781\n0.0315\n0.3598\n0.2876\n0.8363\n0.0685\n0.5543\n0.9194\n0.3232\n0.0243\n0.2689\n0.8404\n0.9788\n0.9912\n0.0846\n0.1129\n0.0503\n\n\ny_2\n0.8881\n0.6638\n0.5292\n0.3452\n0.4999\n0.6894\n0.7628\n0.4233\n0.4219\n0.3110\n0.1801\n0.5059\n0.2597\n0.9244\n0.6246\n0.8295\n0.5742\n0.7359\n\n\ny_3\n0.2917\n0.2912\n0.9906\n0.3964\n0.5851\n0.0647\n0.3191\n0.0659\n0.9295\n0.0189\n0.8553\n0.6701\n0.6306\n0.6152\n0.5295\n0.9469\n0.9927\n0.7433\n\n\ny_4\n0.5977\n0.7385\n0.9348\n0.8533\n0.6523\n0.7823\n0.7676\n0.4763\n0.6374\n0.8520\n0.4391\n0.5353\n0.9097\n0.7429\n0.2067\n0.4188\n0.0382\n0.9770\n\n\ny_5\n0.6669\n0.7935\n0.4501\n0.6770\n0.0361\n0.3082\n0.9436\n0.8420\n0.2966\n0.6996\n0.8092\n0.0206\n0.9509\n0.0499\n0.3504\n0.8491\n0.5674\n0.8691\n\n\ny_6\n0.4429\n0.2004\n0.3868\n0.9650\n0.0220\n0.4891\n0.0179\n0.3229\n0.1670\n0.6188\n0.6477\n0.0439\n0.3738\n0.3988\n0.6175\n0.9562\n0.6395\n0.7886\n\n\ny_7\n0.6403\n0.2487\n0.6137\n0.2387\n0.7919\n0.1610\n0.2259\n0.9336\n0.8569\n0.6710\n0.9067\n0.1028\n0.7898\n0.3126\n0.5972\n0.3078\n0.3259\n0.5631\n\n\ny_8\n0.5374\n0.9159\n0.0255\n0.7863\n0.0953\n0.7248\n0.3355\n0.1565\n0.2010\n0.3647\n0.3080\n0.8794\n0.2877\n0.2028\n0.8040\n0.8565\n0.2100\n0.2746\n\n\n\n\n\n$R$\n\n\n\nvariable\ny_0\ny_1\ny_2\ny_3\ny_4\ny_5\ny_6\ny_7\ny_8\n\n\n\n\ny_0\n0.5106\n0.3847\n0.4957\n0.2641\n0.0725\n0.3685\n0.7145\n0.0334\n0.6538\n\n\ny_1\n0.3847\n1.1931\n1.0307\n0.4238\n0.1569\n0.4712\n0.6801\n0.9019\n0.7750\n\n\ny_2\n0.4957\n1.0307\n1.9865\n0.5453\n0.3404\n0.8202\n1.0589\n0.7202\n1.1494\n\n\ny_3\n0.2641\n0.4238\n0.5453\n1.3356\n0.2462\n0.9538\n0.8949\n1.0863\n1.4202\n\n\ny_4\n0.0725\n0.1569\n0.3404\n0.2462\n0.9155\n0.8953\n0.5091\n0.3793\n0.3845\n\n\ny_5\n0.3685\n0.4712\n0.8202\n0.9538\n0.8953\n1.9467\n1.7410\n1.1834\n1.4452\n\n\ny_6\n0.7145\n0.6801\n1.0589\n0.8949\n0.5091\n1.7410\n2.9640\n1.6060\n1.6805\n\n\ny_7\n0.0334\n0.9019\n0.7202\n1.0863\n0.3793\n1.1834\n1.6060\n3.7560\n2.1550\n\n\ny_8\n0.6538\n0.7750\n1.1494\n1.4202\n0.3845\n1.4452\n1.6805\n2.1550\n3.0331\n\n\n\n\n\n$d$\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.4399\n\n\ny_1\n0.8723\n\n\ny_2\n0.2250\n\n\ny_3\n0.0971\n\n\ny_4\n0.6572\n\n\ny_5\n0.7544\n\n\ny_6\n0.5670\n\n\ny_7\n0.7409\n\n\ny_8\n0.7357\n\n\n\n\n\n$B$\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\nc_6\nc_7\nc_8\nc_9\nc_10\nc_11\nc_12\nc_13\n\n\n\n\nx_0\n0.0135\n0.9418\n0.6751\n0.3042\n0.0136\n0.7803\n0.2302\n0.5920\n0.7610\n0.8504\n0.2033\n0.5990\n0.8954\n0.0604\n\n\nx_1\n0.2530\n0.1406\n0.4280\n0.1165\n0.5817\n0.2896\n0.4509\n0.2735\n0.8425\n0.5123\n0.4176\n0.5773\n0.3222\n0.5276\n\n\nx_2\n0.4523\n0.6324\n0.4716\n0.0785\n0.9462\n0.5346\n0.9771\n0.4970\n0.5893\n0.5292\n0.6864\n0.5196\n0.8370\n0.3849\n\n\nx_3\n0.4922\n0.3260\n0.1990\n0.6217\n0.7287\n0.4589\n0.8500\n0.1375\n0.9457\n0.8397\n0.5203\n0.8416\n0.1371\n0.5186\n\n\nx_4\n0.4377\n0.2392\n0.4949\n0.4146\n0.3028\n0.6810\n0.1177\n0.1563\n0.2588\n0.8996\n0.9248\n0.5575\n0.2553\n0.0631\n\n\nx_5\n0.5413\n0.5853\n0.4166\n0.9482\n0.0665\n0.4683\n0.0348\n0.6635\n0.0501\n0.1221\n0.1268\n0.7322\n0.3311\n0.0151\n\n\nx_6\n0.1452\n0.5820\n0.8673\n0.3090\n0.8065\n0.7325\n0.1682\n0.5885\n0.1180\n0.4120\n0.2043\n0.8200\n0.5015\n0.3238\n\n\nx_7\n0.3842\n0.4433\n0.3052\n0.4963\n0.4459\n0.9266\n0.6286\n0.8807\n0.3252\n0.0861\n0.7891\n0.1666\n0.1766\n0.0318\n\n\nx_8\n0.8600\n0.8088\n0.8600\n0.5418\n0.4772\n0.9634\n0.3191\n0.1484\n0.6377\n0.0586\n0.5372\n0.8380\n0.8808\n0.2243\n\n\nx_9\n0.4277\n0.0173\n0.9436\n0.3526\n0.1852\n0.2433\n0.8409\n0.7467\n0.4969\n0.2585\n0.3466\n0.4240\n0.1253\n0.2661\n\n\nx_10\n0.9022\n0.0314\n0.0804\n0.7244\n0.3651\n0.0938\n0.8409\n0.0069\n0.3613\n0.6663\n0.1531\n0.9582\n0.1326\n0.9434\n\n\nx_11\n0.8686\n0.9671\n0.1879\n0.7194\n0.3153\n0.5075\n0.6469\n0.0551\n0.2449\n0.5830\n0.3328\n0.4071\n0.2686\n0.4456\n\n\nx_12\n0.5746\n0.1570\n0.5606\n0.7224\n0.6012\n0.4299\n0.0548\n0.3849\n0.0750\n0.4321\n0.9120\n0.4023\n0.5149\n0.5738\n\n\nx_13\n0.1813\n0.1437\n0.8099\n0.2174\n0.2784\n0.7365\n0.5066\n0.1417\n0.6935\n0.0812\n0.0792\n0.1286\n0.6698\n0.1731\n\n\nx_14\n0.3023\n0.8685\n0.0737\n0.2969\n0.0566\n0.7863\n0.9368\n0.2227\n0.0272\n0.9288\n0.2405\n0.8415\n0.4647\n0.5220\n\n\nx_15\n0.2359\n0.5393\n0.3662\n0.9737\n0.1073\n0.0926\n0.9738\n0.8049\n0.2272\n0.4266\n0.4965\n0.2811\n0.5143\n0.1134\n\n\nx_16\n0.8076\n0.4430\n0.9223\n0.0757\n0.7333\n0.1208\n0.4115\n0.5446\n0.8064\n0.5765\n0.2153\n0.4235\n0.2613\n0.2662\n\n\nx_17\n0.4906\n0.6666\n0.1782\n0.4631\n0.4471\n0.4886\n0.6511\n0.1357\n0.9547\n0.8251\n0.5739\n0.0537\n0.9671\n0.1413\n\n\n\n\n\n$m_0$\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.4748\n\n\nx_1\n0.0525\n\n\nx_2\n0.8524\n\n\nx_3\n0.5821\n\n\nx_4\n0.7281\n\n\nx_5\n0.9879\n\n\nx_6\n0.6011\n\n\nx_7\n0.4692\n\n\nx_8\n0.9031\n\n\nx_9\n0.9123\n\n\nx_10\n0.6185\n\n\nx_11\n0.8070\n\n\nx_12\n0.5830\n\n\nx_13\n0.5986\n\n\nx_14\n0.5898\n\n\nx_15\n0.8722\n\n\nx_16\n0.7868\n\n\nx_17\n0.8305\n\n\n\n\n\n$P_0$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\nx_0\n0.7075\n0.4575\n0.1025\n0.7552\n0.4678\n0.4102\n0.6403\n0.2264\n0.0279\n0.6776\n0.8178\n0.4180\n0.7019\n0.4730\n0.7924\n0.5532\n0.6499\n0.0084\n\n\nx_1\n0.4575\n1.3349\n0.8975\n1.0693\n0.9878\n0.5854\n0.6734\n0.6813\n0.5162\n1.2483\n1.1523\n0.6240\n0.8392\n0.6469\n1.1093\n0.4286\n0.6817\n0.5511\n\n\nx_2\n0.1025\n0.8975\n1.7778\n1.4223\n1.0713\n0.5301\n0.4902\n0.9565\n0.7575\n1.1096\n0.6516\n0.6540\n1.3722\n0.9620\n0.6709\n1.0418\n0.7424\n0.5799\n\n\nx_3\n0.7552\n1.0693\n1.4223\n2.9851\n1.4541\n1.0645\n1.6541\n0.9605\n1.5344\n2.4493\n1.9649\n1.5684\n1.7126\n1.3214\n1.6471\n1.7876\n1.5110\n1.4273\n\n\nx_4\n0.4678\n0.9878\n1.0713\n1.4541\n1.6513\n1.2777\n1.3102\n0.7676\n1.0484\n1.4509\n1.6367\n1.4200\n1.9104\n0.8253\n1.1014\n0.9313\n1.6568\n1.2782\n\n\nx_5\n0.4102\n0.5854\n0.5301\n1.0645\n1.2777\n2.0910\n2.0046\n1.1160\n1.0262\n1.7268\n2.2526\n2.0053\n2.1983\n1.2724\n1.6174\n1.5978\n2.3012\n1.8972\n\n\nx_6\n0.6403\n0.6734\n0.4902\n1.6541\n1.3102\n2.0046\n3.2961\n1.3658\n2.1582\n2.6066\n2.7869\n2.4754\n2.3180\n1.8782\n1.9604\n2.2224\n3.0209\n2.9851\n\n\nx_7\n0.2264\n0.6813\n0.9565\n0.9605\n0.7676\n1.1160\n1.3658\n2.2370\n1.1397\n2.0217\n1.7425\n1.4582\n1.7065\n1.2660\n1.8500\n1.3857\n2.3256\n1.4360\n\n\nx_8\n0.0279\n0.5162\n0.7575\n1.5344\n1.0484\n1.0262\n2.1582\n1.1397\n3.1976\n2.4686\n2.6294\n2.0770\n2.1077\n1.4005\n1.6343\n2.0925\n2.2970\n2.9207\n\n\nx_9\n0.6776\n1.2483\n1.1096\n2.4493\n1.4509\n1.7268\n2.6066\n2.0217\n2.4686\n3.8815\n3.3607\n2.4745\n3.0405\n2.4247\n2.8433\n3.0134\n3.4315\n3.3814\n\n\nx_10\n0.8178\n1.1523\n0.6516\n1.9649\n1.6367\n2.2526\n2.7869\n1.7425\n2.6294\n3.3607\n4.7920\n3.0654\n3.5766\n2.7664\n3.4915\n3.4792\n4.0401\n3.2582\n\n\nx_11\n0.4180\n0.6240\n0.6540\n1.5684\n1.4200\n2.0053\n2.4754\n1.4582\n2.0770\n2.4745\n3.0654\n3.1407\n3.3492\n2.5045\n2.9033\n2.5216\n3.3064\n2.7858\n\n\nx_12\n0.7019\n0.8392\n1.3722\n1.7126\n1.9104\n2.1983\n2.3180\n1.7065\n2.1077\n3.0405\n3.5766\n3.3492\n6.7801\n4.6939\n3.9369\n4.9600\n4.8216\n3.7575\n\n\nx_13\n0.4730\n0.6469\n0.9620\n1.3214\n0.8253\n1.2724\n1.8782\n1.2660\n1.4005\n2.4247\n2.7664\n2.5045\n4.6939\n5.2116\n3.8491\n4.6837\n3.8005\n3.5731\n\n\nx_14\n0.7924\n1.1093\n0.6709\n1.6471\n1.1014\n1.6174\n1.9604\n1.8500\n1.6343\n2.8433\n3.4915\n2.9033\n3.9369\n3.8491\n5.2003\n4.0112\n3.8169\n3.0359\n\n\nx_15\n0.5532\n0.4286\n1.0418\n1.7876\n0.9313\n1.5978\n2.2224\n1.3857\n2.0925\n3.0134\n3.4792\n2.5216\n4.9600\n4.6837\n4.0112\n6.7694\n5.0850\n4.5844\n\n\nx_16\n0.6499\n0.6817\n0.7424\n1.5110\n1.6568\n2.3012\n3.0209\n2.3256\n2.2970\n3.4315\n4.0401\n3.3064\n4.8216\n3.8005\n3.8169\n5.0850\n6.7135\n5.3989\n\n\nx_17\n0.0084\n0.5511\n0.5799\n1.4273\n1.2782\n1.8972\n2.9851\n1.4360\n2.9207\n3.3814\n3.2582\n2.7858\n3.7575\n3.5731\n3.0359\n4.5844\n5.3989\n7.9999"
  },
  {
    "objectID": "results/model_training.html#fine-tuning",
    "href": "results/model_training.html#fine-tuning",
    "title": "Training Kalman Filter for Results - 21 Feb",
    "section": "Fine tuning",
    "text": "Fine tuning\n\nFine tune Variable | gap only for one variable | gap len 6-336\nfine tune the model to only one variable\n\nfrom fastcore.basics import *\n\n\nfrom IPython.display import HTML\n\n\nvar_learning = {\n    'TA': [{'lr': 1e-3, 'n': 3}],  \n    'SW_IN': [{'lr': 1e-3, 'n': 4}],  \n    'SW_IN': [{'lr': 1e-3, 'n': 4}],  \n    'LW_IN': [{'lr': 1e-3, 'n': 3}],  \n    'VPD': [{'lr': 1e-3, 'n': 3}],  \n    'WS': [{'lr': 1e-3, 'n': 3}],  \n    'PA': [{'lr': 1e-3, 'n': 3}],  \n    # 'P': [{'lr': 1e-3, 'n': 3}], missing on purpose  \n    'SWC' : [{'lr': 1e-3, 'n': 5}, {'lr': 1e-5, 'n': 1}],\n    'TS' : [{'lr': 1e-3, 'n': 5}],\n\n\n}\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\ndef fine_tune(var_learning, learn):\n    spec_models = {}\n    spec_dls = {}\n    spec_learn = {}\n    spec_items = {}\n    for var in tqdm(var_learning.keys()):\n        display(HTML(f\"&lt;h4&gt; {var} | Gap len 6-336  finetune&lt;/h4&gt;\"))\n        spec_dls[var] = imp_dataloader(hai, hai_era, var_sel = var, block_len=100+336, gap_len=gen_gap_len(6, 336), bs=20, control_lags=[1], n_rep=3, shifts=gen_shifts(50)).cpu()\n        if show_metrics:\n            display(HTML(\"Metrics generic model\"))\n            display(metric_valid(learn, dls=spec_dls[var].valid))\n        for i, train in enumerate(var_learning[var]):\n            lr, n = train\n            display(HTML(f\"train {i}\"))\n            spec_models[var] = learn.model.copy()\n            spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], lr, n, base / f\"{var}_specialized_gap_6-336_v1_{i}\")\n            plt.show()\n    return spec_models, spec_dls, spec_learn, spec_items\n\n\nspec_models, spec_dls, spec_learn, spec_items = fine_tune(var_learning, learn_A1v)\n\n\n\n\n TA | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-73.308072\n-59.729135\n0.155103\n23:04\n\n\n1\n1\n-87.049737\n-73.262853\n0.139768\n20:37\n\n\n2\n2\n-92.947376\n-82.557740\n0.131374\n21:16\n\n\n\n\n\n\n\n\n\n\n SW_IN | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n53.445060\n43.464660\n0.286705\n23:25\n\n\n1\n1\n49.458273\n42.814378\n0.285042\n23:47\n\n\n2\n2\n48.186476\n43.087234\n0.283170\n22:43\n\n\n\n\n\n\n\n\n\n\n LW_IN | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n104.665918\n106.041969\n0.414556\n22:34\n\n\n1\n1\n101.284686\n107.526856\n0.419593\n23:28\n\n\n2\n2\n99.767878\n108.885340\n0.420613\n23:43\n\n\n\n\n\n\n\n\n\n\n VPD | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n52.165032\n50.852617\n0.295505\n23:31\n\n\n1\n1\n45.131744\n36.635116\n0.272213\n22:41\n\n\n2\n2\n41.990330\n32.914802\n0.264974\n23:06\n\n\n\n\n\n\n\n\n\n\n WS | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n175.891940\n246.176982\n0.761467\n22:58\n\n\n1\n1\n165.224262\n235.823659\n0.719208\n23:03\n\n\n2\n2\n159.586716\n246.720599\n0.727202\n21:00\n\n\n\n\n\n\n\n\n\n\n PA | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-99.014311\n-97.738537\n0.127062\n21:53\n\n\n1\n1\n-123.031773\n-104.182228\n0.120468\n22:45\n\n\n2\n2\n-130.901483\n-133.160406\n0.104076\n24:11\n\n\n\n\n\n\n\n\n\n\n SWC | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n172.303569\n200.127139\n0.599423\n23:57\n\n\n1\n1\n132.754943\n76.512459\n0.305065\n22:01\n\n\n2\n2\n93.005439\n57.391193\n0.270830\n20:58\n\n\n\n\n\n\n\n\n\n\ntrain 1\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n172.303569\n200.127139\n0.599423\n23:57\n\n\n1\n1\n132.754943\n76.512459\n0.305065\n22:01\n\n\n2\n2\n93.005439\n57.391193\n0.270830\n20:58\n\n\n\n\n\n\n\n\n\n\n TS | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n68.246805\n54.157492\n0.268725\n23:05\n\n\n1\n1\n57.107431\n39.662085\n0.247396\n21:34\n\n\n2\n2\n57.394308\n53.605346\n0.260903\n20:52\n\n\n\n\n\n\n\n\n\n\n\nvar_learning2 = {\n    'TA': [{'lr': 1e-3, 'n': 3}],  \n    'VPD': [{'lr': 1e-3, 'n': 2}],  \n    'PA': [{'lr': 1e-3, 'n': 2}],  \n    'SWC' : [{'lr': 1e-3, 'n': 3}, {'lr': 1e-5, 'n': 1}],\n    'TS' : [{'lr': 1e-3, 'n': 2}],\n}\n\n\ndef fine_tune2(var_learning, spec_dls, spec_learn, spec_items):\n    spec_learn = spec_learn.copy()\n    for var in tqdm(var_learning.keys()):\n        display(HTML(f\"&lt;h4&gt; {var} | Gap len 6-336  finetune 2 &lt;/h4&gt;\"))\n        for i, train in enumerate(var_learning[var]):\n            lr, n = train['lr'], train['n']\n            v = train.get('v', 2)\n            display(HTML(f\"train {i}\"))\n            spec_learn[var], _ = train_or_load(spec_learn[var].model, spec_dls[var], n, lr, path=base / f\"{var}_specialized_gap_6-336_v{v}_{i}\")\n            plt.show()\n    return spec_dls, spec_learn, spec_items\n\n\nspec_dls2, spec_learn2, spec_items2 = fine_tune2(var_learning2, spec_dls, spec_learn, spec_items)\n\n\n\n\n TA | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-97.718002\n-90.171141\n0.125761\n21:44\n\n\n1\n1\n-100.493585\n-88.338616\n0.126856\n20:23\n\n\n2\n2\n-103.769092\n-73.432097\n0.135440\n20:41\n\n\n\n\n\n\n\n\n\n\n SW_IN | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n45.373127\n42.291928\n0.282366\n20:34\n\n\n1\n1\n46.487908\n41.780965\n0.283791\n20:26\n\n\n2\n2\n46.903155\n45.729009\n0.290985\n20:43\n\n\n3\n3\n46.917554\n39.832071\n0.278899\n20:29\n\n\n\n\n\n\n\n\n\n\n LW_IN | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n98.188426\n107.646729\n0.414824\n20:22\n\n\n1\n1\n95.761911\n108.694974\n0.416340\n20:30\n\n\n2\n2\n97.285924\n106.717304\n0.415862\n20:30\n\n\n\n\n\n\n\n\n\n\n VPD | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n42.370522\n31.185345\n0.254092\n20:27\n\n\n1\n1\n37.933048\n29.823322\n0.255506\n20:25\n\n\n2\n2\n35.904875\n25.101849\n0.245977\n20:28\n\n\n\n\n\n\n\n\n\n\n WS | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n158.971839\n247.131486\n0.730278\n20:37\n\n\n1\n1\n158.584698\n253.701959\n0.736434\n20:34\n\n\n2\n2\n156.970299\n263.529629\n0.749744\n20:22\n\n\n\n\n\n\n\n\n\n\n PA | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-131.680055\n-112.508634\n0.115626\n23:56\n\n\n1\n1\n-145.849450\n-95.348580\n0.114896\n24:38\n\n\n2\n2\n-145.281960\n-107.879192\n0.116886\n24:47\n\n\n\n\n\n\n\n\n\n\n SWC | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n35.588624\n-20.680066\n0.174256\n24:56\n\n\n1\n1\n10.683595\n-23.630941\n0.167582\n24:32\n\n\n2\n2\n-34.885516\n-76.834793\n0.124991\n25:11\n\n\n3\n3\n-47.704883\n0.936394\n0.166095\n24:47\n\n\n4\n4\n-61.871826\n-75.499625\n0.120001\n24:45\n\n\n\n\n\n\n\n\n\n\ntrain 1\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-82.762576\n-101.858651\n0.103941\n24:41\n\n\n\n\n\n\n\n\n\n\n TS | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n40.478151\n29.605951\n0.230160\n25:47\n\n\n1\n1\n36.143091\n21.822619\n0.211396\n28:18\n\n\n2\n2\n30.493916\n-0.701642\n0.181059\n24:59\n\n\n3\n3\n26.646244\n17.138843\n0.201734\n27:48\n\n\n4\n4\n19.231903\n10.231487\n0.202251\n29:09\n\n\n\n\n\n\n\n\n\n\n\nvar_learning3 = {\n    'TA': [{'lr': 1e-5, 'n': 1, 'v': 3}],  \n    'PA': [{'lr': 1e-5, 'n': 1, 'v': 3}],  \n}\n\n\nspec_dls3, spec_learn3, spec_items3 = fine_tune2(var_learning3, spec_dls, spec_learn, spec_items)\n\n\n\n\n TA | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-98.172894\n-86.769272\n0.131181\n28:21\n\n\n\n\n\n\n\n\n PA | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-155.452174\n-135.116148\n0.105047\n27:51"
  },
  {
    "objectID": "results/model_training.html#oooollllldddd",
    "href": "results/model_training.html#oooollllldddd",
    "title": "Training Kalman Filter for Results - 21 Feb",
    "section": "OOOOLLLLLDDDD",
    "text": "OOOOLLLLLDDDD\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(hai.columns)):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 &lt;/h4&gt;\"))\n    spec_dls[var] = imp_dataloader(hai, hai_era, var_sel = var, block_len=100+336, gap_len=gen_gap_len(12, 336), bs=20, control_lags=[1], n_rep=3, shifts=gen_shifts(50)).cpu()\n    spec_models[var] = learn_A1v.model.copy()\n    if show_metrics:\n        display(HTML(\"Metrics generic model\"))\n        display(metric_valid(learn_A1v, dls=spec_dls[var].valid))\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 3, 1e-3, base / f\"{var}_specialized_gap_12-336_v1\")\n    plt.show()\n\n\n\n\n TA | Gap len 12-336 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-31.277383\n-59.727349\n0.155292\n23:16\n\n\n1\n1\n-57.410593\n-83.423924\n0.135217\n24:38\n\n\n2\n2\n-69.265146\n-81.197435\n0.137000\n21:50\n\n\n\n\n\n\n\n\n\n\n SW_IN | Gap len 12-336 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n72.838760\n63.622077\n0.317182\n22:33\n\n\n1\n1\n60.514370\n53.416147\n0.296670\n22:32\n\n\n2\n2\n55.572469\n48.728888\n0.287208\n25:43\n\n\n\n\n\n\n\n\n\n\n LW_IN | Gap len 12-336 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n110.011858\n112.541507\n0.435194\n24:28\n\n\n1\n1\n105.027452\n106.923631\n0.417517\n24:23\n\n\n2\n2\n102.755247\n106.489712\n0.415183\n24:53\n\n\n\n\n\n\n\n\n\n\n VPD | Gap len 12-336 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n67.771759\n71.121287\n0.306581\n25:12\n\n\n1\n1\n58.432390\n49.351550\n0.275882\n23:40\n\n\n2\n2\n49.884968\n41.432351\n0.264706\n25:51\n\n\n\n\n\n\n\n\n\n\n WS | Gap len 12-336 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n169.196013\n241.890813\n0.734134\n24:18\n\n\n1\n1\n165.496950\n235.745148\n0.711773\n23:20\n\n\n2\n2\n161.570979\n223.982837\n0.696192\n22:10\n\n\n\n\n\n\n\n\n\n\n PA | Gap len 12-336 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-73.724512\n-103.872036\n0.124549\n27:03\n\n\n1\n1\n-111.420458\n-130.842109\n0.104550\n30:01\n\n\n2\n2\n-133.129972\n-140.989809\n0.100966\n24:07\n\n\n\n\n\n\n\n\n\n\n P | Gap len 12-336 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n259.814830\n216.069186\n0.602446\n24:34\n\n\n1\n1\n248.654373\n209.606269\n0.607980\n23:06\n\n\n2\n2\n242.001831\n206.787110\n0.605378\n25:11\n\n\n\n\n\n\n\n\n\n\n SWC | Gap len 12-336 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n207.952983\n194.924715\n0.646890\n24:35\n\n\n1\n1\n192.451676\n184.576861\n0.595377\n24:16\n\n\n2\n2\n169.086573\n117.395571\n0.388920\n22:44\n\n\n\n\n\n\n\n\n\n\n TS | Gap len 12-336 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n63.610211\n40.488381\n0.255747\n21:39\n\n\n1\n1\n57.377678\n39.280813\n0.247606\n21:41\n\n\n2\n2\n50.239735\n23.977059\n0.231418\n21:34\n\n\n\n\n\n\n\n\n\n\n\nAdditional training\n\nspec_learn2 = {}\nfor var in tqdm(['TA', 'SW_IN', 'WS', 'PA', 'VPD', 'TS', 'SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 &lt;/h4&gt; | Training 2\"))\n    spec_learn2[var], _ = train_or_load(spec_learn[var].model.copy(), spec_dls[var], 2, 1e-3, base / f\"{var}_specialized_gap_12-336_v2.pickle\")\n    plt.show()\n    \n\n\n\n\n TA | Gap len 12-336  | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-82.582890\n-78.921598\n0.137891\n21:39\n\n\n1\n1\n-86.119136\n-89.580120\n0.128288\n21:18\n\n\n\n\n\n\n\n\n\n\n SW_IN | Gap len 12-336  | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n52.668345\n46.674095\n0.288313\n21:53\n\n\n1\n1\n51.089022\n44.378815\n0.283374\n22:28\n\n\n\n\n\n\n\n\n\n\n WS | Gap len 12-336  | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n164.019476\n232.919238\n0.710805\n22:37\n\n\n1\n1\n164.185435\n225.276840\n0.707446\n22:50\n\n\n\n\n\n\n\n\n\n\n PA | Gap len 12-336  | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-147.345027\n-136.658365\n0.103830\n22:42\n\n\n1\n1\n-157.427610\n-119.642745\n0.108054\n22:36\n\n\n\n\n\n\n\n\n\n\n VPD | Gap len 12-336  | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n46.489590\n30.196986\n0.249948\n22:27\n\n\n1\n1\n45.226931\n46.153450\n0.273411\n21:01\n\n\n\n\n\n\n\n\n\n\n TS | Gap len 12-336  | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n39.863970\n17.692497\n0.221164\n20:40\n\n\n1\n1\n31.964869\n3.826767\n0.209781\n20:34\n\n\n\n\n\n\n\n\n\n\n SWC | Gap len 12-336  | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n114.122129\n96.942318\n0.352660\n20:44\n\n\n1\n1\n67.486398\n34.946166\n0.239454\n20:39\n\n\n\n\n\n\n\n\n\n\nPA and VPD are overfitting so repeat training 2 with only one batch\n\nspec_learn3 = {}\nfor var in tqdm(['PA', 'VPD']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn[var].model.copy(), spec_dls[var], 1, 1e-3, base / f\"{var}_specialized_gap_12-336_v3.pickle\")\n    plt.show()\n    \n\n\n\n\n PA | Gap len 12-336 | Training 3 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-135.310833\n-106.027287\n0.120461\n24:33\n\n\n\n\n\n\n\n\n\n\n VPD | Gap len 12-336 | Training 3 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n41.435748\n32.315761\n0.254877\n24:13\n\n\n\n\n\n\n\n\n\n\n\nspec_learn3 = {}\nfor var in tqdm(['TS']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n TS | Gap len 12-336 | Training 1+2+3 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n16.858002\n-7.042792\n0.189336\n23:31\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n SWC | Gap len 12-336 | Training 1+2+3 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n3.990423\n-0.629562\n0.185285\n24:57\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 2, 1e-3, base / f\"{var}_specialized_gap_12-336_v3_2\")\n    plt.show()\n    \n\n\n\n\n SWC | Gap len 12-336 | Training 1+2+3 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-1.621724\n-38.955631\n0.146205\n23:45\n\n\n1\n1\n-35.639923\n-57.006918\n0.130909\n22:09\n\n\n\n\n\n\n\n\n\n\nthe training loss is getting worse … so tring with smaller learning rate\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3_3\")\n    plt.show()\n    \n\n\n\n\n SWC | Gap len 12-336 | Training 1+2+3 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-75.585425\n-79.693433\n0.113264\n25:09\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3+4 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3_4\")\n    plt.show()\n    \n\n\n\n\n SWC | Gap len 12-336 | Training 1+2+3+4 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-79.65696\n-81.566511\n0.113204\n25:25\n\n\n\n\n\n\n\n\n\n\n\nspec_learn4 = {}\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3+4 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v4\")\n    plt.show()\n    \n\n\n\n\n SWC | Gap len 12-336 | Training 1+2+3+4 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-0.957575\n-7.084297\n0.173571\n24:35\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['WS']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n WS | Gap len 12-336 | Training 1+2+3 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n158.930217\n224.42966\n0.701698\n24:04\n\n\n\n\n\n\n\n\n\n\nthis is overfitting PA\n\nfor var in tqdm(['PA']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1++3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn[var].model.copy(), spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n PA | Gap len 12-336 | Training 1++3 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-168.729458\n-122.183789\n0.10763\n23:40\n\n\n\n\n\n\n\n\n\n\n\nspec_learn4 = {}\nfor var in tqdm(['PA']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+3+4 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_12-336_v4\")\n    plt.show()\n    \n\n\n\n\n PA | Gap len 12-336 | Training 1+3+4 \n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-169.911168\n-117.59576\n0.109397\n21:33"
  },
  {
    "objectID": "results/model_training.html#result-visualization",
    "href": "results/model_training.html#result-visualization",
    "title": "Training Kalman Filter for Results - 21 Feb",
    "section": "Result visualization",
    "text": "Result visualization\nthe goal of this section is to figure why the the generic model for all gaps works better then the generic model with gap in only one variable\n\nshow_results(learn_A1v, items = spec_items['TA'], hide_no_gap=True)\n\nNameError: name 'spec_items' is not defined\n\n\n\nshow_results(spec_learn2['TA'], items = spec_items['TA'], hide_no_gap=True)\n\nNameError: name 'spec_learn2' is not defined\n\n\n\nshow_results(learn_Aa, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls)\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\nno control to correlation is bad\n\nwith with_settings(learn_Aa.model, use_control=False):\n    display(show_results(learn_Aa, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\nthere is correlation here but no control,\nthe error is huggher and the uncertainty is higher\n\ndisplay(show_results(learn_A1v_nc, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_A1v_nc, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_A1v, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\nthis has the control\n\nitems_Aa_TA = [MeteoImpItem(i.i, i.shift, 'TA', i.gap_len) for i in items_Aa]\n\n\ndisplay(show_results(learn_Aa, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_Aa, items = items_Aa_TA, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA'], gap_len=30)]\n\n\n\n\n\n\n\nso the problem is that this is worse than the one above, even though it should not be the case\n\ndisplay(show_results(learn_A1v, items = items_Aa_TA, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA'], gap_len=30)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_Aa, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\neven for *** longer gaps is the same issue where the generic model trained with gaps in all variables is worse than the generic model with gaps in none\n\ndisplay(show_results(learn_A1v, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\nthis is just very bad!\n\ndisplay(show_results(learn_A1v, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\n\ncloser look at parameters\n\nimport seaborn as sns\n\n\nGeneric 1 gap\n\ndef plot_model_params(model):\n    sns.set(rc={\"figure.figsize\":(15, 10)})\n\n    sns.heatmap(array2df(model.H.squeeze(0)), annot=True,     vmin=-1, vmax=1.5, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"H\")\n    plt.show()\n    \n    sns.set(rc={\"figure.figsize\":(15, 15)})\n\n    sns.heatmap(array2df(model.A.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"A\")\n    plt.show()\n    \n    \n    sns.set(rc={\"figure.figsize\":(15, 15)})\n\n    sns.heatmap(array2df(model.B.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"B\")\n    plt.show()\n    \n    sns.heatmap(array2df(model.P0.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"P0\")\n    plt.show()\n    \n    sns.heatmap(array2df(model.m0.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"m0\")\n    plt.show()\n    \n#     sns.set(rc={\"figure.figsize\":(15, 15)})\n\n#     sns.heatmap(array2df(model.Q.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n#     cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n#     plt.title(\"Q\")\n#     plt.show()\n    \n\n\nplot_model_params(learn_A1v.model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(learn_Aa.model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(learn_A1v30.model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(spec_learn2['TA'].model)\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(learn_Aa.model)\n\n\n\n\n\n\n\n\n\n\n\nsns.heatmap(array2df((learn_Aa.model.H - learn_A1v30.model.H).squeeze(0)), annot=True,  center=0,\ncmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \nplt.title(\"diff H\")\n\nText(0.5, 1.0, 'diff H')\n\n\n\n\n\n\nsns.heatmap(array2df((learn_Aa.model.A - learn_A1v30.model.A).squeeze(0)), annot=True,  center=0,\ncmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \nplt.title(\"diff A\")\n\nText(0.5, 1.0, 'diff A')\n\n\n\n\n\n\nsns.heatmap(array2df((learn_Aa.model.B - learn_A1v30.model.B).squeeze(0)), annot=True,  center=0,\ncmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \nplt.title(\"diff B\")\n\nText(0.5, 1.0, 'diff B')\n\n\n\n\n\n\nsns.set(rc={\"figure.figsize\":(15, 10)})\n\nsns.heatmap(array2df(learn_A1v.model.H.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\nsns.set(rc={\"figure.figsize\":(15, 15)})\n\nsns.heatmap(array2df(learn_A1v.model.A.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nthis is okay but not much better\n\ndisplay(show_results(spec_learn2['TA'], items = items_Aa_TA, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA'], gap_len=30)]\n\n\n\n\n\n\n\n\n\n\nFinetune gap length\n\ngap_models = {}\ngap_dls = {}\ngap_learn = {}\ngap_items = {}\nfor gap_len in tqdm([6,24,48,7*48]):\n    print(f\"Gap len: {gap_len}\")\n    gap_dls[gap_len] = imp_dataloader(hai, hai_era, var_sel = gen_var_sel(list(hai.columns), n_var=1), block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=1, shifts=gen_shifts(50)).cpu()\n    gap_models[gap_len] = learn_A1v.model.copy()\n    if display_metric: display(metric_valid(learn_A1v, dls=gap_dls[gap_len].valid))\n    gap_learn[gap_len], gap_items[gap_len] = train_or_load(gap_models[gap_len], gap_dls[gap_len], 3, 2e-5, base / f\"gap_1_any_var_gap_len_{gap_len}_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nGap len: 6\n\n\nNameError: name 'display_metric' is not defined\n\n\n\n\nFine tune TA for diff gap lens\nas an experiment TA for a gap of 24 fine tuned\n\ndls_TA24 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'TA',\n    block_len=100+24,\n    gap_len=24,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=1).cpu()\n\n\nmodel_TA24 = learn_A1v.model.copy()\n\n\nmetric_valid(learn_A1v, dls= dls_TA24.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.338763\n0.184055\n\n\nstd\n2.551372\n0.034550\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 1e-4, base / \"TA_gap_24_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-2.51402\n-3.892453\n0.174431\n02:41\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 1e-4, base / \"TA_gap_24_v1_2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-4.713286\n-5.854372\n0.161322\n02:37\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 1e-4, base / \"TA_gap_24_v1_3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-6.744339\n-7.335044\n0.152876\n02:39\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 5e-5, base / \"TA_gap_24_v1_4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.932901\n-8.451432\n0.144813\n02:42\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 3, 3e-5, base / \"TA_gap_24_v1_4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.932901\n-8.451432\n0.144813\n02:42\n\n\n\n\n\n\n\n\n\n\n\nmetric_valid(learn_TA24, dls= dls_TA24.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-9.049459\n0.140859\n\n\nstd\n2.104846\n0.023429\n\n\n\n\n\n\n\n\ndls_TA48 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'TA',\n    block_len=100+48,\n    gap_len=48,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=1).cpu()\n\n\nmodel_TA48 = learn_TA24.model.copy()\n\n\nmetric_valid(learn_TA24, dls= dls_TA48.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-11.247340\n0.170793\n\n\nstd\n8.504741\n0.035950\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 2e-4, base / \"TA_gap_48_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-11.938200\n-13.136611\n0.119232\n02:38\n\n\n1\n1\n-13.676607\n-14.978353\n0.111058\n02:38\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 1e-4, base / \"TA_gap_48_v1_2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-15.563658\n-15.840925\n0.107003\n02:44\n\n\n1\n1\n-15.958545\n-16.626610\n0.103149\n02:54\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 1e-4, base / \"TA_gap_48_v1_3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-16.665217\n-17.394887\n0.099992\n02:40\n\n\n1\n1\n-17.165897\n-17.974066\n0.097933\n02:43\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 1e-4, base / \"TA_gap_48_v1_4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-18.111798\n-18.427618\n0.096457\n02:49\n\n\n1\n1\n-18.591682\n-19.413278\n0.092474\n02:52\n\n\n\n\n\n\n\n\n\n\n\nmodel_TA24_v2 = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca = hai, \n    pred_only_gap=True)\n\n\nlearn_TA24_v2, items_TA24_v2 = train_or_load(model_TA24, dls_TA24, 3, 1e-3, base / \"TA_gap_24_v2_1\")\n\n\nTA_models = {}\nTA_dls = {}\nTA_learn = {}\nTA_items = {}\nfor gap_len in tqdm([6,24,48,7*48]):\n    display(HTML(f\"&lt;h4&gt; TA | Gap len: {gap_len} &lt;/h4&gt;\"))\n    TA_dls[gap_len] = imp_dataloader(hai, hai_era, var_sel = 'TA', block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=2, shifts=gen_shifts(50)).cpu()\n    TA_models[gap_len] = learn_A1v.model.copy()\n    display(metric_valid(learn_A1v, dls=TA_dls[gap_len].valid))\n    TA_learn[gap_len], TA_items[gap_len] = train_or_load(TA_models[gap_len], TA_dls[gap_len], 4, 1e-4, base / f\"TA_gap_len_{gap_len}_v1.pickle\")\n    plt.show()\n    \n\n\n\n\n TA | Gap len: 6 \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.093156\n0.102643\n\n\nstd\n0.403933\n0.025537\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-3.623101\n-4.139968\n0.074969\n03:57\n\n\n1\n-5.193550\n-5.537112\n0.060948\n04:14\n\n\n2\n-6.309246\n-6.533323\n0.052087\n04:33\n\n\n3\n-7.058736\n-7.211129\n0.046813\n04:22\n\n\n\n\n\n\n\n\n TA | Gap len: 24 \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-0.660455\n0.200111\n\n\nstd\n2.088128\n0.031036\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-4.815664\n-5.862635\n0.162549\n05:23\n\n\n1\n-8.257145\n-9.265403\n0.141516\n05:27\n\n\n2\n-10.721531\n-11.764655\n0.126987\n05:17\n\n\n3\n-13.121913\n-13.537241\n0.119559\n05:31\n\n\n\n\n\n\n\n\n TA | Gap len: 48 \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n2.231489\n0.227216\n\n\nstd\n4.624743\n0.032391\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-5.077721\n-6.730453\n0.189337\n06:57\n\n\n1\n-10.696784\n-12.647214\n0.166578\n06:37\n\n\n2\n-15.724886\n-16.361526\n0.153221\n06:56\n\n\n3\n-18.714646\n-19.920346\n0.142614\n06:41\n\n\n\n\n\n\n\n\n TA | Gap len: 336 \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n30.665105\n0.248255\n\n\nstd\n18.911530\n0.021534\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n18.701340\n-1.243045\n0.226205\n17:41\n\n\n1\n-0.477330\n-23.318493\n0.212453\n17:30\n\n\n2\n-19.797870\n-42.075069\n0.200830\n47:23\n\n\n3\n-36.470670\n-55.291288\n0.192844\n18:24\n\n\n\n\n\n\n\n\n\n\nTA multiple gap len\n\ndls_TA = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'TA',\n    block_len=70+336,\n    gap_len=gen_gap_len(12,336),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=3).cpu()\n\n\nmodel_TA = learn_A1v.model.copy()\n\n\nmetric_valid(learn_A1v, dls= dls_TA.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n18.120834\n0.240742\n\n\nstd\n19.321141\n0.032218\n\n\n\n\n\n\n\n\nlearn_TA, items_TA = train_or_load(model_TA, dls_TA, 2, 5e-4, base / \"TA_gap_12-336_v1,\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-26.097331\n-37.440739\n0.175528\n25:36\n\n\n1\n1\n-50.443927\n-66.472712\n0.149401\n26:02\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA, items_TA = train_or_load(learn_TA.model, dls_TA, 2, 5e-4, base / \"TA_gap_12-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-70.902275\n-72.341130\n0.137253\n24:20\n\n\n1\n-78.379055\n-76.962878\n0.132967\n23:55\n\n\n\n\n\n\n\n\n\nlearn_TA, items_TA = train_or_load(learn_TA.model, dls_TA, 2, 5e-4, base / \"TA_gap_12-336_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-85.106441\n-82.588917\n0.128094\n24:21\n\n\n1\n-86.267741\n-83.305525\n0.129777\n23:29\n\n\n\n\n\n\n\n\n\n\nDetailed metrics\n\ndef metrics_valid_gap_lens(learn, var, gaps = [6,12,24,48,7*48]):\n    for gap_len in tqdm(gaps):\n        dls = imp_dataloader(hai, hai_era, var_sel = var, block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=1, shifts=gen_shifts(50)).cpu()\n        display(HTML(f\"&lt;strong&gt; Metrics | gap len: {gap_len} | Var: {var} &lt;/strong&gt;\"))\n        display(metric_valid(learn, dls=dls.valid))\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\nmetrics_valid_gap_lens(learn_A1v, 'TA')\n\n\n\n\n Metrics | gap len: 6 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.649933\n0.095968\n\n\nstd\n0.338268\n0.018444\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.243049\n0.134065\n\n\nstd\n0.710149\n0.021311\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.125709\n0.188222\n\n\nstd\n2.047966\n0.027886\n\n\n\n\n\n\n\n Metrics | gap len: 48 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n0.638268\n0.217725\n\n\nstd\n6.192707\n0.031585\n\n\n\n\n\n\n\n Metrics | gap len: 336 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n21.469734\n0.242877\n\n\nstd\n9.829365\n0.010609\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(learn_A1v_nc, 'TA')\n\n\n\n\n Metrics | gap len: 6 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.430541\n0.055689\n\n\nstd\n0.097171\n0.009796\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.790838\n0.113533\n\n\nstd\n0.833381\n0.030206\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-0.992221\n0.187918\n\n\nstd\n4.834171\n0.068321\n\n\n\n\n\n\n\n Metrics | gap len: 48 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n5.597832\n0.235436\n\n\nstd\n12.132120\n0.075510\n\n\n\n\n\n\n\n Metrics | gap len: 336 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n96.691128\n0.293088\n\n\nstd\n98.837141\n0.085536\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(spec_learn['TA'], 'TA')\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n Metrics | gap len: 6 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-6.332682\n0.049233\n\n\nstd\n0.226535\n0.007010\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-10.478126\n0.072882\n\n\nstd\n0.762210\n0.010581\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-15.977039\n0.103628\n\n\nstd\n4.186457\n0.024085\n\n\n\n\n\n\n\n Metrics | gap len: 48 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-25.748188\n0.125292\n\n\nstd\n11.229092\n0.031731\n\n\n\n\n\n\n\n Metrics | gap len: 336 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-136.930894\n0.152361\n\n\nstd\n61.110281\n0.026692\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(spec_learn[\"SWC\"], 'SWC', gaps=[6,12,24,30])\n\n\n\n\n Metrics | gap len: 6 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-4.637960\n0.051318\n\n\nstd\n0.299619\n0.012608\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-6.503471\n0.078820\n\n\nstd\n0.751687\n0.020815\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-6.256985\n0.114699\n\n\nstd\n1.533914\n0.030592\n\n\n\n\n\n\n\n Metrics | gap len: 30 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.957636\n0.144434\n\n\nstd\n2.228843\n0.032844\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(spec_learn3[\"SWC\"], 'SWC', gaps=[6,12,24,30])\n\n\n\n\n Metrics | gap len: 6 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-7.409364\n0.040791\n\n\nstd\n0.602997\n0.008078\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-14.166100\n0.049997\n\n\nstd\n1.298232\n0.009736\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-26.025482\n0.058914\n\n\nstd\n2.035570\n0.009232\n\n\n\n\n\n\n\n Metrics | gap len: 30 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-31.526816\n0.064446\n\n\nstd\n2.601954\n0.008842\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(learn_Aa, 'SWC', gaps=[6,12,24,30])\n\n\n\n\n Metrics | gap len: 6 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-10.621700\n0.020779\n\n\nstd\n1.637564\n0.009605\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-21.012826\n0.024119\n\n\nstd\n1.456359\n0.007166\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-36.451721\n0.034461\n\n\nstd\n4.163576\n0.011231\n\n\n\n\n\n\n\n Metrics | gap len: 30 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-43.786023\n0.038519\n\n\nstd\n5.267340\n0.010467\n\n\n\n\n\n\n\nthis is pretty weird the model with gaps in all variables is performing better that the one with only partial gaps ….\nlet’s so some finetuning\n\n\nSWC | Gap len 30\n\ndls_SWC_30 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'SWC',\n    block_len=120,\n    gap_len=30,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=1\n).cpu()\n\n\nmodel_SWC_30 = spec_learn['SWC'].copy()\n\n\nif show_metrics: metric_valid(model_SWC_30, dls=model_SWC_30.valid)\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 3e-4, base / \"SWC_gap_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-10.998339\n-15.672505\n0.099315\n02:51\n\n\n1\n-17.301334\n-22.166437\n0.085715\n02:51\n\n\n\n\n\n\n\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 1e-3, base / \"SWC_gap_30_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-27.642977\n-34.401863\n0.056773\n02:56\n\n\n1\n-33.920003\n-38.631508\n0.050980\n02:57\n\n\n\n\n\n\n\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 1e-3, base / \"SWC_gap_30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-38.766184\n-42.612718\n0.041158\n02:55\n\n\n1\n-39.593180\n-42.984206\n0.041554\n02:52\n\n\n\n\n\n\n\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 1e-3, base / \"SWC_gap_30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-38.766184\n-42.612718\n0.041158\n02:55\n\n\n1\n-39.593180\n-42.984206\n0.041554\n02:52\n\n\n\n\n\n\n\n\n\nshow_results(learn_SWC_30, hide_no_gap=True, items=items_SWC_30)\n\n[MeteoImpItem(i=1674, shift=30, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1693, shift=46, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1570, shift=-83, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1682, shift=54, var_sel=['SWC'], gap_len=30)]\n\n\n\n\n\n\n\n\nshow_results(learn_Aa, hide_no_gap=True, items=items_SWC_30)\n\n[MeteoImpItem(i=1674, shift=30, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1693, shift=46, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1570, shift=-83, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1682, shift=54, var_sel=['SWC'], gap_len=30)]\n\n\n\n\n\n\n\n\nshow_results(learn_A1v, hide_no_gap=True, items=items_SWC_30, dls=dls_SWC_30)\n\n[MeteoImpItem(i=1674, shift=30, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1693, shift=46, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1570, shift=-83, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1682, shift=54, var_sel=['SWC'], gap_len=30)]\n\n\n\n\n\n\n\n\nshow_results(spec_learn['SWC'], hide_no_gap=True, items=spec_items['SWC'], dls=spec_dls['SWC'])\n\n[MeteoImpItem(i=510, shift=10, var_sel=['SWC'], gap_len=243), MeteoImpItem(i=504, shift=0, var_sel=['SWC'], gap_len=109), MeteoImpItem(i=494, shift=-94, var_sel=['SWC'], gap_len=116), MeteoImpItem(i=481, shift=-17, var_sel=['SWC'], gap_len=148)]\n\n\n\n\n\n\n\n\nshow_results(learn_SWC_30)\n\n[MeteoImpItem(i=1892, shift=-50, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1667, shift=68, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1761, shift=41, var_sel=['SWC'], gap_len=30)]"
  },
  {
    "objectID": "results/result_plots.html",
    "href": "results/result_plots.html",
    "title": "Plotting for results",
    "section": "",
    "text": "This notebook produces all results plots. It generates some gap in the data, fill with a method (filter, MDS …), compute metrics and then makes all relevant plots\n%load_ext autoreload\n%autoreload 2\nimport altair as alt\nfrom meteo_imp.kalman.results import *\nfrom meteo_imp.data import *\nfrom meteo_imp.utils import *\nimport pandas as pd\nimport numpy as np\nfrom pyprojroot import here\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG, Image\n\nfrom meteo_imp.kalman.results import _plot_timeseries, _get_labels\nfrom functools import partial\nfrom contextlib import redirect_stderr\nimport io\n\nimport polars as pl\nfrom fastai.vision.data import get_grid\nimport cairosvg\nimport vl_convert as vlc\nfrom pyprojroot import here\nbase_path_img = here(\"manuscript/Master Thesis - Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications - Simone Massaro/images/\")\nbase_path_tbl = here(\"manuscript/Master Thesis - Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications - Simone Massaro/tables/\")\n\nbase_path_img.mkdir(exist_ok=True), base_path_tbl.mkdir(exist_ok=True)\n\ndef save_show_plot(plot, path):\n    plt_json = plot.to_json()\n    svg_data = vlc.vegalite_to_svg(vl_spec=plt_json)\n    with open(base_path_img / (path + \".svg\"), 'w') as f:\n        f.write(svg_data)\n    \n    cairosvg.svg2pdf(file_obj=open(base_path_img / (path + \".svg\")), write_to=str(base_path_img / (path + \".pdf\")))\n    # with redirect_stderr(io.StringIO()):\n        # plot.save(base_path_img / (path + \".pdf\"))\n    # render to image for displaying in notebook\n    png_data = vlc.vegalite_to_png(vl_spec=plot.to_json(), scale=1)\n    return Image(png_data)\nreset_seed()\nn_rep = 500\nhai = pd.read_parquet(hai_big_path).reindex(columns=var_type.categories)\nhai_era = pd.read_parquet(hai_era_big_path)\nalt.data_transformers.disable_max_rows() # it is safe to do so as the plots are rendered using vl-convert and then showed as images\n\nDataTransformerRegistry.enable('default')"
  },
  {
    "objectID": "results/result_plots.html#correlation",
    "href": "results/result_plots.html#correlation",
    "title": "Plotting for results",
    "section": "Correlation",
    "text": "Correlation\n\nimport matplotlib.pyplot as plt\n\n\nimport statsmodels.api as sm\n\n\ndef auto_corr_df(data, nlags=96):\n    autocorr = {}\n    for col in data.columns:\n        autocorr[col] = sm.tsa.acf(data[col], nlags=nlags)\n    return pd.DataFrame(autocorr)\n\n\nauto_corr = auto_corr_df(hai).reset_index(names=\"gap_len\").melt(id_vars=\"gap_len\")\nauto_corr.gap_len = auto_corr.gap_len / 2\n\n\nauto_corr\n\n\n\n\n\n\n\n\ngap_len\nvariable\nvalue\n\n\n\n\n0\n0.0\nTA\n1.000000\n\n\n1\n0.5\nTA\n0.998595\n\n\n2\n1.0\nTA\n0.995814\n\n\n3\n1.5\nTA\n0.992141\n\n\n4\n2.0\nTA\n0.987630\n\n\n...\n...\n...\n...\n\n\n868\n46.0\nTS\n0.959680\n\n\n869\n46.5\nTS\n0.961116\n\n\n870\n47.0\nTS\n0.962085\n\n\n871\n47.5\nTS\n0.962551\n\n\n872\n48.0\nTS\n0.962480\n\n\n\n\n873 rows × 3 columns\n\n\n\n\np = (alt.Chart(auto_corr).mark_line().encode(\n    x = alt.X('gap_len', title=\"Gap length [h]\", axis = alt.Axis(values= [12, 24, 36, 48])),\n    y = alt.Y(\"value\", title=\"correlation\"),\n    color=alt.Color(\"variable\", scale=meteo_scale, title=\"Variable\"),\n    facet =alt.Facet('variable', columns=3, sort = meteo_scale.domain, title=None,\n                     header = alt.Header(labelFontWeight=\"bold\", labelFontSize=14))\n)\n    .properties(height=120, width=250)\n    .resolve_scale(y='independent', x = 'independent')\n    .pipe(plot_formatter))\n\nsave_show_plot(p, \"temporal_autocorrelation\")\n\n\n\n\n\naxes = get_grid(1,1,1, figsize=(10,8))\nsns.set(font_scale=1.25)\nsns.heatmap(hai.corr(), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), ax=axes[0], square=True, cbar=True)\n# axes[0].set(xlabel=\"Variable\", ylabel=\"Variable\", title=\"Inter-variable Correlation\");\nplt.tight_layout()\nplt.savefig(base_path_img / \"correlation.pdf\")"
  },
  {
    "objectID": "results/result_plots.html#comparison-imputation-methods",
    "href": "results/result_plots.html#comparison-imputation-methods",
    "title": "Plotting for results",
    "section": "Comparison Imputation methods",
    "text": "Comparison Imputation methods\n\nbase_path = here(\"analysis/results/trained_models\")\n\n\ndef l_model(x, base_path=base_path): return torch.load(base_path / x)\n\n\nmodels_var = pd.DataFrame.from_records([\n    {'var': 'TA',    'model': l_model(\"TA_specialized_gap_6-336_v3_0.pickle\",base_path)},\n    {'var': 'SW_IN', 'model': l_model(\"SW_IN_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'LW_IN', 'model': l_model(\"LW_IN_specialized_gap_6-336_v1.pickle\",base_path)},\n    {'var': 'VPD',   'model': l_model(\"VPD_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'WS',    'model': l_model(\"WS_specialized_gap_6-336_v1.pickle\",base_path)},\n    {'var': 'PA',    'model': l_model(\"PA_specialized_gap_6-336_v3_0.pickle\",base_path)},\n    {'var': 'P',     'model': l_model(\"1_gap_varying_6-336_v3.pickle\",base_path)},\n    {'var': 'TS',    'model': l_model(\"TS_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'SWC',   'model': l_model(\"SWC_specialized_gap_6-336_v2_1.pickle\",base_path)},\n])\n\n\n@cache_disk(cache_dir / \"the_results\")\ndef get_the_results(n_rep=20):\n    reset_seed()\n    comp_Av = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 446, time_series=False)\n    results_Av = comp_Av.compare(gap_len = [12,24, 48, 336], var=list(hai.columns), n_rep=n_rep) \n    return results_Av\n\nresults_Av = get_the_results(n_rep)\n\n\nState of the art\nthe first plot is a time series using only state-of-the-art methods\n\nreset_seed()\ncomp_ts = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 48+100, time_series=True, rmse=False)\nresults_ts = comp_ts.compare(gap_len = [48], var=list(hai.columns), n_rep=1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nres_ts = results_ts.query(\"method != 'Kalman Filter'\")\nres_ts_plot = pd.concat([unnest_predictions(row, ctx_len=72) for _,row in res_ts.iterrows()])\n\n\nscale_sota = alt.Scale(domain=[\"ERA-I\", \"MDS\"], range=list(sns.color_palette('Dark2', 3).as_hex())[1:])\n\n\np = (facet_wrap(res_ts_plot, partial(_plot_timeseries, scale_color=scale_sota, err_band = False), col=\"var\",\n                y_labels = _get_labels(res_ts_plot, 'mean', None),\n               )\n            .pipe(plot_formatter)\n    )\nsave_show_plot(p, \"timeseries_sota\")\n\n\n\n\n\n\nPercentage improvement\n\nresults_Av.method.unique()\n\n['Kalman Filter', 'ERA-I', 'MDS']\nCategories (3, object): ['Kalman Filter' &lt; 'ERA-I' &lt; 'MDS']\n\n\n\nall_res = results_Av.query('var != \"P\"').groupby(['method']).agg({'rmse_stand': 'mean'}).T\n\n\nall_res\n\n\n\n\n\n\n\nmethod\nKalman Filter\nERA-I\nMDS\n\n\n\n\nrmse_stand\n0.204628\n0.307361\n0.482837\n\n\n\n\n\n\n\npercentage of improvement across all variables\n\n(all_res[\"ERA-I\"] - all_res[\"Kalman Filter\"]) / all_res[\"ERA-I\"] * 100 \n\nrmse_stand    33.42398\ndtype: float64\n\n\n\n(all_res[\"MDS\"] - all_res[\"Kalman Filter\"]) / all_res[\"MDS\"] * 100 \n\nrmse_stand    57.619542\ndtype: float64\n\n\n\nres_var = results_Av.groupby(['method', 'var']).agg({'rmse_stand': 'mean'}) \n\n\nres_var = res_var.reset_index().pivot(columns='method', values='rmse_stand', index='var')\n\n\npd.DataFrame({'ERA': (res_var[\"ERA-I\"] - res_var[\"Kalman Filter\"]) / res_var[\"ERA-I\"] * 100, 'MDS': (res_var[\"MDS\"] - res_var[\"Kalman Filter\"]) / res_var[\"MDS\"] * 100 })\n\n\n\n\n\n\n\n\nERA\nMDS\n\n\nvar\n\n\n\n\n\n\nTA\n54.540802\n77.713711\n\n\nSW_IN\n12.004508\n35.516142\n\n\nLW_IN\n5.166063\n52.289627\n\n\nVPD\n44.402821\n65.407769\n\n\nWS\n21.064305\n40.321732\n\n\nPA\n28.784191\n90.751559\n\n\nP\n-18.544370\n-22.084360\n\n\nSWC\nNaN\n41.543006\n\n\nTS\nNaN\n25.772326\n\n\n\n\n\n\n\n\nres_var2 = results_Av.groupby(['method', 'var', 'gap_len']).agg({'rmse_stand': 'mean'}) \n\n\nres_var2 = res_var2.reset_index().pivot(columns='method', values='rmse_stand', index=['var', 'gap_len'])\n\n\npd.DataFrame({'ERA': (res_var2[\"ERA-I\"] - res_var2[\"Kalman Filter\"]) / res_var2[\"ERA-I\"] * 100, 'MDS': (res_var2[\"MDS\"] - res_var2[\"Kalman Filter\"]) / res_var2[\"MDS\"] * 100 })\n\n\n\n\n\n\n\n\n\nERA\nMDS\n\n\nvar\ngap_len\n\n\n\n\n\n\nTA\n6\n69.897582\n85.052698\n\n\n12\n58.766166\n79.376385\n\n\n24\n51.538443\n75.395970\n\n\n168\n41.823614\n73.000401\n\n\nSW_IN\n6\n9.519984\n29.746651\n\n\n12\n11.165399\n30.639223\n\n\n24\n14.232051\n34.811941\n\n\n168\n12.305658\n42.651906\n\n\nLW_IN\n6\n21.023524\n59.136518\n\n\n12\n9.110040\n52.211404\n\n\n24\n-3.553292\n50.720632\n\n\n168\n-4.260023\n48.223005\n\n\nVPD\n6\n66.980942\n79.449579\n\n\n12\n47.785633\n69.081018\n\n\n24\n33.663749\n56.728120\n\n\n168\n32.272332\n57.702579\n\n\nWS\n6\n32.402977\n45.724043\n\n\n12\n25.209162\n43.275430\n\n\n24\n15.543672\n37.142502\n\n\n168\n12.735569\n36.436106\n\n\nPA\n6\n39.823585\n91.511486\n\n\n12\n30.995845\n90.532461\n\n\n24\n24.727301\n89.319180\n\n\n168\n20.691181\n91.421434\n\n\nP\n6\n-18.485009\n-13.917879\n\n\n12\n-28.935358\n-37.127331\n\n\n24\n-24.423076\n-29.998707\n\n\n168\n-7.725322\n-11.587796\n\n\nSWC\n6\nNaN\n61.302664\n\n\n12\nNaN\n47.976950\n\n\n24\nNaN\n42.535719\n\n\n168\nNaN\n23.301469\n\n\nTS\n6\nNaN\n64.264901\n\n\n12\nNaN\n46.699870\n\n\n24\nNaN\n27.050291\n\n\n168\nNaN\n-15.268479\n\n\n\n\n\n\n\n\n\nMain plot\n\nfrom itertools import product\nimport altair as alt\n\n\np = the_plot(results_Av)\nsave_show_plot(p, \"the_plot\")\n\n\n\n\n\np = the_plot_stand(results_Av)\nsave_show_plot(p, \"the_plot_stand\")\n\n\n\nTable\n\nt = the_table(results_Av)\nthe_table_latex(t, base_path_tbl / \"the_table.tex\", label=\"tbl:the_table\",\n                caption=\"\\\\CapTheTable\")\nt\n\n\nt = the_table(results_Av, 'rmse_stand')\nthe_table_latex(t, base_path_tbl / \"the_table_stand.tex\", stand = True, label=\"tbl:the_table_stand\", \n                caption = \"\\\\CapTheTableStand\")\nt\n\n\n\nTimeseries\n\n@cache_disk(cache_dir / \"the_results_ts\")\ndef get_the_results_ts():\n    reset_seed()\n    comp_Av = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 446, time_series=True, rmse=False)\n    results_Av = comp_Av.compare(gap_len = [12,24, 336], var=list(hai.columns), n_rep=4) \n    return results_Av\n\nresults_ts = get_the_results_ts()\n\n\nts = plot_timeseries(results_ts.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=0)\nsave_show_plot(ts, \"timeseries_1\")\n\n\n%time ts = plot_timeseries(results_ts.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=0)\n%time save_show_plot(ts, \"timeseries_2\")\n\n\nfrom tqdm.auto import tqdm\n\n\n@cache_disk(cache_dir / \"ts_plot\")\ndef plot_additional_ts():\n    for idx in tqdm(results_ts.idx_rep.unique()):\n        if idx == 0: continue # skip first plot as is done above\n        ts1 = plot_timeseries(results_ts.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=idx)\n        save_show_plot(ts1, f\"timeseries_1_{idx}\")\n        ts2 = plot_timeseries(results_ts.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=idx)\n        save_show_plot(ts2, f\"timeseries_2_{idx}\")        \n\n\nplot_additional_ts()"
  },
  {
    "objectID": "results/result_plots.html#kalman-filter-analysis",
    "href": "results/result_plots.html#kalman-filter-analysis",
    "title": "Plotting for results",
    "section": "Kalman Filter analysis",
    "text": "Kalman Filter analysis\n\nGap len\n\n@cache_disk(\"gap_len\")\ndef get_g_len(n_rep=n_rep):\n    reset_seed()\n    return KalmanImpComparison(models_var, hai, hai_era, block_len=48*7+100).compare(gap_len = [2,6,12,24,48,48*2, 48*3, 48*7], var=list(hai.columns), n_rep=n_rep)\n\n\ngap_len = get_g_len(n_rep)\n\n\np = plot_gap_len(gap_len, hai, hai_era)\nsave_show_plot(p, \"gap_len\")\n\n\nt = table_gap_len(gap_len)\ntable_gap_len_latex(t, base_path_tbl / \"gap_len.tex\", label=\"gap_len\",\n                caption=\"\\\\CapGapLen\")\nt\n\n\ng_len_agg = gap_len.groupby('gap_len').agg({'rmse_stand': 'mean'})\n(g_len_agg.iloc[0])/g_len_agg.iloc[-1]\n\n\ng_len_agg = gap_len.groupby(['gap_len', 'var']).agg({'rmse_stand': 'mean'})\n(g_len_agg.loc[1.])/g_len_agg.loc[168.]\n\n\ng_len_agg\n\n\ng_len_agg_std = gap_len.groupby('gap_len').agg({'rmse_stand': 'std'})\n(g_len_agg_std.iloc[0])/g_len_agg_std.iloc[-1]\n\n\n(gap_len.groupby(['gap_len', 'var']).agg({'rmse_stand': 'std'})\n    .unstack(\"var\")\n    .drop(columns=('rmse_stand', \"P\"))\n    .droplevel(0, 1) \n    .plot(subplots=True, layout=(3,3), figsize=(10,10)))\n\n\n# with open(base_path_tbl / \"gap_len.tex\") as f:\n    # print(f.readlines())\n\n\n\nControl\n\nmodels_nc = pd.DataFrame({'model': [ l_model(\"1_gap_varying_336_no_control_v1.pickle\"), l_model(\"1_gap_varying_6-336_v3.pickle\")],\n                          'type':   [ 'No Control',                                       'Use Control'                         ]})                                        \n\n\n@cache_disk(\"use_control\")\ndef get_control(n_rep=n_rep):\n    reset_seed()\n    \n    kcomp_control = KalmanImpComparison(models_nc, hai, hai_era, block_len=100+48*7)\n\n    k_results_control = kcomp_control.compare(n_rep =n_rep, gap_len = [12, 24, 48, 48*7], var = list(hai.columns))\n    \n    return k_results_control\n\n\nfrom time import sleep\n\n\nk_results_control = get_control(n_rep)\n\n\nk_results_control\n\n\np = plot_compare(k_results_control, 'type', y = 'rmse', scale_domain=[\"Use Control\", \"No Control\"])\nsave_show_plot(p, \"use_control\")\np\n\n\nfrom functools import partial\n\n\nt = table_compare(k_results_control, 'type')\ntable_compare_latex(t, base_path_tbl / \"control.tex\", label=\"tbl:control\",\n                caption=\"\\\\CapControl\")\nt\n\n\n\nGap in Other variables\n\nmodels_gap_single = pd.DataFrame.from_records([\n    {'Gap':  'All variables', 'gap_single_var': False, 'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")},\n    {'Gap':  'Only one var',  'gap_single_var': True,  'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")},\n])\n\n\n@cache_disk(\"gap_single\")\ndef get_gap_single(n_rep):\n    kcomp_single = KalmanImpComparison(models_gap_single, hai, hai_era, block_len=130)\n\n    return kcomp_single.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nres_single = get_gap_single(n_rep)\n\n\np = plot_compare(res_single, \"Gap\", y = 'rmse', scale_domain=[\"Only one var\", \"All variables\"])\nsave_show_plot(p, \"gap_single_var\")\n\n\nt = table_compare(res_single, 'Gap')\ntable_compare_latex(t, base_path_tbl / \"gap_single_var.tex\", caption=\"\\\\CapGapSingle\", label=\"tbl:gap_single_var\")\nt\n\n\nres_singl_perc = res_single.groupby(['Gap', 'var', 'gap_len']).agg({'rmse_stand': 'mean'}).reset_index().pivot(columns = 'Gap', values='rmse_stand', index=['var', 'gap_len'])\n\n\npd.DataFrame({'Only one var': (res_singl_perc[\"All variables\"] - res_singl_perc[\"Only one var\"]) / res_singl_perc[\"All variables\"] * 100})\n\n\nres_singl_perc = res_single.groupby(['Gap', 'var']).agg({'rmse_stand': 'mean'}).reset_index().pivot(columns = 'Gap', values='rmse_stand', index=['var'])\n\n\npd.DataFrame({'Only one var': (res_singl_perc[\"All variables\"] - res_singl_perc[\"Only one var\"]) / res_singl_perc[\"All variables\"] * 100})\n\n\n\nGeneric vs Specialized\n\nmodels_generic = models_var.copy()\n\n\nmodels_generic.model = l_model(\"1_gap_varying_6-336_v3.pickle\") \nmodels_generic['type'] = 'Generic'\n\n\nmodels_generic\n\n\nmodels_var['type'] = 'Fine-tuned one var'\n\n\nmodels_gen_vs_spec = pd.concat([models_generic, models_var])\n\n\nmodels_gen_vs_spec\n\n\n@cache_disk(\"generic\")\ndef get_generic(n_rep=n_rep):\n    reset_seed()\n\n    comp_generic = KalmanImpComparison(models_gen_vs_spec, hai, hai_era, block_len=100+48*7)\n\n    return comp_generic.compare(n_rep =n_rep, gap_len = [12, 24, 48, 48*7], var = list(hai.columns))\nk_results_generic = get_generic(n_rep)\n\n\nplot_formatter.legend_symbol_size = 300\n\n\np = plot_compare(k_results_generic, 'type', y = 'rmse', scale_domain=[\"Fine-tuned one var\", \"Generic\"])\nsave_show_plot(p, \"generic\")\np\n\n\nt = table_compare(k_results_generic, 'type')\ntable_compare_latex(t, base_path_tbl / \"generic.tex\", label='tbl:generic', caption=\"\\\\CapGeneric\")\nt\n\n\nres_singl_perc = k_results_generic.groupby(['type', 'var']).agg({'rmse_stand': 'mean'}).reset_index().pivot(columns = 'type', values='rmse_stand', index=['var'])\n\n\n(res_singl_perc[\"Generic\"] - res_singl_perc[\"Fine-tuned one var\"]) / res_singl_perc[\"Generic\"] * 100\n\n\n\nTraining\n\nmodels_train = pd.DataFrame.from_records([\n    # {'Train':  'All variables',  'model': l_model(\"All_gap_all_30_v1.pickle\")  },\n    {'Train':  'Only one var',   'model': l_model(\"1_gap_varying_6-336_v3.pickle\")  },\n    {'Train':  'Multi vars',     'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")  },\n    {'Train':  'Random params',  'model': l_model(\"rand_all_varying_gap_varying_len_6-30_v4.pickle\")  }\n])\n\n\nmodels_train\n\n\n@cache_disk(\"train\")\ndef get_train(n_rep):\n    reset_seed()\n    kcomp = KalmanImpComparison(models_train, hai, hai_era, block_len=130)\n\n    return kcomp.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nres_train = get_train(n_rep)\n\n\nres_train_agg = res_train.groupby(['Train', 'gap_len']).agg({'rmse_stand': 'mean'}).reset_index()\n\n\nres_train_agg\n\n\np = plot_compare(res_train, \"Train\", y='rmse', scale_domain=[\"Multi vars\", \"Only one var\", \"Random params\"])\nsave_show_plot(p, \"train_compare\")\n\n\nt = table_compare3(res_train, 'Train')\ntable_compare3_latex(t, base_path_tbl / \"train.tex\", label=\"tbl:train_compare\", caption=\"\\\\CapTrain\")\nt"
  },
  {
    "objectID": "results/result_plots.html#extra-results",
    "href": "results/result_plots.html#extra-results",
    "title": "Plotting for results",
    "section": "Extra results",
    "text": "Extra results\n\nStandard deviations\n\nhai_std = hai.std().to_frame(name='std')\nhai_std.index.name = \"Variable\"\nhai_std\n\n\nhai_std = hai_std.rename(index = renames_table_latex)\nlatex = hai_std.style.to_latex(hrules=True, clines=\"skip-last;data\", caption=\"\\\\CapStd\", label=\"tbl:hai_std\", position_float=\"centering\")\n\nwith open(base_path_tbl / \"hai_std.tex\", 'w') as f:\n    f.write(latex)\n\n\n\nGap distribution\n\nout_dir = here(\"../fluxnet/gap_stat\")\n\n\nsite_info = pl.read_parquet(out_dir / \"../site_info.parquet\").select([\n    pl.col(\"start\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"end\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"site\").cast(pl.Categorical).sort()\n])\n\n\ndef duration_n_obs(duration):\n    \"converts a duration into a n of fluxnet observations\"\n    return abs(int(duration.total_seconds() / (30 * 60)))\n\n\nfiles = out_dir.ls()\nfiles.sort() # need to sort to match the site_info\nsites = []\nfor i, path in enumerate(files):\n    sites.append(pl.scan_parquet(path).with_columns([\n        pl.lit(site_info[i, \"site\"]).alias(\"site\"),\n        pl.lit(duration_n_obs(site_info[i, \"start\"] -  site_info[i, \"end\"])).alias(\"total_obs\"),\n        pl.col(\"TIMESTAMP_END\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\").alias(\"end\"),\n    ]).drop(\"TIMESTAMP_END\"))\n\ngap_stat = pl.concat(sites)\n\n\npl.read_parquet(files[0])\n\n\ngap_stat.head().collect()\n\n\ndef plot_var_dist(var, small=False, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).with_column(pl.col(\"gap_len\") / (24 *2 * 7)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    if not small: ax.set_yscale('log')\n    ax.set_xlabel(\"gap length (weeks)\")\n    ax.set_ylabel(f\"{'Log' if not small else ''} n gaps\")\n    # plt.xscale('log') \n\n\nplot_var_dist('TA_F_QC')\n\n\ncolor_map = dict(zip(scale_meteo.domain, list(sns.color_palette('Set2', n_colors=len(hai.columns)).as_hex())))\n\n\nqc_map = {\n    'TA': 'TA_F_QC',\n    'SW_IN': 'SW_IN_F_QC',\n    'LW_IN': 'LW_IN_F_QC',\n    'VPD': 'VPD_F_QC',\n    'WS': 'WS_F_QC',\n    'PA': 'PA_F_QC',\n    'P': 'P_F_QC',\n    'TS': 'TS_F_MDS_1_QC',\n    'SWC': 'SWC_F_MDS_1_QC',\n}\n\n\ndef pl_in(col, values):\n    expr = False\n    for val in values:\n        expr |= pl.col(col) == val\n    return expr\n\n\ngap_stat.filter(pl_in('variable', qc_map.values())\n               ).with_columns([\n    pl.when(pl.col(\"gap_len\") &lt; 48*7).then(True).otherwise(False).alias(\"short\"),\n    pl.count().alias(\"total\"),\n    pl.count().alias(\"total len\"),\n]).groupby(\"short\").agg([\n    (pl.col(\"gap_len\").count() / pl.col(\"total\")).alias(\"frac_num\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total len\")).alias(\"frac_len\")\n]).collect()\n\n\ngap_stat.filter(pl_in('variable', qc_map.values())\n               ).with_columns([\n    pl.when(pl.col(\"gap_len\") &lt; 48*7).then(True).otherwise(False).alias(\"short\"),\n    pl.count().alias(\"total\"),\n    pl.count().alias(\"total len\"),\n]).groupby(\"short\").agg([\n    (pl.col(\"gap_len\").count() / pl.col(\"total\")).alias(\"frac_num\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total len\")).alias(\"frac_len\")\n]).collect()\n\n\nfrac_miss = gap_stat.filter(\n    pl_in('variable', qc_map.values())\n).groupby([\"site\", \"variable\"]).agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()).alias(\"frac_gap\")\n])\n\n\nfrac_miss.groupby('variable').agg([\n    pl.col(\"frac_gap\").max().alias(\"max\"),\n    pl.col(\"frac_gap\").min().alias(\"min\"),\n    pl.col(\"frac_gap\").std().alias(\"std\"),\n    pl.col(\"frac_gap\").mean().alias(\"mean\"),\n]).collect()\n\n\nfrac_miss.sort(\"frac_gap\", reverse=True).collect()\n\n\nsite_info.filter((pl.col(\"site\") == \"US-LWW\"))\n\n\ngap_stat.filter((pl.col(\"site\") == \"US-LWW\") & (pl.col(\"variable\") == \"LW_IN_F_QC\" )).collect()\n\n\nimport matplotlib\n\n\nmatplotlib.rcParams.update({'font.size': 22})\nsns.set_style(\"whitegrid\")\n\n\ndef plot_var_dist(var, ax=None, small=True):\n    if ax is None: ax = get_grid(1)[0]\n    \n    color = color_map[var]\n    var_qc = qc_map[var]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var_qc)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) if small else True \n    ).with_column(pl.col(\"gap_len\") / (2 if small else 48 * 7)\n                 ).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax, edgecolor=\"white\", color=color)\n    ax.set_title(f\"{var} - { 'gap length &lt;  1 week' if small else 'all gaps'}\")\n    ax.set_xlabel(f\"Gap length ({ 'hour' if small else 'week'})\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n    plt.tight_layout()\n\n\nvars = gap_stat.select(pl.col(\"variable\").unique()).collect()\n\n\nvars.filter(pl.col(\"variable\").str.contains(\"TA\"))\n\n\nfor ax, var in zip(get_grid(9,3,3, figsize=(15,12), sharey=False), list(var_type.categories)):\n    plot_var_dist(var, ax=ax)\nplt.savefig(base_path_img / \"gap_len_dist_small.pdf\")\n\n\nfor ax, var in zip(get_grid(9,3,3, figsize=(15,12), sharey=False), list(var_type.categories)):\n    plot_var_dist(var, ax=ax, small=False)\nplt.savefig(base_path_img / \"gap_len_dist.pdf\")\n\n\n\nSquare Root Filter\n\nNumerical Stability\n\nfrom meteo_imp.kalman.performance import *\n\n\nerr = cache_disk(\"fuzz_sr\")(fuzz_filter_SR)(100, 110)\n\n\np = plot_err_sr_filter(err)\nsave_show_plot(p, \"numerical_stability\")\n\n\n\nPerformance\n\n@cache_disk(\"perf_sr\")\ndef get_perf_sr():\n    reset_seed()\n    return perf_comb_params('filter', use_sr_filter=[True, False], rep=range(100)) \n\n\nperf1 = (get_perf_sr()\n    .groupby('use_sr_filter')\n    .agg(pl.col(\"time\").mean())\n    .with_column(\n        pl.when(pl.col(\"use_sr_filter\"))\n        .then(pl.lit(\"Square Root Filter\"))\n        .otherwise(pl.lit(\"Standard Filter\"))\n        .alias(\"Filter type\")\n    ))\n\n\nperf1\n\n\nplot_perf_sr = alt.Chart(perf1.to_pandas()).mark_bar(size = 50).encode(\n    x=alt.X('Filter type', axis=alt.Axis(labelAngle=0)),\n    y=alt.Y('time', scale=alt.Scale(zero=False), title=\"time [s]\"),\n    color=alt.Color('Filter type',\n                    scale = alt.Scale(scheme = 'accent'))\n).properties(width=300)\n\n\nsave_show_plot(plot_perf_sr, \"perf_sr\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPFA Imputation",
    "section": "",
    "text": "GPFA Imputation\nThis is the homepage of the website that contains all the analysis. Use the sidebar for nagivation\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nAnalyze gaps fluxnet\n\n\n\n\nCode for presentation of 18th January\n\n\n\n\nHainich with ERA-Interim\n\n\n\n\nMeterological Time series Imputation using Kalman Filters\n\n\n\n\nPlotting for results\n\n\n\n\nTraining Kalman Filter for Results - 21 Feb\n\n\n\n\nVariable distribution\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\nundefined\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html",
    "href": "presentations/Hainich_results_pres18_jan.html",
    "title": "Hainich with ERA-Interim",
    "section": "",
    "text": "Manual fine tuning learning process for presentation 18 Jan 2023\n%load_ext autoreload\n%autoreload 2\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom sklearn.decomposition import PCA\nreset_seed()\nhai = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path64)\n# dls = imp_dataloader(hai64, hai_era64, var_sel = gen_var_sel(['TA', 'SW_IN', 'VPD']), block_len=200, gap_len=10, bs=20, control_lags=[1])\npc = PCA().fit(hai)\nd0 = hai.iloc[0:1]\ntr0 = pc.transform(d0)\ntr0\n\narray([[-121.11917652,   -7.06844313,    0.87975241]])\nt0 = np.random.randn(5,1) * 10\ntt = np.hstack([t0, 2 * t0 + np.random.randn(5,1), 3 * t0 + np.random.randn(5,1)])\ntt = np.vstack([tt, -tt])\ntt.mean(0)\n\narray([1.77635684e-16, 0.00000000e+00, 0.00000000e+00])\ndef pca(X):\n    # Data matrix X, assumes 0-centered\n    n, m = X.shape\n    assert np.allclose(X.mean(axis=0), np.zeros(m))\n    # Compute covariance matrix\n    C = np.dot(X.T, X) / (n-1)\n    # Eigen decomposition\n    eigen_vals, eigen_vecs = np.linalg.eig(C)\n    # Project X onto PC space\n    X_pca = np.dot(X, eigen_vecs)\n    return X_pca, eigen_vecs\npca(tt)\n\n(array([[-9.15947487e+00, -3.03773243e-01,  3.81626348e-01],\n        [ 2.00045215e+01, -1.99595583e-02,  1.53745860e+00],\n        [ 1.33744310e+01,  2.01395206e-02, -3.19819214e-02],\n        [-8.99057811e+00,  1.24208069e-01,  9.57988123e-01],\n        [ 3.14616875e+01, -4.88141095e-02, -5.79117681e-01],\n        [ 9.15947487e+00,  3.03773243e-01, -3.81626348e-01],\n        [-2.00045215e+01,  1.99595583e-02, -1.53745860e+00],\n        [-1.33744310e+01, -2.01395206e-02,  3.19819214e-02],\n        [ 8.99057811e+00, -1.24208069e-01, -9.57988123e-01],\n        [-3.14616875e+01,  4.88141095e-02,  5.79117681e-01]]),\n array([[-0.26545847, -0.95659856,  0.12021234],\n        [-0.53518386,  0.04249433, -0.84366608],\n        [-0.80194142,  0.28829401,  0.52323659]]))\nsk_pc = PCA(2).fit(tt)\ntt @ sk_pc.components_.T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\ntr = sk_pc.transform(tt)\ntr\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\nsk_pc.components_.T\n\narray([[-0.26545847,  0.12021234],\n       [-0.53518386, -0.84366608],\n       [-0.80194142,  0.52323659]])\n(sk_pc.components_ @ tt.T).T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\ntt[0, None]\n\narray([[2.76792539, 4.56712931, 7.45746711]])\nsk_pc.components_.shape\n\n(3, 3)\ntt[0].shape\n\n(3,)\n(sk_pc.components_ @ tt[0])\n\narray([-9.15947487,  0.38162635])\ntt[0]\n\narray([2.76792539, 4.56712931, 7.45746711])\nsk_pc.components_.T @ tr[0]\n\narray([2.47733634, 4.58003795, 7.54504311])\nsk_pc.inverse_transform(tr[0])\n\narray([2.47733634, 4.58003795, 7.54504311])\nd0.to_numpy()\n\narray([[-0.6  ,  0.   ,  0.222]])\nhai.iloc[0]\n\nTA      -0.600\nSW_IN    0.000\nVPD      0.222\nName: 2000-01-01 00:30:00, dtype: float64\n\\[ x = y\\Lambda \\]\npc.components_\n\narray([[ 0.01681572,  0.99979324,  0.01143269],\n       [ 0.93010891, -0.01983747,  0.36674772],\n       [-0.36689868, -0.00446652,  0.93025018]])\nnp.linalg.inv(pc.components_)\n\narray([[ 0.01681572,  0.93010891, -0.36689868],\n       [ 0.99979324, -0.01983747, -0.00446652],\n       [ 0.01143269,  0.36674772,  0.93025018]])\nassuming that the control has the same dimensions of the observations then if we are doing a local slope model we have \\(B \\in \\mathbb{R}^{state \\times contr}\\): \\[ B = \\begin{bmatrix} -I & I \\\\ 0 & 0 \\end{bmatrix}\\]\nfrom torch import hstack, eye, vstack, ones, zeros, tensor\nfrom functools import partial\ndef set_dtype(*args, dtype=torch.float64):\n    return [partial(arg, dtype=dtype) for arg in args]\neye, ones, zeros, tensor = set_dtype(eye, ones, zeros, tensor)\ndef init_smart(n_dim_obs, n_dim_state, df, pca=True):\n    # n_dim_obs == n_dim_contr\n    if pca:\n        comp = PCA(n_dim_state).fit(df).components_\n        obs_matrix = tensor(comp.T) # transform state -&gt; obs\n        contr_matrix = tensor(comp) # transform obs -&gt; state\n    else:\n        obs_matrix, contr_matrix = eye(n_dim_obs), eye(n_dim_obs)\n        \n    return KalmanFilter(\n        trans_matrix =     vstack([hstack([eye(n_dim_state),                eye(n_dim_state)]),\n                                   hstack([zeros(n_dim_state, n_dim_state), eye(n_dim_state)])]),\n        trans_off =        zeros(n_dim_state * 2),        \n        trans_cov =        eye(n_dim_state * 2)*.1,        \n        obs_matrix =       hstack([obs_matrix, zeros(n_dim_obs, n_dim_state)]),\n        obs_off =          zeros(n_dim_obs),          \n        obs_cov =          eye(n_dim_obs)*.01,            \n        contr_matrix =     vstack([hstack([-contr_matrix,                  contr_matrix]),\n                                   hstack([ zeros(n_dim_state,n_dim_obs), zeros(n_dim_state, n_dim_obs)])]),\n        init_state_mean =  zeros(n_dim_state * 2),        \n        init_state_cov =   eye(n_dim_state * 2) * 3,\n    )\nnp.hstack([np.eye(2), np.eye(2)])\n\narray([[1., 0., 1., 0.],\n       [0., 1., 0., 1.]])\ninit_smart(3,2, hai)\n\n\nKalman Filter (3 obs, 4 state, 6 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n1.0000\n0.0000\n1.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n1.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\n\n\n\n\ny_0\n0.0168\n0.9301\n0.0000\n0.0000\n\n\ny_1\n0.9998\n-0.0198\n0.0000\n0.0000\n\n\ny_2\n0.0114\n0.3667\n0.0000\n0.0000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0168\n-0.9998\n-0.0114\n0.0168\n0.9998\n0.0114\n\n\nx_1\n-0.9301\n0.0198\n-0.3667\n0.9301\n-0.0198\n0.3667\n\n\nx_2\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\nclass PersistentRecorder(Callback):\n    order = 70\n    name = \"per_recorder\"\n    attrs = ['lrs', 'iters', 'losses', 'values']\n    def before_fit(self):\n        \"Prepare state for training\"\n        for attr in self.attrs:\n            if not hasattr(self.per_recorder, attr): setattr(self.per_recorder, attr, [])\n\n    def after_batch(self):\n        for attr in self.attrs:\n            setattr(self.per_recorder, attr, getattr(self.recorder, attr))\nmodels = []\ndls = imp_dataloader(hai, hai_era, var_sel = ['TA', 'SW_IN', 'VPD'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\nlen(dls.valid.items)\nitems = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\ndef train_show_save(learn, n_iter, lr):\n    learn.fit(n_iter, lr)\n    models.append(learn.model.state_dict().copy())\n    learn.recorder.plot_loss()\n    items = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n    return show_results(learn, items = items, control=hai_control)"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html#gaps-all-variables",
    "href": "presentations/Hainich_results_pres18_jan.html#gaps-all-variables",
    "title": "Hainich with ERA-Interim",
    "section": "Gaps all variables",
    "text": "Gaps all variables\n\nmodel = init_smart(3,3,3, hai, pca=False).cuda()\n\n\nloss = loss_func=KalmanLoss(only_gap=False)\nlearn = Learner(dls, model, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\nlearn.model.use_smooth = True\n\n\nshow_results(learn, items=items, control=hai_control)\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-87.370314\n-96.212915\n0.069196\n0.210351\n0.946861\n-250571814513532732431918956544.000000\n02:37\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1489\n0.0042\n0.0120\n0.8404\n-0.0794\n-0.1127\n\n\nx_1\n-0.0954\n1.1464\n-0.0546\n0.1177\n0.8560\n0.0599\n\n\nx_2\n-0.0225\n0.0420\n1.1357\n-0.0717\n-0.1190\n0.8475\n\n\nx_3\n0.1201\n0.0661\n0.1170\n1.1840\n-0.1150\n-0.1073\n\n\nx_4\n0.0770\n0.1299\n0.0068\n-0.1847\n1.0987\n-0.1500\n\n\nx_5\n0.1084\n0.0358\n0.1133\n-0.1809\n-0.0281\n1.1533\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0341\n0.0493\n0.0361\n-0.0008\n-0.0312\n0.0026\n\n\nx_1\n0.0493\n0.1125\n0.0531\n0.0475\n0.0047\n0.0509\n\n\nx_2\n0.0361\n0.0531\n0.0399\n0.0001\n-0.0327\n0.0058\n\n\nx_3\n-0.0008\n0.0475\n0.0001\n0.0608\n0.0614\n0.0585\n\n\nx_4\n-0.0312\n0.0047\n-0.0327\n0.0614\n0.0901\n0.0554\n\n\nx_5\n0.0026\n0.0509\n0.0058\n0.0585\n0.0554\n0.0594\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0008\n\n\nx_1\n-0.0083\n\n\nx_2\n0.0067\n\n\nx_3\n0.0023\n\n\nx_4\n-0.0007\n\n\nx_5\n0.0005\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.8238\n-0.1897\n-0.1014\n-0.0841\n0.1914\n0.1286\n\n\ny_1\n0.0814\n0.8353\n0.1048\n0.1040\n0.0127\n0.0689\n\n\ny_2\n-0.0742\n-0.1502\n0.8296\n0.1282\n0.1811\n-0.0746\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0084\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0084\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0084\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0017\n\n\ny_1\n-0.0046\n\n\ny_2\n0.0026\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0290\n-0.0485\n-0.0022\n0.9297\n0.0792\n-0.0247\n\n\nx_1\n-0.0267\n-0.8778\n-0.0233\n0.0113\n0.9099\n0.0261\n\n\nx_2\n-0.0018\n-0.0592\n-1.0044\n-0.0310\n0.0674\n0.9204\n\n\nx_3\n-0.0503\n-0.0312\n-0.0556\n-0.0440\n-0.0529\n-0.0587\n\n\nx_4\n-0.0375\n-0.0537\n-0.0246\n-0.0412\n-0.0352\n-0.0338\n\n\nx_5\n-0.0653\n-0.0179\n-0.0575\n-0.0655\n-0.0307\n-0.0449\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0393\n\n\nx_1\n0.0044\n\n\nx_2\n-0.0092\n\n\nx_3\n0.0207\n\n\nx_4\n-0.0094\n\n\nx_5\n-0.0021\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.5500\n-0.1402\n-0.0082\n0.5233\n0.3011\n0.0882\n\n\nx_1\n-0.1402\n2.4578\n0.1502\n0.0355\n0.5714\n-0.0459\n\n\nx_2\n-0.0082\n0.1502\n2.5045\n0.2739\n0.0634\n0.5195\n\n\nx_3\n0.5233\n0.0355\n0.2739\n2.3814\n-0.2201\n-0.1842\n\n\nx_4\n0.3011\n0.5714\n0.0634\n-0.2201\n2.3880\n-0.1162\n\n\nx_5\n0.0882\n-0.0459\n0.5195\n-0.1842\n-0.1162\n2.3874\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-108.023250\n-113.113034\n0.073622\n0.198386\n0.956828\n-105578612000578019271909572608.000000\n02:37\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1440\n-0.0057\n0.0107\n0.8262\n-0.0845\n-0.1205\n\n\nx_1\n-0.0534\n1.2089\n-0.0223\n0.0948\n0.7937\n0.0494\n\n\nx_2\n-0.0289\n0.0176\n1.1417\n-0.0702\n-0.1154\n0.8351\n\n\nx_3\n0.1277\n0.0589\n0.1074\n1.1787\n-0.1203\n-0.1024\n\n\nx_4\n0.0654\n0.1560\n0.0043\n-0.1781\n1.1087\n-0.1415\n\n\nx_5\n0.0959\n0.0439\n0.1197\n-0.1725\n-0.0340\n1.1496\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0207\n0.0254\n0.0210\n-0.0052\n-0.0232\n-0.0033\n\n\nx_1\n0.0254\n0.0548\n0.0255\n0.0259\n0.0034\n0.0281\n\n\nx_2\n0.0210\n0.0255\n0.0222\n-0.0056\n-0.0241\n-0.0023\n\n\nx_3\n-0.0052\n0.0259\n-0.0056\n0.0473\n0.0504\n0.0465\n\n\nx_4\n-0.0232\n0.0034\n-0.0241\n0.0504\n0.0695\n0.0473\n\n\nx_5\n-0.0033\n0.0281\n-0.0023\n0.0465\n0.0473\n0.0477\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0004\n\n\nx_1\n-0.0055\n\n\nx_2\n0.0048\n\n\nx_3\n0.0027\n\n\nx_4\n-0.0033\n\n\nx_5\n0.0021\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7902\n-0.2032\n-0.1217\n-0.0986\n0.2164\n0.1090\n\n\ny_1\n0.0160\n0.7417\n0.0555\n0.1016\n0.1041\n0.0374\n\n\ny_2\n-0.1103\n-0.1594\n0.7916\n0.1185\n0.2096\n-0.0949\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0077\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0077\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0077\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0009\n\n\ny_1\n-0.0053\n\n\ny_2\n-0.0021\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0277\n-0.0595\n-0.0019\n0.9181\n0.0838\n-0.0194\n\n\nx_1\n-0.0414\n-0.8661\n-0.0323\n0.0110\n0.8741\n0.0262\n\n\nx_2\n-0.0029\n-0.0647\n-1.0045\n-0.0319\n0.0757\n0.8957\n\n\nx_3\n-0.0678\n-0.0368\n-0.0564\n-0.0559\n-0.0610\n-0.0639\n\n\nx_4\n-0.0374\n-0.0690\n-0.0209\n-0.0485\n-0.0406\n-0.0354\n\n\nx_5\n-0.0591\n-0.0267\n-0.0753\n-0.0633\n-0.0407\n-0.0555\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0396\n\n\nx_1\n-0.0009\n\n\nx_2\n-0.0049\n\n\nx_3\n0.0153\n\n\nx_4\n-0.0139\n\n\nx_5\n-0.0094\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.6364\n-0.1456\n-0.0020\n0.5695\n0.3986\n0.1628\n\n\nx_1\n-0.1456\n2.3292\n0.1064\n0.2074\n0.7591\n0.1475\n\n\nx_2\n-0.0020\n0.1064\n2.4205\n0.3489\n0.1488\n0.6143\n\n\nx_3\n0.5695\n0.2074\n0.3489\n2.2351\n-0.4355\n-0.3277\n\n\nx_4\n0.3986\n0.7591\n0.1488\n-0.4355\n2.1625\n-0.3445\n\n\nx_5\n0.1628\n0.1475\n0.6143\n-0.3277\n-0.3445\n2.2518\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps\")\n\nPath('models/17_jan_all_gaps.pth')\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-122.620241\n-126.621236\n0.077944\n0.202366\n0.948695\n-115274990280631988956159803392.000000\n02:38\n\n\n1\n-134.455010\n-139.768441\n0.074152\n0.188695\n0.959962\n-103653315974387814635399020544.000000\n02:31\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1347\n-0.0147\n0.0100\n0.8021\n-0.1122\n-0.1444\n\n\nx_1\n-0.0388\n1.2480\n-0.0185\n0.0773\n0.7576\n0.0414\n\n\nx_2\n-0.0257\n-0.0018\n1.1500\n-0.0814\n-0.1272\n0.8203\n\n\nx_3\n0.1332\n0.0451\n0.0866\n1.1612\n-0.1103\n-0.0917\n\n\nx_4\n0.0559\n0.1720\n0.0056\n-0.1625\n1.1052\n-0.1248\n\n\nx_5\n0.0721\n0.0457\n0.1226\n-0.1523\n-0.0385\n1.1357\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0146\n0.0183\n0.0142\n-0.0081\n-0.0202\n-0.0067\n\n\nx_1\n0.0183\n0.0445\n0.0187\n0.0148\n-0.0014\n0.0168\n\n\nx_2\n0.0142\n0.0187\n0.0143\n-0.0065\n-0.0187\n-0.0044\n\n\nx_3\n-0.0081\n0.0148\n-0.0065\n0.0340\n0.0393\n0.0343\n\n\nx_4\n-0.0202\n-0.0014\n-0.0187\n0.0393\n0.0550\n0.0378\n\n\nx_5\n-0.0067\n0.0168\n-0.0044\n0.0343\n0.0378\n0.0361\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0014\n\n\nx_1\n-0.0059\n\n\nx_2\n0.0039\n\n\nx_3\n0.0016\n\n\nx_4\n-0.0036\n\n\nx_5\n0.0032\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7310\n-0.1642\n-0.0983\n-0.1030\n0.2236\n0.0907\n\n\ny_1\n0.0158\n0.6777\n0.0705\n0.1196\n0.1642\n0.0316\n\n\ny_2\n-0.1124\n-0.1249\n0.7399\n0.1211\n0.2193\n-0.0846\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0064\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0065\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0065\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0019\n\n\ny_1\n-0.0046\n\n\ny_2\n-0.0026\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0316\n-0.0708\n-0.0019\n0.8851\n0.0973\n-0.0094\n\n\nx_1\n-0.0727\n-0.8659\n-0.0584\n0.0075\n0.7964\n0.0142\n\n\nx_2\n0.0098\n-0.0734\n-1.0045\n-0.0156\n0.0845\n0.8489\n\n\nx_3\n-0.0977\n-0.0434\n-0.0574\n-0.0723\n-0.0760\n-0.0730\n\n\nx_4\n-0.0309\n-0.1114\n-0.0185\n-0.0559\n-0.0634\n-0.0420\n\n\nx_5\n-0.0503\n-0.0424\n-0.1056\n-0.0644\n-0.0571\n-0.0729\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0585\n\n\nx_1\n-0.0141\n\n\nx_2\n-0.0070\n\n\nx_3\n0.0239\n\n\nx_4\n-0.0043\n\n\nx_5\n-0.0069\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.9257\n0.1245\n0.1987\n0.4475\n0.2435\n0.0893\n\n\nx_1\n0.1245\n2.4393\n0.3641\n0.2107\n0.7802\n0.1895\n\n\nx_2\n0.1987\n0.3641\n2.4862\n0.2959\n0.0089\n0.6053\n\n\nx_3\n0.4475\n0.2107\n0.2959\n2.1058\n-0.5771\n-0.4644\n\n\nx_4\n0.2435\n0.7802\n0.0089\n-0.5771\n1.9818\n-0.5064\n\n\nx_5\n0.0893\n0.1895\n0.6053\n-0.4644\n-0.5064\n2.0952\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-149.288297\n-152.804403\n0.072760\n0.179190\n0.956827\n-61340364986143763995601928192.000000\n02:35\n\n\n1\n-160.099637\n-165.002097\n0.071263\n0.181697\n0.963587\n-48387608947773502947627892736.000000\n02:37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps_final\")\n\nPath('models/17_jan_all_gaps_final.pth')\n\n\n\np = show_results(learn, control=hai_control, items = [items[i] for i in [0,5,4]])"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html#gap-in-only-1-variable",
    "href": "presentations/Hainich_results_pres18_jan.html#gap-in-only-1-variable",
    "title": "Hainich with ERA-Interim",
    "section": "Gap in only 1 variable",
    "text": "Gap in only 1 variable\n\ndls2 = imp_dataloader(hai, hai_era, var_sel = ['TA'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nmodel2 = init_smart(3,3, hai, pca=True).cuda()\n\n\nloss2 = loss_func=KalmanLoss(only_gap=False)\nlearn2 = Learner(dls2, model2, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\ntrain_show_save(learn2, 1, 1e-3)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/91 00:00&lt;?]\n    \n    \n\n\nNotImplementedError: Module [KalmanFilter] is missing the required \"forward\" function\n\n\nDisable smoother\n\nmodel.use_smooth = False\n\n\ntrain_show_save(learn, 3, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n534.159578\n465.194522\n0.284379\n0.329570\n-1.919198\n-241825956681560181426503024640.000000\n02:24\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.6761\n0.1157\n-0.0095\n0.5895\n0.0728\n0.0850\n\n\nx_1\n0.0309\n0.6675\n0.1639\n0.2296\n0.5542\n-0.1129\n\n\nx_2\n0.1836\n-0.1476\n0.6498\n0.1684\n-0.2142\n0.5899\n\n\nx_3\n-0.1246\n0.0361\n0.0138\n0.6844\n-0.0830\n0.1501\n\n\nx_4\n-0.0167\n-0.0399\n0.0044\n0.0089\n0.7537\n0.0580\n\n\nx_5\n0.0341\n0.0300\n-0.0436\n-0.0362\n0.2878\n0.6487\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.5106\n0.3768\n0.4383\n-0.1720\n-0.2863\n-0.1749\n\n\nx_1\n0.3768\n0.5687\n0.1931\n-0.2781\n0.0461\n0.0798\n\n\nx_2\n0.4383\n0.1931\n0.4406\n-0.0734\n-0.3650\n-0.2384\n\n\nx_3\n-0.1720\n-0.2781\n-0.0734\n0.5150\n0.3608\n0.3418\n\n\nx_4\n-0.2863\n0.0461\n-0.3650\n0.3608\n0.8281\n0.6975\n\n\nx_5\n-0.1749\n0.0798\n-0.2384\n0.3418\n0.6975\n0.6156\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0166\n\n\nx_1\n-0.0196\n\n\nx_2\n-0.0446\n\n\nx_3\n0.0033\n\n\nx_4\n-0.0022\n\n\nx_5\n0.0107\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n-0.3096\n0.5414\n-0.0652\n0.4906\n-0.0643\n-0.3834\n\n\ny_1\n0.6288\n-0.3366\n-0.3681\n0.0321\n-0.2062\n0.3634\n\n\ny_2\n-0.2813\n0.0500\n0.4738\n-0.0601\n0.2872\n-0.1334\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.3678\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.3707\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.3693\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0197\n\n\ny_1\n-0.0452\n\n\ny_2\n-0.0451\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9410\n-0.5681\n-0.8722\n0.9998\n1.2958\n1.0828\n\n\nx_1\n-0.7019\n-0.7354\n-0.9277\n1.2348\n1.1344\n0.9620\n\n\nx_2\n-0.9322\n-0.7328\n-0.8307\n1.0310\n1.1112\n1.1017\n\n\nx_3\n0.0914\n0.2592\n0.0286\n0.0638\n0.1644\n-0.0007\n\n\nx_4\n0.2175\n-0.0398\n0.0226\n0.2072\n-0.1205\n-0.0117\n\n\nx_5\n-0.0578\n0.1485\n0.0908\n-0.0613\n0.0627\n0.0792\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0290\n\n\nx_1\n-0.0320\n\n\nx_2\n-0.0494\n\n\nx_3\n0.0296\n\n\nx_4\n0.0387\n\n\nx_5\n0.1010\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n3.7171\n2.7054\n2.8366\n2.0078\n1.3904\n2.3275\n\n\nx_1\n2.7054\n4.5189\n2.1010\n1.4436\n1.8804\n2.8057\n\n\nx_2\n2.8366\n2.1010\n3.3027\n1.3323\n0.9307\n2.0280\n\n\nx_3\n2.0078\n1.4436\n1.3323\n3.3867\n2.1461\n3.0821\n\n\nx_4\n1.3904\n1.8804\n0.9307\n2.1461\n2.5441\n2.6815\n\n\nx_5\n2.3275\n2.8057\n2.0280\n3.0821\n2.6815\n3.7653\n\n\n\n\n\n \n\n\nIndexError: list index out of range\n\n\n\ntrain_show_save(learn, 1, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n272.526685\n207.314376\n0.279615\n0.290300\n-3.962859\n-113462721102480788295669252096.000000\n06:35\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5320\n0.0984\n0.2567\n\n\nx_1\n0.5386\n-0.1615\n-0.1019\n\n\nx_2\n0.4759\n0.4640\n0.4202\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.4043\n0.5126\n1.8864\n\n\nx_1\n0.5126\n0.1871\n0.6885\n\n\nx_2\n1.8864\n0.6885\n2.5339\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0972\n\n\nx_1\n-0.2735\n\n\nx_2\n0.0298\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1792\n0.7014\n-0.3164\n\n\ny_1\n-0.5045\n0.1311\n0.3455\n\n\ny_2\n-0.0962\n0.0705\n0.1246\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0190\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.1624\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0297\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1664\n\n\ny_1\n-0.0433\n\n\ny_2\n0.0232\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0331\n-0.4960\n0.2924\n0.5946\n-0.1127\n-0.1933\n\n\nx_1\n0.5991\n0.3053\n0.2248\n0.7649\n0.9446\n0.0394\n\n\nx_2\n0.0060\n0.0013\n-0.0667\n-0.1143\n0.7781\n0.6409\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0627\n\n\nx_1\n-0.6392\n\n\nx_2\n-1.2803\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n9.0224\n3.6789\n10.5469\n\n\nx_1\n3.6789\n2.3654\n5.9687\n\n\nx_2\n10.5469\n5.9687\n21.9915\n\n\n\n\n\n \n\n\nNameError: name 'learn1' is not defined\n\n\n\ntrain_show_save(learn, 1, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n160.675568\n150.361449\n0.266633\n0.269324\n-1.959343\n-53151966368643636872563654656.000000\n05:30\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5363\n0.1070\n0.2723\n\n\nx_1\n0.5375\n-0.1534\n-0.0948\n\n\nx_2\n0.4909\n0.4638\n0.4301\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.3408\n0.4970\n1.7922\n\n\nx_1\n0.4970\n0.1842\n0.6643\n\n\nx_2\n1.7922\n0.6643\n2.3955\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0948\n\n\nx_1\n-0.2568\n\n\nx_2\n0.0393\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1742\n0.6954\n-0.3174\n\n\ny_1\n-0.5094\n0.1551\n0.3429\n\n\ny_2\n-0.1147\n0.0583\n0.1163\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0121\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.1100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0190\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1697\n\n\ny_1\n-0.0123\n\n\ny_2\n0.0387\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0363\n-0.5225\n0.3103\n0.5948\n-0.0692\n-0.1628\n\n\nx_1\n0.5955\n0.3523\n0.1332\n0.7587\n0.9820\n-0.0509\n\n\nx_2\n-0.0159\n0.0573\n-0.0471\n-0.1409\n0.7802\n0.6457\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0500\n\n\nx_1\n-0.6014\n\n\nx_2\n-1.2993\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n10.9221\n3.8061\n12.4742\n\n\nx_1\n3.8061\n1.7574\n5.6427\n\n\nx_2\n12.4742\n5.6427\n24.9669\n\n\n\n\n\n \n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n115.738528\n104.632719\n0.257947\n0.259673\n-1.679687\n-24519279833493970234084687872.000000\n05:09\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5303\n0.1124\n0.2795\n\n\nx_1\n0.5333\n-0.1470\n-0.0908\n\n\nx_2\n0.4963\n0.4602\n0.4322\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.2399\n0.4645\n1.6526\n\n\nx_1\n0.4645\n0.1740\n0.6191\n\n\nx_2\n1.6526\n0.6191\n2.2026\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0883\n\n\nx_1\n-0.2505\n\n\nx_2\n0.0440\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1715\n0.6898\n-0.3162\n\n\ny_1\n-0.5142\n0.1782\n0.3403\n\n\ny_2\n-0.1173\n0.0543\n0.1176\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0079\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0732\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0124\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1699\n\n\ny_1\n-0.0058\n\n\ny_2\n0.0383\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0449\n-0.5621\n0.3391\n0.5937\n0.0285\n-0.1154\n\n\nx_1\n0.5860\n0.3917\n0.0584\n0.7468\n1.0129\n-0.1247\n\n\nx_2\n-0.0479\n0.1664\n-0.0280\n-0.1814\n0.7807\n0.6422\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0488\n\n\nx_1\n-0.5958\n\n\nx_2\n-1.3038\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n12.1503\n4.1501\n13.5248\n\n\nx_1\n4.1501\n1.7544\n5.9730\n\n\nx_2\n13.5248\n5.9730\n27.6423\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nlearn.model.use_smooth = True\nshow_results(learn, items = [learn.dls.valid.items[i] for i in [10, 100, 300, 500, 700, 1000]])\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n30.829145\n26.059960\n0.201186\n0.215770\n-0.034140\n-48287381455002891037683220480.000000\n06:32\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5309\n0.1159\n0.2797\n\n\nx_1\n0.5313\n-0.1385\n-0.0881\n\n\nx_2\n0.4961\n0.4560\n0.4324\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.1473\n0.4769\n1.5832\n\n\nx_1\n0.4769\n0.2008\n0.6682\n\n\nx_2\n1.5832\n0.6682\n2.2249\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0894\n\n\nx_1\n-0.2426\n\n\nx_2\n0.0426\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1711\n0.6883\n-0.3146\n\n\ny_1\n-0.5151\n0.1887\n0.3414\n\n\ny_2\n-0.1155\n0.0454\n0.1145\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0063\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0583\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0097\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1747\n\n\ny_1\n0.0044\n\n\ny_2\n0.0287\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0475\n-0.5920\n0.3650\n0.5948\n0.0563\n-0.0801\n\n\nx_1\n0.5812\n0.4322\n-0.0088\n0.7377\n1.0076\n-0.1962\n\n\nx_2\n-0.0461\n0.1875\n-0.0228\n-0.1820\n0.7675\n0.6408\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0477\n\n\nx_1\n-0.5959\n\n\nx_2\n-1.2990\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n12.4282\n4.5316\n13.6935\n\n\nx_1\n4.5316\n2.1406\n6.4461\n\n\nx_2\n13.6935\n6.4461\n27.6478\n\n\n\n\n\n \n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n2.643850\n-2.886783\n0.193556\n0.212050\n0.165402\n-29540765662186335322106757120.000000\n06:27\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5339\n0.1210\n0.2783\n\n\nx_1\n0.5270\n-0.1285\n-0.0862\n\n\nx_2\n0.4976\n0.4502\n0.4350\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.0913\n0.4682\n1.5174\n\n\nx_1\n0.4682\n0.2037\n0.6628\n\n\nx_2\n1.5174\n0.6628\n2.1595\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0873\n\n\nx_1\n-0.2397\n\n\nx_2\n0.0401\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1690\n0.6841\n-0.3145\n\n\ny_1\n-0.5175\n0.1944\n0.3405\n\n\ny_2\n-0.1099\n0.0372\n0.1142\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0051\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0467\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0078\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1766\n\n\ny_1\n0.0070\n\n\ny_2\n0.0210\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0614\n-0.6233\n0.4026\n0.5869\n0.0964\n-0.0301\n\n\nx_1\n0.5842\n0.4624\n-0.0654\n0.7358\n0.9837\n-0.2582\n\n\nx_2\n-0.0390\n0.2132\n-0.0130\n-0.1775\n0.7580\n0.6438\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0447\n\n\nx_1\n-0.5957\n\n\nx_2\n-1.2961\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n13.1380\n4.8428\n14.4531\n\n\nx_1\n4.8428\n2.3677\n6.8288\n\n\nx_2\n14.4531\n6.8288\n28.0978"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html#loss-only-gap",
    "href": "presentations/Hainich_results_pres18_jan.html#loss-only-gap",
    "title": "Hainich with ERA-Interim",
    "section": "Loss only gap",
    "text": "Loss only gap\n\nlearn.model.use_smooth = True\nlearn.loss_func = KalmanLoss(only_gap=True)\n\n\ntrain_show_save(learn, 1,  5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-3.892116\n-2.872949\n0.120294\n0.204105\n-2.923830\n-85436991145187843135533744128.000000\n04:26\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.2027\n0.3761\n0.2969\n\n\nx_1\n0.3594\n0.3686\n0.3606\n\n\nx_2\n0.4679\n0.2295\n0.2852\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0220\n0.0113\n-0.0224\n\n\nx_1\n0.0113\n0.0064\n-0.0097\n\n\nx_2\n-0.0224\n-0.0097\n0.0290\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.3025\n\n\nx_1\n-0.5231\n\n\nx_2\n0.2064\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.1489\n0.3517\n-0.0053\n\n\ny_1\n0.2922\n0.1890\n-0.4105\n\n\ny_2\n0.3920\n-0.0638\n0.2473\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0001\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0054\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0003\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.2038\n\n\ny_1\n0.1323\n\n\ny_2\n-0.1988\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9856\n0.6178\n0.4694\n-0.5368\n0.5367\n-0.0159\n\n\nx_1\n0.6667\n0.5327\n-0.3882\n0.6907\n-0.0209\n-0.2365\n\n\nx_2\n0.0330\n-0.7175\n-0.1714\n0.2048\n-0.6491\n0.1349\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0922\n\n\nx_1\n-0.6262\n\n\nx_2\n0.2368\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n8.1062\n12.7070\n6.0908\n\n\nx_1\n12.7070\n21.6312\n10.2860\n\n\nx_2\n6.0908\n10.2860\n5.5679"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html#smoother",
    "href": "presentations/Hainich_results_pres18_jan.html#smoother",
    "title": "Hainich with ERA-Interim",
    "section": "Smoother",
    "text": "Smoother\n\nlearn1.model.use_smooth = True\n\n\ntrain_show_save(learn1, 1,  5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-415.543654\n-449.643886\n0.055635\n0.200575\n0.759219\n-45104963579554009935054372864.000000\n07:43\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.1815\n0.3765\n0.3138\n\n\nx_1\n0.3401\n0.3500\n0.3764\n\n\nx_2\n0.4904\n0.2297\n0.2438\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0491\n0.0170\n-0.0614\n\n\nx_1\n0.0170\n0.0127\n-0.0243\n\n\nx_2\n-0.0614\n-0.0243\n0.0889\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.3080\n\n\nx_1\n-0.5210\n\n\nx_2\n0.2044\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.1630\n0.3552\n-0.0202\n\n\ny_1\n0.3143\n0.1963\n-0.4623\n\n\ny_2\n0.4022\n-0.0653\n0.2466\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0001\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0054\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0003\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.2096\n\n\ny_1\n0.1459\n\n\ny_2\n-0.2079\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0006\n0.5988\n0.4673\n-0.5447\n0.5317\n-0.0122\n\n\nx_1\n0.6825\n0.5156\n-0.3620\n0.7188\n-0.0186\n-0.1805\n\n\nx_2\n0.0303\n-0.7130\n-0.1672\n0.2100\n-0.6278\n0.1642\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0933\n\n\nx_1\n-0.6332\n\n\nx_2\n0.2392\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n8.2929\n13.0004\n6.2311\n\n\nx_1\n13.0004\n22.1310\n10.5205\n\n\nx_2\n6.2311\n10.5205\n5.6921\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n# learn1.save(\"model_16_jan1\")\n\nPath('models/model_16_jan1.pth')"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#outline",
    "href": "presentations/presentation_bioclim_18_jan_23.html#outline",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\nKalman Filter\nPreliminary results\nNext steps"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#background",
    "href": "presentations/presentation_bioclim_18_jan_23.html#background",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Background",
    "text": "Background\n\nEC tower measures also meteorological variables (eg. air temperature, wind speed)\ntechnical issues (eg. broken sensor) result in meteo time series with gaps\nPresence of gaps is a problem in many EC data applications (eg. ecosystem modelling)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#dataset",
    "href": "presentations/presentation_bioclim_18_jan_23.html#dataset",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Dataset",
    "text": "Dataset\n\nFluxnet 2015 data from Hainich (more 20 years)\nGlobal meteo dataset (downscaled ERA-Interim from Fluxnet 2015)\nmeteorological measurements every 30 mins\nfocusing on 3 variables\n\nAir temperature: TA\nIncoming shortwave radiation: SW_IN\nVapour Pressure Deficit: VPD"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of gaps for all TA, SW_IN and VPD for all sites. 26 millions total obs"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of small gaps (&lt;1 week) for all TA, SW_IN and VPD for all sites. 26 millions total obs"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-fill-gaps",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-fill-gaps",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to fill gaps",
    "text": "How to fill gaps\n\nuse previous and following measurements for one variable and temporal auto-correlation (eg. diurnal cycles)\ncorrelation with other variables measures (eg. solar radiation and temperature)\nother measurements of meteo variables (eg. nearby station)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#state-of-the-art",
    "href": "presentations/presentation_bioclim_18_jan_23.html#state-of-the-art",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "State of the art",
    "text": "State of the art\nOneFlux pipeline (Fluxnet + ICOS + AmeriFlux)\n\nShort and medium gaps using Marginal Distribution Sampling (MDS)\nLong gaps filled with ERA data (global meteo dataset) using linear transformation to reduce site bias"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-mds-marginal-distribution-sampling-works",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-mds-marginal-distribution-sampling-works",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How MDS (Marginal Distribution Sampling) works",
    "text": "How MDS (Marginal Distribution Sampling) works\n\n\n\ntake a time window (7 days) around the gap\nuse 3 predictors variables (TA, SW_IN and VPD) and divide them in n discrete bins\nfor each bin (combination of conditions) find the average value of the missing variable\nfor each gap find the closest condition and fill with the average value\nif necessary increase the time window\nquality flag depends on the time window size"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-ta",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-ta",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling TA",
    "text": "MDS - gap filling TA"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-sw_in",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-sw_in",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling SW_IN",
    "text": "MDS - gap filling SW_IN"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-vpd",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-vpd",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling VPD",
    "text": "MDS - gap filling VPD"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#current-approaches-limitations",
    "href": "presentations/presentation_bioclim_18_jan_23.html#current-approaches-limitations",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Current approaches limitations",
    "text": "Current approaches limitations\n\ndon’t consider the observations before and after the gap\nEither MDS (variable correlation) or ERA data, don’t combine the information\nNo uncertainty for the predictions (only a quality flag)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#thesis-goal",
    "href": "presentations/presentation_bioclim_18_jan_23.html#thesis-goal",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Thesis goal",
    "text": "Thesis goal\n\ndevelop model to impute missing data in meteorological time series\ninclude all 3 imputation approaches\nprovide an uncertainty of the predictions"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-kalman-filter-works",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-kalman-filter-works",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How Kalman Filter works",
    "text": "How Kalman Filter works\n\n\nModels over time a latent variable (we are not observing it), the state of the system.\nThe current state \\(x_t\\) depends using:\n\nthe previous state \\(\\color{blue}{x_{t-1}}\\)\ncurrent observation \\(\\color{green}{y_t}\\)\ncontrol variable \\(\\color{purple}{c_t}\\) (ERA data)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#previous-state",
    "href": "presentations/presentation_bioclim_18_jan_23.html#previous-state",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "1. Previous state",
    "text": "1. Previous state\n\n\n\\[ x_t = A\\color{blue}{x_{t-1}} + \\varepsilon \\] where:\n\n\\(x_{t}\\) is the current state\n\\(\\color{blue}{x_{t-1}}\\) is the previous state\n\\(A\\) is a linear transformation of \\(\\color{blue}{x_{t-1}}\\)\n\\(\\varepsilon\\) is the “process” noise which is a random variable with a normal distribution with mean 0\n\n\n\n\n\n\nExample of Kalman Filter \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#current-observation",
    "href": "presentations/presentation_bioclim_18_jan_23.html#current-observation",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "2. Current Observation",
    "text": "2. Current Observation\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(\\color{green}{y_{t}}\\) is the current observation\n\\(H\\) is a linear transformation of \\(\\color{green}{y_{t}}\\)\n\\(\\nu\\) is the “observation” noise which is a random variable with a normal distribution with mean 0\n\nusing the rules of probabilistic inference if we observe \\(y_t\\) you can update the distribution of \\(x_t\\)\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gaps",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gaps",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gaps",
    "text": "Gaps\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#control-variable",
    "href": "presentations/presentation_bioclim_18_jan_23.html#control-variable",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "3. Control variable",
    "text": "3. Control variable\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + B\\color{purple}{c_t} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(B\\) is a linear transformation of \\(\\color{purple}{c_t}\\)\n\nUse the difference between current and previous value of control variable\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\) \\(B=[-1,1]\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#extra-variable-correlation",
    "href": "presentations/presentation_bioclim_18_jan_23.html#extra-variable-correlation",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Extra: Variable correlation",
    "text": "Extra: Variable correlation\n\n\nGap in two variables\n\n\n\n\n\n\n\nGap in only one variable"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-find-model-parameters",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-find-model-parameters",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to find model parameters",
    "text": "How to find model parameters\n\ncreate artificial gaps\npredicting gap in the model\ncompute the log likelihood of the predictions\nmaximise the log likelihood"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filter",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filter",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filter",
    "text": "Kalman Filter\npros:\n\nProbabilist model: the output of the model is a distribution of predictions, not a single value\nCombines all 3 approaches to gap filling in one model\ninterpretable paramters\ncomputationally efficient\n\ncons:\n\nkeeps tracks only of the local state"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #1",
    "text": "Kalman Filters gap #1"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-2",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-2",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #2",
    "text": "Kalman Filters gap #2"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-3",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-3",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #3",
    "text": "Kalman Filters gap #3"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#what-is-missing-in-the-model-development",
    "href": "presentations/presentation_bioclim_18_jan_23.html#what-is-missing-in-the-model-development",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "What is missing in the model development",
    "text": "What is missing in the model development\n\nimprove numerical stability of model (work in progress)\nfind optimal settings for training and inference\n\nn observations before after/gap\nhow to best generate artificial gaps\n\nHow to assess the model?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#use-of-gap-filling",
    "href": "presentations/presentation_bioclim_18_jan_23.html#use-of-gap-filling",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Use of gap filling",
    "text": "Use of gap filling\n\nwhat is the impact of better gap filling for data users?\n\nwhy better filling for short/medium gaps is useful\nhow can the uncertainty be used"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-settings",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-settings",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? settings",
    "text": "How to assess the model? settings\n\nhow to choose gap lengths?\nhow to choose number of variables missing?\nwhich variable to focus on?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-metrics",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-metrics",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Metrics",
    "text": "How to assess the model? Metrics\n\nRMSE - interpretation difficult as it’s relative to the variable\nr2 - gaps are often too short to interpret properly\n?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nTime series\n\n\n\n\n\n\nScatter plots"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nGap length / mean RMSE\n\n\n\n\n\n\nDistribution gaps vs filled"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#future-outlook",
    "href": "presentations/presentation_bioclim_18_jan_23.html#future-outlook",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Future outlook",
    "text": "Future outlook\n\noptimize performance model\nprovide pre-trained model on Fluxnet 2015 and then to fine-tune to local site\nprovide web-service for filling gaps\nreprocess Fluxnet 2015 dataset"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html",
    "href": "presentations/plots_for_presentation_18_jan_23.html",
    "title": "Code for presentation of 18th January",
    "section": "",
    "text": "::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n:::\nfrom meteo_imp.kalman.filter import *\nimport torch\nimport numpy as np\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.fastai import plot_variable\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\nfrom pyprojroot import here\nn_obs = 20\nbase_dir = here(\"presentations/plots_18_jan\")\nbase_dir.mkdir(exist_ok=True)\ndef save_plot(p, path):\n    f_name = base_dir / (path + \".vl.json\")\n    with open(f_name, 'w') as f:\n        f.write(p.to_json())\n    return f_name\nplt_props = {'width': 460, 'height': 400}"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#only-state",
    "href": "presentations/plots_for_presentation_18_jan_23.html#only-state",
    "title": "Code for presentation of 18th January",
    "section": "Only state",
    "text": "Only state\n\\[ x_t = A\\color{blue}{x_{-1}} + \\varepsilon\\]\n\ntorch.tensor([[0.]]).shape\n\ntorch.Size([1, 1])\n\n\n\nk0 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([0.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.3]]),\n    obs_cov = torch.tensor([[0.1]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.01]]),\n)\n\n\npred0 = k0.smooth(torch.ones(1,n_obs,1), torch.zeros(1,n_obs,1, dtype=bool), torch.zeros(1,n_obs,1))\n\n\ntime = pd.Index(range(n_obs), name=\"time\")\n\n\npred0.cov.shape\n\ntorch.Size([1, 20, 1, 1])\n\n\n\npred0_df = NormalsDf(pd.DataFrame(pred0.mean.squeeze(0), index=time, columns=[\"state\"]),\n                     pd.DataFrame(pred0.cov.squeeze(0).squeeze(-1), index=time, columns=[\"state\"]))\n\n\np0 = facet_variable(pred0_df.tidy(), ys=[\"mean\", \"mean\"], error=True, point=False, gap_area=False, props=plt_props)\np0\n\n\n\n\n\n\n\nsave_plot(p0, \"only_state\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/only_state.vl.json')"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#observations",
    "href": "presentations/plots_for_presentation_18_jan_23.html#observations",
    "title": "Code for presentation of 18th January",
    "section": "Observations",
    "text": "Observations\n\ntorch.tensor([[0.]]).shape\n\ntorch.Size([1, 1])\n\n\n\nk1 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nobs = 2 * torch.sin(torch.arange(n_obs) * 2 * torch.pi / (n_obs-2))\n\n\nplt.plot(obs)\n\n\n\n\n\nmask1 = torch.ones(1,n_obs,1, dtype=bool)\n\n\npred1 = k1.smooth(obs.unsqueeze(0).unsqueeze(-1), mask1, torch.zeros(1,n_obs,1))\n\n\ntime = pd.Index(range(n_obs), name=\"time\")\n\n\npred1.cov.shape\n\ntorch.Size([1, 20, 1, 1])\n\n\n\npred_df1 = NormalsDf(pd.DataFrame(pred1.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred1.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df1 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask1[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df1 = pd.merge(obs_df1, pred_df1, on=['time', 'variable'])\n\n\np1 = facet_variable(plot_df1, ys=[\"value\", \"mean\"], error=True, gap_area=False, props=plt_props)\np1\n\n\n\n\n\n\n\nsave_plot(p1, \"obs\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/obs.vl.json')"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#gaps",
    "href": "presentations/plots_for_presentation_18_jan_23.html#gaps",
    "title": "Code for presentation of 18th January",
    "section": "Gaps",
    "text": "Gaps\n\nk2 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nmask2 = torch.ones(1,n_obs,1, dtype=bool)\nmask2[0,11:16,0] = False\nmask2\n\ntensor([[[ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False],\n         [ True],\n         [ True],\n         [ True],\n         [ True]]])\n\n\n\npred2 = k2.smooth(obs.unsqueeze(0).unsqueeze(-1), mask2, torch.zeros(1,n_obs,1))\n\n\npred_df2 = NormalsDf(pd.DataFrame(pred2.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred2.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df2 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask2[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df2 = pd.merge(obs_df2, pred_df2, on=['time', 'variable'])\n\n\np2 = facet_variable(plot_df2, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 750, 'height': 500})\np2\n\n\n\n\n\n\n\nsave_plot(p2, \"gaps\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/gaps.vl.json')"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#control",
    "href": "presentations/plots_for_presentation_18_jan_23.html#control",
    "title": "Code for presentation of 18th January",
    "section": "Control",
    "text": "Control\n\nk3 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([-1., 1]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nmask3 = torch.ones(1,n_obs,1, dtype=bool)\nmask3[0,11:16,0] = False\nmask3\n\ntensor([[[ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False],\n         [ True],\n         [ True],\n         [ True],\n         [ True]]])\n\n\n\ncontr = torch.stack([(obs + 0.5)*1.2, ((obs + 0.5)*1.2).roll(-1)], dim=-1)\n\n\ncontr.shape\n\ntorch.Size([20, 2])\n\n\n\nplt.plot(contr[:,0])\nplt.plot(contr[:,1])\n\n\n\n\n\npred3 = k3.smooth(obs.unsqueeze(0).unsqueeze(-1), mask3, contr.unsqueeze(0))\n\n\npred_df3 = NormalsDf(pd.DataFrame(pred3.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred3.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df3 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask3[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df3 = pd.merge(obs_df3, pred_df3, on=['time', 'variable'])\n\n\np3 = (plot_variable(plot_df3, variable=\"var\", ys=[\"value\", \"mean\"], error=True, gap_area=False, props=plt_props) +\nalt.Chart(pd.DataFrame({'contr':contr[:,0], 'time': time, 'col': 'control'})).mark_line(strokeDash=[6,3], color='orange').encode(x='time', y='contr'))\np3\n\n\n\n\n\n\n\nsave_plot(p3, \"control\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/control.vl.json')"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#variables-correlation",
    "href": "presentations/plots_for_presentation_18_jan_23.html#variables-correlation",
    "title": "Code for presentation of 18th January",
    "section": "Variables correlation",
    "text": "Variables correlation\n\nk4 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([[.7], [.3]]),        \n    contr_matrix = torch.tensor([[0.]]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.diag(torch.tensor([0.1, 0.1])),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0., 0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.01]]),\n)\n\n\nk4\n\n\nKalman Filter (2 obs, 1 state, 1 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n0.5000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\n\n\n\n\ny_0\n0.7000\n\n\ny_1\n0.3000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\n\n\n\n\ny_0\n0.1000\n0.0000\n\n\ny_1\n0.0000\n0.1000\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\n\n\n\n\nx_0\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n0.0100\n\n\n\n\n\n \n\n\n\nmask4 = torch.ones(1,n_obs,2, dtype=bool)\nmask4[0,11:16,0] = False\nmask4\n\ntensor([[[ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True]]])\n\n\n\nobs4 = torch.stack([\n    obs *.7 + torch.randn_like(obs) * .1,\n    obs *.3 + torch.randn_like(obs) * .1\n], dim=-1)\n\n\nplt.plot(obs4)\nplt.plot(obs)\n\n\n\n\n\nobs4.shape\n\ntorch.Size([20, 2])\n\n\n\npred4 = k4.predict(obs4.unsqueeze(0), mask4, torch.zeros(1,n_obs,1))\n\n\npred_df4 = NormalsDf(pd.DataFrame(pred4.mean.squeeze(0), index=time, columns=[\"var1\", \"var2\"]),\n                     pd.DataFrame(pred4.std.squeeze(0).squeeze(-1), index=time, columns=[\"var1\", \"var2\"])).tidy()\n\n\nobs_df4_0 = pd.DataFrame({'time': time, 'var1': obs4[:,0].numpy(),\n                        'is_present': mask4[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \nobs_df4_1 = pd.DataFrame({'time': time, 'var2': obs4[:,1].numpy(),\n                        'is_present': mask4[0,:,1].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nobs_df4 = pd.concat([obs_df4_0, obs_df4_1])\n\n\nplot_df4 = pd.merge(obs_df4, pred_df4, on=['time', 'variable'])\n\n\np4 = facet_variable(plot_df4, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 350, 'height': 220}, n_cols=1)\np4\n\n\n\n\n\n\n\nsave_plot(p4, \"var_corr\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/var_corr.vl.json')\n\n\n\nmask5 = torch.ones(1,n_obs,2, dtype=bool)\nmask5[0,11:16,:] = False\nmask5\n\ntensor([[[ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [False, False],\n         [False, False],\n         [False, False],\n         [False, False],\n         [False, False],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True]]])\n\n\n\npred5 = k4.predict(obs4.unsqueeze(0), mask5, torch.zeros(1,n_obs,1))\n\n\npred_df5 = NormalsDf(pd.DataFrame(pred5.mean.squeeze(0), index=time, columns=[\"var1\", \"var2\"]),\n                     pd.DataFrame(pred5.std.squeeze(0).squeeze(-1), index=time, columns=[\"var1\", \"var2\"])).tidy()\n\n\nobs_df5_0 = pd.DataFrame({'time': time, 'var1': obs4[:,0].numpy(),\n                        'is_present': mask5[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \nobs_df5_1 = pd.DataFrame({'time': time, 'var2': obs4[:,1].numpy(),\n                        'is_present': mask5[0,:,1].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nobs_df5 = pd.concat([obs_df5_0, obs_df5_1])\n\n\nplot_df5 = pd.merge(obs_df4, pred_df5, on=['time', 'variable'])\n\n\np5 = facet_variable(plot_df5, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 350, 'height': 220}, n_cols=1)\np5\n\n\n\n\n\n\n\nsave_plot(p5, \"var_no_corr\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/var_no_corr.vl.json')"
  },
  {
    "objectID": "var_distribution.html",
    "href": "var_distribution.html",
    "title": "Variable distribution",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport polars as pl\n\nfrom meteo_imp.fluxnet.gap_finder import scan_fluxnet_csv\n\nfrom meteo_imp.utils import cache_disk"
  },
  {
    "objectID": "var_distribution.html#load",
    "href": "var_distribution.html#load",
    "title": "Variable distribution",
    "section": "Load",
    "text": "Load\nload Hainich dataset\n\nhai_path = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\"\n# hai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=20_000)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = scan_fluxnet_csv(hai_path, convert_dates=True).rename(meteo_vars).select([pl.col(\"end\").alias(\"time\"), *meteo_vars.values()])\n\nhai.fetch(10)\n\n\n\n\nshape: (10, 5)\ntime\nTA\nSW_IN\nLW_IN\nVPD\ndatetime[μs]\nf64\nf64\nf64\nf64\n2000-01-01 00:30:00\n-0.6\n0.0\n302.475\n0.222\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.09\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.11\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n2000-01-01 03:00:00\n-0.4\n0.0\n301.677\n0.111\n2000-01-01 03:30:00\n-0.36\n0.0\n301.677\n0.109\n2000-01-01 04:00:00\n-0.35\n0.0\n301.677\n0.107\n2000-01-01 04:30:00\n-0.28\n0.0\n308.046\n0.122\n2000-01-01 05:00:00\n-0.27\n0.0\n308.046\n0.138\n\n\n\n\n\nhai_td = hai.melt('time')\n\n\nhai_td.fetch(3)\n\n\n\n\nshape: (12, 3)\ntime\nvariable\nvalue\ndatetime[μs]\nstr\nf64\n2000-01-01 00:30:00\n\"TA\"\n-0.6\n2000-01-01 01:00:00\n\"TA\"\n-0.65\n2000-01-01 01:30:00\n\"TA\"\n-0.58\n2000-01-01 00:30:00\n\"SW_IN\"\n0.0\n2000-01-01 01:00:00\n\"SW_IN\"\n0.0\n2000-01-01 01:30:00\n\"SW_IN\"\n0.0\n2000-01-01 00:30:00\n\"LW_IN\"\n302.475\n2000-01-01 01:00:00\n\"LW_IN\"\n302.475\n2000-01-01 01:30:00\n\"LW_IN\"\n301.677\n2000-01-01 00:30:00\n\"VPD\"\n0.222\n2000-01-01 01:00:00\n\"VPD\"\n0.122\n2000-01-01 01:30:00\n\"VPD\"\n0.09"
  },
  {
    "objectID": "var_distribution.html#distribution",
    "href": "var_distribution.html#distribution",
    "title": "Variable distribution",
    "section": "Distribution",
    "text": "Distribution\n\nhai.drop('time').collect().to_pandas().hist(figsize=(15,10));\n\n\n\n\n\n# should to the binning before the plot\n# alt.Chart(hai_td.collect().to_pandas()).mark_line().encode(\n#     x = 'value',\n#     y = 'density()',\n#     facet = alt.Facet('variable', columns=2)\n# )"
  },
  {
    "objectID": "var_distribution.html#correlation",
    "href": "var_distribution.html#correlation",
    "title": "Variable distribution",
    "section": "Correlation",
    "text": "Correlation\nCode inspired from source: https://towardsdatascience.com/altair-plot-deconstruction-visualizing-the-correlation-structure-of-weather-data-38fb5668c5b1\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_p.corr()\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\n\n\nTA\n1.000000\n0.432321\n0.639556\n0.735412\n\n\nSW_IN\n0.432321\n1.000000\n0.126278\n0.533506\n\n\nLW_IN\n0.639556\n0.126278\n1.000000\n0.270424\n\n\nVPD\n0.735412\n0.533506\n0.270424\n1.000000\n\n\n\n\n\n\n\n\ndef corr_mask(size):\n    corr_mask = np.zeros((size,size), dtype=bool)\n    for i in range(size):\n        for j in range(size):\n            corr_mask[i,j] = True if i &gt;= j else False\n    return corr_mask\n\n\ncorr_mask(len(hai_p.columns))\n\narray([[ True, False, False, False],\n       [ True,  True, False, False],\n       [ True,  True,  True, False],\n       [ True,  True,  True,  True]])\n\n\n\nhai_p = hai_p[sorted(hai_p.columns)] # need to properly plot half a corr matrix\n\n\ncor_hai = (hai_p\n              .corr().mask(~corr_mask(len(hai_p.columns))).stack()\n              .reset_index()     # The stacking results in an index on the correlation values, we need the index as normal columns for Altair\n              .rename(columns={0: 'correlation', 'level_0': 'variable', 'level_1': 'variable2'}))\ncor_hai['correlation_label'] = cor_hai['correlation'].map('{:.2f}'.format)  # Round to 2 decimal\ncor_hai\n\n\n\n\n\n\n\n\nvariable\nvariable2\ncorrelation\ncorrelation_label\n\n\n\n\n0\nLW_IN\nLW_IN\n1.000000\n1.00\n\n\n1\nSW_IN\nLW_IN\n0.126278\n0.13\n\n\n2\nSW_IN\nSW_IN\n1.000000\n1.00\n\n\n3\nTA\nLW_IN\n0.639556\n0.64\n\n\n4\nTA\nSW_IN\n0.432321\n0.43\n\n\n5\nTA\nTA\n1.000000\n1.00\n\n\n6\nVPD\nLW_IN\n0.270424\n0.27\n\n\n7\nVPD\nSW_IN\n0.533506\n0.53\n\n\n8\nVPD\nTA\n0.735412\n0.74\n\n\n9\nVPD\nVPD\n1.000000\n1.00\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef compute_2d_histogram(var1, var2, df, density=True, bins=20):\n    H, xedges, yedges = np.histogram2d(df[var1], df[var2], bins=bins, density=density)\n    H[H == 0] = np.nan\n\n    # Create a nice variable that shows the bin boundaries\n    \n    x_width = xedges[1] - xedges[0] # all bins have same width\n    xedges = pd.Series(xedges[:-1] + x_width /2)\n    \n    y_width = yedges[1] - yedges[0] # all bins have same width\n    yedges = pd.Series(yedges[:-1] + y_width /2)\n    \n    # Cast to long format using melt\n    res = pd.DataFrame(H, \n                       index=yedges, \n                       columns=xedges).reset_index().melt(\n                            id_vars='index'\n                       ).rename(columns={'index': 'value2', \n                                         'value': 'count',\n                                         'variable': 'value'})\n    \n\n    res['variable'] = var1\n    res['variable2'] = var2 \n    return res.dropna() # Drop all combinations for which no values where found\n\n\nh, xe, ye = np.histogram2d(hai_p['VPD'], hai_p['TA'])\n\n\nx_width = xe[1] - xe[0] # all bins have same width\nxe = pd.Series(xe + x_width /2)\n\n\nh.shape\n\n(10, 10)\n\n\n\nxe\n\n0      2.38335\n1      7.15005\n2     11.91675\n3     16.68345\n4     21.45015\n5     26.21685\n6     30.98355\n7     35.75025\n8     40.51695\n9     45.28365\n10    50.05035\ndtype: float64\n\n\n\nhai_binned = pd.concat([compute_2d_histogram(var1, var2, hai_p) for var1 in meteo_vars.values() for var2 in meteo_vars.values()])\nhai_binned.head()\n\n\n\n\n\n\n\n\nvalue2\nvalue\ncount\nvariable\nvariable2\n\n\n\n\n0\n-17.19025\n-17.19025\n0.000083\nTA\nTA\n\n\n21\n-14.47075\n-14.47075\n0.000224\nTA\nTA\n\n\n42\n-11.75125\n-11.75125\n0.000574\nTA\nTA\n\n\n63\n-9.03175\n-9.03175\n0.001188\nTA\nTA\n\n\n84\n-6.31225\n-6.31225\n0.003781\nTA\nTA\n\n\n\n\n\n\n\n\n# Define selector\nvar_sel_cor = alt.selection_single(fields=['variable', 'variable2'], clear=False, \n                                  init={'variable': 'TA', 'variable2': 'SW_IN'})\n\n# Define correlation heatmap\nbase = alt.Chart(cor_hai).encode(\n    x='variable2:O',\n    y='variable:O'    \n)\n\ntext = base.mark_text().encode(\n    text='correlation_label',\n    color=alt.condition(\n        alt.datum.correlation &gt; 0.5, \n        alt.value('white'),\n        alt.value('black')\n    )\n)\n\ncor_plot = base.mark_rect().encode(\n    color=alt.condition(var_sel_cor, alt.value('pink'), 'correlation:Q')\n).add_selection(var_sel_cor)\n\n# Define 2d binned histogram plot\nscat_plot = alt.Chart(hai_binned).transform_filter(\n    var_sel_cor\n).mark_rect().encode(\n    alt.X('value:N', axis=alt.Axis(format=\".4\")), \n    alt.Y('value2:N', axis=alt.Axis(format=\".4\"), sort='descending'),\n    alt.Color('count:Q', scale=alt.Scale(scheme='blues'))\n)\n\n# Combine all plots. hconcat plots both side-by-side \nalt.hconcat((cor_plot + text).properties(width=350, height=350), scat_plot.properties(width=350, height=350)).resolve_scale(color='independent')\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_td_p = hai_td.collect().to_pandas().set_index('time')"
  },
  {
    "objectID": "fluxnet/analyze_gaps_fluxnet.html",
    "href": "fluxnet/analyze_gaps_fluxnet.html",
    "title": "Analyze gaps fluxnet",
    "section": "",
    "text": "from IPython.display import display\nfrom ipywidgets import widgets, interact\nfrom pathlib import Path\nimport polars as pl\nfrom datetime import datetime\nfrom fastcore.utils import * # support of ls for paths\nimport matplotlib.pyplot as plt\nimport altair as alt\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\nsite_info = pl.read_parquet(out_dir / \"../site_info.parquet\").select([\n    pl.col(\"start\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"end\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"site\").cast(pl.Categorical).sort()\n])\nsite_info.head()\n\n\n\n\nshape: (5, 3)\nstart\nend\nsite\ndatetime[μs]\ndatetime[μs]\ncat\n2009-01-01 00:30:00\n2012-01-01 00:00:00\n\"AR-SLu\"\n2009-01-01 00:30:00\n2013-01-01 00:00:00\n\"AR-Vir\"\n2002-01-01 00:30:00\n2013-01-01 00:00:00\n\"AT-Neu\"\n2007-01-01 00:30:00\n2010-01-01 00:00:00\n\"AU-Ade\"\n2010-01-01 00:30:00\n2015-01-01 00:00:00\n\"AU-ASM\"\ndef duration_n_obs(duration):\n    \"converts a duration into a n of fluxnet observations\"\n    return abs(int(duration.total_seconds() / (30 * 60)))\nduration_n_obs(site_info[1, \"start\"] - site_info[1, \"end\"])\n\n70127\nduration_n_obs(site_info[0, \"start\"] - site_info[0, \"end\"])\n\n52559\nsite_info.select((pl.col(\"end\")-pl.col(\"start\")).dt.minutes() // 30).sum()\n\n\n\n\nshape: (1, 1)\nend\ni64\n26175287\nsp = site_info.to_pandas()\n((sp.end - sp.start).dt.total_seconds() / (30*60)).astype(int).sum()\n\n26175287\n# maybe this code should actually go in 20_gap_finding\nfiles = out_dir.ls()\nfiles.sort() # need to sort to match the site_info\nsites = []\nfor i, path in enumerate(files):\n    sites.append(pl.scan_parquet(path).with_columns([\n        pl.lit(site_info[i, \"site\"]).alias(\"site\"),\n        pl.lit(duration_n_obs(site_info[i, \"start\"] -  site_info[i, \"end\"])).alias(\"total_obs\"),\n        pl.col(\"TIMESTAMP_END\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\").alias(\"end\"),\n    ]).drop(\"TIMESTAMP_END\"))\ngap_stat = pl.concat(sites)\ngap_stat.head().fetch(5)\n\n\n\n\nshape: (5, 5)\ngap_len\nvariable\nsite\ntotal_obs\nend\nu32\nstr\nstr\ni32\ndatetime[μs]\n16992\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-01-01 00:30:00\n5\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-12-21 11:00:00\n1\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-12-21 17:00:00\n1\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2010-01-06 13:00:00\n3\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2010-01-07 13:00:00\ndef filter_variables(variables = [\"TA_F_QC\", \"SW_IN_QC\", \"LW_IN_QC\", \"VPD_F_QC\"]):\n    expr = False\n    for var in variables:\n        expr |= pl.col(\"variable\") == var\n    return expr\ndef pl_in(col, values):\n    expr = False\n    for val in values:\n        expr |= pl.col(col) == val\n    return expr\nsome sites have a lot of data missing, with the avg gap length of several years, so is seems that the year can have an impact\nImportant! here the 3 possibles gap value of a QC variable are considered as one (null, 1, 2) we should co\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).groupby(\"site\").agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()).alias(\"frac_gap\")\n]).collect().describe()\n\n\n\n\nshape: (7, 4)\ndescribe\nsite\nmean\nfrac_gap\nstr\nstr\nf64\nf64\n\"count\"\n\"205\"\n205.0\n205.0\n\"null_count\"\n\"0\"\n0.0\n0.0\n\"mean\"\nnull\n1327.107552\n0.198846\n\"std\"\nnull\n4421.109085\n0.256043\n\"min\"\n\"AR-SLu\"\n1.0\n0.000011\n\"max\"\n\"ZM-Mon\"\n52608.0\n2.299027\n\"median\"\nnull\n214.365854\n0.141903"
  },
  {
    "objectID": "fluxnet/analyze_gaps_fluxnet.html#interactive-histograms",
    "href": "fluxnet/analyze_gaps_fluxnet.html#interactive-histograms",
    "title": "Analyze gaps fluxnet",
    "section": "Interactive histograms",
    "text": "Interactive histograms\n\nall_vars = gap_stat.select(pl.col(\"variable\").unique().sort()).collect()[\"variable\"]\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var):\n    ta_gaps = gap_stat.filter(\n        pl.col(\"variable\") == var \n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n\n\n\nall_sites = gap_stat.select(pl.col(\"site\").unique().sort()).collect()[\"site\"]\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site, small=False):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.title(f\"{site}: {var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n\n\n\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var, small=False):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).collect().to_pandas().hist(\"gap_len\", bins=30)\n    plt.title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    plt.yscale('log')\n    plt.xscale('log') \n\n\n\n\n\nfrom fastai.vision.data import get_grid\nfrom pyprojroot import here\n\n\ndef plot_var_dist(var, small=False, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).with_column(pl.col(\"gap_len\") / (24 *2 * 7)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    if not small: ax.set_yscale('log')\n    ax.set_xlabel(\"gap length (weeks)\")\n    ax.set_ylabel(f\"{'Log' if not small else ''} n gaps\")\n    # plt.xscale('log') \n\n\nfor ax, var in zip(get_grid(3,1,3, figsize=(15,8), sharey=True), ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC']):\n    plot_var_dist(var, ax=ax)\nplt.savefig(here(\"analysis/presentations/plots_18_jan/gap_len_dist.png\", warn=False))\n\n\n\n\n\ndef plot_var_dist_small(var, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) \n    ).with_column(pl.col(\"gap_len\") / (24 *2)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - gap len &lt; 1 week\")\n    ax.set_xlabel(\"gap length (days)\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n\n\nfor ax, var in zip(get_grid(3,1,3, figsize=(15,8), sharey=True), ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC']):\n    plot_var_dist_small(var, ax=ax)\nplt.savefig(here(\"analysis/presentations/plots_18_jan/gap_len_dist_small.png\", warn=False))\n\n\n\n\n\ndef plot_var_dist_cum(var, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    \n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) \n    ).collect() #.to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    bins = pl.cut(ta_gaps[\"gap_len\"], bins = pl.arange(0, 24 * 2 * 7, (24 * 2 * 7) // 50, eager=True))\n    return ta_gaps\n    ax.set_title(f\"{var} - gap len &lt; 1 week\")\n    ax.set_xlabel(\"gap length (days)\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log')"
  },
  {
    "objectID": "fluxnet/analyze_gaps_fluxnet.html#difference-sites",
    "href": "fluxnet/analyze_gaps_fluxnet.html#difference-sites",
    "title": "Analyze gaps fluxnet",
    "section": "Difference sites",
    "text": "Difference sites\n\nvar_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == 'TA_F_QC')\n    ).filter(\n        pl.col(\"gap_len\") &lt; 20000\n    ).sort(pl.col(\"gap_len\")\n        \n    ).collect().to_pandas()\n\n\nalt.data_transformers.enable('data_server')\n\n\n \nalt.Chart(var_gaps).mark_boxplot().encode(\n    y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n    x='gap_len'\n)\n\n\nwidgets.IntSlider?\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var, max_len=widgets.IntSlider(1000, 100, 20_000, 100)):\n    var_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; max_len\n    ).collect().to_pandas()\n    \n    display(alt.Chart(var_gaps).mark_boxplot().encode(\n        y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n        x='gap_len'\n    ))\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(pl.col(\"gap_len\").sum()).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).groupby(\"variable\").agg(pl.col(\"gap_len\").mean()).collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).filter(pl.col(\"variable\") == \"TA_F_QC\").collect()"
  },
  {
    "objectID": "fluxnet/extract_gap_all_fluxnet.html",
    "href": "fluxnet/extract_gap_all_fluxnet.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.fluxnet.gap_finder import *\n\nlinks are obtained from this https://fluxnet.org/data-and-manifest/ page(requires login) by running in the browser console this code\nvar x = document.querySelectorAll(\"a\");\nvar myarray = []\nfor (var i=0; i&lt;x.length; i++){\nvar nametext = x[i].textContent;\nvar cleantext = nametext.replace(/\\s+/g, ' ').trim();\nvar cleanlink = x[i].href;\nmyarray.push([cleantext,cleanlink]);\n};\nfunction make_table() {\n    var table = '&lt;table&gt;&lt;thead&gt;&lt;th&gt;Links&lt;/th&gt;&lt;/thead&gt;&lt;tbody&gt;';\n   for (var i=0; i&lt;myarray.length; i++) {\n            table += '&lt;tr&gt;&lt;td&gt;'+myarray[i][1]+'&lt;/td&gt;&lt;/tr&gt;';\n    };\n \n    var w = window.open(\"\");\nw.document.write(table); \n}\nmake_table()\ncode inspired from https://towardsdatascience.com/quickly-extract-all-links-from-a-web-page-using-javascript-and-the-browser-console-49bb6f48127b\nand then doing some smart copy pasting\nacually download in parallel all files with, so is faster than download with python\nparallel -a fluxnet_parallel_wget.txt --jobs 10 wget\n\nfrom fluxnet_links import all_fluxnet_link\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\n\ntest_file = Path(\"../../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip\")\ntmp_dir = Path(\"/tmp\")\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\n\n\nsite_info = download_and_find_gaps(all_fluxnet_link, download_dir, out_dir, tmp_dir)\n\n\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/run/media/simone/Simone DATI/fluxnet_all/FLX_AR-SLu_FLUXNET2015_FULLSET_2009-2011_1-4.zip'\n\n\n\nsite_info\n\n\n\n\nshape: (206, 3)\nstart\nend\nsite\ni64\ni64\nstr\n200901010030\n201201010000\n\"AR-SLu\"\n200901010030\n201301010000\n\"AR-Vir\"\n200201010030\n201301010000\n\"AT-Neu\"\n200701010030\n201001010000\n\"AU-Ade\"\n201001010030\n201501010000\n\"AU-ASM\"\n201001010030\n201501010000\n\"AU-Cpr\"\n201201010030\n201501010000\n\"AU-Cum\"\n200701010030\n201401010000\n\"AU-DaP\"\n200801010030\n201501010000\n\"AU-DaS\"\n200801010030\n201501010000\n\"AU-Dry\"\n201101010030\n201401010000\n\"AU-Emr\"\n200601010030\n200901010000\n\"AU-Fog\"\n...\n...\n...\n200301010030\n200401010000\n\"US-Wi1\"\n200301010030\n200401010000\n\"US-Wi2\"\n200201010030\n200501010000\n\"US-Wi3\"\n200201010030\n200601010000\n\"US-Wi4\"\n200401010030\n200501010000\n\"US-Wi5\"\n200201010030\n200401010000\n\"US-Wi6\"\n200501010030\n200601010000\n\"US-Wi7\"\n200201010030\n200301010000\n\"US-Wi8\"\n200401010030\n200601010000\n\"US-Wi9\"\n200401010030\n201501010000\n\"US-Wkg\"\n201101010030\n201401010000\n\"US-WPT\"\n200001010030\n201001010000\n\"ZM-Mon\"\n\n\n\n\n\nsite_info.write_parquet(out_dir / \"../site_info.parquet\")"
  }
]