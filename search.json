[
  {
    "objectID": "Multi latent - GPFA Hainich.html",
    "href": "Multi latent - GPFA Hainich.html",
    "title": "Multiple Latent",
    "section": "",
    "text": "Trying to use more than 1 latent variable\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\n\ndef reset_seed():\n    torch.manual_seed(27);\n    np.random.seed(27);\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      LW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      302.475\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      302.475\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      301.677\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      301.677\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      301.677\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-05 02:00:00\n      4.74\n      0.0\n      330.202\n      1.191\n    \n    \n      2000-01-05 02:30:00\n      4.75\n      0.0\n      330.202\n      1.057\n    \n    \n      2000-01-05 03:00:00\n      4.76\n      0.0\n      330.202\n      0.935\n    \n    \n      2000-01-05 03:30:00\n      4.62\n      0.0\n      330.202\n      1.162\n    \n    \n      2000-01-05 04:00:00\n      4.51\n      0.0\n      330.202\n      1.636\n    \n  \n\n200 rows × 4 columns\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values())\n\n\ncache_file_gaps = cache_path / \"hai_diff_latents.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\n@cache_disk(cache_file_gaps)\ndef compute_multiple_latent():\n    hai_r_gaps = [GPFAImputation(\n        data_r_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_r_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    hai_c_gaps = [GPFAImputationExplorer(\n        data_c_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_c_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    return hai_r_gaps, hai_c_gaps\n\n\nhai_r_gaps, hai_c_gaps = compute_multiple_latent()\n\n\n\n\n\n\nhai_r_gaps\n\n[<gpfa_imputation.results.ImputationResult>,\n <gpfa_imputation.results.ImputationResult>,\n <gpfa_imputation.results.ImputationResult>]\n\n\n\nhai_r_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.6601\n    \n    \n      SW_IN\n      0.0865\n    \n    \n      LW_IN\n      -0.0341\n    \n    \n      VPD\n      0.9569\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.5224\n      °C\n    \n    \n      SW_IN\n      46.1695\n      W m-2\n    \n    \n      LW_IN\n      21.2045\n      W m-2\n    \n    \n      VPD\n      0.0578\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      0\n      z0\n    \n  \n  \n    \n      TA\n      0.8015\n    \n    \n      SW_IN\n      0.4251\n    \n    \n      LW_IN\n      -0.1879\n    \n    \n      VPD\n      1.0596\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      5.6057\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.4174\n    \n    \n      SW_IN\n      0.8049\n    \n    \n      LW_IN\n      0.9361\n    \n    \n      VPD\n      0.0103\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0224\n    \n  \n\n \n\n\n\nai_r_gaps[0].plot_pred()\n\n\nhai_r_gaps[1].display_results()\n\n\nhai_r_gaps[2].display_results()\n\n\n\n\n\nhai_c_gaps[0].display_results()\n\n\nhai_c_gaps[1].display_results()\n\n\nhai_c_gaps[2].display_results()"
  },
  {
    "objectID": "Additional latent kernel.html",
    "href": "Additional latent kernel.html",
    "title": "Additional latent kernel",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.gpfa.data_preparation import *\nfrom meteo_imp.gpfa.results import *\nfrom meteo_imp.gpfa.gpfa import *\nfrom meteo_imp.gpfa.results import _display_as_row\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import *\nfrom gpytorch.kernels import *\nimport gpytorch\n\ncp = here() / \".cache\" / \"add_kernel.pickle\"\n\n\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=1000)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-21 18:00:00\n      -0.81\n      0.0\n      1.892\n    \n    \n      2000-01-21 18:30:00\n      -0.81\n      0.0\n      1.744\n    \n    \n      2000-01-21 19:00:00\n      -0.90\n      0.0\n      2.118\n    \n    \n      2000-01-21 19:30:00\n      -1.09\n      0.0\n      2.528\n    \n    \n      2000-01-21 20:00:00\n      -1.23\n      0.0\n      2.677\n    \n  \n\n1000 rows × 3 columns\n\n\n\n\n\n\n\ndef _get_lengthscale_info(kernel: RBFKernel, suffix=''):\n        ls = kernel.lengthscale.detach().item()\n        return pd.DataFrame({\n            'lengthscale'+suffix: [ls]\n        }) \n\n\ndef _get_outscale_info(kernel: ScaleKernel, suffix=''):\n        ls = kernel.outputscale.detach().item()\n        return pd.DataFrame({\n            'outscale'+suffix: [ls]\n        }) \n\n\nclass GPFAMultiRbf(GPFA):\n    latent_kernel = lambda x: AdditiveKernel(RBFKernel(), ScaleKernel(RBFKernel()))\n    \n    \n    def get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -> dict[str, pd.DataFrame]:\n        \"Model info for a GPFA with a RBFKernel\"\n        out = {}\n\n        latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n\n        out[\"Lambda\"] = pd.concat([\n            None if var_names is None else pd.Series(var_names),\n            pd.DataFrame(\n                self.covar_module.Lambda.detach().cpu().numpy(),\n                columns=latent_names)],\n            axis=1)\n\n        ls_all = []\n        l_kernels = self.covar_module.latent_kernels\n        for kernel in list(l_kernels):\n            ls_s = [_get_lengthscale_info(kernel.kernels[0], \"_k0\")]\n            ls_s.append(_get_lengthscale_info(kernel.kernels[1].base_kernel, \"_k1\")) # this is a scale kernel\n            ls_all.append(pd.concat(ls_s, axis=1)) # attach multiple columns\n\n        ls_all = pd.concat(ls_all)\n        ls_all.insert(0, 'latent', latent_names)\n        out[\"Lengthscale\"] = ls_all\n        \n        os_all = []\n        l_kernels = self.covar_module.latent_kernels\n        for kernel in list(l_kernels):\n            os_s = [pd.DataFrame({'outscale_k0': [1] })] # there is no scaling here\n            os_s.append(_get_outscale_info(kernel.kernels[1], \"_k1\"))\n            os_all.append(pd.concat(os_s, axis=1)) # attach multiple columns\n\n        os_all = pd.concat(os_all)\n        os_all.insert(0, 'latent', latent_names)\n        out[\"Outscale\"] = os_all\n\n        psi = self.covar_module.psi.detach().cpu().numpy()\n        out[\"Psi\"] = pd.DataFrame({\n            'variable': var_names,\n            'psi': psi \n        })\n\n        out[\"Likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.item()]})\n\n        return out\n\n\nk = GPFAMultiRbf(torch.tensor([1,2,3]), torch.tensor([1,2,3]), gpytorch.likelihoods.GaussianLikelihood(), 2)\n\n\nk\n\nGPFAMultiRbf(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): AdditiveKernel(\n        (kernels): ModuleList(\n          (0): RBFKernel(\n            (raw_lengthscale_constraint): Positive()\n          )\n          (1): ScaleKernel(\n            (base_kernel): RBFKernel(\n              (raw_lengthscale_constraint): Positive()\n            )\n            (raw_outputscale_constraint): Positive()\n          )\n        )\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\n\n_display_as_row(k.get_info())\n\n\n  Lambda \n\n  \n    \n      z0\n    \n  \n  \n    \n      0.5875\n    \n    \n      0.4568\n    \n  \n\n  Lengthscale \n\n  \n    \n      latent\n      lengthscale_k0\n      lengthscale_k1\n    \n  \n  \n    \n      z0\n      0.6931\n      0.6931\n    \n  \n\n  Outscale \n\n  \n    \n      latent\n      outscale_k0\n      outscale_k1\n    \n  \n  \n    \n      z0\n      1\n      0.6931\n    \n  \n\n  Psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      None\n      0.6931\n    \n    \n      None\n      0.6931\n    \n  \n\n  Likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.6932\n    \n  \n\n \n\n\n\nreset_seed()\ndata = GPFADataTest(hai[:150]).add_random_missing()\n\n\n\n\n\n@cache_disk(cp)\ndef compute_small():\n    reset_seed()\n    data = GPFADataTest(hai[:150]).add_random_missing()\n    imp = GPFAImputationExplorer(data.data, latent_dims=3, model=GPFAMultiRbf)\n    return imp.fit().to_result(data.data_compl_tidy)\n\n\nres_small = compute_small()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n@cache_disk(cp)\ndef compute_large():\n    reset_seed()\n    data = GPFADataTest(hai[:1500]).add_random_missing()\n    imp = GPFAImputationExplorer(data.data, latent_dims=2, model=GPFAMultiRbf)\n    return imp.fit(), data\n\n\n# imp_large = compute_large()\n\n\n\n\n\n\nres_small.display_results(plot_args={'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9917\n    \n    \n      SW_IN\n      0.9725\n    \n    \n      VPD\n      0.9708\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n    \n  \n  \n    \n      TA\n      0.0842\n    \n    \n      SW_IN\n      6.1635\n    \n    \n      VPD\n      0.0437\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9833\n    \n    \n      SW_IN\n      0.9712\n    \n    \n      VPD\n      0.9618\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n    \n  \n  \n    \n      TA\n      0.1155\n    \n    \n      SW_IN\n      8.3501\n    \n    \n      VPD\n      0.0538\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      0\n      z0\n      z1\n      z2\n    \n  \n  \n    \n      TA\n      -0.1227\n      0.8634\n      0.1707\n    \n    \n      SW_IN\n      0.8996\n      0.5550\n      -0.6289\n    \n    \n      VPD\n      0.5291\n      0.7391\n      0.5174\n    \n  \n\n  Lengthscale \n\n  \n    \n      latent\n      lengthscale_k0\n      lengthscale_k1\n    \n  \n  \n    \n      z0\n      5.7209\n      5.1237\n    \n    \n      z1\n      6.9906\n      6.2228\n    \n    \n      z2\n      4.5014\n      4.3961\n    \n  \n\n  Outscale \n\n  \n    \n      latent\n      outscale_k0\n      outscale_k1\n    \n  \n  \n    \n      z0\n      1\n      0.1827\n    \n    \n      z1\n      1\n      0.1025\n    \n    \n      z2\n      1\n      0.2387\n    \n  \n\n  Psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0014\n    \n    \n      SW_IN\n      0.0344\n    \n    \n      VPD\n      0.0238\n    \n  \n\n  Likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0050"
  },
  {
    "objectID": "Log transform - Multi latent - Imputation GPFA - Hainich.html",
    "href": "Log transform - Multi latent - Imputation GPFA - Hainich.html",
    "title": "Log Transform",
    "section": "",
    "text": "Trying to lo transform\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import *\n\n\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'log_SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'log_VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-05 02:00:00\n      4.74\n      0.0\n      1.191\n    \n    \n      2000-01-05 02:30:00\n      4.75\n      0.0\n      1.057\n    \n    \n      2000-01-05 03:00:00\n      4.76\n      0.0\n      0.935\n    \n    \n      2000-01-05 03:30:00\n      4.62\n      0.0\n      1.162\n    \n    \n      2000-01-05 04:00:00\n      4.51\n      0.0\n      1.636\n    \n  \n\n200 rows × 3 columns\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).log_transform(['SW_IN', 'VPD']).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).log_transform(['SW_IN', 'VPD']).add_gap(15, units.keys())\n\n\ndata_r_gaps.data\n\n\n\n\n\n  \n    \n      \n      TA\n      log_SW_IN\n      log_VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      0.200489\n    \n    \n      2000-01-01 01:00:00\n      NaN\n      NaN\n      NaN\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      0.086178\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      0.104360\n    \n    \n      2000-01-01 02:30:00\n      NaN\n      0.0\n      0.097127\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-04 01:00:00\n      NaN\n      NaN\n      NaN\n    \n    \n      2000-01-04 01:30:00\n      2.13\n      NaN\n      0.530628\n    \n    \n      2000-01-04 02:00:00\n      2.10\n      0.0\n      0.594983\n    \n    \n      2000-01-04 02:30:00\n      2.19\n      0.0\n      0.668854\n    \n    \n      2000-01-04 03:00:00\n      2.27\n      0.0\n      NaN\n    \n  \n\n150 rows × 3 columns\n\n\n\n\ncache_file_gaps = cache_path / \"hai_lo_transform.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\n@cache_disk(cache_file_gaps)\ndef compute_multiple_latent():\n    hai_r_gaps = [GPFAImputationExplorer(\n        data_r_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_r_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    hai_c_gaps = [GPFAImputationExplorer(\n        data_c_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_c_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    return hai_r_gaps, hai_c_gaps # cc\n\n\nhai_r_gaps, hai_c_gaps = compute_multiple_latent()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhai_r_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9929\n    \n    \n      log_SW_IN\n      0.9888\n    \n    \n      log_VPD\n      0.9838\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0778\n      °C\n    \n    \n      log_SW_IN\n      0.1859\n      W m-2\n    \n    \n      log_VPD\n      0.0244\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9858\n    \n    \n      log_SW_IN\n      0.9867\n    \n    \n      log_VPD\n      0.9841\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1064\n      °C\n    \n    \n      log_SW_IN\n      0.2308\n      W m-2\n    \n    \n      log_VPD\n      0.0255\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n      z2\n    \n  \n  \n    \n      TA\n      -0.0231\n      0.1967\n      0.9051\n    \n    \n      log_SW_IN\n      0.8899\n      0.3733\n      0.0379\n    \n    \n      log_VPD\n      -0.1494\n      0.7971\n      0.5105\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      4.2271\n    \n    \n      z1\n      2.9433\n    \n    \n      z2\n      6.6462\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0013\n    \n    \n      log_SW_IN\n      0.0102\n    \n    \n      log_VPD\n      0.0141\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0041\n    \n  \n\n \n\n\n\nhai_r_gaps[1].units = units\nhai_r_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9919\n    \n    \n      log_SW_IN\n      0.9906\n    \n    \n      log_VPD\n      0.5973\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0832\n      °C\n    \n    \n      log_SW_IN\n      0.1700\n      W m-2\n    \n    \n      log_VPD\n      0.1216\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9844\n    \n    \n      log_SW_IN\n      0.9890\n    \n    \n      log_VPD\n      0.6195\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1115\n      °C\n    \n    \n      log_SW_IN\n      0.2099\n      W m-2\n    \n    \n      log_VPD\n      0.1251\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.0841\n      -0.9069\n    \n    \n      log_SW_IN\n      0.9343\n      0.0371\n    \n    \n      log_VPD\n      0.2455\n      -0.6557\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      3.7979\n    \n    \n      z1\n      6.6354\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0013\n    \n    \n      log_SW_IN\n      0.0098\n    \n    \n      log_VPD\n      0.4257\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0053\n    \n  \n\n \n\n\n\nhai_r_gaps[0].units = units\nhai_r_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9917\n    \n    \n      log_SW_IN\n      -0.0027\n    \n    \n      log_VPD\n      0.5397\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0844\n      °C\n    \n    \n      log_SW_IN\n      1.7565\n      W m-2\n    \n    \n      log_VPD\n      0.1300\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9834\n    \n    \n      log_SW_IN\n      -0.0389\n    \n    \n      log_VPD\n      0.5384\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1151\n      °C\n    \n    \n      log_SW_IN\n      2.0372\n      W m-2\n    \n    \n      log_VPD\n      0.1378\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n    \n  \n  \n    \n      TA\n      0.9099\n    \n    \n      log_SW_IN\n      0.0142\n    \n    \n      log_VPD\n      0.6696\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      6.5268\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0021\n    \n    \n      log_SW_IN\n      0.9885\n    \n    \n      log_VPD\n      0.4629\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0043\n    \n  \n\n \n\n\n\n\n\n\nhai_c_gaps[2].units = units\nhai_c_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9357\n    \n    \n      log_SW_IN\n      0.9046\n    \n    \n      log_VPD\n      0.9007\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.2342\n      °C\n    \n    \n      log_SW_IN\n      0.5418\n      W m-2\n    \n    \n      log_VPD\n      0.0604\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      -11.2260\n    \n    \n      log_SW_IN\n      -2.0597\n    \n    \n      log_VPD\n      -1027.6723\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.7308\n      °C\n    \n    \n      log_SW_IN\n      1.6771\n      W m-2\n    \n    \n      log_VPD\n      0.1880\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n      z2\n    \n  \n  \n    \n      TA\n      0.2127\n      -0.4678\n      0.6172\n    \n    \n      log_SW_IN\n      -0.0392\n      0.5399\n      0.7212\n    \n    \n      log_VPD\n      0.6975\n      -0.0231\n      0.5030\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      2.3790\n    \n    \n      z1\n      3.0309\n    \n    \n      z2\n      4.2982\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0009\n    \n    \n      log_SW_IN\n      0.0038\n    \n    \n      log_VPD\n      0.0044\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0022\n    \n  \n\n \n\n\n\nhai_c_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9919\n    \n    \n      log_SW_IN\n      0.9906\n    \n    \n      log_VPD\n      0.5973\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0832\n      °C\n    \n    \n      log_SW_IN\n      0.1700\n      W m-2\n    \n    \n      log_VPD\n      0.1216\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9844\n    \n    \n      log_SW_IN\n      0.9890\n    \n    \n      log_VPD\n      0.6195\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1115\n      °C\n    \n    \n      log_SW_IN\n      0.2099\n      W m-2\n    \n    \n      log_VPD\n      0.1251\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.0841\n      -0.9069\n    \n    \n      log_SW_IN\n      0.9343\n      0.0371\n    \n    \n      log_VPD\n      0.2455\n      -0.6557\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      3.7979\n    \n    \n      z1\n      6.6354\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0013\n    \n    \n      log_SW_IN\n      0.0098\n    \n    \n      log_VPD\n      0.4257\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0053\n    \n  \n\n \n\n\n\nhai_c_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9917\n    \n    \n      log_SW_IN\n      -0.0027\n    \n    \n      log_VPD\n      0.5397\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0844\n      °C\n    \n    \n      log_SW_IN\n      1.7565\n      W m-2\n    \n    \n      log_VPD\n      0.1300\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9834\n    \n    \n      log_SW_IN\n      -0.0389\n    \n    \n      log_VPD\n      0.5384\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1151\n      °C\n    \n    \n      log_SW_IN\n      2.0372\n      W m-2\n    \n    \n      log_VPD\n      0.1378\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n    \n  \n  \n    \n      TA\n      0.9099\n    \n    \n      log_SW_IN\n      0.0142\n    \n    \n      log_VPD\n      0.6696\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      6.5268\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0021\n    \n    \n      log_SW_IN\n      0.9885\n    \n    \n      log_VPD\n      0.4629\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0043"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Notes\nNotes: - Kernel Lengthscale: too short to handle big gaps - how to encode time? - normalization - parameters init\nnext steps:\n\n\n\nthings to try: - more variables - more kernels - basic variable dist - log - deploy to website\nThings to consider: - different latent kernel for different latent variable?\n\nTA\nSW_IN\nLW_IN\nVPD\nWS\nWD\nP\nPA\nSWC\nPPFD\nCO2 flux\nH02 flux\nLE\n\nCode improvements: - move normalization out of the learner class - use something like fastai pipelines for data transformation …"
  },
  {
    "objectID": "Init_parameters_effect.html",
    "href": "Init_parameters_effect.html",
    "title": "Init Lambda",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\nfrom meteo_imp.results import _display_as_row\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\nfrom IPython.display import HTML\n\n\ndef reset_seed():\n    torch.manual_seed(27);\n    np.random.seed(27);\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\ncache = True\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      LW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      302.475\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      302.475\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      301.677\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      301.677\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      301.677\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-05 02:00:00\n      4.74\n      0.0\n      330.202\n      1.191\n    \n    \n      2000-01-05 02:30:00\n      4.75\n      0.0\n      330.202\n      1.057\n    \n    \n      2000-01-05 03:00:00\n      4.76\n      0.0\n      330.202\n      0.935\n    \n    \n      2000-01-05 03:30:00\n      4.62\n      0.0\n      330.202\n      1.162\n    \n    \n      2000-01-05 04:00:00\n      4.51\n      0.0\n      330.202\n      1.636\n    \n  \n\n200 rows × 4 columns\n\n\n\n\nreset_seed()\ndata = GPFADataTest(hai[:150]).add_random_missing()\n\n\nimp = GPFAImputationExplorer(hai[:20], latent_dims=2)\n\n\nimp\n\nGPFA Imputation Explorer:\n    N obs: 20\n    N features 4 (TA, SW_IN, LW_IN, VPD)\n    N missing observations 0\n    N latent: 2\n\n\n\nimp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.ones(4,2))\n\nimp.fit()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nGPFA Imputation Explorer:\n    N obs: 20\n    N features 4 (TA, SW_IN, LW_IN, VPD)\n    N missing observations 0\n    N latent: 2\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n\npca = PCA(2).fit(data.data_complete)\n\n\nPCA(2).fit(data.data_complete).components_\n\narray([[ 2.64510596e-03,  9.72190612e-01, -2.34163954e-01,\n         2.37925628e-03],\n       [-8.47181130e-03, -2.34134422e-01, -9.72167258e-01,\n        -3.50525961e-04]])\n\n\n\nimp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.tensor(PCA(2).fit(data.data_complete).components_, dtype=torch.float).T)\n\n\nimp.fit()\n\n\n\n\nGPFA Imputation Explorer:\n    N obs: 20\n    N features 4 (TA, SW_IN, LW_IN, VPD)\n    N missing observations 0\n    N latent: 2\n\n\n\n\n\n\ncache = here() / \".cache\" / \"hai_test_init_values.pickle\"\n# cache.unlink()\n\n\n@cache_disk(cache)\ndef compute():\n    out = {}\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    out[\"normal\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.ones(4,2))\n    out[\"ones\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.zeros(4,2))\n    out[\"zeros\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.rand(4,2))\n    out[\"rand1\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.rand(4,2))\n    out[\"rand2\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.rand(4,2))\n    out[\"rand3\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.tensor(PCA(2).fit(data.data_complete).components_, dtype=torch.float).T)\n    out[\"pca\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    return out\n\n\nresults = compute()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor key, result in results.items():\n    display(HTML(f\"<h4>{key}</h4>\"))\n    _display_as_row({'r2': result.r2(), **result.model_info})\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9913\n    \n    \n      SW_IN\n      0.1858\n    \n    \n      LW_IN\n      0.9682\n    \n    \n      VPD\n      0.5829\n    \n  \n\n  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.5849\n      0.8590\n    \n    \n      SW_IN\n      0.3494\n      -0.0086\n    \n    \n      LW_IN\n      -0.6155\n      0.4553\n    \n    \n      VPD\n      0.5600\n      0.5320\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      7.3226\n    \n    \n      z1\n      7.1896\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0009\n    \n    \n      SW_IN\n      0.7995\n    \n    \n      LW_IN\n      0.0192\n    \n    \n      VPD\n      0.4386\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0053\n    \n  \n\n \n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.6200\n    \n    \n      SW_IN\n      0.1293\n    \n    \n      LW_IN\n      0.0110\n    \n    \n      VPD\n      0.9652\n    \n  \n\n  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.5621\n      0.5621\n    \n    \n      SW_IN\n      0.2967\n      0.2967\n    \n    \n      LW_IN\n      -0.1298\n      -0.1298\n    \n    \n      VPD\n      0.7420\n      0.7420\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      5.5764\n    \n    \n      z1\n      5.5764\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.4161\n    \n    \n      SW_IN\n      0.8069\n    \n    \n      LW_IN\n      0.9339\n    \n    \n      VPD\n      0.0106\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0226\n    \n  \n\n \n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      -0.0176\n    \n    \n      SW_IN\n      -0.0140\n    \n    \n      LW_IN\n      -0.0032\n    \n    \n      VPD\n      -0.0225\n    \n  \n\n  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.0000\n      0.0000\n    \n    \n      SW_IN\n      0.0000\n      0.0000\n    \n    \n      LW_IN\n      0.0000\n      0.0000\n    \n    \n      VPD\n      0.0000\n      0.0000\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      0.6931\n    \n    \n      z1\n      0.6931\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.4942\n    \n    \n      SW_IN\n      0.4942\n    \n    \n      LW_IN\n      0.4942\n    \n    \n      VPD\n      0.4942\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.4943\n    \n  \n\n \n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9916\n    \n    \n      SW_IN\n      0.9688\n    \n    \n      LW_IN\n      0.1685\n    \n    \n      VPD\n      0.6162\n    \n  \n\n  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.1113\n      0.8822\n    \n    \n      SW_IN\n      -1.2775\n      0.4726\n    \n    \n      LW_IN\n      0.5127\n      -0.0920\n    \n    \n      VPD\n      -0.2842\n      0.7279\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      5.2701\n    \n    \n      z1\n      6.5753\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0011\n    \n    \n      SW_IN\n      0.0177\n    \n    \n      LW_IN\n      0.8398\n    \n    \n      VPD\n      0.4049\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0051\n    \n  \n\n \n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9913\n    \n    \n      SW_IN\n      0.1856\n    \n    \n      LW_IN\n      0.9680\n    \n    \n      VPD\n      0.5827\n    \n  \n\n  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.9612\n      0.3015\n    \n    \n      SW_IN\n      0.0887\n      0.3397\n    \n    \n      LW_IN\n      0.2557\n      -0.7323\n    \n    \n      VPD\n      0.6497\n      0.3773\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      7.0300\n    \n    \n      z1\n      7.5588\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0009\n    \n    \n      SW_IN\n      0.8078\n    \n    \n      LW_IN\n      0.0191\n    \n    \n      VPD\n      0.4389\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0051\n    \n  \n\n \n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9915\n    \n    \n      SW_IN\n      0.1840\n    \n    \n      LW_IN\n      0.9684\n    \n    \n      VPD\n      0.5822\n    \n  \n\n  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.2080\n      0.9447\n    \n    \n      SW_IN\n      -0.2592\n      0.2189\n    \n    \n      LW_IN\n      0.7847\n      -0.0903\n    \n    \n      VPD\n      -0.0126\n      0.7127\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      7.8303\n    \n    \n      z1\n      6.8435\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0009\n    \n    \n      SW_IN\n      0.7976\n    \n    \n      LW_IN\n      0.0190\n    \n    \n      VPD\n      0.4412\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0051\n    \n  \n\n \n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9915\n    \n    \n      SW_IN\n      0.1845\n    \n    \n      LW_IN\n      0.9683\n    \n    \n      VPD\n      0.5823\n    \n  \n\n  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.9431\n      -0.1357\n    \n    \n      SW_IN\n      0.2026\n      0.2780\n    \n    \n      LW_IN\n      -0.0436\n      -0.7956\n    \n    \n      VPD\n      0.7012\n      0.0679\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      6.8095\n    \n    \n      z1\n      7.8881\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0008\n    \n    \n      SW_IN\n      0.7966\n    \n    \n      LW_IN\n      0.0195\n    \n    \n      VPD\n      0.4391\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0052\n    \n  \n\n \n\n\n\n\n\n\nfor key, result in results.items():\n    display(HTML(f\"<h4>{key}</h4>\"))\n    result.display_results()\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9913\n    \n    \n      SW_IN\n      0.1858\n    \n    \n      LW_IN\n      0.9682\n    \n    \n      VPD\n      0.5829\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0863\n      °C\n    \n    \n      SW_IN\n      33.5182\n      W m-2\n    \n    \n      LW_IN\n      3.3613\n      W m-2\n    \n    \n      VPD\n      0.1651\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9819\n    \n    \n      SW_IN\n      0.1773\n    \n    \n      LW_IN\n      0.9495\n    \n    \n      VPD\n      0.5814\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1205\n      °C\n    \n    \n      SW_IN\n      43.8162\n      W m-2\n    \n    \n      LW_IN\n      4.6838\n      W m-2\n    \n    \n      VPD\n      0.1803\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.5849\n      0.8590\n    \n    \n      SW_IN\n      0.3494\n      -0.0086\n    \n    \n      LW_IN\n      -0.6155\n      0.4553\n    \n    \n      VPD\n      0.5600\n      0.5320\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      7.3226\n    \n    \n      z1\n      7.1896\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0009\n    \n    \n      SW_IN\n      0.7995\n    \n    \n      LW_IN\n      0.0192\n    \n    \n      VPD\n      0.4386\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0053\n    \n  \n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.6200\n    \n    \n      SW_IN\n      0.1293\n    \n    \n      LW_IN\n      0.0110\n    \n    \n      VPD\n      0.9652\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.5693\n      °C\n    \n    \n      SW_IN\n      34.6616\n      W m-2\n    \n    \n      LW_IN\n      18.7311\n      W m-2\n    \n    \n      VPD\n      0.0477\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.6607\n    \n    \n      SW_IN\n      0.0861\n    \n    \n      LW_IN\n      -0.0338\n    \n    \n      VPD\n      0.9568\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.5220\n      °C\n    \n    \n      SW_IN\n      46.1813\n      W m-2\n    \n    \n      LW_IN\n      21.2015\n      W m-2\n    \n    \n      VPD\n      0.0579\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.5621\n      0.5621\n    \n    \n      SW_IN\n      0.2967\n      0.2967\n    \n    \n      LW_IN\n      -0.1298\n      -0.1298\n    \n    \n      VPD\n      0.7420\n      0.7420\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      5.5764\n    \n    \n      z1\n      5.5764\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.4161\n    \n    \n      SW_IN\n      0.8069\n    \n    \n      LW_IN\n      0.9339\n    \n    \n      VPD\n      0.0106\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0226\n    \n  \n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      -0.0176\n    \n    \n      SW_IN\n      -0.0140\n    \n    \n      LW_IN\n      -0.0032\n    \n    \n      VPD\n      -0.0225\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.9315\n      °C\n    \n    \n      SW_IN\n      37.4040\n      W m-2\n    \n    \n      LW_IN\n      18.8651\n      W m-2\n    \n    \n      VPD\n      0.2585\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      -0.1121\n    \n    \n      SW_IN\n      -0.0709\n    \n    \n      LW_IN\n      -0.0378\n    \n    \n      VPD\n      -0.1479\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.9450\n      °C\n    \n    \n      SW_IN\n      49.9906\n      W m-2\n    \n    \n      LW_IN\n      21.2424\n      W m-2\n    \n    \n      VPD\n      0.2985\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.0000\n      0.0000\n    \n    \n      SW_IN\n      0.0000\n      0.0000\n    \n    \n      LW_IN\n      0.0000\n      0.0000\n    \n    \n      VPD\n      0.0000\n      0.0000\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      0.6931\n    \n    \n      z1\n      0.6931\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.4942\n    \n    \n      SW_IN\n      0.4942\n    \n    \n      LW_IN\n      0.4942\n    \n    \n      VPD\n      0.4942\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.4943\n    \n  \n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9916\n    \n    \n      SW_IN\n      0.9688\n    \n    \n      LW_IN\n      0.1685\n    \n    \n      VPD\n      0.6162\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0847\n      °C\n    \n    \n      SW_IN\n      6.5572\n      W m-2\n    \n    \n      LW_IN\n      17.1756\n      W m-2\n    \n    \n      VPD\n      0.1584\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9825\n    \n    \n      SW_IN\n      0.9670\n    \n    \n      LW_IN\n      0.2287\n    \n    \n      VPD\n      0.6275\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1185\n      °C\n    \n    \n      SW_IN\n      8.7722\n      W m-2\n    \n    \n      LW_IN\n      18.3136\n      W m-2\n    \n    \n      VPD\n      0.1700\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.1113\n      0.8822\n    \n    \n      SW_IN\n      -1.2775\n      0.4726\n    \n    \n      LW_IN\n      0.5127\n      -0.0920\n    \n    \n      VPD\n      -0.2842\n      0.7279\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      5.2701\n    \n    \n      z1\n      6.5753\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0011\n    \n    \n      SW_IN\n      0.0177\n    \n    \n      LW_IN\n      0.8398\n    \n    \n      VPD\n      0.4049\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0051\n    \n  \n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9913\n    \n    \n      SW_IN\n      0.1856\n    \n    \n      LW_IN\n      0.9680\n    \n    \n      VPD\n      0.5827\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0859\n      °C\n    \n    \n      SW_IN\n      33.5210\n      W m-2\n    \n    \n      LW_IN\n      3.3695\n      W m-2\n    \n    \n      VPD\n      0.1651\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9820\n    \n    \n      SW_IN\n      0.1771\n    \n    \n      LW_IN\n      0.9495\n    \n    \n      VPD\n      0.5812\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1203\n      °C\n    \n    \n      SW_IN\n      43.8197\n      W m-2\n    \n    \n      LW_IN\n      4.6882\n      W m-2\n    \n    \n      VPD\n      0.1803\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.9612\n      0.3015\n    \n    \n      SW_IN\n      0.0887\n      0.3397\n    \n    \n      LW_IN\n      0.2557\n      -0.7323\n    \n    \n      VPD\n      0.6497\n      0.3773\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      7.0300\n    \n    \n      z1\n      7.5588\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0009\n    \n    \n      SW_IN\n      0.8078\n    \n    \n      LW_IN\n      0.0191\n    \n    \n      VPD\n      0.4389\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0051\n    \n  \n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9915\n    \n    \n      SW_IN\n      0.1840\n    \n    \n      LW_IN\n      0.9684\n    \n    \n      VPD\n      0.5822\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0852\n      °C\n    \n    \n      SW_IN\n      33.5551\n      W m-2\n    \n    \n      LW_IN\n      3.3494\n      W m-2\n    \n    \n      VPD\n      0.1652\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9822\n    \n    \n      SW_IN\n      0.1742\n    \n    \n      LW_IN\n      0.9509\n    \n    \n      VPD\n      0.5801\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1194\n      °C\n    \n    \n      SW_IN\n      43.8981\n      W m-2\n    \n    \n      LW_IN\n      4.6204\n      W m-2\n    \n    \n      VPD\n      0.1805\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.2080\n      0.9447\n    \n    \n      SW_IN\n      -0.2592\n      0.2189\n    \n    \n      LW_IN\n      0.7847\n      -0.0903\n    \n    \n      VPD\n      -0.0126\n      0.7127\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      7.8303\n    \n    \n      z1\n      6.8435\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0009\n    \n    \n      SW_IN\n      0.7976\n    \n    \n      LW_IN\n      0.0190\n    \n    \n      VPD\n      0.4412\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0051\n    \n  \n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9915\n    \n    \n      SW_IN\n      0.1845\n    \n    \n      LW_IN\n      0.9683\n    \n    \n      VPD\n      0.5823\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0852\n      °C\n    \n    \n      SW_IN\n      33.5447\n      W m-2\n    \n    \n      LW_IN\n      3.3548\n      W m-2\n    \n    \n      VPD\n      0.1652\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9822\n    \n    \n      SW_IN\n      0.1750\n    \n    \n      LW_IN\n      0.9508\n    \n    \n      VPD\n      0.5801\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1195\n      °C\n    \n    \n      SW_IN\n      43.8775\n      W m-2\n    \n    \n      LW_IN\n      4.6273\n      W m-2\n    \n    \n      VPD\n      0.1805\n      hPa\n    \n  \n\n \n\n\n\nModel Info  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      0.9431\n      -0.1357\n    \n    \n      SW_IN\n      0.2026\n      0.2780\n    \n    \n      LW_IN\n      -0.0436\n      -0.7956\n    \n    \n      VPD\n      0.7012\n      0.0679\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      6.8095\n    \n    \n      z1\n      7.8881\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0008\n    \n    \n      SW_IN\n      0.7966\n    \n    \n      LW_IN\n      0.0195\n    \n    \n      VPD\n      0.4391\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0052"
  },
  {
    "objectID": "var_distribution.html",
    "href": "var_distribution.html",
    "title": "Variable distribution",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport polars as pl\n\nfrom meteo_imp.fluxnet.gap_finder import scan_fluxnet_csv\n\nfrom meteo_imp.utils import cache_disk\n\n\n\nload Hainich dataset\n\nhai_path = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\"\n# hai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=20_000)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = scan_fluxnet_csv(hai_path, convert_dates=True).rename(meteo_vars).select([pl.col(\"end\").alias(\"time\"), *meteo_vars.values()])\n\nhai.fetch(10)\n\n\n\n\nshape: (10, 5)\n\n\n\n\ntime\n\n\nTA\n\n\nSW_IN\n\n\nLW_IN\n\n\nVPD\n\n\n\n\ndatetime[μs]\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n2000-01-01 00:30:00\n\n\n-0.6\n\n\n0.0\n\n\n302.475\n\n\n0.222\n\n\n\n\n2000-01-01 01:00:00\n\n\n-0.65\n\n\n0.0\n\n\n302.475\n\n\n0.122\n\n\n\n\n2000-01-01 01:30:00\n\n\n-0.58\n\n\n0.0\n\n\n301.677\n\n\n0.09\n\n\n\n\n2000-01-01 02:00:00\n\n\n-0.51\n\n\n0.0\n\n\n301.677\n\n\n0.11\n\n\n\n\n2000-01-01 02:30:00\n\n\n-0.49\n\n\n0.0\n\n\n301.677\n\n\n0.102\n\n\n\n\n2000-01-01 03:00:00\n\n\n-0.4\n\n\n0.0\n\n\n301.677\n\n\n0.111\n\n\n\n\n2000-01-01 03:30:00\n\n\n-0.36\n\n\n0.0\n\n\n301.677\n\n\n0.109\n\n\n\n\n2000-01-01 04:00:00\n\n\n-0.35\n\n\n0.0\n\n\n301.677\n\n\n0.107\n\n\n\n\n2000-01-01 04:30:00\n\n\n-0.28\n\n\n0.0\n\n\n308.046\n\n\n0.122\n\n\n\n\n2000-01-01 05:00:00\n\n\n-0.27\n\n\n0.0\n\n\n308.046\n\n\n0.138\n\n\n\n\n\n\n\n\nhai_td = hai.melt('time')\n\n\nhai_td.fetch(3)\n\n\n\n\nshape: (12, 3)\n\n\n\n\ntime\n\n\nvariable\n\n\nvalue\n\n\n\n\ndatetime[μs]\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n2000-01-01 00:30:00\n\n\n\"TA\"\n\n\n-0.6\n\n\n\n\n2000-01-01 01:00:00\n\n\n\"TA\"\n\n\n-0.65\n\n\n\n\n2000-01-01 01:30:00\n\n\n\"TA\"\n\n\n-0.58\n\n\n\n\n2000-01-01 00:30:00\n\n\n\"SW_IN\"\n\n\n0.0\n\n\n\n\n2000-01-01 01:00:00\n\n\n\"SW_IN\"\n\n\n0.0\n\n\n\n\n2000-01-01 01:30:00\n\n\n\"SW_IN\"\n\n\n0.0\n\n\n\n\n2000-01-01 00:30:00\n\n\n\"LW_IN\"\n\n\n302.475\n\n\n\n\n2000-01-01 01:00:00\n\n\n\"LW_IN\"\n\n\n302.475\n\n\n\n\n2000-01-01 01:30:00\n\n\n\"LW_IN\"\n\n\n301.677\n\n\n\n\n2000-01-01 00:30:00\n\n\n\"VPD\"\n\n\n0.222\n\n\n\n\n2000-01-01 01:00:00\n\n\n\"VPD\"\n\n\n0.122\n\n\n\n\n2000-01-01 01:30:00\n\n\n\"VPD\"\n\n\n0.09\n\n\n\n\n\n\n\n\n\n\n\nhai.drop('time').collect().to_pandas().hist(figsize=(15,10));\n\n\n\n\n\n# should to the binning before the plot\n# alt.Chart(hai_td.collect().to_pandas()).mark_line().encode(\n#     x = 'value',\n#     y = 'density()',\n#     facet = alt.Facet('variable', columns=2)\n# )\n\n\n\n\nCode inspired from source: https://towardsdatascience.com/altair-plot-deconstruction-visualizing-the-correlation-structure-of-weather-data-38fb5668c5b1\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_p.corr()\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      LW_IN\n      VPD\n    \n  \n  \n    \n      TA\n      1.000000\n      0.432321\n      0.639556\n      0.735412\n    \n    \n      SW_IN\n      0.432321\n      1.000000\n      0.126278\n      0.533506\n    \n    \n      LW_IN\n      0.639556\n      0.126278\n      1.000000\n      0.270424\n    \n    \n      VPD\n      0.735412\n      0.533506\n      0.270424\n      1.000000\n    \n  \n\n\n\n\n\ndef corr_mask(size):\n    corr_mask = np.zeros((size,size), dtype=bool)\n    for i in range(size):\n        for j in range(size):\n            corr_mask[i,j] = True if i >= j else False\n    return corr_mask\n\n\ncorr_mask(len(hai_p.columns))\n\narray([[ True, False, False, False],\n       [ True,  True, False, False],\n       [ True,  True,  True, False],\n       [ True,  True,  True,  True]])\n\n\n\nhai_p = hai_p[sorted(hai_p.columns)] # need to properly plot half a corr matrix\n\n\ncor_hai = (hai_p\n              .corr().mask(~corr_mask(len(hai_p.columns))).stack()\n              .reset_index()     # The stacking results in an index on the correlation values, we need the index as normal columns for Altair\n              .rename(columns={0: 'correlation', 'level_0': 'variable', 'level_1': 'variable2'}))\ncor_hai['correlation_label'] = cor_hai['correlation'].map('{:.2f}'.format)  # Round to 2 decimal\ncor_hai\n\n\n\n\n\n  \n    \n      \n      variable\n      variable2\n      correlation\n      correlation_label\n    \n  \n  \n    \n      0\n      LW_IN\n      LW_IN\n      1.000000\n      1.00\n    \n    \n      1\n      SW_IN\n      LW_IN\n      0.126278\n      0.13\n    \n    \n      2\n      SW_IN\n      SW_IN\n      1.000000\n      1.00\n    \n    \n      3\n      TA\n      LW_IN\n      0.639556\n      0.64\n    \n    \n      4\n      TA\n      SW_IN\n      0.432321\n      0.43\n    \n    \n      5\n      TA\n      TA\n      1.000000\n      1.00\n    \n    \n      6\n      VPD\n      LW_IN\n      0.270424\n      0.27\n    \n    \n      7\n      VPD\n      SW_IN\n      0.533506\n      0.53\n    \n    \n      8\n      VPD\n      TA\n      0.735412\n      0.74\n    \n    \n      9\n      VPD\n      VPD\n      1.000000\n      1.00\n    \n  \n\n\n\n\n\nimport numpy as np\n\ndef compute_2d_histogram(var1, var2, df, density=True, bins=20):\n    H, xedges, yedges = np.histogram2d(df[var1], df[var2], bins=bins, density=density)\n    H[H == 0] = np.nan\n\n    # Create a nice variable that shows the bin boundaries\n    \n    x_width = xedges[1] - xedges[0] # all bins have same width\n    xedges = pd.Series(xedges[:-1] + x_width /2)\n    \n    y_width = yedges[1] - yedges[0] # all bins have same width\n    yedges = pd.Series(yedges[:-1] + y_width /2)\n    \n    # Cast to long format using melt\n    res = pd.DataFrame(H, \n                       index=yedges, \n                       columns=xedges).reset_index().melt(\n                            id_vars='index'\n                       ).rename(columns={'index': 'value2', \n                                         'value': 'count',\n                                         'variable': 'value'})\n    \n\n    res['variable'] = var1\n    res['variable2'] = var2 \n    return res.dropna() # Drop all combinations for which no values where found\n\n\nh, xe, ye = np.histogram2d(hai_p['VPD'], hai_p['TA'])\n\n\nx_width = xe[1] - xe[0] # all bins have same width\nxe = pd.Series(xe + x_width /2)\n\n\nh.shape\n\n(10, 10)\n\n\n\nxe\n\n0      2.38335\n1      7.15005\n2     11.91675\n3     16.68345\n4     21.45015\n5     26.21685\n6     30.98355\n7     35.75025\n8     40.51695\n9     45.28365\n10    50.05035\ndtype: float64\n\n\n\nhai_binned = pd.concat([compute_2d_histogram(var1, var2, hai_p) for var1 in meteo_vars.values() for var2 in meteo_vars.values()])\nhai_binned.head()\n\n\n\n\n\n  \n    \n      \n      value2\n      value\n      count\n      variable\n      variable2\n    \n  \n  \n    \n      0\n      -17.19025\n      -17.19025\n      0.000083\n      TA\n      TA\n    \n    \n      21\n      -14.47075\n      -14.47075\n      0.000224\n      TA\n      TA\n    \n    \n      42\n      -11.75125\n      -11.75125\n      0.000574\n      TA\n      TA\n    \n    \n      63\n      -9.03175\n      -9.03175\n      0.001188\n      TA\n      TA\n    \n    \n      84\n      -6.31225\n      -6.31225\n      0.003781\n      TA\n      TA\n    \n  \n\n\n\n\n\n# Define selector\nvar_sel_cor = alt.selection_single(fields=['variable', 'variable2'], clear=False, \n                                  init={'variable': 'TA', 'variable2': 'SW_IN'})\n\n# Define correlation heatmap\nbase = alt.Chart(cor_hai).encode(\n    x='variable2:O',\n    y='variable:O'    \n)\n\ntext = base.mark_text().encode(\n    text='correlation_label',\n    color=alt.condition(\n        alt.datum.correlation > 0.5, \n        alt.value('white'),\n        alt.value('black')\n    )\n)\n\ncor_plot = base.mark_rect().encode(\n    color=alt.condition(var_sel_cor, alt.value('pink'), 'correlation:Q')\n).add_selection(var_sel_cor)\n\n# Define 2d binned histogram plot\nscat_plot = alt.Chart(hai_binned).transform_filter(\n    var_sel_cor\n).mark_rect().encode(\n    alt.X('value:N', axis=alt.Axis(format=\".4\")), \n    alt.Y('value2:N', axis=alt.Axis(format=\".4\"), sort='descending'),\n    alt.Color('count:Q', scale=alt.Scale(scheme='blues'))\n)\n\n# Combine all plots. hconcat plots both side-by-side \nalt.hconcat((cor_plot + text).properties(width=350, height=350), scat_plot.properties(width=350, height=350)).resolve_scale(color='independent')\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_td_p = hai_td.collect().to_pandas().set_index('time')"
  },
  {
    "objectID": "gap length variation.html",
    "href": "gap length variation.html",
    "title": "Gap Length Variation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\nfrom meteo_imp.results import _display_as_row \n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\nfrom itertools import combinations, repeat, zip_longest\n\nfrom ipywidgets import interact\nfrom tqdm.auto import tqdm\n\nfrom multiprocessing import Pool\n\nimport pickle\n\nModuleNotFoundError: No module named 'meteo_imp.gpfa.data_preparation'\n\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    #\"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nn_obs = 200\nn_latent = 1\ntotal_iter = 100\n\n\nmodel_save_dir = here() / \"analysis/trained_models\"\n\nmodel_path = model_save_dir / f\"GPFA_l_{n_latent}_train_{total_iter}_1ker_{n_obs}_obs.pickle\"\n\n\ndata = GPFADataTest(hai[:n_obs])\n\n\n# inspired from https://datagy.io/python-combinations-of-a-list/\ndef all_comb(l):\n    list_combinations = []\n    for n in range(1, len(l) + 1):\n        list_combinations += list(combinations(l, n))\n    return list_combinations\n\n\nall_comb(meteo_vars.values())\n\n\ndef to_result_pretrained(gap_len, n_latent, var_sel, gap_start=None):\n    data = GPFADataTest(hai[:n_obs]).add_gap(gap_len, var_sel, gap_start)\n    imp = GPFAImputationExplorer(data.data, latent_dims = n_latent)\n    model_path = model_save_dir / f\"GPFA_l_{n_latent}_train_{total_iter}_1ker_{n_obs}_obs.pickle\"\n    imp.learner.load(model_path)\n    return imp.to_result(data.data_compl_tidy, units=units)\n\n\n# to_result_pretrained(10, 1, ['TA'])\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# @cache_disk(here() / \".cache/diff_gaps\")\n# def diff_gaps(gap_start=30):\n#     return {n_lat:\n#             {var_sel:{ \n#                 gap_len: to_result_pretrained(GPFADataTest(hai[:n_obs]).add_gap(gap_len, ['TA'], gap_start), n_lat)\n#                 for gap_len in [2, 4, 5, 7, 10 , 15, 20, 30, 50, 100]\n#                 }\n#                 for var_sel in all_comb(meteo_vars.values())}\n#             for n_lat in range(1,4)}\n:::\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\npath_base = here() / \".cache/diff_gap_partial\"\n# path_base.rmdir()\n\n\ndef process_var_sel(args, path_base=path_base):\n    var_sel, n_lat = args # limitations in python map...\n    f_name = path_base / f\"{'-'.join(var_sel)}__l_{n_lat}.pickle\"\n    if f_name.exists(): return\n    out = {}\n    for gap_len in gaps:\n        out[gap_len] = {}\n        for gap_start in gap_starts:\n            out[gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start) \n    with open(f_name, \"wb\") as f:\n        pickle.dump(out, f)    \n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# # this is going to run on the process\n# # @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\n# def process_n_lat(n_lat):\n#     out = {}\n#     for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n#         out[var_sel] = {}\n#         for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n#             out[var_sel][gap_len] = {}\n#             for gap_start in tqdm(gap_starts, desc=f\"gap len: {gap_len}\"):\n#                 out[var_sel][gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n#     return out\n:::\n\n# @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\ndef compute_diff_gaps(gap_start=30):\n    for n_lat in tqdm(range(1,4)):\n        with Pool(processes=4) as pool:\n            list(pool.imap(process_var_sel, zip(all_comb(meteo_vars.values()), repeat(n_lat,))))\n\n\n\n\nthis is memory intensive! (maybe there is a leak to fix somewhere …)\n\n# compute_diff_gaps()\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\n# def diff_gaps(gap_start=30):\n#     with Pool(processes=4) as pool:\n#         out = pool.map(res, range(1,4)\n#         for n_lat in tqdm(range(1,4)):\n#             out[n_lat] = {}\n#             for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n#                 out[n_lat][var_sel] = {}\n#                 for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n#                     out[n_lat][var_sel][gap_len] = {}\n#                     for gap_start in tqdm(gap_starts, desc=f\"gap len: {gap_len}\"):\n#                         out[n_lat][var_sel][gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n#     return out\n:::\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# @cache_disk(here() / \".cache/diff_gaps\")\n# def diff_gaps(gap_start=30):\n#     with Pool(processes=4) as pool:\n#         out = {}  \n#         for n_lat in tqdm(range(1,4)):\n#             out[n_lat] = {}\n#             for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n#                 out[n_lat][var_sel] = {}\n#                 for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n#                     out[n_lat][var_sel][gap_len] = {}\n#                     f = lambda gap_start: to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n#                     results = pool.map(f, gap_starts)\n#                     for gap_start, res in zip(gap_starts, results):\n#                         out[n_lat][var_sel][gap_len][gap_start] = res\n#     return out\n:::\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# diff_gaps_res = diff_gaps()\n:::\n\n# loads computations from disk\ndef load_diff_gaps():\n    out = {}\n    for n_lat in tqdm(range(1,4)):\n        out[n_lat] = {}\n        for var_sel in all_comb(meteo_vars.values()):\n            f_name = path_base / f\"{'-'.join(var_sel)}__l_{n_lat}.pickle\"\n            with open(f_name, \"rb\") as f:\n                out[n_lat][var_sel] = pickle.load(f)  \n    return out\n\n\ndiff_gaps_res = load_diff_gaps()\n\n\n\n\n\nWhat I am doing here:\n\ntake a dataset with 200 obs and 3 variables\ndistribution and correlation between vars\nfit the kernel parameters using gradient descend on whole dataset and save trained model notebook\ncreate a dataset with all combinations of gap_len, gap_start, n latents and variable missing\npredict the model for all 200 Obs also when there are no gaps!\nNote: in case there is a gap in not all variable, the variable with the gap have the (correct) prediction conditioned on the other variables, but the variables with no gap have the base model prediction (which is often bad), which should not be considered\n\n\n\nWhat we can see from this result:\n\n\n\n1 latent the Lambda is almost 1 for TA, 0 for SW_IN and .4 for VPD. Hence is good for TA, horrible for SW_IN and somehow okayish forVPD`\n2 latent2: good for TA and SW_IN, still limited for VPD\n3 latents: quite good fit for all 3 models\n\ncomments\n\ncorrelation between SW_IN and the others variables is pretty low\ntherefore with 1-2 latents the model cannot model accurately more then 1 variable\n\n\n\n\n\nwhen gaps are short <~10 the model works kind of well, but there are issues in some locations (eg: TA, len: 10 start: 60)\n\ncomments\nthe lengthscale of the kernel is quite small (3 latents):\n\n5.2 z0\n1.8 z1\n4.0 z2\n\nso for longer gaps (in only one var) the main driver for the predictions are observations from the other variables, otherwise the models predictions are contastant as there is no way to use more information (eg. gap_len: 50, gap_start: 30, gaps in all vars)\nnotes - when SW_IN and VPD are close to 0 the gap filling is not that great also for shorter gaps (eg. SW_IN, len: 7, start: 30, n_lat: 3)\n\n\n\nthe interesting aspect is when there is a long gap, but in only 1-2 variables\n\nfor gaps only in TA with len up to 50 the models manages to follow the variations in the measurements, but with an error\n\nthis is pretty similar if there are gaps also in SW_IN, but not if there are gaps in VPD\nwith gap len over 100 it get way worse\n\n\nfor gaps only in VP with len up to 50 the models overall manages to follow the variations in the measurements, but with a considerable error (measurements are still in error bar) and the models has a lot of variations which are not present in the data\n\nthe predictions for SW_IN are bad (underestimates a lot the values) during the day for long gaps\n\n\n\n\n\n\n\n\n\n\n\n\nmore kernels -> can have different timescales. However with 150 obs both kernels have the same timescale, should use more data but then there are computation issues (with 1500 it would take more then 20hours to do the training)\nlog transform\nmore variables\n\n\n\n\nmodel performance: at the moment it takes ~8 minutes to train with 200 obs and ~20 seconds for inference\n\nprofile current model\nuse SparseGP\nCUDA support\n\nparameters init\nlearning rate and stability of parameters over training\nvariable transformation:\n\nall vars are now normalized (0 mean, 1 std)\ntime is enconded as integer increasing at steps of 1. Maybe not a good idea?\n\n\n\n\n\n\n\nuse ERA5-Land (world-wide dataset with complete meteo vars, but a coarse spatial-temporal scale)\ncompare performance with state of art models\nmodel where relation between variables changes over time\nunderstand gap distribution in real world:\n\naverage gap len (tentative results are: a lot of short gaps(<10) and some pretty long gap (>10.0000)\ncorrelation between variable gaps\nsite distributions of gaps?\n\n\n\n\n\n\n\n%time r = to_result_pretrained(12, 3, ['SW_IN', 'TA'], gap_start=60)\n\n\nr.display_results()\n\n\ndata = GPFADataTest(hai[:n_obs]).add_gap(50, ['TA'], 30)\n\n\nimp = GPFAImputationExplorer(data.data, latent_dims = n_latent)\nimp.learner.load(model_path)\n\n\nresult_pretrained(data)"
  },
  {
    "objectID": "GPFA Hainich.html",
    "href": "GPFA Hainich.html",
    "title": "Single latent",
    "section": "",
    "text": "First analysis of the Hainich data using GPFA for filling the gaps\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      LW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      302.475\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      302.475\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      301.677\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      301.677\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      301.677\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-05 02:00:00\n      4.74\n      0.0\n      330.202\n      1.191\n    \n    \n      2000-01-05 02:30:00\n      4.75\n      0.0\n      330.202\n      1.057\n    \n    \n      2000-01-05 03:00:00\n      4.76\n      0.0\n      330.202\n      0.935\n    \n    \n      2000-01-05 03:30:00\n      4.62\n      0.0\n      330.202\n      1.162\n    \n    \n      2000-01-05 04:00:00\n      4.51\n      0.0\n      330.202\n      1.636\n    \n  \n\n200 rows × 4 columns\n\n\n\n\n\n\n\ngpfa_data = GPFADataTest(hai).add_random_missing()\n\n\ngpfa_hai = GPFAImputation(gpfa_data.data, gpfa_data.tidy_df(complete=True, is_missing=True))\n\nTypeError: rand(): argument 'size' must be tuple of ints, but found element of type DataFrame at pos 2\n\n\n\ngpfa_hai\n\n\n%time imputed = gpfa_hai.impute()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nCPU times: user 5min 57s, sys: 485 ms, total: 5min 57s\nWall time: 5min 59s\n\n\n\nimputed\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      TA\n      -0.600000\n      NaN\n    \n    \n      1\n      2.0\n      TA\n      -0.580000\n      NaN\n    \n    \n      2\n      3.0\n      TA\n      -0.510000\n      NaN\n    \n    \n      3\n      4.0\n      TA\n      -0.490000\n      NaN\n    \n    \n      4\n      11.0\n      TA\n      -0.230000\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      403\n      189.0\n      VPD\n      0.826632\n      0.252326\n    \n    \n      404\n      190.0\n      VPD\n      0.827371\n      0.252322\n    \n    \n      405\n      192.0\n      VPD\n      1.213000\n      0.000000\n    \n    \n      406\n      193.0\n      VPD\n      0.826446\n      0.252319\n    \n    \n      407\n      197.0\n      VPD\n      0.820434\n      0.252332\n    \n  \n\n800 rows × 4 columns\n\n\n\n\nhai_plot = gpfa_hai.plot_pred(units=units, properties =  {'height': 190 , 'width': 380})\n\nhai_plot.save(\"plots/plot_hai_winter_4_var_200_obs_random_gaps_row_20_value_10.vl.json\")\nhai_plot\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n#gpfa_hai.plot_pred(complete= gpfa_data.tidy_df(complete=True, is_missing=True) )\n\n\ngpfa_hai.rmse()\n\nAttributeError: module 'sklearn' has no attribute 'metrics'\n\n\n\ngpfa_hai.r2()\n\n\nlosses = pd.DataFrame(gpfa_hai.learner.losses.cpu().numpy(), columns=['loss'])\n\np = losses.plot()\nplt.savefig(here('analysis/plots/loss_plot_hai_winter_4_var_200_obs_random_gaps_row_20_value_10.png'))\np\n\nLambda parameter, the latent variable is very similar to the\n\ngpfa_hai.data.corr()\n\nNameError: name 'gpfa_hai' is not defined\n\n\n\ngpfa_hai.learner.model.covar_module.Lambda.detach()\n\nthis is the value of the length scale of the RBF latent kernel\n\ngpfa_hai.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\ngpfa_hai.learner.model.covar_module.psi.detach()\n\n\n\nThe low correlation between SW_IN and TA is likely due to cloud cover, which is hard to predict with a dialy cycle. Hence we are looking at summer days and there is a much better correlation\n\nhai_raw2 = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows= 7 * 30 * 24 * 2)\n\nNameError: name 'pd' is not defined\n\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai2 = (hai_raw2\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai2\n\n\nhai2[-800:-500].SW_IN.plot()\n\n\nhai2[-800:-500].corr()\n\n\ngpdata2 = GPFADataTest(hai2[-800:-500].copy()).add_random_missing()\n\n\ngp_imp2 = GPFAImputation(gpdata2.data, gpdata2.tidy_df(complete=True, is_missing=True))\n\n\n%time data_imp2 = gp_imp2.impute()\n\n\ngp_imp2.plot_pred(units=units)\n\n\ndata_imp2\n\n\ngp_imp2.rmse()\n\n\ngpdata2.data.corr()\n\n\ngp_imp2.learner.model.covar_module.Lambda.detach()\n\n\ngp_imp2.learner.model.covar_module.psi.detach()\n\nthis is the value of the length scale of the RBF latent kernel\n\ngp_imp2.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\n\n\ngpdata3 = GPFADataTest(hai2[-800:-500].loc[:, [\"TA\", \"SW_IN\"]].copy()).add_random_missing()\n\n\ngp_imp3 = GPFAImputation(gpdata3.data, gpdata3.tidy_df(complete=True, is_missing=True))\n\n\n%time data_imp3 = gp_imp3.impute()\n\n\ngp_imp3.plot_pred(units=units, bind_interaction=False)\n\n\ndata_imp3\n\n\ngpdata3.data.corr()\n\n\ngp_imp3.learner.model.covar_module.Lambda.detach()\n\nthis is the value of the length scale of the RBF latent kernel\n\ngp_imp3.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\ngp_imp3.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\n\n\n\n\nTrying to see how the model works with a continous gap of 10% the length of the dataset for all variables\n\ngpd_gap = GPFADataTest(hai).add_gap(20, variables = ['TA', 'SW_IN', 'LW_IN', 'VPD'])\n\n\ngp_gap = GPFAImputation(gpd_gap.data, gpd_gap.tidy_df(complete=True, is_missing=True))\n\n\ngp_gap\n\n\n%time gp_gap.impute()\n\n\ngap_plot= gp_gap.plot_pred(units=units, properties =  {'height': 190 , 'width': 380})\n\ngap_plot.save(here(\"analysis/plots\") /\" plot_hai_winter_4_var_200_obs_gap_20.vl.json\")\ngap_plot\n\n\nprint(gp_gap.rmse().to_markdown(index=False))\n\n\nprint(pd.DataFrame(gp_gap.learner.model.covar_module.Lambda.detach().numpy()).to_markdown(index=False))\n\n\npsi = pd.DataFrame(gp_gap.learner.model.covar_module.psi.detach().numpy())\npsi.insert(0, \"variable\", meteo_vars.values())\nprint(psi.to_markdown(index=False))\n\n\ngp_gap.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\nlosses = pd.DataFrame(gp_gap.learner.losses.cpu().numpy(), columns=['loss'])\n\np = losses.plot()\nplt.savefig(here('analysis/plots/') /'loss_plot_hai_winter_4_var_200_obs_gap_20.png')\np"
  },
  {
    "objectID": "kalman/Loss_comparison.html",
    "href": "kalman/Loss_comparison.html",
    "title": "Compare Loss functions",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\n\nIn order to optimize the parameters of a KalmanFilter there are multiple ways, one is to use EM like pykalman another is to optimize using the likelihood like statsmodels is doing.\nStatsmodels doesn’t support gap in the dataset\nThere are 2 main approach for the loss:\n\nuse only the filter. This means that the models runs only the kalman filter and for every observations tries to predict the next one. This doesn’t consider the gaps and doesn’t use the smoother. The loss is calculated for the whole period\npredict the gap after smoothing. Run the Kalman smoother and then compute the loglikelihood only for the gap\n\nin addition between different batches the loss can be averaged or summed\n\n\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai = load_data()\nhai64 = load_data(np.float64)\n\n\ndef train_loss(reduction, use_smooth):\n    model = KalmanFilter.init_random(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1], dtype=torch.float64).cuda()\n    model.use_smooth = use_smooth\n    only_gap = use_smooth\n    dls = make_dataloader(hai64[:5*20_000], block_len=200, gap_len=10, bs=20) # about 5 year of data\n    loss_func = loss_func=KalmanLoss(only_gap=only_gap, reduction=reduction)\n    learn = Learner(dls, model, loss_func, cbs=[ShowGraphCallback, Float64Callback], metrics=[msk_rmse, msk_r2])\n    learn.fit(20, 1e-2)\n    return learn\n\n\n\n\n\nl_sum_smooth = train_loss('sum', use_smooth=True)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      10799.557130\n      7761.787664\n      0.494636\n      0.706476\n      00:33\n    \n    \n      1\n      6721.645770\n      1878.776931\n      0.464390\n      0.739616\n      00:33\n    \n    \n      2\n      4177.969558\n      509.344728\n      0.412180\n      0.793374\n      00:33\n    \n    \n      3\n      2260.037030\n      -1837.433033\n      0.246094\n      0.927043\n      00:33\n    \n    \n      4\n      -719.709847\n      -9555.917596\n      0.199754\n      0.952821\n      00:33\n    \n    \n      5\n      -4476.204630\n      -13089.933634\n      0.149999\n      0.973490\n      00:34\n    \n    \n      6\n      -8455.999841\n      -17691.481401\n      0.131274\n      0.979583\n      00:33\n    \n    \n      7\n      -11678.127028\n      -17865.444855\n      0.147398\n      0.972290\n      00:34\n    \n    \n      8\n      -14031.962195\n      -18773.763584\n      0.137897\n      0.976673\n      00:34\n    \n    \n      9\n      -15678.972540\n      -18799.284560\n      0.141924\n      0.975796\n      00:35\n    \n    \n      10\n      -16803.071401\n      -18882.290928\n      0.112587\n      0.985104\n      00:36\n    \n    \n      11\n      -17558.598880\n      -18924.760868\n      0.105362\n      0.986287\n      00:33\n    \n    \n      12\n      -18067.875428\n      -18897.525877\n      0.093465\n      0.989777\n      00:33\n    \n    \n      13\n      -18430.414265\n      -19040.231475\n      0.090060\n      0.990478\n      00:35\n    \n    \n      14\n      -18694.256430\n      -19050.077039\n      0.092960\n      0.989644\n      00:33\n    \n    \n      15\n      -18885.238713\n      -19185.962157\n      0.080832\n      0.992125\n      00:33\n    \n    \n      16\n      -19050.434482\n      -19113.437583\n      0.114341\n      0.984587\n      00:33\n    \n    \n      17\n      -19185.244701\n      -19220.514641\n      0.082792\n      0.992028\n      00:33\n    \n    \n      18\n      -19271.169882\n      -19315.973143\n      0.066270\n      0.994715\n      00:33\n    \n    \n      19\n      -19338.463470\n      -19355.567454\n      0.068022\n      0.994381\n      00:33\n    \n  \n\n\n\n\n\n\n\nl_sum_smooth.recorder.plot_loss()\n\n\n\n\n\nshow_results(l_sum_smooth, [10, 50, 100])\n\n\n\n\n\n\n\ndisplay_as_row(l_sum_smooth.model.get_info(var_names=hai64.columns))\n\n\n  trans_matrix (A) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.8793\n      0.1644\n      -0.0996\n    \n    \n      z_1\n      0.0674\n      0.9061\n      0.0915\n    \n    \n      z_2\n      -0.1359\n      0.1807\n      0.8870\n    \n  \n\n  trans_cov (Q) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.1691\n      -0.1189\n      -0.1084\n    \n    \n      z_1\n      -0.1189\n      0.1337\n      0.0929\n    \n    \n      z_2\n      -0.1084\n      0.0929\n      0.3573\n    \n  \n\n  trans_off \n\n  \n    \n      latent\n      offset\n    \n  \n  \n    \n      z_0\n      0.0393\n    \n    \n      z_1\n      0.0015\n    \n    \n      z_2\n      0.1445\n    \n  \n\n  obs_matrix (H) \n\n  \n    \n      variable\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      TA\n      0.1359\n      0.1995\n      -0.0180\n    \n    \n      SW_IN\n      -0.0492\n      0.3400\n      0.0636\n    \n    \n      VPD\n      0.3182\n      0.0204\n      0.3036\n    \n  \n\n  obs_cov (R) \n\n  \n    \n      variable\n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      TA\n      0.0000\n      0.0000\n      -0.0000\n    \n    \n      SW_IN\n      0.0000\n      0.0000\n      -0.0000\n    \n    \n      VPD\n      -0.0000\n      -0.0000\n      0.0000\n    \n  \n\n  obs_off \n\n  \n    \n      variable\n      offset\n    \n  \n  \n    \n      TA\n      0.2448\n    \n    \n      SW_IN\n      0.3635\n    \n    \n      VPD\n      0.2137\n    \n  \n\n  init_state_mean \n\n  \n    \n      latent\n      mean\n    \n  \n  \n    \n      z_0\n      0.2512\n    \n    \n      z_1\n      0.6348\n    \n    \n      z_2\n      0.9272\n    \n  \n\n  init_state_cov \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      1.0244\n      0.4137\n      -0.1930\n    \n    \n      z_1\n      0.4137\n      1.5316\n      0.9529\n    \n    \n      z_2\n      -0.1930\n      0.9529\n      2.2897\n    \n  \n\n \n\n\n\n\n\n\nl_sum_n_smooth = train_loss('sum', use_smooth=False)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      93576.557506\n      49094.936151\n      40.175671\n      -1609.196695\n      00:22\n    \n    \n      1\n      55729.858957\n      25465.931891\n      13.037290\n      -168.944314\n      00:22\n    \n    \n      2\n      40901.222382\n      23625.398854\n      9.227300\n      -81.796224\n      00:23\n    \n    \n      3\n      32801.561799\n      21803.994577\n      6.553250\n      -44.308130\n      00:23\n    \n    \n      4\n      28134.386174\n      20767.311986\n      4.905621\n      -25.004391\n      00:23\n    \n    \n      5\n      25155.250779\n      19650.187167\n      3.522742\n      -11.580414\n      00:23\n    \n    \n      6\n      23134.458851\n      18852.613585\n      2.670879\n      -6.114421\n      00:24\n    \n    \n      7\n      21590.111416\n      18724.717731\n      2.463030\n      -4.972687\n      00:23\n    \n    \n      8\n      20488.952666\n      18046.332419\n      1.743591\n      -2.261957\n      00:24\n    \n    \n      9\n      19599.282756\n      17686.631493\n      1.554881\n      -1.386024\n      00:24\n    \n    \n      10\n      18864.805491\n      17168.486468\n      1.231224\n      -0.576305\n      00:23\n    \n    \n      11\n      18219.402631\n      16717.755567\n      0.993929\n      0.016635\n      00:24\n    \n    \n      12\n      17652.747846\n      16358.387093\n      0.948614\n      0.086813\n      00:23\n    \n    \n      13\n      17166.811259\n      15819.057481\n      0.743849\n      0.461778\n      00:22\n    \n    \n      14\n      16669.927076\n      15380.745776\n      0.729747\n      0.474362\n      00:23\n    \n    \n      15\n      16170.134417\n      14886.565936\n      0.694190\n      0.532979\n      00:24\n    \n    \n      16\n      15616.390677\n      14280.504826\n      0.653767\n      0.585480\n      00:25\n    \n    \n      17\n      15008.060510\n      13473.336895\n      0.579920\n      0.672648\n      00:23\n    \n    \n      18\n      14325.386795\n      12561.507037\n      0.566265\n      0.686859\n      00:23\n    \n    \n      19\n      13516.621992\n      11399.619833\n      0.543082\n      0.710991\n      00:23\n    \n  \n\n\n\n\n\n\n\nl_sum_n_smooth.recorder.plot_loss()\n\n\n\n\nfit a bit more\n\nl_sum_n_smooth.fit(5, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      10346.725174\n      9452.293934\n      0.518965\n      0.736902\n      00:23\n    \n    \n      1\n      9009.084750\n      7346.894680\n      0.498220\n      0.756269\n      00:23\n    \n    \n      2\n      7605.196719\n      6058.022931\n      0.528030\n      0.725869\n      00:23\n    \n    \n      3\n      6738.237530\n      5312.245515\n      0.501561\n      0.751644\n      00:23\n    \n    \n      4\n      5908.156025\n      4639.401604\n      0.500624\n      0.751768\n      00:23\n    \n  \n\n\n\n\n\n\n\nl_sum_n_smooth.fit(10, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      4075.124830\n      4743.987561\n      0.504186\n      0.748768\n      00:22\n    \n    \n      1\n      4534.079512\n      5471.174814\n      0.494402\n      0.757260\n      00:22\n    \n    \n      2\n      4162.734111\n      4298.111266\n      0.494588\n      0.758349\n      00:22\n    \n    \n      3\n      3856.497019\n      4796.030030\n      0.499238\n      0.751841\n      00:23\n    \n    \n      4\n      3702.818910\n      3923.179320\n      0.493548\n      0.759028\n      00:23\n    \n    \n      5\n      3538.964910\n      4498.824727\n      0.483678\n      0.767376\n      00:24\n    \n    \n      6\n      3410.967823\n      3545.588596\n      0.478077\n      0.772404\n      00:24\n    \n    \n      7\n      3237.693183\n      3850.488723\n      0.495080\n      0.754614\n      00:24\n    \n    \n      8\n      3111.747285\n      4198.825180\n      0.487525\n      0.764516\n      00:23\n    \n    \n      9\n      3083.048830\n      3667.951800\n      0.490212\n      0.759747\n      00:24\n    \n  \n\n\n\n\n\n\n\nshow_results(l_sum_n_smooth, [10, 50, 100])\n\n\n\n\n\n\nwith smoother enabled just for predictions\n\nl_sum_n_smooth.model.use_smooth = True\ndisplay(show_results(l_sum_n_smooth, [10, 50, 100]))\nl_sum_n_smooth.model.use_smooth = False\n\n\n\n\n\n\n\ndisplay_as_row(l_sum_n_smooth.model.get_info(var_names=hai64.columns))\n\n\n  trans_matrix (A) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.9738\n      0.0646\n      0.0733\n    \n    \n      z_1\n      0.3341\n      0.5742\n      0.3603\n    \n    \n      z_2\n      -0.3077\n      0.4121\n      0.6017\n    \n  \n\n  trans_cov (Q) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.5887\n      0.1503\n      -0.3697\n    \n    \n      z_1\n      0.1503\n      0.0416\n      -0.0951\n    \n    \n      z_2\n      -0.3697\n      -0.0951\n      0.2325\n    \n  \n\n  trans_off \n\n  \n    \n      latent\n      offset\n    \n  \n  \n    \n      z_0\n      0.0393\n    \n    \n      z_1\n      -0.0971\n    \n    \n      z_2\n      0.0904\n    \n  \n\n  obs_matrix (H) \n\n  \n    \n      variable\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      TA\n      0.4275\n      0.1589\n      0.7232\n    \n    \n      SW_IN\n      0.1958\n      0.8560\n      0.3983\n    \n    \n      VPD\n      0.3929\n      0.3593\n      0.7669\n    \n  \n\n  obs_cov (R) \n\n  \n    \n      variable\n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      TA\n      0.1843\n      -0.0151\n      -0.0008\n    \n    \n      SW_IN\n      -0.0151\n      0.0013\n      0.0001\n    \n    \n      VPD\n      -0.0008\n      0.0001\n      0.0000\n    \n  \n\n  obs_off \n\n  \n    \n      variable\n      offset\n    \n  \n  \n    \n      TA\n      0.5145\n    \n    \n      SW_IN\n      0.8793\n    \n    \n      VPD\n      0.6626\n    \n  \n\n  init_state_mean \n\n  \n    \n      latent\n      mean\n    \n  \n  \n    \n      z_0\n      -0.0205\n    \n    \n      z_1\n      -0.6986\n    \n    \n      z_2\n      -0.6000\n    \n  \n\n  init_state_cov \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      1.8298\n      1.1307\n      -0.6162\n    \n    \n      z_1\n      1.1307\n      1.2855\n      0.1820\n    \n    \n      z_2\n      -0.6162\n      0.1820\n      0.7764\n    \n  \n\n \n\n\n\n\n\n\nl_mean_smooth = train_loss('mean', use_smooth=True)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      697.162786\n      630.356973\n      0.503804\n      0.739012\n      00:36\n    \n    \n      1\n      635.893690\n      554.948438\n      0.397441\n      0.837237\n      00:33\n    \n    \n      2\n      573.158679\n      438.329959\n      0.397376\n      0.839449\n      00:33\n    \n    \n      3\n      443.483282\n      104.015152\n      0.415593\n      0.825035\n      00:34\n    \n    \n      4\n      279.839509\n      -134.222046\n      0.440517\n      0.801995\n      00:33\n    \n    \n      5\n      73.469916\n      -312.444000\n      0.405823\n      0.831356\n      00:33\n    \n    \n      6\n      -80.695937\n      -384.546196\n      0.409707\n      0.827130\n      00:33\n    \n    \n      7\n      -197.950462\n      -445.921780\n      0.409722\n      0.827629\n      00:33\n    \n    \n      8\n      -283.184402\n      -451.063827\n      0.409284\n      0.828156\n      00:34\n    \n    \n      9\n      -340.951170\n      -454.172981\n      0.406012\n      0.830759\n      00:34\n    \n    \n      10\n      -379.005242\n      -456.886911\n      0.399631\n      0.835787\n      00:33\n    \n    \n      11\n      -417.681836\n      -581.048350\n      0.137346\n      0.980816\n      00:33\n    \n    \n      12\n      -524.525933\n      -843.416952\n      0.122917\n      0.984063\n      00:33\n    \n    \n      13\n      -655.300585\n      -940.494837\n      0.112013\n      0.987288\n      00:33\n    \n    \n      14\n      -754.148642\n      -954.246318\n      0.106991\n      0.988218\n      00:33\n    \n    \n      15\n      -823.230645\n      -964.190164\n      0.093932\n      0.990694\n      00:33\n    \n    \n      16\n      -870.296853\n      -964.511931\n      0.097596\n      0.990023\n      00:33\n    \n    \n      17\n      -902.821560\n      -966.711134\n      0.083416\n      0.992802\n      00:33\n    \n    \n      18\n      -925.295414\n      -974.465392\n      0.070724\n      0.994710\n      00:36\n    \n    \n      19\n      -942.501159\n      -981.922380\n      0.061404\n      0.996135\n      00:34\n    \n  \n\n\n\n\n\n\n\nl_mean_smooth.recorder.plot_loss()\n\n\n\n\n\nshow_results(l_mean_smooth, [10, 50, 100])\n\n\n\n\n\n\n\ndisplay_as_row(l_mean_smooth.model.get_info(var_names=hai64.columns))\n\n\n  trans_matrix (A) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      1.0959\n      -0.1656\n      -0.0462\n    \n    \n      z_1\n      0.1858\n      0.6684\n      -0.1010\n    \n    \n      z_2\n      -0.0301\n      0.0735\n      0.9870\n    \n  \n\n  trans_cov (Q) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.2404\n      0.3451\n      0.0008\n    \n    \n      z_1\n      0.3451\n      0.5604\n      -0.0763\n    \n    \n      z_2\n      0.0008\n      -0.0763\n      0.3327\n    \n  \n\n  trans_off \n\n  \n    \n      latent\n      offset\n    \n  \n  \n    \n      z_0\n      0.5843\n    \n    \n      z_1\n      0.5746\n    \n    \n      z_2\n      0.3402\n    \n  \n\n  obs_matrix (H) \n\n  \n    \n      variable\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      TA\n      0.2666\n      -0.1751\n      -0.0240\n    \n    \n      SW_IN\n      -0.0178\n      0.2368\n      0.1899\n    \n    \n      VPD\n      0.3308\n      -0.2863\n      0.0118\n    \n  \n\n  obs_cov (R) \n\n  \n    \n      variable\n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      TA\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      SW_IN\n      0.0000\n      0.0000\n      -0.0000\n    \n    \n      VPD\n      0.0000\n      -0.0000\n      0.0000\n    \n  \n\n  obs_off \n\n  \n    \n      variable\n      offset\n    \n  \n  \n    \n      TA\n      0.5414\n    \n    \n      SW_IN\n      0.4390\n    \n    \n      VPD\n      0.6652\n    \n  \n\n  init_state_mean \n\n  \n    \n      latent\n      mean\n    \n  \n  \n    \n      z_0\n      0.7301\n    \n    \n      z_1\n      0.3346\n    \n    \n      z_2\n      0.2617\n    \n  \n\n  init_state_cov \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      1.4340\n      0.7932\n      0.2754\n    \n    \n      z_1\n      0.7932\n      1.1764\n      0.2383\n    \n    \n      z_2\n      0.2754\n      0.2383\n      0.5386\n    \n  \n\n \n\n\n\n\n\n\nl_mean_n_smooth = train_loss('mean', use_smooth=False)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      1057.629457\n      888.496646\n      1.018840\n      -0.055726\n      00:22\n    \n    \n      1\n      923.925165\n      796.762860\n      0.674683\n      0.544355\n      00:22\n    \n    \n      2\n      851.088850\n      747.840067\n      0.596738\n      0.645535\n      00:23\n    \n    \n      3\n      798.429587\n      705.430505\n      0.606329\n      0.634376\n      00:23\n    \n    \n      4\n      754.539451\n      668.123294\n      0.616092\n      0.622592\n      00:23\n    \n    \n      5\n      715.262041\n      636.254135\n      0.611814\n      0.628103\n      00:23\n    \n    \n      6\n      681.105511\n      615.883004\n      0.617857\n      0.620452\n      00:23\n    \n    \n      7\n      651.706290\n      591.641635\n      0.601030\n      0.640745\n      00:23\n    \n    \n      8\n      625.154468\n      570.554773\n      0.592957\n      0.650125\n      00:23\n    \n    \n      9\n      593.910834\n      477.784763\n      0.467038\n      0.783076\n      00:23\n    \n    \n      10\n      542.761901\n      414.206483\n      0.429209\n      0.817010\n      00:23\n    \n    \n      11\n      488.455552\n      339.528152\n      0.422597\n      0.821515\n      00:24\n    \n    \n      12\n      404.451756\n      144.976959\n      0.433326\n      0.812972\n      00:23\n    \n    \n      13\n      308.818492\n      107.266816\n      0.447009\n      0.800872\n      00:23\n    \n    \n      14\n      228.120486\n      59.730369\n      0.401824\n      0.838990\n      00:23\n    \n    \n      15\n      164.371803\n      39.778540\n      0.411961\n      0.830528\n      00:23\n    \n    \n      16\n      121.075008\n      31.160392\n      0.421206\n      0.823260\n      00:23\n    \n    \n      17\n      84.520273\n      4.141408\n      0.420663\n      0.823536\n      00:24\n    \n    \n      18\n      57.648469\n      4.640894\n      0.418233\n      0.825792\n      00:24\n    \n    \n      19\n      36.316722\n      3.784453\n      0.421853\n      0.822295\n      00:23\n    \n  \n\n\n\n\n\n\n\nl_mean_n_smooth.recorder.plot_loss()\n\n\n\n\n\nshow_results(l_mean_n_smooth, [10, 50, 100])\n\n\n\n\n\n\n\nl_mean_n_smooth.model.use_smooth = True\ndisplay(show_results(l_mean_n_smooth, [10, 50, 100]))\nl_mean_n_smooth.model.use_smooth = False\n\n\n\n\n\n\n\ndisplay_as_row(l_mean_n_smooth.model.get_info(var_names=hai64.columns))\n\n\n  trans_matrix (A) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.8564\n      0.3303\n      0.2741\n    \n    \n      z_1\n      0.0411\n      0.5239\n      -0.3802\n    \n    \n      z_2\n      0.1810\n      -0.3956\n      0.6654\n    \n  \n\n  trans_cov (Q) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.1339\n      0.1567\n      -0.1591\n    \n    \n      z_1\n      0.1567\n      0.1839\n      -0.1865\n    \n    \n      z_2\n      -0.1591\n      -0.1865\n      0.1893\n    \n  \n\n  trans_off \n\n  \n    \n      latent\n      offset\n    \n  \n  \n    \n      z_0\n      -0.1275\n    \n    \n      z_1\n      0.0851\n    \n    \n      z_2\n      0.1455\n    \n  \n\n  obs_matrix (H) \n\n  \n    \n      variable\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      TA\n      0.3125\n      0.3005\n      0.5561\n    \n    \n      SW_IN\n      0.1724\n      0.6300\n      0.5023\n    \n    \n      VPD\n      0.1824\n      0.6207\n      0.7000\n    \n  \n\n  obs_cov (R) \n\n  \n    \n      variable\n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      TA\n      0.0000\n      -0.0000\n      0.0001\n    \n    \n      SW_IN\n      -0.0000\n      0.0000\n      -0.0016\n    \n    \n      VPD\n      0.0001\n      -0.0016\n      0.1801\n    \n  \n\n  obs_off \n\n  \n    \n      variable\n      offset\n    \n  \n  \n    \n      TA\n      0.4672\n    \n    \n      SW_IN\n      0.0952\n    \n    \n      VPD\n      0.2722\n    \n  \n\n  init_state_mean \n\n  \n    \n      latent\n      mean\n    \n  \n  \n    \n      z_0\n      -1.0630\n    \n    \n      z_1\n      0.2947\n    \n    \n      z_2\n      -0.3536\n    \n  \n\n  init_state_cov \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      3.4376\n      0.9167\n      0.0107\n    \n    \n      z_1\n      0.9167\n      0.5092\n      -0.3225\n    \n    \n      z_2\n      0.0107\n      -0.3225\n      0.4000\n    \n  \n\n \n\n\n\n\n\n\nimport dill\n\n\nwith open(\"models_loss_comparison_30dec.pickle\", 'wb') as f:\n    # dill.dump([l_sum_smooth, l_sum_n_smooth, l_mean_smooth, l_mean_n_smooth], f)"
  },
  {
    "objectID": "kalman/kalman_fastai.html",
    "href": "kalman/kalman_fastai.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2"
  },
  {
    "objectID": "kalman/kalman_fastai.html#filter-loss---float32",
    "href": "kalman/kalman_fastai.html#filter-loss---float32",
    "title": "Meteo Imputation",
    "section": "Filter loss - (float32)",
    "text": "Filter loss - (float32)\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai = load_data()\n\n\nmodel = KalmanFilter.init_random(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1]).cuda()\n\n\nmodel.use_smooth = False\n\n\ndls = make_dataloader(hai[:5*20_000], block_len=200, gap_len=1, bs=30) # about 5 year of data \n\n\ndls.one_batch()[0][0].dtype, dls.one_batch()[0][0].device\n\n(torch.float32, device(type='cuda', index=0))\n\n\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False, reduction='sum'), cbs=[ShowGraphCallback], metrics=[msk_rmse, msk_r2])\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      28334.117188\n      24884.890625\n      1.044047\n      -0.270700\n      00:10\n    \n    \n      1\n      27505.285156\n      23820.916016\n      0.936844\n      -0.022787\n      00:09\n    \n    \n      2\n      26766.705078\n      22994.548828\n      0.868220\n      0.122282\n      00:09\n    \n    \n      3\n      26070.080078\n      22316.949219\n      0.818773\n      0.219742\n      00:09\n    \n    \n      4\n      25482.609375\n      21709.919922\n      0.778877\n      0.294147\n      00:09\n    \n    \n      5\n      24909.001953\n      21147.484375\n      0.743662\n      0.356575\n      00:09\n    \n    \n      6\n      24372.490234\n      20604.521484\n      0.716608\n      0.402541\n      00:09\n    \n    \n      7\n      23862.095703\n      20079.589844\n      0.694325\n      0.439092\n      00:09\n    \n    \n      8\n      23340.068359\n      19563.205078\n      0.676239\n      0.467748\n      00:09\n    \n    \n      9\n      22824.031250\n      19030.230469\n      0.661495\n      0.490591\n      00:09\n    \n  \n\n\n\n\n\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      20513.478516\n      18492.871094\n      0.646592\n      0.512978\n      00:09\n    \n    \n      1\n      20331.521484\n      17940.644531\n      0.636259\n      0.528079\n      00:09\n    \n    \n      2\n      20015.253906\n      17363.677734\n      0.626718\n      0.541712\n      00:09\n    \n    \n      3\n      19642.833984\n      16716.089844\n      0.616619\n      0.556175\n      00:10\n    \n    \n      4\n      19214.849609\n      15884.863281\n      0.600617\n      0.578533\n      00:09\n    \n    \n      5\n      18648.689453\n      14677.888672\n      0.547030\n      0.651080\n      00:10\n    \n    \n      6\n      17847.457031\n      13356.031250\n      0.496326\n      0.713167\n      00:09\n    \n    \n      7\n      17063.013672\n      12896.752930\n      0.455412\n      0.758161\n      00:09\n    \n    \n      8\n      16343.417969\n      12419.463867\n      0.433779\n      0.779173\n      00:10\n    \n    \n      9\n      15676.717773\n      11876.413086\n      0.415929\n      0.795271\n      00:13\n    \n  \n\n\n\n\n\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      13020.457031\n      11358.107422\n      0.402912\n      0.806103\n      00:11\n    \n    \n      1\n      12650.278320\n      10791.625000\n      0.392657\n      0.815727\n      00:10\n    \n    \n      2\n      12294.549805\n      10184.606445\n      0.383261\n      0.823566\n      00:10\n    \n    \n      3\n      11870.844727\n      9569.536133\n      0.376152\n      0.828884\n      00:10\n    \n    \n      4\n      11425.293945\n      8862.309570\n      0.366659\n      0.839007\n      00:11\n    \n    \n      5\n      10938.154297\n      8113.016602\n      0.363477\n      0.842085\n      00:11\n    \n    \n      6\n      10377.956055\n      7253.085938\n      0.363612\n      0.841904\n      00:10\n    \n    \n      7\n      9712.108398\n      5912.989746\n      0.365726\n      0.841298\n      00:10\n    \n    \n      8\n      8721.796875\n      1818.676758\n      0.379970\n      0.830193\n      00:11\n    \n    \n      9\n      6890.971191\n      -1213.163086\n      0.375065\n      0.835149\n      00:14\n    \n  \n\n\n\n\n\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      991.025146\n      -375.008484\n      0.374033\n      0.836182\n      00:10\n    \n    \n      1\n      382.140930\n      -1426.713867\n      0.376774\n      0.833246\n      00:09\n    \n    \n      2\n      -10.901829\n      -1699.835083\n      0.374207\n      0.835683\n      00:09\n    \n    \n      3\n      -249.258392\n      -1820.056519\n      0.373657\n      0.836395\n      00:09\n    \n    \n      4\n      -377.162689\n      -1793.390137\n      0.375005\n      0.834909\n      00:10\n    \n    \n      5\n      -475.895691\n      -1815.924683\n      0.375007\n      0.834904\n      00:09\n    \n    \n      6\n      -518.640808\n      -1835.481812\n      0.374665\n      0.835544\n      00:09\n    \n    \n      7\n      -586.902527\n      -1827.023193\n      0.374206\n      0.835841\n      00:10\n    \n    \n      8\n      -655.092407\n      -1729.346924\n      0.375481\n      0.834565\n      00:10\n    \n    \n      9\n      -669.299683\n      -1798.226685\n      0.374862\n      0.835274\n      00:09\n    \n  \n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\ndisplay_as_row(learn.model.get_info(var_names=hai.columns))\n\n\n  trans_matrix (A) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.1146\n      0.5802\n      0.1363\n    \n    \n      z_1\n      0.1496\n      0.6129\n      0.3537\n    \n    \n      z_2\n      0.3043\n      0.6128\n      0.0236\n    \n  \n\n  trans_cov (Q) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.7217\n      0.1717\n      0.0886\n    \n    \n      z_1\n      0.1717\n      0.1250\n      -0.0129\n    \n    \n      z_2\n      0.0886\n      -0.0129\n      0.0281\n    \n  \n\n  trans_off \n\n  \n    \n      latent\n      offset\n    \n  \n  \n    \n      z_0\n      0.2728\n    \n    \n      z_1\n      -0.0485\n    \n    \n      z_2\n      0.4645\n    \n  \n\n  obs_matrix (H) \n\n  \n    \n      variable\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      TA\n      0.4154\n      0.6550\n      0.3770\n    \n    \n      SW_IN\n      0.0119\n      0.5495\n      0.5917\n    \n    \n      VPD\n      0.1603\n      0.6821\n      -0.0631\n    \n  \n\n  obs_cov (R) \n\n  \n    \n      variable\n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      TA\n      0.8515\n      0.1525\n      0.9209\n    \n    \n      SW_IN\n      0.1525\n      0.2047\n      0.2631\n    \n    \n      VPD\n      0.9209\n      0.2631\n      1.8438\n    \n  \n\n  obs_off \n\n  \n    \n      variable\n      offset\n    \n  \n  \n    \n      TA\n      0.6057\n    \n    \n      SW_IN\n      0.4861\n    \n    \n      VPD\n      0.1418\n    \n  \n\n  init_state_mean \n\n  \n    \n      latent\n      mean\n    \n  \n  \n    \n      z_0\n      -0.0218\n    \n    \n      z_1\n      0.3442\n    \n    \n      z_2\n      0.0868\n    \n  \n\n  init_state_cov \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      1.1216\n      1.0998\n      0.7476\n    \n    \n      z_1\n      1.0998\n      1.9714\n      0.9649\n    \n    \n      z_2\n      0.7476\n      0.9649\n      0.6006\n    \n  \n\n \n\n\nThis is filtering! this is not smoothing\n\n# torch.save(learn.model, \"model_trained_30dec.pickle\")\n\n\nshow_results(learn, bind_interaction=False)\n\n\n\n\n\n\n\nlearn.model.use_smooth = True\nshow_results(learn, items=[10, 110, 130])\nlearn.model.use_smooth = False\n\n\n\n\n\n\n\nshow_results(learn, items=[10, 110, 130])\n\n\n\n\n\n\n\n# learn.fit(10, 1e-2)\n\n\n# learn.recorder.plot_loss()\n\n\nlearn.model.use_smooth = False\n\n\nshow_results(learn, items=[1,2,3])\n\n\n\n\n\n\n\n# torch.save(learn.model, \"trained_filter_29_dec_1.pickle\")"
  },
  {
    "objectID": "kalman/kalman_fastai.html#filter-loss---float64",
    "href": "kalman/kalman_fastai.html#filter-loss---float64",
    "title": "Meteo Imputation",
    "section": "Filter loss - (float64)",
    "text": "Filter loss - (float64)\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai64 = load_data(np.float64)\n\n\nmodel64 = KalmanFilter.init_random(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1], dtype=torch.float64).cuda()\n\n\nmodel64.use_smooth = False\n\n\ndls64 = make_dataloader(hai64[:5*20_000], block_len=200, gap_len=1, bs=10) # about 5 year of data \n\n\ndls64.one_batch()[0][0].dtype, dls64.one_batch()[0][0].device\n\n(torch.float64, device(type='cuda', index=0))\n\n\n\nlearn64 = Learner(dls, model, loss_func=KalmanLoss(only_gap=False, reduction='sum'), cbs=[ShowGraphCallback, Float16Callback], metrics=[msk_rmse, msk_r2])\n\nNameError: name 'Float16Callback' is not defined\n\n\n\nlearn64.fit(10, 2e-3)\n\n\nlearn64.recorder.plot_loss()\n\n\ndisplay_as_row(learn64.model.get_info(var_names=hai64.columns))\n\n\nshow_results(learn64, bind_interaction=False)\n\n\n# learn.fit(10, 1e-2)\n\n\n# learn.recorder.plot_loss()\n\n\nlearn.model.use_smooth = False\n\n\nshow_results(learn, items=[1,2,3])"
  },
  {
    "objectID": "kalman/KalmanFilter_PyTorch.html",
    "href": "kalman/KalmanFilter_PyTorch.html",
    "title": "Kalman Filter Hainich",
    "section": "",
    "text": "Kalman filter model on Hainich data using PyTorch\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.data import hai, units\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\nfrom meteo_imp.kalman.model import *\nfrom ipywidgets import interact, interact_manual, IntSlider\n\n\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n    \n  \n  \n    \n      200001011712.0\n      -0.60\n      0.0\n      0.222\n    \n    \n      200001011712.0\n      -0.65\n      0.0\n      0.122\n    \n    \n      200001011712.0\n      -0.58\n      0.0\n      0.090\n    \n    \n      200001011712.0\n      -0.51\n      0.0\n      0.110\n    \n    \n      200001011712.0\n      -0.49\n      0.0\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      200001044480.0\n      4.74\n      0.0\n      1.191\n    \n    \n      200001044480.0\n      4.75\n      0.0\n      1.057\n    \n    \n      200001044480.0\n      4.76\n      0.0\n      0.935\n    \n    \n      200001044480.0\n      4.62\n      0.0\n      1.162\n    \n    \n      200001044480.0\n      4.51\n      0.0\n      1.636\n    \n  \n\n200 rows × 3 columns\n\n\n\n\ndata = MeteoDataTest(hai).add_gap(10, ['TA', 'SW_IN', 'VPD'], 30)\n\n\ndata.data\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n    \n  \n  \n    \n      200001011712.0\n      -0.60\n      0.0\n      0.222\n    \n    \n      200001011712.0\n      -0.65\n      0.0\n      0.122\n    \n    \n      200001011712.0\n      -0.58\n      0.0\n      0.090\n    \n    \n      200001011712.0\n      -0.51\n      0.0\n      0.110\n    \n    \n      200001011712.0\n      -0.49\n      0.0\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      200001044480.0\n      4.74\n      0.0\n      1.191\n    \n    \n      200001044480.0\n      4.75\n      0.0\n      1.057\n    \n    \n      200001044480.0\n      4.76\n      0.0\n      0.935\n    \n    \n      200001044480.0\n      4.62\n      0.0\n      1.162\n    \n    \n      200001044480.0\n      4.51\n      0.0\n      1.636\n    \n  \n\n200 rows × 3 columns\n\n\n\n\nimport numpy as np\n\n\nimp = KalmanImputation(data.data)\n\n\nimp.fit(n_iter = 50)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat iteration 9 expected positive definite matrix. \ntensor([[ 1.8868, -0.4285, -1.4917],\n        [-0.4203,  1.1536, -0.5259],\n        [-1.4795, -0.5314,  1.7019]], grad_fn=<SubBackward0>)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat iteration 9 expected positive definite matrix. \ntensor([[ 0.2549, -0.1689, -0.1888],\n        [-0.1536,  0.4337, -0.2591],\n        [-0.1773, -0.2612,  0.2829]], grad_fn=<AddBackward0>)\nat iteration 9 expected positive definite matrix. \ntensor([[ 0.6063, -0.3421, -0.3056],\n        [-0.3361,  0.6454, -0.3535],\n        [-0.3007, -0.3570,  0.5838]], grad_fn=<AddBackward0>)\n\n\nValueError: Expected parameter covariance_matrix (Tensor of shape (3, 3)) of distribution MultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[ 0.6063, -0.3421, -0.3056],\n        [-0.3361,  0.6454, -0.3535],\n        [-0.3007, -0.3570,  0.5838]], grad_fn=<ExpandBackward0>)\n\n\n\nimp.model.plot_loss()\n\n\nimp.impute()\n\n\ndata.data.columns\n\n\nres = imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nres.display_results()\n\n\n\n\ndef gap2res(var_sel, gap_len, gap_start, n_iter):\n    data = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\n    return KalmanImputation(data.data).fit(n_iter=n_iter).to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10, 5)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]"
  },
  {
    "objectID": "kalman/KalmanFilter_simple.html",
    "href": "kalman/KalmanFilter_simple.html",
    "title": "Local Level Hainich",
    "section": "",
    "text": "simple local level model on Hainich data\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport altair as alt\nalt.renderers.enable('mimetype')\n\nRendererRegistry.enable('mimetype')\n\n\n\nfrom meteo_imp.data import hai, units\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\nfrom meteo_imp.kalman.model import *\nfrom ipywidgets import interact, interact_manual, IntSlider\n\n\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-05 02:00:00\n      4.74\n      0.0\n      1.191\n    \n    \n      2000-01-05 02:30:00\n      4.75\n      0.0\n      1.057\n    \n    \n      2000-01-05 03:00:00\n      4.76\n      0.0\n      0.935\n    \n    \n      2000-01-05 03:30:00\n      4.62\n      0.0\n      1.162\n    \n    \n      2000-01-05 04:00:00\n      4.51\n      0.0\n      1.636\n    \n  \n\n200 rows × 3 columns\n\n\n\n\ndata = MeteoDataTest(hai).add_gap(10, ['TA', 'SW_IN', 'VPD'], 30)\n\n\ndata.data\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-05 02:00:00\n      4.74\n      0.0\n      1.191\n    \n    \n      2000-01-05 02:30:00\n      4.75\n      0.0\n      1.057\n    \n    \n      2000-01-05 03:00:00\n      4.76\n      0.0\n      0.935\n    \n    \n      2000-01-05 03:30:00\n      4.62\n      0.0\n      1.162\n    \n    \n      2000-01-05 04:00:00\n      4.51\n      0.0\n      1.636\n    \n  \n\n200 rows × 3 columns\n\n\n\nWarning WIP. Data is not standardized (at it should)\n\nimp = KalmanImputation(data.data, LocalLevelModel)\n\n\nimp.fit()\n\n<meteo_imp.kalman.imputation.KalmanImputation>\n\n\n\nimp.impute()\n\nTypeError: KalmanModel.predict() takes 2 positional arguments but 3 were given\n\n\n\ndata.data.columns\n\n\nres = imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nres.display_results()\n\n\n\n\ndef gap2res(var_sel, gap_len, gap_start, model, n_iter):\n    data = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\n    return KalmanImputation(data.data, model).fit(n_iter=n_iter).to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10, LocalLevelModel, 5)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\n\n\n\ngap_len = 30\ngap_start = 40\nvar_sel = ('TA', 'SW_IN', 'VPD')\nn_iter = 10\n\n\ndata = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\nimp_ls = KalmanImputation(data.data, LocalSlopeModel).fit(n_iter=n_iter)\nres_ls = imp_ls.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nres_ls.display_results()\n\n\nimp\n\n\ndata = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\nimp_ls = KalmanImputation(data.data, LocalSlopeModel).fit(n_iter=n_iter, smooth=False).to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nimp_ls.display_results()"
  },
  {
    "objectID": "kalman/kalman_obs_cov.html",
    "href": "kalman/kalman_obs_cov.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nshortcut for having randomly initialized and with correct type paramters\nmore sensible default to the obs_cov"
  },
  {
    "objectID": "kalman/kalman_obs_cov.html#results",
    "href": "kalman/kalman_obs_cov.html#results",
    "title": "Meteo Imputation",
    "section": "Results",
    "text": "Results\n\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\n\n\ndef gap2res(var_sel, gap_len, gap_start, block_start=1000, block_end=1200):\n    data = MeteoDataTest(hai.iloc[block_start:block_end, :]).add_gap(gap_len, var_sel, gap_start)\n    imp = KalmanImputation(data.data)\n    imp.model.filter.load_state_dict(trained_state)\n    return imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\ndata = MeteoDataTest(hai)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\nfrom ipywidgets import interact_manual, IntSlider"
  },
  {
    "objectID": "kalman/kalman_float64.html",
    "href": "kalman/kalman_float64.html",
    "title": "Train Kalman filter using Fastai using float64",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nimport pandas as pd\nimport torch\nimport numpy as np\n\nNameError: name 'ListNormal' is not defined\n\n\n\n@cache_disk(\"full_hai\")\ndef load_data():\n    return read_fluxnet_csv(hai_path, None, num_dtype=np.float64)\n\nhai = load_data()\n\n\nmodel = KalmanFilter(**KalmanFilterTester(dtype=torch.float64).params)\n\n\ndls = make_dataloader(hai, 200, 10, bs=10) \n\n\nlen(hai) / 200 / 10 * .8\n\n91.1808\n\n\n\nlen(dls.train)\n\n91\n\n\n\nlearn = Learner(dls, model, loss_func=imp_ll_loss, cbs=[ShowGraphCallback, Float64Callback])\n\n\nlen(dls.train)\n\n91\n\n\n\n\n\nlearn.fit(10, 1e-3)\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n  \n\n\n    \n      \n      0.00% [0/91 00:00<?]\n    \n    \n\n\nNotImplementedError: Module [KalmanFilter] is missing the required \"forward\" function\n\n\n\nlearn.recorder.plot_loss()\n\n\n# learn.fit(10, 1e-2)\n\n\n# learn.recorder.plot_loss()\n\n\n# torch.save(learn.model, \"trained_model_20_dec_f64.pickle\")\n\n\ntrained_state = learn.model.state_dict()\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\n\n\ndef gap2res(var_sel, gap_len, gap_start, block_start=1000, block_end=1200):\n    data = MeteoDataTest(hai.iloc[block_start:block_end, :]).add_gap(gap_len, var_sel, gap_start)\n    imp = KalmanImputation(data.data)\n    imp.model.filter = learn.model # ensure float64 support\n    imp.model.filter.load_state_dict(trained_state)\n    return imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\ndisplay_as_row(learn.model.get_info())\n\n\ncheck_posdef(learn.model.obs_cov.to(torch.float32))\n\n\ngap2res(var_sel, gap_start=30, gap_len=15, block_start=500, block_end=700).display_results()\n\n\n\n\n\ncompute the loss for all predictions not only the gap\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False), cbs=[ShowGraphCallback, Float64Callback])\n\n\nlearn.fit(10, 2e-3)\n\n\nlearn.recorder.plot_loss()\n\nHorrible idea … probably by smoothing is too easy to predict the points that the parameters of the model have basically no influence.\nNeed to try with filtering\n\n\n\ncompute the loss for all predictions not only the gap\n\nmodel = KalmanFilter(**KalmanFilterTester(dtype=torch.float64).params)\n\n\nmodel.use_smooth = False\n\n\nmodel.check_args = None\n\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False), cbs=[ShowGraphCallback, Float64Callback])\n\n\nlearn.fit(12, 2e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      7412.622489\n      6652.202241\n      03:45\n    \n    \n      1\n      5049.731810\n      2825.582725\n      03:44\n    \n    \n      2\n      1962.345019\n      903.253100\n      03:47\n    \n    \n      3\n      318.255686\n      -117.658645\n      03:44\n    \n    \n      4\n      -323.398913\n      -448.278246\n      03:47\n    \n    \n      5\n      -654.908696\n      -679.999570\n      03:43\n    \n    \n      6\n      -803.473786\n      -873.061342\n      03:46\n    \n    \n      7\n      -1016.195842\n      -1018.383004\n      03:45\n    \n    \n      8\n      -1142.367950\n      -1148.339634\n      03:46\n    \n    \n      9\n      -1260.992943\n      -1299.262096\n      03:41\n    \n    \n      10\n      -1431.255912\n      -1435.102765\n      03:46\n    \n    \n      11\n      -1686.184436\n      -1593.321410\n      03:40\n    \n  \n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nlearn.fit(10, 1e-3)\n\n\nlearn.recorder.plot_loss()\n\n\nlearn.fit(10, 5e-4)\n\n\nlearn.recorder.plot_loss()\n\n\ntrained_state = learn.model.state_dict()\n\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\n\n\ndef gap2res(var_sel, gap_len, gap_start, block_start=1000, block_end=1200):\n    data = MeteoDataTest(hai.iloc[block_start:block_end, :]).add_gap(gap_len, var_sel, gap_start)\n    imp = KalmanImputation(data.data)\n    imp.model.filter = learn.model # ensure float64 support\n    imp.model.filter.load_state_dict(trained_state)\n    return imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\nfrom ipywidgets import interact_manual, IntSlider"
  },
  {
    "objectID": "kalman/Process_noise.html",
    "href": "kalman/Process_noise.html",
    "title": "Process Noise estimation",
    "section": "",
    "text": "Process Noise estimation\n\nfor a local level model using data from Hainich\n\n\nfrom meteo_imp.data import hai\n\n\nta_diff = (hai.TA - hai.TA.shift(-1))\nta_diff.hist(bins=20)\n\n<AxesSubplot: >\n\n\n\n\n\nthis is the difference between TA and the previous time step which is not too far from a normal distribution\n\nta_diff.std()\n\n0.12497036855024059\n\n\nso This should be the value of the Q in the kalman filter"
  },
  {
    "objectID": "analyze_gaps_fluxnet.html",
    "href": "analyze_gaps_fluxnet.html",
    "title": "Analyze gaps fluxnet",
    "section": "",
    "text": "from IPython.display import display\nfrom ipywidgets import widgets, interact\n\n\nfrom pathlib import Path\nimport polars as pl\nfrom datetime import datetime\nfrom fastcore.utils import * # support of ls for paths\nimport matplotlib.pyplot as plt\nimport altair as alt\n\n\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\n\n\nsite_info = pl.read_parquet(out_dir / \"../site_info.parquet\").select([\n    pl.col(\"start\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"end\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"site\").cast(pl.Categorical).sort()\n])\n\n\nsite_info.head()\n\n\n\n\nshape: (5, 3)\n\n\n\n\nstart\n\n\nend\n\n\nsite\n\n\n\n\ndatetime[μs]\n\n\ndatetime[μs]\n\n\ncat\n\n\n\n\n\n\n2009-01-01 00:30:00\n\n\n2012-01-01 00:00:00\n\n\n\"AR-SLu\"\n\n\n\n\n2009-01-01 00:30:00\n\n\n2013-01-01 00:00:00\n\n\n\"AR-Vir\"\n\n\n\n\n2002-01-01 00:30:00\n\n\n2013-01-01 00:00:00\n\n\n\"AT-Neu\"\n\n\n\n\n2007-01-01 00:30:00\n\n\n2010-01-01 00:00:00\n\n\n\"AU-Ade\"\n\n\n\n\n2010-01-01 00:30:00\n\n\n2015-01-01 00:00:00\n\n\n\"AU-ASM\"\n\n\n\n\n\n\n\n\ndef duration_n_obs(duration):\n    \"converts a duration into a n of fluxnet observations\"\n    return abs(int(duration.total_seconds() / (30 * 60)))\n\n\nduration_n_obs(site_info[1, \"start\"] - site_info[1, \"end\"])\n\n70127\n\n\n\n# maybe this code should actually go in 20_gap_finding\nfiles = out_dir.ls()\nfiles.sort() # need to sort to match the site_info\nsites = []\nfor i, path in enumerate(files):\n    sites.append(pl.scan_parquet(path).with_columns([\n        pl.lit(site_info[i, \"site\"]).alias(\"site\"),\n        pl.lit(duration_n_obs(site_info[i, \"start\"] -  site_info[i, \"end\"])).alias(\"total_obs\"),\n        pl.col(\"TIMESTAMP_END\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\").alias(\"end\"),\n    ]).drop(\"TIMESTAMP_END\"))\n\n\ngap_stat = pl.concat(sites)\n\n\ngap_stat.head().fetch(5)\n\n\n\n\nshape: (5, 5)\n\n\n\n\ngap_len\n\n\nvariable\n\n\nsite\n\n\ntotal_obs\n\n\nend\n\n\n\n\nu32\n\n\nstr\n\n\nstr\n\n\ni32\n\n\ndatetime[μs]\n\n\n\n\n\n\n16992\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2009-01-01 00:30:00\n\n\n\n\n5\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2009-12-21 11:00:00\n\n\n\n\n1\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2009-12-21 17:00:00\n\n\n\n\n1\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2010-01-06 13:00:00\n\n\n\n\n3\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2010-01-07 13:00:00\n\n\n\n\n\n\n\n\ndef filter_variables(variables = [\"TA_F_QC\", \"SW_IN_QC\", \"LW_IN_QC\", \"VPD_F_QC\"]):\n    expr = False\n    for var in variables:\n        expr |= pl.col(\"variable\") == var\n    return expr\n\n\ndef pl_in(col, values):\n    expr = False\n    for val in values:\n        expr |= pl.col(col) == val\n    return expr\n\nsome sites have a lot of data missing, with the avg gap length of several years, so is seems that the year can have an impact\nImportant! here the 3 possibles gap value of a QC variable are considered as one (null, 1, 2) we should co\n\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).groupby(\"site\").agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()).alias(\"frac_gap\")\n]).collect()\n\n\n\n\nshape: (205, 3)\n\n\n\n\nsite\n\n\nmean\n\n\nfrac_gap\n\n\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"SJ-Blv\"\n\n\n293.0\n\n\n0.242176\n\n\n\n\n\"AU-Rob\"\n\n\n3.727273\n\n\n0.011702\n\n\n\n\n\"CA-NS7\"\n\n\n1015.733333\n\n\n0.217263\n\n\n\n\n\"US-GLE\"\n\n\n1955.285714\n\n\n0.070967\n\n\n\n\n\"RU-Ha1\"\n\n\n3396.571429\n\n\n0.451955\n\n\n\n\n\"US-SRM\"\n\n\n45.59322\n\n\n0.013948\n\n\n\n\n\"US-MMS\"\n\n\n236.205128\n\n\n0.03284\n\n\n\n\n\"DE-Seh\"\n\n\n104.758065\n\n\n0.185235\n\n\n\n\n\"SN-Dhr\"\n\n\n1113.466667\n\n\n0.238168\n\n\n\n\n\"GL-NuF\"\n\n\n2274.5\n\n\n0.074127\n\n\n\n\n\"CN-Sw2\"\n\n\n8356.5\n\n\n0.635391\n\n\n\n\n\"US-AR1\"\n\n\n236.962963\n\n\n0.091234\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\"AU-Whr\"\n\n\n632.384615\n\n\n0.23446\n\n\n\n\n\"US-WPT\"\n\n\n152.0\n\n\n0.135799\n\n\n\n\n\"GF-Guy\"\n\n\n53.333333\n\n\n0.00083\n\n\n\n\n\"US-KS1\"\n\n\n2043.5\n\n\n0.23329\n\n\n\n\n\"DE-Gri\"\n\n\n155.428571\n\n\n0.022565\n\n\n\n\n\"IT-Ren\"\n\n\n214.107143\n\n\n0.021372\n\n\n\n\n\"BE-Vie\"\n\n\n114.333333\n\n\n0.094729\n\n\n\n\n\"US-ARb\"\n\n\n131.582524\n\n\n0.386798\n\n\n\n\n\"CH-Lae\"\n\n\n101.416667\n\n\n0.075722\n\n\n\n\n\"CA-NS6\"\n\n\n898.65\n\n\n0.205061\n\n\n\n\n\"FI-Lom\"\n\n\n1.0\n\n\n0.000038\n\n\n\n\n\"AU-Fog\"\n\n\n214.365854\n\n\n0.167069\n\n\n\n\n\n\n\n\n\n\nshort_gaps = gap_stat.filter(pl_in('variable', ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC'])).filter(pl.col('gap_len') < 200).groupby('variable').agg(pl.col('gap_len').mean()).collect()\n\n\nshort_gaps\n\n\n\n\nshape: (3, 2)\n\n\n\n\nvariable\n\n\ngap_len\n\n\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"TA_F_QC\"\n\n\n11.33915\n\n\n\n\n\"VPD_F_QC\"\n\n\n9.642444\n\n\n\n\n\"SW_IN_F_QC\"\n\n\n5.726891\n\n\n\n\n\n\n\n\nprint(short_gaps.to_pandas().to_markdown())\n\n|    | variable   |   gap_len |\n|---:|:-----------|----------:|\n|  0 | TA_F_QC    |  11.3392  |\n|  1 | VPD_F_QC   |   9.64244 |\n|  2 | SW_IN_F_QC |   5.72689 |\n\n\n\ngaps_year_site_ta = gap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).with_column(\n  pl.col(\"end\").dt.year().alias(\"year\")  \n).groupby([\"site\", \"year\"]).agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / (48 * 365)).alias(\"frac_gap\")\n]).sort([\"site\", \"year\"]).collect()\n\n\ngaps_year_site_ta.describe()\n\n\n\n\nshape: (7, 5)\n\n\n\n\ndescribe\n\n\nsite\n\n\nyear\n\n\nmean\n\n\nfrac_gap\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"count\"\n\n\n\"1232\"\n\n\n1232.0\n\n\n1232.0\n\n\n1232.0\n\n\n\n\n\"null_count\"\n\n\n\"0\"\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n\"mean\"\n\n\nnull\n\n\n2007.18263\n\n\n826.696712\n\n\n0.156607\n\n\n\n\n\"std\"\n\n\nnull\n\n\n4.633427\n\n\n3339.649175\n\n\n0.350794\n\n\n\n\n\"min\"\n\n\n\"AR-SLu\"\n\n\n1991.0\n\n\n1.0\n\n\n0.000057\n\n\n\n\n\"max\"\n\n\n\"ZM-Mon\"\n\n\n2015.0\n\n\n52608.0\n\n\n7.686073\n\n\n\n\n\"median\"\n\n\nnull\n\n\n2008.0\n\n\n42.166667\n\n\n0.030822\n\n\n\n\n\n\n\n\ngaps_year_site_ta.filter(pl.col(\"frac_gap\") >=1)\n\n\n\n\nshape: (15, 4)\n\n\n\n\nsite\n\n\nyear\n\n\nmean\n\n\nfrac_gap\n\n\n\n\nstr\n\n\ni32\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"BE-Bra\"\n\n\n2003\n\n\n17738.0\n\n\n1.012443\n\n\n\n\n\"BR-Sa1\"\n\n\n2006\n\n\n4167.4\n\n\n1.189326\n\n\n\n\n\"CA-NS1\"\n\n\n2001\n\n\n1219.941176\n\n\n1.183733\n\n\n\n\n\"CA-NS3\"\n\n\n2002\n\n\n4580.8\n\n\n1.307306\n\n\n\n\n\"DE-Lnf\"\n\n\n2007\n\n\n52608.0\n\n\n3.00274\n\n\n\n\n\"IT-Cpz\"\n\n\n1998\n\n\n228.990566\n\n\n1.385445\n\n\n\n\n\"IT-Ro2\"\n\n\n2008\n\n\n1464.5\n\n\n1.504623\n\n\n\n\n\"RU-Cok\"\n\n\n2003\n\n\n411.729167\n\n\n1.128025\n\n\n\n\n\"SJ-Adv\"\n\n\n2011\n\n\n7235.0\n\n\n1.23887\n\n\n\n\n\"US-Blo\"\n\n\n1997\n\n\n292.784615\n\n\n1.086244\n\n\n\n\n\"US-CRT\"\n\n\n2004\n\n\n17545.0\n\n\n1.001427\n\n\n\n\n\"US-LWW\"\n\n\n2011\n\n\n52608.0\n\n\n3.00274\n\n\n\n\n\"US-Syv\"\n\n\n2009\n\n\n52560.0\n\n\n3.0\n\n\n\n\n\"US-WCr\"\n\n\n2007\n\n\n17520.0\n\n\n1.0\n\n\n\n\n\"ZM-Mon\"\n\n\n2000\n\n\n22443.333333\n\n\n7.686073\n\n\n\n\n\n\n\n\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).filter(pl_in(\"site\", [\"ZM-Mon\"])).collect()\n\n\n\n\nshape: (8, 5)\n\n\n\n\ngap_len\n\n\nvariable\n\n\nsite\n\n\ntotal_obs\n\n\nend\n\n\n\n\nu32\n\n\nstr\n\n\nstr\n\n\ni32\n\n\ndatetime[μs]\n\n\n\n\n\n\n2913\n\n\n\"TA_F_QC\"\n\n\n\"ZM-Mon\"\n\n\n175343\n\n\n2000-01-01 00:30:00\n\n\n\n\n1\n\n\n\"TA_F_QC\"\n\n\n\"ZM-Mon\"\n\n\n175343\n\n\n2000-03-02 17:30:00\n\n\n\n\n1\n\n\n\"TA_F_QC\"\n\n\n\"ZM-Mon\"\n\n\n175343\n\n\n2000-03-04 04:30:00\n\n\n\n\n1\n\n\n\"TA_F_QC\"\n\n\n\"ZM-Mon\"\n\n\n175343\n\n\n2000-03-04 09:00:00\n\n\n\n\n1\n\n\n\"TA_F_QC\"\n\n\n\"ZM-Mon\"\n\n\n175343\n\n\n2000-03-10 11:30:00\n\n\n\n\n131743\n\n\n\"TA_F_QC\"\n\n\n\"ZM-Mon\"\n\n\n175343\n\n\n2000-03-11 02:00:00\n\n\n\n\n1\n\n\n\"TA_F_QC\"\n\n\n\"ZM-Mon\"\n\n\n175343\n\n\n2008-01-30 12:00:00\n\n\n\n\n7753\n\n\n\"TA_F_QC\"\n\n\n\"ZM-Mon\"\n\n\n175343\n\n\n2009-07-23 12:00:00\n\n\n\n\n\n\n\n\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).filter(pl_in(\"site\", [\"DE-Lnf\"])).collect()\n\n\n\n\nshape: (6, 5)\n\n\n\n\ngap_len\n\n\nvariable\n\n\nsite\n\n\ntotal_obs\n\n\nend\n\n\n\n\nu32\n\n\nstr\n\n\nstr\n\n\ni32\n\n\ndatetime[μs]\n\n\n\n\n\n\n5116\n\n\n\"TA_F_QC\"\n\n\n\"DE-Lnf\"\n\n\n192863\n\n\n2002-01-01 00:30:00\n\n\n\n\n52608\n\n\n\"TA_F_QC\"\n\n\n\"DE-Lnf\"\n\n\n192863\n\n\n2007-01-01 00:30:00\n\n\n\n\n1\n\n\n\"TA_F_QC\"\n\n\n\"DE-Lnf\"\n\n\n192863\n\n\n2010-04-15 10:00:00\n\n\n\n\n20\n\n\n\"TA_F_QC\"\n\n\n\"DE-Lnf\"\n\n\n192863\n\n\n2012-08-14 13:00:00\n\n\n\n\n45\n\n\n\"TA_F_QC\"\n\n\n\"DE-Lnf\"\n\n\n192863\n\n\n2012-10-09 14:00:00\n\n\n\n\n2\n\n\n\"TA_F_QC\"\n\n\n\"DE-Lnf\"\n\n\n192863\n\n\n2012-10-16 13:30:00\n\n\n\n\n\n\n\n\ndef visualize_by_site(df):\n    sites = df[\"site\"].unique()\n    for site in sites:\n        yield df.filter(pl.col(\"site\") == site)\n\n\nby_site = list(visualize_by_site(gaps_year_site_ta))\n\n\nbutton_next = widgets.Button(description=\"Next\", icon=\"arrow-right\")\nbutton_prev = widgets.Button(description=\"Previous\", icon=\"arrow-left\")\noutput = widgets.Output()\n\ndisplay(button_next, button_prev, output)\n\ni = 0\n\ndef update_view():\n    with output:\n        print(f\"{i} of {len(by_site)}\")\n        display(by_site[i])\n    output.clear_output(wait=True)\n\ndef on_next(b):\n    global i\n    if i < len(by_site):\n        i +=1\n    else:\n        button_next.disabled = True\n    update_view()\n\ndef on_prev(b):\n    global i\n    if i > 0:\n        i -=1\n    else:\n        button_prev.disabled = True\n    update_view()\n    \n\nbutton_next.on_click(on_next)\nbutton_prev.on_click(on_prev)\n\n\n\n\n\n\n\n\n\n\n\nta_gaps = gap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).collect()\n\n\nta_gaps.to_pandas().hist(column=\"gap_len\")\nplt.yscale('log')\n\n\n\n\n\n\n\n\nall_vars = gap_stat.select(pl.col(\"variable\").unique().sort()).collect()[\"variable\"]\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var):\n    ta_gaps = gap_stat.filter(\n        pl.col(\"variable\") == var \n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n\n\n\nall_sites = gap_stat.select(pl.col(\"site\").unique().sort()).collect()[\"site\"]\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site, small=False):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).filter(\n        pl.col(\"gap_len\") < 200 if small else True\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.title(f\"{site}: {var} - { 'gaps < 200' if small else 'all gaps'}\")\n\n\n\n\n\nvar_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == 'TA_F_QC')\n    ).filter(\n        pl.col(\"gap_len\") < 20000\n    ).sort(pl.col(\"gap\n        \n    ).collect().to_pandas()\n\n\nalt.data_transformers.enable('data_server')\n\n\n \nalt.Chart(var_gaps).mark_boxplot().encode(\n    y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n    x='gap_len'\n)\n\n\nwidgets.IntSlider?\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var, max_len=widgets.IntSlider(1000, 100, 20_000, 100)):\n    var_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") < max_len\n    ).collect().to_pandas()\n    \n    display(alt.Chart(var_gaps).mark_boxplot().encode(\n        y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n        x='gap_len'\n    ))\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(pl.col(\"gap_len\").sum()).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).groupby(\"variable\").agg(pl.col(\"gap_len\").mean()).collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).filter(pl.col(\"variable\") == \"TA_F_QC\").collect()"
  },
  {
    "objectID": "Train multiple latent.html",
    "href": "Train multiple latent.html",
    "title": "Train multiple latents",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\nfrom meteo_imp.results import _display_as_row \n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\n\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=300)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    #\"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-07 04:00:00\n      3.48\n      0.0\n      0.065\n    \n    \n      2000-01-07 04:30:00\n      3.48\n      0.0\n      0.063\n    \n    \n      2000-01-07 05:00:00\n      3.47\n      0.0\n      0.063\n    \n    \n      2000-01-07 05:30:00\n      3.43\n      0.0\n      0.057\n    \n    \n      2000-01-07 06:00:00\n      3.41\n      0.0\n      0.055\n    \n  \n\n300 rows × 3 columns\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nn_obs = 200\nn_latents = range(1,4) \n\n\ndata = GPFADataTest(hai[:n_obs])\n\n\ncache_file_gaps = cache_path / \"hai_train_multi_latents.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\nmodel_save_dir = here() / \"analysis/trained_models\"\n\n\nimps = [GPFAImputation(data.data, latent_dims=i) for i in n_latents]\n\n\ntotal_iter = 0\n\n\ndef train_save(n_iter=10):\n    global total_iter\n    total_iter += n_iter\n    for imp, n_lat in zip(imps, n_latents):\n        imp.fit(n_iter)\n        imp.learner.save(model_save_dir / f\"GPFA_l_{n_lat}_train_{total_iter}_1ker_{n_obs}_obs.pickle\")\n\n\n\n\n\ntrain_save(100)\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_idx=0\nprint(\"n_latent: \" + str(imps[p_idx].learner.latent_dims))\ndisplay(imps[p_idx].learner.plot_progress())\n_display_as_row(imps[p_idx].learner.model.get_info(imps[p_idx].learner.var_names))\n\nn_latent: 1\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  Lambda \n\n  \n    \n      variable\n      z0\n    \n  \n  \n    \n      TA\n      0.7965\n    \n    \n      SW_IN\n      -0.0502\n    \n    \n      VPD\n      0.4993\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      6.5707\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0013\n    \n    \n      SW_IN\n      0.9871\n    \n    \n      VPD\n      0.6120\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0027\n    \n  \n\n \n\n\n\n\n\n\np_idx=1\nprint(\"n_latent: \" + str(imps[p_idx].learner.latent_dims))\ndisplay(imps[p_idx].learner.plot_progress())\n_display_as_row(imps[p_idx].learner.model.get_info(imps[p_idx].learner.var_names))\n\nn_latent: 2\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      TA\n      -0.1784\n      0.7830\n    \n    \n      SW_IN\n      0.9565\n      0.3294\n    \n    \n      VPD\n      0.0957\n      0.5738\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      4.7033\n    \n    \n      z1\n      6.8953\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0007\n    \n    \n      SW_IN\n      0.0261\n    \n    \n      VPD\n      0.5750\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0031\n    \n  \n\n \n\n\n\n\n\n\np_idx=2\nprint(\"n_latent: \" + str(imps[p_idx].learner.latent_dims))\ndisplay(imps[p_idx].learner.plot_progress())\n_display_as_row(imps[p_idx].learner.model.get_info(imps[p_idx].learner.var_names))\n\nn_latent: 3\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  Lambda \n\n  \n    \n      variable\n      z0\n      z1\n      z2\n    \n  \n  \n    \n      TA\n      0.5749\n      0.1098\n      0.6824\n    \n    \n      SW_IN\n      -0.7796\n      -0.0461\n      0.6270\n    \n    \n      VPD\n      0.1813\n      0.7577\n      0.5405\n    \n  \n\n  lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      5.2282\n    \n    \n      z1\n      1.8765\n    \n    \n      z2\n      4.0832\n    \n  \n\n  psi \n\n  \n    \n      variable\n      psi\n    \n  \n  \n    \n      TA\n      0.0006\n    \n    \n      SW_IN\n      0.0285\n    \n    \n      VPD\n      0.0035\n    \n  \n\n  likelihood \n\n  \n    \n      noise\n    \n  \n  \n    \n      0.0017"
  },
  {
    "objectID": "GPFA Hainich - multi latent var.html",
    "href": "GPFA Hainich - multi latent var.html",
    "title": "Multiple latent …",
    "section": "",
    "text": "Trying to use more than 1 latent variable\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\n\ndef reset_seed():\n    torch.manual_seed(27);\n    np.random.seed(27);\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      LW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      302.475\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      302.475\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      301.677\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      301.677\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      301.677\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-05 02:00:00\n      4.74\n      0.0\n      330.202\n      1.191\n    \n    \n      2000-01-05 02:30:00\n      4.75\n      0.0\n      330.202\n      1.057\n    \n    \n      2000-01-05 03:00:00\n      4.76\n      0.0\n      330.202\n      0.935\n    \n    \n      2000-01-05 03:30:00\n      4.62\n      0.0\n      330.202\n      1.162\n    \n    \n      2000-01-05 04:00:00\n      4.51\n      0.0\n      330.202\n      1.636\n    \n  \n\n200 rows × 4 columns\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values())\n\n\ncache_file_gaps = cache_path / \"hai_diff_latents.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\n@cache_disk(cache_file_gaps)\ndef compute_multiple_latent():\n    hai_r_gaps = [GPFAImputationExplorer(\n        data_r_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_r_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    hai_c_gaps = [GPFAImputationExplorer(\n        data_c_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_c_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    return hai_r_gaps, hai_c_gaps\n\n\nhai_r_gaps, hai_c_gaps = compute_multiple_latent()\n\n\n\n\n\n\nhai_r_gaps\n\n[<gpfa_imputation.imputation.ImputationResult>,\n <gpfa_imputation.imputation.ImputationResult>,\n <gpfa_imputation.imputation.ImputationResult>]\n\n\n\nhai_r_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.6243\n    \n    \n      SW_IN\n      0.1244\n    \n    \n      LW_IN\n      0.0072\n    \n    \n      VPD\n      0.9597\n    \n  \n\n  Λ \n\n  \n    \n      variable\n      z0\n    \n  \n  \n    \n      time\n      0.8015\n    \n    \n      variable\n      0.4251\n    \n    \n      mean\n      -0.1879\n    \n    \n      std\n      1.0596\n    \n  \n\n  Lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      5.6057\n    \n  \n\n \n\n\n\nhai_r_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9913\n    \n    \n      SW_IN\n      0.1884\n    \n    \n      LW_IN\n      0.9673\n    \n    \n      VPD\n      0.5811\n    \n  \n\n  Λ \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      time\n      0.8553\n      0.5997\n    \n    \n      variable\n      0.3110\n      -0.1375\n    \n    \n      mean\n      -0.3717\n      0.6710\n    \n    \n      std\n      0.7057\n      0.2981\n    \n  \n\n  Lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      7.0575\n    \n    \n      z1\n      7.4311\n    \n  \n\n \n\n\n\nhai_r_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9915\n    \n    \n      SW_IN\n      0.9635\n    \n    \n      LW_IN\n      0.9679\n    \n    \n      VPD\n      0.6259\n    \n  \n\n  Λ \n\n  \n    \n      variable\n      z0\n      z1\n      z2\n    \n  \n  \n    \n      time\n      0.8547\n      0.4558\n      -0.0728\n    \n    \n      variable\n      0.4540\n      -0.3101\n      1.2743\n    \n    \n      mean\n      -0.2829\n      0.7159\n      0.0165\n    \n    \n      std\n      0.7268\n      0.1550\n      0.2461\n    \n  \n\n  Lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      6.8582\n    \n    \n      z1\n      7.3777\n    \n    \n      z2\n      5.3624\n    \n  \n\n \n\n\n\n\n\n\nhai_c_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9793\n    \n    \n      SW_IN\n      0.0152\n    \n    \n      LW_IN\n      -0.0507\n    \n    \n      VPD\n      0.5711\n    \n  \n\n  Λ \n\n  \n    \n      variable\n      z0\n    \n  \n  \n    \n      time\n      0.8542\n    \n    \n      variable\n      0.1202\n    \n    \n      mean\n      0.1860\n    \n    \n      std\n      0.6229\n    \n  \n\n  Lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      6.0641\n    \n  \n\n \n\n\n\nhai_c_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9789\n    \n    \n      SW_IN\n      0.1882\n    \n    \n      LW_IN\n      0.9552\n    \n    \n      VPD\n      0.6114\n    \n  \n\n  Λ \n\n  \n    \n      variable\n      z0\n      z1\n    \n  \n  \n    \n      time\n      0.5927\n      0.6728\n    \n    \n      variable\n      0.4136\n      -0.0936\n    \n    \n      mean\n      -0.5082\n      0.5232\n    \n    \n      std\n      0.5586\n      0.4200\n    \n  \n\n  Lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      5.8459\n    \n    \n      z1\n      6.5770\n    \n  \n\n \n\n\n\nhai_c_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9785\n    \n    \n      SW_IN\n      0.9640\n    \n    \n      LW_IN\n      0.9562\n    \n    \n      VPD\n      0.6513\n    \n  \n\n  Λ \n\n  \n    \n      variable\n      z0\n      z1\n      z2\n    \n  \n  \n    \n      time\n      -0.0527\n      -0.4903\n      0.7231\n    \n    \n      variable\n      0.8765\n      0.1260\n      0.2360\n    \n    \n      mean\n      -0.0042\n      -0.6728\n      -0.3668\n    \n    \n      std\n      0.2132\n      -0.2787\n      0.5878\n    \n  \n\n  Lengthscale \n\n  \n    \n      latent\n      lengthscale\n    \n  \n  \n    \n      z0\n      4.5862\n    \n    \n      z1\n      6.9706\n    \n    \n      z2\n      5.3607"
  },
  {
    "objectID": "Simple GP Hainich.html",
    "href": "Simple GP Hainich.html",
    "title": "Simple GP Hainich",
    "section": "",
    "text": "imputation using simple GP\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.simple_gp_imputation import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import *\n\n\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      TIMESTAMP_END\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      -0.60\n      0.0\n      0.222\n    \n    \n      2000-01-01 01:00:00\n      -0.65\n      0.0\n      0.122\n    \n    \n      2000-01-01 01:30:00\n      -0.58\n      0.0\n      0.090\n    \n    \n      2000-01-01 02:00:00\n      -0.51\n      0.0\n      0.110\n    \n    \n      2000-01-01 02:30:00\n      -0.49\n      0.0\n      0.102\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2000-01-05 02:00:00\n      4.74\n      0.0\n      1.191\n    \n    \n      2000-01-05 02:30:00\n      4.75\n      0.0\n      1.057\n    \n    \n      2000-01-05 03:00:00\n      4.76\n      0.0\n      0.935\n    \n    \n      2000-01-05 03:30:00\n      4.62\n      0.0\n      1.162\n    \n    \n      2000-01-05 04:00:00\n      4.51\n      0.0\n      1.636\n    \n  \n\n200 rows × 3 columns\n\n\n\n\n\n\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(20, meteo_vars.values(), 60)\n\n\ndata = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values(), 60)\nres = SimpleGPImputationExplorer(data.data).fit().to_result(data.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres.display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9186\n    \n    \n      SW_IN\n      0.9267\n    \n    \n      VPD\n      0.9880\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.2635\n      °C\n    \n    \n      SW_IN\n      10.0560\n      W m-2\n    \n    \n      VPD\n      0.0280\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      -9.1010\n    \n    \n      SW_IN\n      0.7756\n    \n    \n      VPD\n      0.5680\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.8120\n      °C\n    \n    \n      SW_IN\n      30.2152\n      W m-2\n    \n    \n      VPD\n      0.0796\n      hPa\n    \n  \n\n \n\n\n\nModel Info  lengthscale \n\n  \n    \n      variable\n      lengthscale\n    \n  \n  \n    \n      TA\n      4.7362\n    \n    \n      SW_IN\n      3.5688\n    \n    \n      VPD\n      2.6062\n    \n  \n\n  outputscale \n\n  \n    \n      variable\n      outputscale\n    \n  \n  \n    \n      TA\n      0.6055\n    \n    \n      SW_IN\n      1.4438\n    \n    \n      VPD\n      0.7713\n    \n  \n\n  likelihood \n\n  \n    \n      variable\n      noise\n    \n  \n  \n    \n      TA\n      0.0057\n    \n    \n      SW_IN\n      0.0237\n    \n    \n      VPD\n      0.0046\n    \n  \n\n \n\n\n\ndata = GPFADataTest(hai[:150]).add_gap(20, meteo_vars.values(), 60)\nres = SimpleGPImputationExplorer(data.data).fit().to_result(data.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres.display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9600\n    \n    \n      SW_IN\n      0.3226\n    \n    \n      VPD\n      0.9833\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1846\n      °C\n    \n    \n      SW_IN\n      30.5714\n      W m-2\n    \n    \n      VPD\n      0.0330\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      -0.9281\n    \n    \n      SW_IN\n      -0.9669\n    \n    \n      VPD\n      0.5433\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.4883\n      °C\n    \n    \n      SW_IN\n      83.2934\n      W m-2\n    \n    \n      VPD\n      0.0841\n      hPa\n    \n  \n\n \n\n\n\nModel Info  lengthscale \n\n  \n    \n      variable\n      lengthscale\n    \n  \n  \n    \n      TA\n      4.5675\n    \n    \n      SW_IN\n      3.7809\n    \n    \n      VPD\n      2.6307\n    \n  \n\n  outputscale \n\n  \n    \n      variable\n      outputscale\n    \n  \n  \n    \n      TA\n      0.6024\n    \n    \n      SW_IN\n      0.7026\n    \n    \n      VPD\n      0.7619\n    \n  \n\n  likelihood \n\n  \n    \n      variable\n      noise\n    \n  \n  \n    \n      TA\n      0.0040\n    \n    \n      SW_IN\n      0.0249\n    \n    \n      VPD\n      0.0045\n    \n  \n\n \n\n\n\n\n\nres_r_gaps = SimpleGPImputationExplorer(data_r_gaps.data).fit().to_result(data_r_gaps.data_compl_tidy, units=units)\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n\n\n\n\n\n\n\n\n\nres_r_gaps.display_results(plot_args = {'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9911\n    \n    \n      SW_IN\n      0.9661\n    \n    \n      VPD\n      0.9675\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0873\n      °C\n    \n    \n      SW_IN\n      6.8369\n      W m-2\n    \n    \n      VPD\n      0.0461\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9813\n    \n    \n      SW_IN\n      0.9574\n    \n    \n      VPD\n      0.9504\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1222\n      °C\n    \n    \n      SW_IN\n      10.1436\n      W m-2\n    \n    \n      VPD\n      0.0613\n      hPa\n    \n  \n\n \n\n\n\nModel Info  lengthscale \n\n  \n    \n      variable\n      lengthscale\n    \n  \n  \n    \n      TA\n      6.4697\n    \n    \n      SW_IN\n      5.0863\n    \n    \n      VPD\n      5.3424\n    \n  \n\n  outputscale \n\n  \n    \n      variable\n      outputscale\n    \n  \n  \n    \n      TA\n      0.7881\n    \n    \n      SW_IN\n      1.6644\n    \n    \n      VPD\n      1.1420\n    \n  \n\n  likelihood \n\n  \n    \n      variable\n      noise\n    \n  \n  \n    \n      TA\n      0.0068\n    \n    \n      SW_IN\n      0.0376\n    \n    \n      VPD\n      0.0305\n    \n  \n\n \n\n\n\n\n\n\nres_c_gaps = SimpleGPImputationExplorer(data_c_gaps.data).fit().to_result(data_c_gaps.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres_c_gaps.display_results(plot_args = {'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9500\n    \n    \n      SW_IN\n      0.9746\n    \n    \n      VPD\n      0.9401\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.2064\n      °C\n    \n    \n      SW_IN\n      5.9171\n      W m-2\n    \n    \n      VPD\n      0.0626\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      -8.4622\n    \n    \n      SW_IN\n      0.1705\n    \n    \n      VPD\n      -1065.0841\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.6429\n      °C\n    \n    \n      SW_IN\n      10.5049\n      W m-2\n    \n    \n      VPD\n      0.1929\n      hPa\n    \n  \n\n \n\n\n\nModel Info  lengthscale \n\n  \n    \n      variable\n      lengthscale\n    \n  \n  \n    \n      TA\n      3.0968\n    \n    \n      SW_IN\n      3.9848\n    \n    \n      VPD\n      2.5715\n    \n  \n\n  outputscale \n\n  \n    \n      variable\n      outputscale\n    \n  \n  \n    \n      TA\n      0.6266\n    \n    \n      SW_IN\n      0.7329\n    \n    \n      VPD\n      0.7777\n    \n  \n\n  likelihood \n\n  \n    \n      variable\n      noise\n    \n  \n  \n    \n      TA\n      0.0028\n    \n    \n      SW_IN\n      0.0239\n    \n    \n      VPD\n      0.0064\n    \n  \n\n \n\n\n\n\n\n\ntry with a different random seed\n\nreset_seed(101)\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values())\n\n\n\n\nres_r_gaps = SimpleGPImputationExplorer(data_r_gaps.data).fit().to_result(data_r_gaps.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres_r_gaps.display_results(plot_args = {'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9915\n    \n    \n      SW_IN\n      0.9671\n    \n    \n      VPD\n      0.9646\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0849\n      °C\n    \n    \n      SW_IN\n      6.7353\n      W m-2\n    \n    \n      VPD\n      0.0481\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9913\n    \n    \n      SW_IN\n      0.9149\n    \n    \n      VPD\n      0.9433\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.0848\n      °C\n    \n    \n      SW_IN\n      9.2910\n      W m-2\n    \n    \n      VPD\n      0.0680\n      hPa\n    \n  \n\n \n\n\n\nModel Info  lengthscale \n\n  \n    \n      variable\n      lengthscale\n    \n  \n  \n    \n      TA\n      6.4669\n    \n    \n      SW_IN\n      4.6214\n    \n    \n      VPD\n      4.9073\n    \n  \n\n  outputscale \n\n  \n    \n      variable\n      outputscale\n    \n  \n  \n    \n      TA\n      0.7311\n    \n    \n      SW_IN\n      0.9371\n    \n    \n      VPD\n      0.8954\n    \n  \n\n  likelihood \n\n  \n    \n      variable\n      noise\n    \n  \n  \n    \n      TA\n      0.0094\n    \n    \n      SW_IN\n      0.0193\n    \n    \n      VPD\n      0.0215\n    \n  \n\n \n\n\n\n\n\n\nres_c_gaps = SimpleGPImputationExplorer(data_c_gaps.data).fit().to_result(data_c_gaps.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres_c_gaps.display_results(plot_args = {'bind_interaction': False, 'properties': {}})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.9849\n    \n    \n      SW_IN\n      0.9654\n    \n    \n      VPD\n      0.9291\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.1135\n      °C\n    \n    \n      SW_IN\n      6.9104\n      W m-2\n    \n    \n      VPD\n      0.0680\n      hPa\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      TA\n      0.2640\n    \n    \n      SW_IN\n      0.0000\n    \n    \n      VPD\n      -9.7505\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n      units\n    \n  \n  \n    \n      TA\n      0.3417\n      °C\n    \n    \n      SW_IN\n      14.0756\n      W m-2\n    \n    \n      VPD\n      0.2107\n      hPa\n    \n  \n\n \n\n\n\nModel Info  lengthscale \n\n  \n    \n      variable\n      lengthscale\n    \n  \n  \n    \n      TA\n      3.0464\n    \n    \n      SW_IN\n      3.9805\n    \n    \n      VPD\n      2.5759\n    \n  \n\n  outputscale \n\n  \n    \n      variable\n      outputscale\n    \n  \n  \n    \n      TA\n      0.6008\n    \n    \n      SW_IN\n      0.7517\n    \n    \n      VPD\n      0.7584\n    \n  \n\n  likelihood \n\n  \n    \n      variable\n      noise\n    \n  \n  \n    \n      TA\n      0.0025\n    \n    \n      SW_IN\n      0.0284\n    \n    \n      VPD\n      0.0061"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPFA Imputation",
    "section": "",
    "text": "GPFA Imputation\nThis is the homepage of the website that contains all the analysis. Use the sidebar for nagivation\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nAdditional latent kernel\n\n\n\n\nAnalyze gaps fluxnet\n\n\n\n\nCompare Loss functions\n\n\n\n\nExploration of Loglikelihood computations\n\n\n\n\nFilter loss - (float32)\n\n\n\n\nGap Length Variation\n\n\n\n\nInit Lambda\n\n\n\n\nKalman Filter Hainich\n\n\n\n\nKernel visualization\n\n\n\n\nLocal Level Hainich\n\n\n\n\nLog Transform\n\n\n\n\nMultiple Latent\n\n\n\n\nMultiple latent …\n\n\n\n\nNotes\n\n\n\n\nPositive semi-definite\n\n\n\n\nProcess Noise estimation\n\n\n\n\nResults\n\n\n\n\nSimple GP Hainich\n\n\n\n\nSingle latent\n\n\n\n\nTrain Kalman filter using Fastai using float64\n\n\n\n\nTrain multiple latents\n\n\n\n\nVariable distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "extract_gap_all_fluxnet.html",
    "href": "extract_gap_all_fluxnet.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.fluxnet.gap_finder import *\n\nlinks are obtained from this https://fluxnet.org/data-and-manifest/ page(requires login) by running in the browser console this code\nvar x = document.querySelectorAll(\"a\");\nvar myarray = []\nfor (var i=0; i<x.length; i++){\nvar nametext = x[i].textContent;\nvar cleantext = nametext.replace(/\\s+/g, ' ').trim();\nvar cleanlink = x[i].href;\nmyarray.push([cleantext,cleanlink]);\n};\nfunction make_table() {\n    var table = '<table><thead><th>Links</th></thead><tbody>';\n   for (var i=0; i<myarray.length; i++) {\n            table += '<tr><td>'+myarray[i][1]+'</td></tr>';\n    };\n \n    var w = window.open(\"\");\nw.document.write(table); \n}\nmake_table()\ncode inspired from https://towardsdatascience.com/quickly-extract-all-links-from-a-web-page-using-javascript-and-the-browser-console-49bb6f48127b\nand then doing some smart copy pasting\nacually download in parallel all files with, so is faster than download with python\nparallel -a fluxnet_parallel_wget.txt --jobs 10 wget\n\nfrom fluxnet_links import all_fluxnet_link\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\n\ntest_file = Path(\"../../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip\")\ntmp_dir = Path(\"/tmp\")\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\n\n\nsite_info = download_and_find_gaps(all_fluxnet_link, download_dir, out_dir, tmp_dir)\n\n\n\n\nFLX_AR-SLu_FLUXNET2015_FULLSET_HH_2009-2011_1-4\nFLX_AR-Vir_FLUXNET2015_FULLSET_HH_2009-2012_1-4\nFLX_AT-Neu_FLUXNET2015_FULLSET_HH_2002-2012_1-4\nFLX_AU-Ade_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_AU-ASM_FLUXNET2015_FULLSET_HH_2010-2014_2-4\nFLX_AU-Cpr_FLUXNET2015_FULLSET_HH_2010-2014_2-4\nFLX_AU-Cum_FLUXNET2015_FULLSET_HH_2012-2014_2-4\nFLX_AU-DaP_FLUXNET2015_FULLSET_HH_2007-2013_2-4\nFLX_AU-DaS_FLUXNET2015_FULLSET_HH_2008-2014_2-4\nFLX_AU-Dry_FLUXNET2015_FULLSET_HH_2008-2014_2-4\nFLX_AU-Emr_FLUXNET2015_FULLSET_HH_2011-2013_1-4\nFLX_AU-Fog_FLUXNET2015_FULLSET_HH_2006-2008_1-4\nFLX_AU-Gin_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_AU-GWW_FLUXNET2015_FULLSET_HH_2013-2014_1-4\nFLX_AU-How_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_AU-Lox_FLUXNET2015_FULLSET_HH_2008-2009_1-4\nFLX_AU-RDF_FLUXNET2015_FULLSET_HH_2011-2013_1-4\nFLX_AU-Rig_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_AU-Rob_FLUXNET2015_FULLSET_HH_2014-2014_1-4\nFLX_AU-Stp_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_AU-TTE_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_AU-Tum_FLUXNET2015_FULLSET_HR_2001-2014_2-4\nFLX_AU-Wac_FLUXNET2015_FULLSET_HH_2005-2008_1-4\nFLX_AU-Whr_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_AU-Wom_FLUXNET2015_FULLSET_HH_2010-2014_1-4\nFLX_AU-Ync_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_BE-Bra_FLUXNET2015_FULLSET_HH_1996-2014_2-4\nFLX_BE-Lon_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_BE-Vie_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_BR-Sa1_FLUXNET2015_FULLSET_HR_2002-2011_1-4\nFLX_BR-Sa3_FLUXNET2015_FULLSET_HH_2000-2004_1-4\nFLX_CA-Gro_FLUXNET2015_FULLSET_HH_2003-2014_1-4\nFLX_CA-Man_FLUXNET2015_FULLSET_HH_1994-2008_1-4\nFLX_CA-NS1_FLUXNET2015_FULLSET_HH_2001-2005_2-4\nFLX_CA-NS2_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-NS3_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-NS4_FLUXNET2015_FULLSET_HH_2002-2005_1-4\nFLX_CA-NS5_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-NS6_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-NS7_FLUXNET2015_FULLSET_HH_2002-2005_1-4\nFLX_CA-Oas_FLUXNET2015_FULLSET_HH_1996-2010_1-4\nFLX_CA-Obs_FLUXNET2015_FULLSET_HH_1997-2010_1-4\nFLX_CA-Qfo_FLUXNET2015_FULLSET_HH_2003-2010_1-4\nFLX_CA-SF1_FLUXNET2015_FULLSET_HH_2003-2006_1-4\nFLX_CA-SF2_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-SF3_FLUXNET2015_FULLSET_HH_2001-2006_1-4\nFLX_CA-TP1_FLUXNET2015_FULLSET_HH_2002-2014_2-4\nFLX_CA-TP2_FLUXNET2015_FULLSET_HH_2002-2007_1-4\nFLX_CA-TP3_FLUXNET2015_FULLSET_HH_2002-2014_1-4\nFLX_CA-TP4_FLUXNET2015_FULLSET_HH_2002-2014_1-4\nFLX_CA-TPD_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_CG-Tch_FLUXNET2015_FULLSET_HH_2006-2009_1-4\nFLX_CH-Cha_FLUXNET2015_FULLSET_HH_2005-2014_2-4\nFLX_CH-Dav_FLUXNET2015_FULLSET_HH_1997-2014_1-4\nFLX_CH-Fru_FLUXNET2015_FULLSET_HH_2005-2014_2-4\nFLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_CH-Oe1_FLUXNET2015_FULLSET_HH_2002-2008_2-4\nFLX_CH-Oe2_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_CN-Cha_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_CN-Cng_FLUXNET2015_FULLSET_HH_2007-2010_1-4\nFLX_CN-Dan_FLUXNET2015_FULLSET_HH_2004-2005_1-4\nFLX_CN-Din_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_CN-Du2_FLUXNET2015_FULLSET_HH_2006-2008_1-4\nFLX_CN-Du3_FLUXNET2015_FULLSET_HH_2009-2010_1-4\nFLX_CN-Ha2_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_CN-HaM_FLUXNET2015_FULLSET_HH_2002-2004_1-4\nFLX_CN-Qia_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_CN-Sw2_FLUXNET2015_FULLSET_HH_2010-2012_1-4\nFLX_CZ-BK1_FLUXNET2015_FULLSET_HH_2004-2014_2-4\nFLX_CZ-BK2_FLUXNET2015_FULLSET_HH_2004-2012_2-4\nFLX_CZ-wet_FLUXNET2015_FULLSET_HH_2006-2014_1-4\nFLX_DE-Akm_FLUXNET2015_FULLSET_HH_2009-2014_1-4\nFLX_DE-Geb_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_DE-Gri_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4\nFLX_DE-Kli_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_DE-Lkb_FLUXNET2015_FULLSET_HH_2009-2013_1-4\nFLX_DE-Lnf_FLUXNET2015_FULLSET_HH_2002-2012_1-4\nFLX_DE-Obe_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_DE-RuR_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_DE-RuS_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_DE-Seh_FLUXNET2015_FULLSET_HH_2007-2010_1-4\nFLX_DE-SfN_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_DE-Spw_FLUXNET2015_FULLSET_HH_2010-2014_1-4\nFLX_DE-Tha_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_DE-Zrk_FLUXNET2015_FULLSET_HH_2013-2014_2-4\nFLX_DK-Eng_FLUXNET2015_FULLSET_HH_2005-2008_1-4\nFLX_DK-Fou_FLUXNET2015_FULLSET_HH_2005-2005_1-4\nFLX_DK-Sor_FLUXNET2015_FULLSET_HH_1996-2014_2-4\nFLX_ES-Amo_FLUXNET2015_FULLSET_HH_2007-2012_1-4\nFLX_ES-LgS_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_ES-LJu_FLUXNET2015_FULLSET_HH_2004-2013_1-4\nFLX_ES-Ln2_FLUXNET2015_FULLSET_HH_2009-2009_1-4\nFLX_FI-Hyy_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_FI-Jok_FLUXNET2015_FULLSET_HH_2000-2003_1-4\nFLX_FI-Let_FLUXNET2015_FULLSET_HH_2009-2012_1-4\nFLX_FI-Lom_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_FI-Sod_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_FR-Fon_FLUXNET2015_FULLSET_HH_2005-2014_1-4\nFLX_FR-Gri_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_FR-LBr_FLUXNET2015_FULLSET_HH_1996-2008_1-4\nFLX_FR-Pue_FLUXNET2015_FULLSET_HH_2000-2014_2-4\nFLX_GF-Guy_FLUXNET2015_FULLSET_HH_2004-2014_2-4\nFLX_GH-Ank_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_GL-NuF_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_GL-ZaF_FLUXNET2015_FULLSET_HH_2008-2011_2-4\nFLX_GL-ZaH_FLUXNET2015_FULLSET_HH_2000-2014_2-4\nFLX_IT-BCi_FLUXNET2015_FULLSET_HH_2004-2014_2-4\nFLX_IT-CA1_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_IT-CA2_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_IT-CA3_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_IT-Col_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_IT-Cp2_FLUXNET2015_FULLSET_HH_2012-2014_2-4\nFLX_IT-Cpz_FLUXNET2015_FULLSET_HH_1997-2009_1-4\nFLX_IT-Isp_FLUXNET2015_FULLSET_HH_2013-2014_1-4\nFLX_IT-La2_FLUXNET2015_FULLSET_HH_2000-2002_1-4\nFLX_IT-Lav_FLUXNET2015_FULLSET_HH_2003-2014_2-4\nFLX_IT-MBo_FLUXNET2015_FULLSET_HH_2003-2013_1-4\nFLX_IT-Noe_FLUXNET2015_FULLSET_HH_2004-2014_2-4\nFLX_IT-PT1_FLUXNET2015_FULLSET_HH_2002-2004_1-4\nFLX_IT-Ren_FLUXNET2015_FULLSET_HH_1998-2013_1-4\nFLX_IT-Ro1_FLUXNET2015_FULLSET_HH_2000-2008_1-4\nFLX_IT-Ro2_FLUXNET2015_FULLSET_HH_2002-2012_1-4\nFLX_IT-SR2_FLUXNET2015_FULLSET_HH_2013-2014_1-4\nFLX_IT-SRo_FLUXNET2015_FULLSET_HH_1999-2012_1-4\nFLX_IT-Tor_FLUXNET2015_FULLSET_HH_2008-2014_2-4\nFLX_JP-MBF_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_JP-SMF_FLUXNET2015_FULLSET_HH_2002-2006_1-4\nFLX_MY-PSO_FLUXNET2015_FULLSET_HH_2003-2009_1-4\nFLX_NL-Hor_FLUXNET2015_FULLSET_HH_2004-2011_1-4\nFLX_NL-Loo_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_PA-SPn_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_PA-SPs_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_RU-Che_FLUXNET2015_FULLSET_HH_2002-2005_1-4\nFLX_RU-Cok_FLUXNET2015_FULLSET_HH_2003-2014_2-4\nFLX_RU-Fyo_FLUXNET2015_FULLSET_HH_1998-2014_2-4\nFLX_RU-Ha1_FLUXNET2015_FULLSET_HH_2002-2004_1-4\nFLX_SD-Dem_FLUXNET2015_FULLSET_HH_2005-2009_2-4\nFLX_SJ-Adv_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_SJ-Blv_FLUXNET2015_FULLSET_HR_2008-2009_1-4\nFLX_SN-Dhr_FLUXNET2015_FULLSET_HH_2010-2013_1-4\nFLX_US-AR1_FLUXNET2015_FULLSET_HH_2009-2012_1-4\nFLX_US-AR2_FLUXNET2015_FULLSET_HH_2009-2012_1-4\nFLX_US-ARb_FLUXNET2015_FULLSET_HH_2005-2006_1-4\nFLX_US-ARc_FLUXNET2015_FULLSET_HH_2005-2006_1-4\nFLX_US-ARM_FLUXNET2015_FULLSET_HH_2003-2012_1-4\nFLX_US-Atq_FLUXNET2015_FULLSET_HH_2003-2008_1-4\nFLX_US-Blo_FLUXNET2015_FULLSET_HH_1997-2007_1-4\nFLX_US-Cop_FLUXNET2015_FULLSET_HR_2001-2007_1-4\nFLX_US-CRT_FLUXNET2015_FULLSET_HH_2011-2013_1-4\nFLX_US-GBT_FLUXNET2015_FULLSET_HH_1999-2006_1-4\nFLX_US-GLE_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_US-Goo_FLUXNET2015_FULLSET_HH_2002-2006_1-4\nFLX_US-Ha1_FLUXNET2015_FULLSET_HR_1991-2012_1-4\nFLX_US-IB2_FLUXNET2015_FULLSET_HH_2004-2011_1-4\nFLX_US-Ivo_FLUXNET2015_FULLSET_HH_2004-2007_1-4\nFLX_US-KS1_FLUXNET2015_FULLSET_HH_2002-2002_1-4\nFLX_US-KS2_FLUXNET2015_FULLSET_HH_2003-2006_1-4\nFLX_US-Lin_FLUXNET2015_FULLSET_HH_2009-2010_1-4\nFLX_US-Los_FLUXNET2015_FULLSET_HH_2000-2014_2-4\nFLX_US-LWW_FLUXNET2015_FULLSET_HH_1997-1998_1-4\nFLX_US-Me1_FLUXNET2015_FULLSET_HH_2004-2005_1-4\nFLX_US-Me2_FLUXNET2015_FULLSET_HH_2002-2014_1-4\nFLX_US-Me3_FLUXNET2015_FULLSET_HH_2004-2009_1-4\nFLX_US-Me4_FLUXNET2015_FULLSET_HH_1996-2000_1-4\nFLX_US-Me5_FLUXNET2015_FULLSET_HH_2000-2002_1-4\nFLX_US-Me6_FLUXNET2015_FULLSET_HH_2010-2014_2-4\nFLX_US-MMS_FLUXNET2015_FULLSET_HR_1999-2014_1-4\nFLX_US-Myb_FLUXNET2015_FULLSET_HH_2010-2014_2-4\nFLX_US-Ne1_FLUXNET2015_FULLSET_HR_2001-2013_1-4\nFLX_US-Ne2_FLUXNET2015_FULLSET_HR_2001-2013_1-4\nFLX_US-Ne3_FLUXNET2015_FULLSET_HR_2001-2013_1-4\nFLX_US-NR1_FLUXNET2015_FULLSET_HH_1998-2014_1-4\nFLX_US-Oho_FLUXNET2015_FULLSET_HH_2004-2013_1-4\nFLX_US-ORv_FLUXNET2015_FULLSET_HH_2011-2011_1-4\nFLX_US-PFa_FLUXNET2015_FULLSET_HR_1995-2014_1-4\nFLX_US-Prr_FLUXNET2015_FULLSET_HH_2010-2014_1-4\nFLX_US-SRC_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_US-SRG_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_US-SRM_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_US-Sta_FLUXNET2015_FULLSET_HH_2005-2009_1-4\nFLX_US-Syv_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_US-Ton_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_US-Tw1_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_US-Tw2_FLUXNET2015_FULLSET_HH_2012-2013_1-4\nFLX_US-Tw3_FLUXNET2015_FULLSET_HH_2013-2014_2-4\nFLX_US-Tw4_FLUXNET2015_FULLSET_HH_2013-2014_1-4\nFLX_US-Twt_FLUXNET2015_FULLSET_HH_2009-2014_1-4\nFLX_US-UMB_FLUXNET2015_FULLSET_HR_2000-2014_1-4\nFLX_US-UMd_FLUXNET2015_FULLSET_HH_2007-2014_1-4\nFLX_US-Var_FLUXNET2015_FULLSET_HH_2000-2014_1-4\nFLX_US-WCr_FLUXNET2015_FULLSET_HH_1999-2014_1-4\nFLX_US-Whs_FLUXNET2015_FULLSET_HH_2007-2014_1-4\nFLX_US-Wi0_FLUXNET2015_FULLSET_HH_2002-2002_1-4\nFLX_US-Wi1_FLUXNET2015_FULLSET_HH_2003-2003_1-4\nFLX_US-Wi2_FLUXNET2015_FULLSET_HH_2003-2003_1-4\nFLX_US-Wi3_FLUXNET2015_FULLSET_HH_2002-2004_1-4\nFLX_US-Wi4_FLUXNET2015_FULLSET_HH_2002-2005_1-4\nFLX_US-Wi5_FLUXNET2015_FULLSET_HH_2004-2004_1-4\nFLX_US-Wi6_FLUXNET2015_FULLSET_HH_2002-2003_1-4\nFLX_US-Wi7_FLUXNET2015_FULLSET_HH_2005-2005_1-4\nFLX_US-Wi8_FLUXNET2015_FULLSET_HH_2002-2002_1-4\nFLX_US-Wi9_FLUXNET2015_FULLSET_HH_2004-2005_1-4\nFLX_US-Wkg_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_US-WPT_FLUXNET2015_FULLSET_HH_2011-2013_1-4\nFLX_ZM-Mon_FLUXNET2015_FULLSET_HH_2000-2009_2-4\n\n\n\nsite_info\n\n\n\n\nshape: (206, 3)\n\n\n\n\nstart\n\n\nend\n\n\nsite\n\n\n\n\ni64\n\n\ni64\n\n\nstr\n\n\n\n\n\n\n200901010030\n\n\n201201010000\n\n\n\"AR-SLu\"\n\n\n\n\n200901010030\n\n\n201301010000\n\n\n\"AR-Vir\"\n\n\n\n\n200201010030\n\n\n201301010000\n\n\n\"AT-Neu\"\n\n\n\n\n200701010030\n\n\n201001010000\n\n\n\"AU-Ade\"\n\n\n\n\n201001010030\n\n\n201501010000\n\n\n\"AU-ASM\"\n\n\n\n\n201001010030\n\n\n201501010000\n\n\n\"AU-Cpr\"\n\n\n\n\n201201010030\n\n\n201501010000\n\n\n\"AU-Cum\"\n\n\n\n\n200701010030\n\n\n201401010000\n\n\n\"AU-DaP\"\n\n\n\n\n200801010030\n\n\n201501010000\n\n\n\"AU-DaS\"\n\n\n\n\n200801010030\n\n\n201501010000\n\n\n\"AU-Dry\"\n\n\n\n\n201101010030\n\n\n201401010000\n\n\n\"AU-Emr\"\n\n\n\n\n200601010030\n\n\n200901010000\n\n\n\"AU-Fog\"\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n200301010030\n\n\n200401010000\n\n\n\"US-Wi1\"\n\n\n\n\n200301010030\n\n\n200401010000\n\n\n\"US-Wi2\"\n\n\n\n\n200201010030\n\n\n200501010000\n\n\n\"US-Wi3\"\n\n\n\n\n200201010030\n\n\n200601010000\n\n\n\"US-Wi4\"\n\n\n\n\n200401010030\n\n\n200501010000\n\n\n\"US-Wi5\"\n\n\n\n\n200201010030\n\n\n200401010000\n\n\n\"US-Wi6\"\n\n\n\n\n200501010030\n\n\n200601010000\n\n\n\"US-Wi7\"\n\n\n\n\n200201010030\n\n\n200301010000\n\n\n\"US-Wi8\"\n\n\n\n\n200401010030\n\n\n200601010000\n\n\n\"US-Wi9\"\n\n\n\n\n200401010030\n\n\n201501010000\n\n\n\"US-Wkg\"\n\n\n\n\n201101010030\n\n\n201401010000\n\n\n\"US-WPT\"\n\n\n\n\n200001010030\n\n\n201001010000\n\n\n\"ZM-Mon\"\n\n\n\n\n\n\n\n\nsite_info.write_parquet(out_dir / \"../site_info.parquet\")"
  },
  {
    "objectID": "Kernel exploration.html",
    "href": "Kernel exploration.html",
    "title": "Kernel visualization",
    "section": "",
    "text": "Kernel visualization\n\nfrom meteo_imp.gpfa.gpfa import GPFAKernel\nimport gpytorch\nimport torch\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport pandas as pd\n\n\nk = gpytorch.kernels.RBFKernel()\n\n\nk.lengthscale = 3\n\n\nk(torch.tensor([1]), torch.tensor([2, 3])).evaluate()\n\ntensor([[0.9460, 0.8007]], grad_fn=<RBFCovarianceBackward>)\n\n\n\nalt.renderers.enable('mimetype')\n\nRendererRegistry.enable('mimetype')\n\n\n\ndef plot_kernel(kernel, t_range = torch.arange(-10, 10)):\n    y = kernel(torch.tensor([0]), t_range).evaluate().detach().numpy().squeeze()\n    return alt.Chart(pd.DataFrame({'x': t_range, 'y': y})).mark_line().encode(x='x', y ='y')\n    #plt.plot(t_range, y\n    # return y\n\n\nplot_kernel(k)"
  },
  {
    "objectID": "exploration/log_likelihood.html",
    "href": "exploration/log_likelihood.html",
    "title": "Exploration of Loglikelihood computations",
    "section": "",
    "text": "This notebook is not running yet\n\nX[0]\n\ntensor(1)\n\n\n\nk.loglikelihood(X, smooth=False)\n\ntensor(-3.7474)\n\n\n\nfrom scipy import linalg\n\n\ndef log_multivariate_normal_density(X, means, covars, min_covar=1.e-7):\n    \"\"\"Log probability for full covariance matrices. \"\"\"\n    if hasattr(linalg, 'solve_triangular'):\n        # only in scipy since 0.9\n        solve_triangular = linalg.solve_triangular\n    else:\n        # slower, but works\n        solve_triangular = linalg.solve\n    n_samples, n_dim = X.shape\n    nmix = len(means)\n    log_prob = np.empty((n_samples, nmix))\n    for c, (mu, cv) in enumerate(zip(means, covars)):\n        try:\n            cv_chol = linalg.cholesky(cv, lower=True)\n        except linalg.LinAlgError:\n            # The model is most probabily stuck in a component with too\n            # few observations, we need to reinitialize this components\n            cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),\n                                      lower=True)\n        cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))\n        cv_sol = solve_triangular(cv_chol, (X - mu).T, lower=True).T\n        log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) + \\\n                                     n_dim * np.log(2 * np.pi) + cv_log_det)\n\n    return log_prob\n\n\nlog_multivariate_normal_density(np.ones((1,1)), np.zeros(1), np.eye(1))\n\narray([[-1.41893853]])\n\n\n\nMultivariateNormal(torch.zeros(1), torch.eye(1)).log_prob(torch.ones((1)))\n\ntensor(-1.4189)\n\n\n\nlog_multivariate_normal_density(np.ones((1, 2)), np.zeros((1,2)), np.expand_dims(np.eye(2), 0))\n\narray([[-2.83787707]])\n\n\n\nMultivariateNormal(torch.zeros(2), torch.eye(2)).log_prob(torch.ones((2)))\n\ntensor(-2.8379)\n\n\npytorch and pykalman have the same results for computing the loglikelihood of a gaussian distribution\n\ndef _torch_loglikelihoods(observation_matrices, observation_offsets,\n                    observation_covariance, predicted_state_means,\n                    predicted_state_covariances, observations):\n    \n    n_timesteps = observations.shape[0]\n    loglikelihoods = np.zeros(n_timesteps)\n    for t in range(n_timesteps):\n        observation = observations[t]\n        observation_matrix = _last_dims(observation_matrices, t)\n        observation_offset = _last_dims(observation_offsets, t, ndims=1)\n        predicted_state_mean = _last_dims(\n            predicted_state_means, t, ndims=1\n        )\n        predicted_state_covariance = _last_dims(\n            predicted_state_covariances, t\n        )\n\n        predicted_observation_mean = (\n            observation_matrix @ predicted_state_mean\n            + observation_offset\n        )\n        predicted_observation_covariance = (\n            observation_matrix @ predicted_state_covariance @ observation_matrix.T\n            + observation_covariance\n        )\n\n        loglikelihoods[t] = MultivariateNormal(\n            predicted_observation_mean,\n            predicted_observation_covariance\n        ).log_prob(observation.unsqueeze(0))\n    return loglikelihoods.sum()\n\n\nstate = k.filter(X)\n\n\npyk.loglikelihood??\n\n\nSignature: pyk.loglikelihood(X)\nSource:   \n    def loglikelihood(self, X):\n        \"\"\"Calculate the log likelihood of all observations\n        Parameters\n        ----------\n        X : [n_timesteps, n_dim_obs] array\n            observations for time steps [0...n_timesteps-1]\n        Returns\n        -------\n        likelihood : float\n            likelihood of all observations\n        \"\"\"\n        Z = np.array(self._parse_observations(X))\n        # initialize parameters\n        (transition_matrices, transition_offsets,\n         transition_covariance, observation_matrices,\n         observation_offsets, observation_covariance,\n         initial_state_mean, initial_state_covariance) = (\n            self._initialize_parameters()\n        )\n        # apply the Kalman Filter\n        (predicted_state_means, predicted_state_covariances,\n         kalman_gains, filtered_state_means,\n         filtered_state_covariances) = (\n            _filter(\n                transition_matrices, observation_matrices,\n                transition_covariance, observation_covariance,\n                transition_offsets, observation_offsets,\n                initial_state_mean, initial_state_covariance,\n                Z\n            )\n        )\n        # get likelihoods for each time step\n        loglikelihoods = _loglikelihoods(\n          observation_matrices, observation_offsets, observation_covariance,\n          predicted_state_means, predicted_state_covariances, Z\n        )\n        return np.sum(loglikelihoods)\nFile:      ~/anaconda3/envs/data-science/lib/python3.10/site-packages/pykalman/standard.py\nType:      method\n\n\n\n\n\nnX = X.numpy()\n\n\npred_mean, pred_cov = pyk.filter(nX)\n\n\npyk.loglikelihood(nX)\n\n-5.231597970652478\n\n\n\nZ = np.array(pyk._parse_observations(nX))\n\n\n(transition_matrices, transition_offsets,\n         transition_covariance, observation_matrices,\n         observation_offsets, observation_covariance,\n         initial_state_mean, initial_state_covariance) = (\n            pyk._initialize_parameters()\n        )\n\n\n(predicted_state_means, predicted_state_covariances,\n         kalman_gains, filtered_state_means,\n         filtered_state_covariances) = (\n            pykalman.standard._filter(\n                transition_matrices, observation_matrices,\n                transition_covariance, observation_covariance,\n                transition_offsets, observation_offsets,\n                initial_state_mean, initial_state_covariance,\n                Z\n            )\n        )\n\n\npykalman.standard._loglikelihoods(\n observation_matrices, observation_offsets, observation_covariance,\n          pred_mean, pred_cov, Z\n).sum()\n\n-3.7473859589713614\n\n\n\n_torch_loglikelihoods(\n    k.obs_matrices,\n    k.obs_offsets,\n    k.obs_cov,\n    state.mean,\n    state.cov,\n    X\n)\n\n-3.747385859489441\n\n\n\npykalman.standard._loglikelihoods(\n    k.obs_matrices.numpy(),\n    k.obs_offsets.numpy(),\n    k.obs_cov.numpy(),\n    state.mean.numpy(),\n    state.cov.numpy(),\n    Z\n).sum()\n\n-3.747385949780805\n\n\n\npyk.loglikelihood(Z)\n\n-5.231597970652478\n\n\n\ntest_close(pyk.filter(nX), k.filter(X))\n\n\npyk.loglikelihood(nX)\n\n-5.231597970652478\n\n\n\n\n\nfrom torch.distributions import MultivariateNormal\nimport torch\nfrom meteo_imp.gaussian import to_posdef\n\n\nn = 5\n\n\ncov = to_posdef(torch.rand(n,n))\nmean = torch.rand(n)\n\n\ndist = MultivariateNormal(mean, cov)\n\n\nobs = dist.sample()\n\n\nobs\n\ntensor([1.0237, 0.9482, 0.9448, 1.5264, 1.2376])\n\n\n\ndist.log_prob(obs)\n\ntensor(-3.6350)\n\n\n\ndist2 = MultivariateNormal(mean, torch.diag(cov.diag()))\n\n\ndist2.covariance_matrix\n\ntensor([[0.9555, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 1.1688, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 1.7292, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 2.1713, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 1.0093]])\n\n\n\ndist2.log_prob(obs)\n\ntensor(-6.6010)\n\n\nso we see that if there is a diagonal covariance the likelihood is the same of the sum of the likelihoods of the individual variables\n\ntot = 0\nfor i in range(n):\n    dist_n = MultivariateNormal(mean[i:i+1], cov[i:i+1, i:i+1])\n    tot += dist_n.log_prob(obs[i:i+1])\n\n\ntot\n\ntensor(-6.6010)\n\n\n\ndist_n.log_prob(obs[i:i+1])\n\ntensor(-1.3723)\n\n\nperformance big ll vs individual\n\nn = 5000\n\n\nstd = torch.rand(n)\nmean = torch.rand(n)\n\n\ndist = MultivariateNormal(mean, torch.diag(std))\n\n\nobs = dist.sample()\n\n\n%timeit dist.log_prob(obs)\n\n2.61 ms ± 134 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\ndist.log_prob(obs)\n\ntensor(-4552.8486)\n\n\n\ndef single_ll():\n    tot = 0\n    for i in range(n):\n        dist_n = MultivariateNormal(mean[i:i+1], torch.diag(std[i:i+1]))\n        tot += dist_n.log_prob(obs[i:i+1])\n\n\ntot\n\ntensor(-6.6010)\n\n\n\n%timeit single_ll()\n\n1.49 s ± 164 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\ndist = MultivariateNormal(mean.cuda(), torch.diag(std).cuda())\n\n\nobs = dist.sample()\n\n\n%timeit dist.log_prob(obs)\n\n630 µs ± 3.43 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "exploration/pytorch_constraints.html",
    "href": "exploration/pytorch_constraints.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "need to be sure that the matrix is a positive semi-definite, so we can use choleshy decomposition where if \\(A\\) is positive semidefinite it can be decomposed as \\(A=LL^T\\) where \\(L\\) is a lower triangular matrix. Therefore the idea is to have as the raw parameter \\(L\\) which we’ll enforce to be a lower triangular matrix\n\nimport torch\n\n\nR = torch.rand(3,3, requires_grad=True) \n\n\nL = torch.tril(R)\n\n\nL.sum().backward() \n\n\nL\n\ntensor([[0.6097, 0.0000, 0.0000],\n        [0.6872, 0.5714, 0.0000],\n        [0.6097, 0.9560, 0.4841]], grad_fn=<TrilBackward0>)\n\n\n\ntorch.distributions.MultivariateNormal(torch.zeros(3), R @ R.T)\n\nMultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3]))\n\n\n\nfor i in range(10_000):\n    R = torch.rand(3,3)\n    cov = R @ R.T + (torch.eye(3) * 1e-7)\n    torch.distributions.MultivariateNormal(torch.zeros(3), cov)\n\n\nfor i in range(10_000):\n    R = torch.rand(3,3)\n    L = torch.tril(R)\n    torch.distributions.MultivariateNormal(torch.zeros(3), L @ L.T)\n\nValueError: Expected parameter covariance_matrix (Tensor of shape (3, 3)) of distribution MultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[0.3166, 0.4638, 0.4075],\n        [0.4638, 0.6800, 0.6019],\n        [0.4075, 0.6019, 0.5586]])\n\n\n\nfrom gpytorch.constraints import Positive\nimport torch\n\n\ncov = torch.eye(3)\n\n\nconstraint = Positive()\n\n\nraw = constraint.inverse_transform(cov)\nraw\n\ntensor([[0.5413,   -inf,   -inf],\n        [  -inf, 0.5413,   -inf],\n        [  -inf,   -inf, 0.5413]])\n\n\n\nconstraint.transform(raw)\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n\n\n\nconstraint.inverse_transform(cov + torch.fill(cov, 1e-7))\n\ntensor([[  0.5413, -16.1181, -16.1181],\n        [-16.1181,   0.5413, -16.1181],\n        [-16.1181, -16.1181,   0.5413]])\n\n\n\nraw2 = -torch.ones(3,3)\n\n\nraw2\n\ntensor([[-1., -1., -1.],\n        [-1., -1., -1.],\n        [-1., -1., -1.]])\n\n\n\nconstraint.transform(raw2)\n\ntensor([[0.3133, 0.3133, 0.3133],\n        [0.3133, 0.3133, 0.3133],\n        [0.3133, 0.3133, 0.3133]])\n\n\n\n\n\nfrom meteo_imp.kalman.filter import *\n\n\nimport torch\n\n\nA = torch.rand(100, 100)\n\n\nA.dtype\n\ntorch.float32\n\n\n\nsymmetric_upto(A * A.T, -10)\n\n-10\n\n\n\ntorch.tril(A) * torch.tril(A).T\n\ntensor([[0.0088, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.1427, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.5120,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.1121, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3522, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2634]])"
  }
]