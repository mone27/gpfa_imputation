[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Meteo Imp",
    "section": "",
    "text": "Meteo Imp\nCode for the Evaluation of Kalman Filter for meteorological time series imputation for Eddy Covariance applications\nThis website contains only the code of the analysis for the documentation see https://mone27.github.io/meteo_imp/libs.\nfull text of the master thesis\n\n\nContent\n\nresults: Code to procude the obtain the final results\n\nmodel training\nplots for manuscript\n\nfluxnet: code to download all FLUXNET 2015 database and extract information about gaps\npresentations: intermediate results presentations\nvariables distribution: basic analysis of meteorological variables"
  },
  {
    "objectID": "fluxnet/extract_gap_all_fluxnet.html",
    "href": "fluxnet/extract_gap_all_fluxnet.html",
    "title": "Meteo Imp Analsyis",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.fluxnet.gap_finder import *\n\nlinks are obtained from this https://fluxnet.org/data-and-manifest/ page(requires login) by running in the browser console this code\nvar x = document.querySelectorAll(\"a\");\nvar myarray = []\nfor (var i=0; i&lt;x.length; i++){\nvar nametext = x[i].textContent;\nvar cleantext = nametext.replace(/\\s+/g, ' ').trim();\nvar cleanlink = x[i].href;\nmyarray.push([cleantext,cleanlink]);\n};\nfunction make_table() {\n    var table = '&lt;table&gt;&lt;thead&gt;&lt;th&gt;Links&lt;/th&gt;&lt;/thead&gt;&lt;tbody&gt;';\n   for (var i=0; i&lt;myarray.length; i++) {\n            table += '&lt;tr&gt;&lt;td&gt;'+myarray[i][1]+'&lt;/td&gt;&lt;/tr&gt;';\n    };\n \n    var w = window.open(\"\");\nw.document.write(table); \n}\nmake_table()\ncode inspired from https://towardsdatascience.com/quickly-extract-all-links-from-a-web-page-using-javascript-and-the-browser-console-49bb6f48127b\nand then doing some smart copy pasting\nacually download in parallel all files with, so is faster than download with python\nparallel -a fluxnet_parallel_wget.txt --jobs 10 wget\n\nfrom fluxnet_links import all_fluxnet_link\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\n\ntest_file = Path(\"../../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip\")\ntmp_dir = Path(\"/tmp\")\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\n\n\nsite_info = download_and_find_gaps(all_fluxnet_link, download_dir, out_dir, tmp_dir)\n\n\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nInput In [5], in &lt;cell line: 1&gt;()\n----&gt; 1 site_info = download_and_find_gaps(all_fluxnet_link, download_dir, out_dir, tmp_dir)\n\nFile ~/Documents/uni/Thesis/GPFA_imputation/meteo_imp/fluxnet/gap_finder.py:153, in download_and_find_gaps(urls, download_dir, out_dir, tmp_dir)\n    151 site_infos = []\n    152 for url in tqdm(urls):\n--&gt; 153     file_zip = download_fluxnet(url, download_dir)\n    154     file, site_info = find_gaps_fluxnet_archive(file_zip, out_dir, tmp_dir)\n    155     site_infos.append(site_info)\n\nFile ~/Documents/uni/Thesis/GPFA_imputation/meteo_imp/fluxnet/gap_finder.py:133, in download_fluxnet(url, download_dir)\n    131 r = requests.get(url, allow_redirects=True, stream=True)\n    132 n_iter = int(r.headers['Content-Length'])\n--&gt; 133 with open(file_name, 'wb') as file:\n    134     with tqdm(total=n_iter, unit_divisor=1024, unit_scale=True, unit='B') as pbar:\n    135         for chunk in r.iter_content(chunk_size=1024):\n\nFileNotFoundError: [Errno 2] No such file or directory: '/run/media/simone/Simone DATI/fluxnet_all/FLX_AR-SLu_FLUXNET2015_FULLSET_2009-2011_1-4.zip'\n\n\n\n\nsite_info\n\n\n\nshape: (206, 3)\n\n\n\n\nstart\n\n\nend\n\n\nsite\n\n\n\n\ni64\n\n\ni64\n\n\nstr\n\n\n\n\n\n\n200901010030\n\n\n201201010000\n\n\n\"AR-SLu\"\n\n\n\n\n200901010030\n\n\n201301010000\n\n\n\"AR-Vir\"\n\n\n\n\n200201010030\n\n\n201301010000\n\n\n\"AT-Neu\"\n\n\n\n\n200701010030\n\n\n201001010000\n\n\n\"AU-Ade\"\n\n\n\n\n201001010030\n\n\n201501010000\n\n\n\"AU-ASM\"\n\n\n\n\n201001010030\n\n\n201501010000\n\n\n\"AU-Cpr\"\n\n\n\n\n201201010030\n\n\n201501010000\n\n\n\"AU-Cum\"\n\n\n\n\n200701010030\n\n\n201401010000\n\n\n\"AU-DaP\"\n\n\n\n\n200801010030\n\n\n201501010000\n\n\n\"AU-DaS\"\n\n\n\n\n200801010030\n\n\n201501010000\n\n\n\"AU-Dry\"\n\n\n\n\n201101010030\n\n\n201401010000\n\n\n\"AU-Emr\"\n\n\n\n\n200601010030\n\n\n200901010000\n\n\n\"AU-Fog\"\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n200301010030\n\n\n200401010000\n\n\n\"US-Wi1\"\n\n\n\n\n200301010030\n\n\n200401010000\n\n\n\"US-Wi2\"\n\n\n\n\n200201010030\n\n\n200501010000\n\n\n\"US-Wi3\"\n\n\n\n\n200201010030\n\n\n200601010000\n\n\n\"US-Wi4\"\n\n\n\n\n200401010030\n\n\n200501010000\n\n\n\"US-Wi5\"\n\n\n\n\n200201010030\n\n\n200401010000\n\n\n\"US-Wi6\"\n\n\n\n\n200501010030\n\n\n200601010000\n\n\n\"US-Wi7\"\n\n\n\n\n200201010030\n\n\n200301010000\n\n\n\"US-Wi8\"\n\n\n\n\n200401010030\n\n\n200601010000\n\n\n\"US-Wi9\"\n\n\n\n\n200401010030\n\n\n201501010000\n\n\n\"US-Wkg\"\n\n\n\n\n201101010030\n\n\n201401010000\n\n\n\"US-WPT\"\n\n\n\n\n200001010030\n\n\n201001010000\n\n\n\"ZM-Mon\"\n\n\n\n\n\n\n\n\nsite_info.write_parquet(out_dir / \"../site_info.parquet\")"
  },
  {
    "objectID": "exploration/ff crash.html",
    "href": "exploration/ff crash.html",
    "title": "Meteo Imp Analsyis",
    "section": "",
    "text": "import altair as alt\nimport pandas as pd\nimport numpy as np\n\n\nfrom ipywidgets import widgets, HBox, VBox\nfrom typing import Sequence\n\n\nclass InteractiveSequence():\n    def __init__(self,s: Sequence):\n        self.s =s\n        self.i = 0\n        self.output = widgets.Output()\n        self.button_next = widgets.Button(description=\"Next\", icon=\"arrow-right\")\n        self.button_prev = widgets.Button(description=\"Previous\", icon=\"arrow-left\", disabled=True)\n        self.button_next.on_click(self.on_next)\n        self.button_prev.on_click(self.on_prev)\n        self.label = widgets.Label(f\"of {len(self.s)-1}\")\n        self.slider = widgets.IntSlider(0, 0, len(s)-1, 1)\n        self.slider.observe(self.on_slide, names=\"value\")\n    def update_view(self):\n        self.button_enable()\n        self.slider.value = self.i\n        with self.output:\n            display(self.s[self.i])\n        self.output.clear_output(wait=True)\n    def button_enable(self):\n        if self.i &lt; len(self.s) - 1:  self.button_next.disabled = False\n        else: self.button_next.disabled = True\n        \n        if self.i == 0: self.button_prev.disabled = True\n        else: self.button_prev.disabled = False\n        \n    def on_next(self, b):\n        self.i +=1\n        self.update_view()\n    def on_prev(self, b):\n        self.i -=1\n        self.update_view()\n    def on_slide(self, change):\n        self.i = change['new']\n        self.update_view()\n        \n    def __call__(self):\n        self.update_view()\n        display(VBox([HBox([self.slider, self.label]), HBox([self.button_prev, self.button_next])]), self.output)\n\n\nseq = [alt.Chart(pd.DataFrame({'x': np.arange(5000), 'y': np.random.randn(5000)})).mark_point().encode(x='x',y= 'y') for _ in range(200)]\n\n\nInteractiveSequence(seq)()"
  },
  {
    "objectID": "exploration/linalg.html",
    "href": "exploration/linalg.html",
    "title": "Meteo Imp Analsyis",
    "section": "",
    "text": "import torch\n\n\nran = torch.rand(5,5, dtype=torch.float64)\nd = ran @ ran.T + 1e-5\n\n\nd\n\ntensor([[2.1157, 1.6960, 1.3807, 2.0091, 1.2477],\n        [1.6960, 1.5403, 1.4525, 1.9626, 1.0250],\n        [1.3807, 1.4525, 1.6791, 2.0742, 1.0316],\n        [2.0091, 1.9626, 2.0742, 2.8813, 1.6865],\n        [1.2477, 1.0250, 1.0316, 1.6865, 1.5130]], dtype=torch.float64)\n\n\n\na = torch.rand(5,5, dtype=torch.float64)\n\n\nQ = torch.linalg.qr(a).Q\n\n\nQ @ d @ Q.T\n\ntensor([[ 0.9317, -1.4794,  0.9637, -1.2813, -0.1782],\n        [-1.4794,  4.6029, -1.9401,  3.0748,  0.7384],\n        [ 0.9637, -1.9401,  1.2851, -1.4728, -0.4926],\n        [-1.2813,  3.0748, -1.4728,  2.2845,  0.3230],\n        [-0.1782,  0.7384, -0.4926,  0.3230,  0.6251]], dtype=torch.float64)\n\n\n\nA = torch.rand(3,3)\n\n\nQ,R = torch.linalg.qr(A)\n\n\nQ, R\n\n(tensor([[-0.5216,  0.0729, -0.8501],\n         [-0.7627, -0.4865,  0.4262],\n         [-0.3825,  0.8706,  0.3093]]),\n tensor([[-1.1534, -0.7000, -0.7603],\n         [ 0.0000,  0.7366,  0.2398],\n         [ 0.0000,  0.0000, -0.3977]]))\n\n\n\nQ.T @ Q\n\ntensor([[ 1.0000e+00,  0.0000e+00, -2.9802e-08],\n        [ 0.0000e+00,  1.0000e+00, -5.9605e-08],\n        [-2.9802e-08, -5.9605e-08,  1.0000e+00]])\n\n\nCho solve\n\nfrom meteo_imp.gaussian import to_posdef\n\n\nA = to_posdef(torch.rand(3,3))\nB = torch.rand(3,3)\n\n\nx = torch.inverse(A) @ B\nx\n\ntensor([[ 31.5294,   4.7335,  51.6443],\n        [  1.4137,   1.7132,   3.0639],\n        [-18.4855,  -3.2360, -31.1095]])\n\n\n\nC = torch.linalg.cholesky(A)\n\n\ntorch.cholesky_solve(B, C)\n\ntensor([[ 31.5293,   4.7335,  51.6443],\n        [  1.4136,   1.7132,   3.0639],\n        [-18.4855,  -3.2360, -31.1094]])\n\n\n\ntorch.linalg.qr\n\n\nA = torch.randn(3, 3, requires_grad=True)\n\n\nA = A @ A.mT\n\n\nLD, pivots = torch.linalg.ldl_factor(A)\n\n\nLD.sum().backward()\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [9], in &lt;cell line: 1&gt;()\n----&gt; 1 LD.sum().backward()\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/_tensor.py:488, in Tensor.backward(self, gradient, retain_graph, create_graph, inputs)\n    478 if has_torch_function_unary(self):\n    479     return handle_torch_function(\n    480         Tensor.backward,\n    481         (self,),\n   (...)\n    486         inputs=inputs,\n    487     )\n--&gt; 488 torch.autograd.backward(\n    489     self, gradient, retain_graph, create_graph, inputs=inputs\n    490 )\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:197, in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    192     retain_graph = create_graph\n    194 # The reason we repeat same the comment below is that\n    195 # some Python versions print out the first line of a multi-line function\n    196 # calls in the traceback and some print out the last line\n--&gt; 197 Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    198     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n    199     allow_unreachable=True, accumulate_grad=True)\n\nRuntimeError: derivative for aten::linalg_ldl_factor_ex is not implemented"
  },
  {
    "objectID": "exploration/pca.html",
    "href": "exploration/pca.html",
    "title": "PCA",
    "section": "",
    "text": "exploration of PCA for init parameters\n\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\n\nt0 = np.random.randn(5,1) * 10\ntt = np.hstack([t0, 2 * t0 + np.random.randn(5,1), 3 * t0 + np.random.randn(5,1)])\n\n\ntt = np.vstack([tt, -tt])\n\n\ntt.mean(0)\n\narray([-8.88178420e-17,  0.00000000e+00, -1.59872116e-15])\n\n\n\ndef pca(X):\n    # Data matrix X, assumes 0-centered\n    n, m = X.shape\n    assert np.allclose(X.mean(axis=0), np.zeros(m))\n    # Compute covariance matrix\n    C = np.dot(X.T, X) / (n-1)\n    # Eigen decomposition\n    eigen_vals, eigen_vecs = np.linalg.eig(C)\n    # Project X onto PC space\n    X_pca = np.dot(X, eigen_vecs)\n    return X_pca, eigen_vecs\n\n\npca(tt)\n\n(array([[ 4.86387982e+00, -1.41975358e-02,  1.92176031e+00],\n        [ 3.60233996e+01,  2.33725703e-01, -4.02597844e-01],\n        [ 3.09057872e+01, -3.14141537e-01,  1.15917989e+00],\n        [ 1.60022488e+01,  3.52349936e-01,  3.35200954e-01],\n        [ 2.04880798e+01, -2.08908751e-01, -1.75875946e+00],\n        [-4.86387982e+00,  1.41975358e-02, -1.92176031e+00],\n        [-3.60233996e+01, -2.33725703e-01,  4.02597844e-01],\n        [-3.09057872e+01,  3.14141537e-01, -1.15917989e+00],\n        [-1.60022488e+01, -3.52349936e-01, -3.35200954e-01],\n        [-2.04880798e+01,  2.08908751e-01,  1.75875946e+00]]),\n array([[ 0.26875523,  0.96317815, -0.00764675],\n        [ 0.53111826, -0.15481125, -0.83303473],\n        [ 0.80354466, -0.21982111,  0.55316784]]))\n\n\n\nsk_pc = PCA(2).fit(tt)\n\n\ntt @ sk_pc.components_.T\n\narray([[  4.86387982,   1.92176031],\n       [ 36.02339962,  -0.40259784],\n       [ 30.9057872 ,   1.15917989],\n       [ 16.00224879,   0.33520095],\n       [ 20.48807979,  -1.75875946],\n       [ -4.86387982,  -1.92176031],\n       [-36.02339962,   0.40259784],\n       [-30.9057872 ,  -1.15917989],\n       [-16.00224879,  -0.33520095],\n       [-20.48807979,   1.75875946]])\n\n\n\ntr = sk_pc.transform(tt)\ntr\n\narray([[  4.86387982,   1.92176031],\n       [ 36.02339962,  -0.40259784],\n       [ 30.9057872 ,   1.15917989],\n       [ 16.00224879,   0.33520095],\n       [ 20.48807979,  -1.75875946],\n       [ -4.86387982,  -1.92176031],\n       [-36.02339962,   0.40259784],\n       [-30.9057872 ,  -1.15917989],\n       [-16.00224879,  -0.33520095],\n       [-20.48807979,   1.75875946]])\n\n\n\nsk_pc.components_.T\n\narray([[ 0.26875523, -0.00764675],\n       [ 0.53111826, -0.83303473],\n       [ 0.80354466,  0.55316784]])\n\n\n\n(sk_pc.components_ @ tt.T).T\n\narray([[  4.86387982,   1.92176031],\n       [ 36.02339962,  -0.40259784],\n       [ 30.9057872 ,   1.15917989],\n       [ 16.00224879,   0.33520095],\n       [ 20.48807979,  -1.75875946],\n       [ -4.86387982,  -1.92176031],\n       [-36.02339962,   0.40259784],\n       [-30.9057872 ,  -1.15917989],\n       [-16.00224879,  -0.33520095],\n       [-20.48807979,   1.75875946]])\n\n\n\ntt[0, None]\n\narray([[1.27882313, 0.98460024, 4.97452158]])\n\n\n\nsk_pc.components_.shape\n\n(2, 3)\n\n\n\ntt[0].shape\n\n(3,)\n\n\n\n(sk_pc.components_ @ tt[0])\n\narray([4.86387982, 1.92176031])\n\n\n\ntt[0]\n\narray([1.27882313, 0.98460024, 4.97452158])\n\n\n\nsk_pc.components_.T @ tr[0]\n\narray([1.29249789, 0.9824023 , 4.97140066])\n\n\n\nsk_pc.inverse_transform(tr[0])\n\narray([1.29249789, 0.9824023 , 4.97140066])"
  },
  {
    "objectID": "var_distribution.html",
    "href": "var_distribution.html",
    "title": "Variable distribution",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport polars as pl\n\nfrom meteo_imp.fluxnet.gap_finder import scan_fluxnet_csv\n\nfrom meteo_imp.utils import cache_disk"
  },
  {
    "objectID": "var_distribution.html#load",
    "href": "var_distribution.html#load",
    "title": "Variable distribution",
    "section": "Load",
    "text": "Load\nload Hainich dataset\n\nhai_path = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\"\n# hai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=20_000)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = scan_fluxnet_csv(hai_path, convert_dates=True).rename(meteo_vars).select([pl.col(\"end\").alias(\"time\"), *meteo_vars.values()])\n\nhai.fetch(10)\n\n\n\nshape: (10, 5)\n\n\n\n\ntime\n\n\nTA\n\n\nSW_IN\n\n\nLW_IN\n\n\nVPD\n\n\n\n\ndatetime[μs]\n\n\nf64\n\n\nf64\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n2000-01-01 00:30:00\n\n\n-0.6\n\n\n0.0\n\n\n302.475\n\n\n0.222\n\n\n\n\n2000-01-01 01:00:00\n\n\n-0.65\n\n\n0.0\n\n\n302.475\n\n\n0.122\n\n\n\n\n2000-01-01 01:30:00\n\n\n-0.58\n\n\n0.0\n\n\n301.677\n\n\n0.09\n\n\n\n\n2000-01-01 02:00:00\n\n\n-0.51\n\n\n0.0\n\n\n301.677\n\n\n0.11\n\n\n\n\n2000-01-01 02:30:00\n\n\n-0.49\n\n\n0.0\n\n\n301.677\n\n\n0.102\n\n\n\n\n2000-01-01 03:00:00\n\n\n-0.4\n\n\n0.0\n\n\n301.677\n\n\n0.111\n\n\n\n\n2000-01-01 03:30:00\n\n\n-0.36\n\n\n0.0\n\n\n301.677\n\n\n0.109\n\n\n\n\n2000-01-01 04:00:00\n\n\n-0.35\n\n\n0.0\n\n\n301.677\n\n\n0.107\n\n\n\n\n2000-01-01 04:30:00\n\n\n-0.28\n\n\n0.0\n\n\n308.046\n\n\n0.122\n\n\n\n\n2000-01-01 05:00:00\n\n\n-0.27\n\n\n0.0\n\n\n308.046\n\n\n0.138\n\n\n\n\n\n\n\n\nhai_td = hai.melt('time')\n\n\nhai_td.fetch(3)\n\n\n\nshape: (12, 3)\n\n\n\n\ntime\n\n\nvariable\n\n\nvalue\n\n\n\n\ndatetime[μs]\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n2000-01-01 00:30:00\n\n\n\"TA\"\n\n\n-0.6\n\n\n\n\n2000-01-01 01:00:00\n\n\n\"TA\"\n\n\n-0.65\n\n\n\n\n2000-01-01 01:30:00\n\n\n\"TA\"\n\n\n-0.58\n\n\n\n\n2000-01-01 00:30:00\n\n\n\"SW_IN\"\n\n\n0.0\n\n\n\n\n2000-01-01 01:00:00\n\n\n\"SW_IN\"\n\n\n0.0\n\n\n\n\n2000-01-01 01:30:00\n\n\n\"SW_IN\"\n\n\n0.0\n\n\n\n\n2000-01-01 00:30:00\n\n\n\"LW_IN\"\n\n\n302.475\n\n\n\n\n2000-01-01 01:00:00\n\n\n\"LW_IN\"\n\n\n302.475\n\n\n\n\n2000-01-01 01:30:00\n\n\n\"LW_IN\"\n\n\n301.677\n\n\n\n\n2000-01-01 00:30:00\n\n\n\"VPD\"\n\n\n0.222\n\n\n\n\n2000-01-01 01:00:00\n\n\n\"VPD\"\n\n\n0.122\n\n\n\n\n2000-01-01 01:30:00\n\n\n\"VPD\"\n\n\n0.09"
  },
  {
    "objectID": "var_distribution.html#distribution",
    "href": "var_distribution.html#distribution",
    "title": "Variable distribution",
    "section": "Distribution",
    "text": "Distribution\n\nhai.drop('time').collect().to_pandas().hist(figsize=(15,10));\n\n\n\n\n\n\n\n\n\n# should to the binning before the plot\n# alt.Chart(hai_td.collect().to_pandas()).mark_line().encode(\n#     x = 'value',\n#     y = 'density()',\n#     facet = alt.Facet('variable', columns=2)\n# )"
  },
  {
    "objectID": "var_distribution.html#correlation",
    "href": "var_distribution.html#correlation",
    "title": "Variable distribution",
    "section": "Correlation",
    "text": "Correlation\nCode inspired from source: https://towardsdatascience.com/altair-plot-deconstruction-visualizing-the-correlation-structure-of-weather-data-38fb5668c5b1\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_p.corr()\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\n\n\nTA\n1.000000\n0.432321\n0.639556\n0.735412\n\n\nSW_IN\n0.432321\n1.000000\n0.126278\n0.533506\n\n\nLW_IN\n0.639556\n0.126278\n1.000000\n0.270424\n\n\nVPD\n0.735412\n0.533506\n0.270424\n1.000000\n\n\n\n\n\n\n\n\ndef corr_mask(size):\n    corr_mask = np.zeros((size,size), dtype=bool)\n    for i in range(size):\n        for j in range(size):\n            corr_mask[i,j] = True if i &gt;= j else False\n    return corr_mask\n\n\ncorr_mask(len(hai_p.columns))\n\narray([[ True, False, False, False],\n       [ True,  True, False, False],\n       [ True,  True,  True, False],\n       [ True,  True,  True,  True]])\n\n\n\nhai_p = hai_p[sorted(hai_p.columns)] # need to properly plot half a corr matrix\n\n\ncor_hai = (hai_p\n              .corr().mask(~corr_mask(len(hai_p.columns))).stack()\n              .reset_index()     # The stacking results in an index on the correlation values, we need the index as normal columns for Altair\n              .rename(columns={0: 'correlation', 'level_0': 'variable', 'level_1': 'variable2'}))\ncor_hai['correlation_label'] = cor_hai['correlation'].map('{:.2f}'.format)  # Round to 2 decimal\ncor_hai\n\n\n\n\n\n\n\n\nvariable\nvariable2\ncorrelation\ncorrelation_label\n\n\n\n\n0\nLW_IN\nLW_IN\n1.000000\n1.00\n\n\n1\nSW_IN\nLW_IN\n0.126278\n0.13\n\n\n2\nSW_IN\nSW_IN\n1.000000\n1.00\n\n\n3\nTA\nLW_IN\n0.639556\n0.64\n\n\n4\nTA\nSW_IN\n0.432321\n0.43\n\n\n5\nTA\nTA\n1.000000\n1.00\n\n\n6\nVPD\nLW_IN\n0.270424\n0.27\n\n\n7\nVPD\nSW_IN\n0.533506\n0.53\n\n\n8\nVPD\nTA\n0.735412\n0.74\n\n\n9\nVPD\nVPD\n1.000000\n1.00\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef compute_2d_histogram(var1, var2, df, density=True, bins=20):\n    H, xedges, yedges = np.histogram2d(df[var1], df[var2], bins=bins, density=density)\n    H[H == 0] = np.nan\n\n    # Create a nice variable that shows the bin boundaries\n    \n    x_width = xedges[1] - xedges[0] # all bins have same width\n    xedges = pd.Series(xedges[:-1] + x_width /2)\n    \n    y_width = yedges[1] - yedges[0] # all bins have same width\n    yedges = pd.Series(yedges[:-1] + y_width /2)\n    \n    # Cast to long format using melt\n    res = pd.DataFrame(H, \n                       index=yedges, \n                       columns=xedges).reset_index().melt(\n                            id_vars='index'\n                       ).rename(columns={'index': 'value2', \n                                         'value': 'count',\n                                         'variable': 'value'})\n    \n\n    res['variable'] = var1\n    res['variable2'] = var2 \n    return res.dropna() # Drop all combinations for which no values where found\n\n\nh, xe, ye = np.histogram2d(hai_p['VPD'], hai_p['TA'])\n\n\nx_width = xe[1] - xe[0] # all bins have same width\nxe = pd.Series(xe + x_width /2)\n\n\nh.shape\n\n(10, 10)\n\n\n\nxe\n\n0      2.38335\n1      7.15005\n2     11.91675\n3     16.68345\n4     21.45015\n5     26.21685\n6     30.98355\n7     35.75025\n8     40.51695\n9     45.28365\n10    50.05035\ndtype: float64\n\n\n\nhai_binned = pd.concat([compute_2d_histogram(var1, var2, hai_p) for var1 in meteo_vars.values() for var2 in meteo_vars.values()])\nhai_binned.head()\n\n\n\n\n\n\n\n\nvalue2\nvalue\ncount\nvariable\nvariable2\n\n\n\n\n0\n-17.19025\n-17.19025\n0.000083\nTA\nTA\n\n\n21\n-14.47075\n-14.47075\n0.000224\nTA\nTA\n\n\n42\n-11.75125\n-11.75125\n0.000574\nTA\nTA\n\n\n63\n-9.03175\n-9.03175\n0.001188\nTA\nTA\n\n\n84\n-6.31225\n-6.31225\n0.003781\nTA\nTA\n\n\n\n\n\n\n\n\n# Define selector\nvar_sel_cor = alt.selection_single(fields=['variable', 'variable2'], clear=False, \n                                  init={'variable': 'TA', 'variable2': 'SW_IN'})\n\n# Define correlation heatmap\nbase = alt.Chart(cor_hai).encode(\n    x='variable2:O',\n    y='variable:O'    \n)\n\ntext = base.mark_text().encode(\n    text='correlation_label',\n    color=alt.condition(\n        alt.datum.correlation &gt; 0.5, \n        alt.value('white'),\n        alt.value('black')\n    )\n)\n\ncor_plot = base.mark_rect().encode(\n    color=alt.condition(var_sel_cor, alt.value('pink'), 'correlation:Q')\n).add_selection(var_sel_cor)\n\n# Define 2d binned histogram plot\nscat_plot = alt.Chart(hai_binned).transform_filter(\n    var_sel_cor\n).mark_rect().encode(\n    alt.X('value:N', axis=alt.Axis(format=\".4\")), \n    alt.Y('value2:N', axis=alt.Axis(format=\".4\"), sort='descending'),\n    alt.Color('count:Q', scale=alt.Scale(scheme='blues'))\n)\n\n# Combine all plots. hconcat plots both side-by-side \nalt.hconcat((cor_plot + text).properties(width=350, height=350), scat_plot.properties(width=350, height=350)).resolve_scale(color='independent')\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_td_p = hai_td.collect().to_pandas().set_index('time')"
  },
  {
    "objectID": "results/model_training.html",
    "href": "results/model_training.html",
    "title": "Training Kalman Filter for Results",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom meteo_imp.kalman.training import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\nfrom meteo_imp.gaussian import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\nfrom fastcore.foundation import patch\nfrom pathlib import Path, PosixPath\n\nfrom meteo_imp.kalman.training import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image, HTML\n\nfrom tqdm.auto import tqdm\nfrom fastcore.basics import *\nshow_metrics = False\nreset_seed()\nhai = pd.read_parquet(hai_big_path)\nhai_era = pd.read_parquet(hai_era_big_path)\nbase = here(\"analysis/results/trained_models\")\nbase.mkdir(exist_ok=True)\n@patch\ndef add_end(self: PosixPath, end): return self.parent / (self.name + end)\ndef train_or_load(model, dls, lr, n, path, keep=True):\n    save_models = SaveModelsBatch(times_epoch=5)\n    csv_logger = CSVLogger(path.add_end(\"log.csv\"))\n    learn = Learner(dls, model, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models, csv_logger], metrics=rmse_gap) \n    items = random.choices(dls.valid.items, k=4) \n    if path.add_end(\".pickle\").exists() and keep:\n        learn.model = torch.load(path.add_end(\".pickle\"))\n        display(csv_logger.read_log())\n        plot = Image(filename=path.add_end(\"_loss_plot.png\"))\n        display(plot)\n    else:\n        learn.fit(lr, n)\n        \n        torch.save(learn.model, path.add_end(\".pickle\"))\n        learn.recorder.plot_loss()\n        plt.savefig(path.add_end(\"_loss_plot.png\"))\n    return learn, items\ndef metric_valid(learn, dls=None):\n    nrmse = []\n    losses = []\n    dls = ifnone(dls, learn.dls.valid)\n    for input, target in tqdm(dls, leave=False):\n        pred = learn.model(input)\n        nrmse.append(learn.metrics[0](pred, target))\n        losses.append(learn.loss_func(pred, target).item())\n    metric = pd.DataFrame({'loss': losses, 'rmse': nrmse})\n    return metric.agg(['mean', 'std'])\nhai.columns\n\nIndex(['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], dtype='object')"
  },
  {
    "objectID": "results/model_training.html#generic-model-gap-len-3-336-gap-1-random",
    "href": "results/model_training.html#generic-model-gap-len-3-336-gap-1-random",
    "title": "Training Kalman Filter for Results",
    "section": "Generic model | gap len 3-336 | gap 1 random",
    "text": "Generic model | gap len 3-336 | gap 1 random\n\ndls_A1v = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns), n_var=1),\n    block_len=100+336,\n    gap_len=gen_gap_len(6, 336),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5).cpu()\n\n\nlen(hai)\n\n227952\n\n\n\nlen(dls_A1v.train)*20, len(dls_A1v.valid)*20\n\n(2080, 520)\n\n\n\nmodel_A1v = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca = None, \n    pred_only_gap=True)\n\n\nmodel_A1v.B.shape\n\ntorch.Size([1, 18, 14])\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 3, 1e-3, base / \"1_gap_varying_6-336_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n196.765350\n163.493486\n0.579074\n48:33\n\n\n1\n1\n138.298704\n123.299909\n0.490741\n48:14\n\n\n2\n2\n113.640141\n116.746793\n0.488059\n39:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 1, 1e-4, base / \"1_gap_varying_6-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n94.869328\n112.046392\n0.471249\n43:59\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 1, 1e-6, base / \"1_gap_varying_6-336_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n95.239438\n104.268073\n0.467282\n40:19"
  },
  {
    "objectID": "results/model_training.html#var-gap---varying-336---no-control",
    "href": "results/model_training.html#var-gap---varying-336---no-control",
    "title": "Training Kalman Filter for Results",
    "section": "1 var gap - varying 336 - No Control",
    "text": "1 var gap - varying 336 - No Control\n\nmodel_A1v_nc = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca=None,\n    pred_only_gap=True,\n    use_control=False\n)\n\n\nlearn_A1v_nc, items_A1v_nc = train_or_load(model_A1v_nc, dls_A1v, 3, 1e-3, base / \"1_gap_varying_336_no_control_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n219.686355\n178.506325\n0.658579\n37:14\n\n\n1\n1\n176.039201\n160.979378\n0.583213\n37:00\n\n\n2\n2\n166.012525\n158.206468\n0.574111\n36:47"
  },
  {
    "objectID": "results/model_training.html#short-gaps",
    "href": "results/model_training.html#short-gaps",
    "title": "Training Kalman Filter for Results",
    "section": "Short gaps",
    "text": "Short gaps\n\nAll variables - 30 all\n\ndls_Aa = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = list(hai.columns),\n    block_len=120,\n    gap_len=gen_gap_len(6, 30),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5\n).cpu()\n\n\ndls_Aa = imp_dataloader(hai, hai_era, var_sel = list(hai.columns), block_len=120, gap_len=gen_gap_len(6,30), bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa = learn_A1v.model.copy()\n\n\nif show_metrics: display(metric_valid(learn_A1v, dls=dls_Aa.valid))\n\n\ndls_A1v30 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns), n_var=1),\n    block_len=100+30,\n    gap_len=gen_gap_len(6, 30),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5).cpu()\n\n\nif show_metrics: display(metric_valid(learn_A1v, dls=dls_A1v30.valid))\n\n\nlearn_Aa, items_Aa = train_or_load(model_Aa, dls_Aa, 3, 3e-4, base / \"All_gap_all_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n24.688308\n21.049544\n0.365108\n28:05\n\n\n1\n1\n-7.512621\n-4.152000\n0.342344\n27:26\n\n\n2\n2\n-18.230698\n-19.744404\n0.327594\n26:15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v30, items_A1v30 = train_or_load(learn_A1v.model.copy(), dls_A1v30, 3, 3e-4, base / \"1_gap_varying_tuned_6-30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n4.763339\n3.134267\n0.276432\n15:02\n\n\n1\n1\n2.390429\n1.721823\n0.267772\n16:12\n\n\n2\n2\n0.780169\n0.607745\n0.255524\n16:05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso this is not working …\n\n\nVarying number of variables missing | short gaps 6-30\n\ndls_Vv30 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns)),\n    block_len=100+30,\n    gap_len=gen_gap_len(6, 30),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=20).cpu()\n\n\nlearn_Vv30, items_Vv30 = train_or_load(learn_Aa.model, dls_Vv30, 3, 5e-4, base / \"all_varying_gap_varying_len_6-30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-5.794561\n-4.508800\n0.213908\n1:00:55\n\n\n1\n1\n-3.717697\n-5.165062\n0.205841\n1:00:26\n\n\n2\n2\n-1.928287\n-6.012112\n0.202048\n1:00:21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn_Vv30, items_Vv30 = train_or_load(learn_Aa.model, dls_Vv30, 1, 1e-5, base / \"all_varying_gap_varying_len_6-30_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.061014\n-6.663726\n0.192194\n57:55\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn_Vv30, items_Vv30 = train_or_load(learn_Aa.model, dls_Vv30, 1, 1e-5, base / \"all_varying_gap_varying_len_6-30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.417934\n-6.799482\n0.19076\n1:00:23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom parameters\n\nmodel_Vv_rand = KalmanFilterSR.init_random(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=2*len(hai.columns),\n    n_dim_contr = 2*len(hai_era.columns),\n    seed=27,\n    pred_only_gap=True)\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(model_Vv_rand, dls_Vv30, 3, 1e-3, base / \"rand_all_varying_gap_varying_len_6-30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n13.508053\n14.374478\n0.448185\n1:00:35\n\n\n1\n9.766153\n11.389963\n0.395332\n58:51\n\n\n2\n6.503961\n6.754238\n0.305433\n54:16\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(learn_Vv_rand.model, dls_Vv30, 3, 1e-4, base / \"rand_all_varying_gap_varying_len_6-30_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n5.101230\n5.345336\n0.285398\n1:02:15\n\n\n1\n4.838514\n4.907970\n0.281667\n1:02:44\n\n\n2\n4.571287\n4.295109\n0.275344\n1:04:47\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(learn_Vv_rand.model, dls_Vv30, 3, 1e-5, base / \"rand_all_varying_gap_varying_len_6-30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n4.075190\n4.282635\n0.275272\n2:36:25\n\n\n1\n5.134852\n4.244826\n0.274858\n1:17:58\n\n\n2\n3.923739\n4.185355\n0.274057\n1:11:31\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(learn_Vv_rand.model, dls_Vv30, 3, 1e-5, base / \"rand_all_varying_gap_varying_len_6-30_v4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n4.100049\n4.142156\n0.273928\n56:48\n\n\n1\n3.712223\n4.112651\n0.273566\n56:59\n\n\n2\n3.942678\n4.073334\n0.273349\n57:03\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_Vv_rand\n\nKalman Filter (9 obs, 18 state, 14 contr)  $A$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\nx_0\n0.8775\n0.2675\n0.0937\n0.6706\n0.1638\n0.9272\n0.2620\n0.4967\n0.2630\n0.1175\n0.1694\n0.2100\n0.4890\n0.0564\n0.4760\n0.7606\n0.7759\n0.5243\n\n\nx_1\n0.3714\n0.0426\n0.2343\n0.9991\n0.1775\n0.6319\n0.6734\n0.7937\n0.6468\n0.5825\n0.4599\n0.7960\n0.9038\n0.9735\n0.6428\n0.3725\n0.2052\n0.0507\n\n\nx_2\n0.4448\n0.5775\n0.7237\n0.5927\n0.3217\n0.6441\n0.2801\n0.9132\n0.0329\n0.4856\n0.9927\n0.5895\n0.2611\n0.9413\n0.1371\n0.8726\n0.5590\n0.8451\n\n\nx_3\n0.1253\n0.9434\n0.0462\n0.2360\n0.0239\n0.8950\n0.7419\n0.9471\n0.6690\n0.1554\n0.0821\n0.7309\n0.7764\n0.9769\n0.0196\n0.0384\n0.4294\n0.3438\n\n\nx_4\n0.5494\n0.8238\n0.9845\n0.6826\n0.9001\n0.3022\n0.7509\n0.0926\n0.0328\n0.4798\n0.5335\n0.0434\n0.3530\n0.4157\n0.7495\n0.1716\n0.1980\n0.4298\n\n\nx_5\n0.9201\n0.6883\n0.5342\n0.7847\n0.3137\n0.1778\n0.5838\n0.9799\n0.3611\n0.3155\n0.7475\n0.5450\n0.5641\n0.2493\n0.8323\n0.9723\n0.1883\n0.3605\n\n\nx_6\n0.5344\n0.3443\n0.7696\n0.3410\n0.7553\n0.3177\n0.0315\n0.5209\n0.6514\n0.3131\n0.4510\n0.3550\n0.4790\n0.0676\n0.3606\n0.7299\n0.6713\n0.3134\n\n\nx_7\n0.7460\n0.1291\n0.4653\n0.5693\n0.9906\n0.8288\n0.9039\n0.5240\n0.6277\n0.3574\n0.0076\n0.6530\n0.8667\n0.9368\n0.8667\n0.6749\n0.3526\n0.6618\n\n\nx_8\n0.0837\n0.7188\n0.7247\n0.3211\n0.4898\n0.9030\n0.0358\n0.1662\n0.7741\n0.7937\n0.7183\n0.5141\n0.4918\n0.2773\n0.6901\n0.8565\n0.3723\n0.3410\n\n\nx_9\n0.4035\n0.0591\n0.6836\n0.8306\n0.4312\n0.0210\n0.0032\n0.9010\n0.6741\n0.3875\n0.3683\n0.5337\n0.0706\n0.8516\n0.7304\n0.8507\n0.6829\n0.6900\n\n\nx_10\n0.1059\n0.0500\n0.5736\n0.9595\n0.8101\n0.7397\n0.5282\n0.1294\n0.2746\n0.5556\n0.6463\n0.0023\n0.1761\n0.3391\n0.3346\n0.4655\n0.8172\n0.4176\n\n\nx_11\n0.1349\n0.0519\n0.1180\n0.9767\n0.1679\n0.8635\n0.3753\n0.9760\n0.2125\n0.8049\n0.2124\n0.6794\n0.0037\n0.9711\n0.5679\n0.9474\n0.8593\n0.6385\n\n\nx_12\n0.8770\n0.0469\n0.1582\n0.6694\n0.5670\n0.9794\n0.6498\n0.3257\n0.8462\n0.7727\n0.3213\n0.7318\n0.3665\n0.9550\n0.7188\n0.2660\n0.5867\n0.1134\n\n\nx_13\n0.7401\n0.1982\n0.4165\n0.3814\n0.5263\n0.6516\n0.9604\n0.8996\n0.8318\n0.7448\n0.6912\n0.5938\n0.0929\n0.5298\n0.2637\n0.8722\n0.5430\n0.2217\n\n\nx_14\n0.3495\n0.3756\n0.1251\n0.4052\n0.0638\n0.0588\n0.4379\n0.4891\n0.2796\n0.0740\n0.2123\n0.1370\n0.4477\n0.3628\n0.9125\n0.4047\n0.8130\n0.2332\n\n\nx_15\n0.8424\n0.0816\n0.8791\n0.3892\n0.2923\n0.8603\n0.1172\n0.6212\n0.6087\n0.6072\n0.8778\n0.6758\n0.5495\n0.8240\n0.7461\n0.1555\n0.2950\n0.0365\n\n\nx_16\n0.8060\n0.8602\n0.9453\n0.7811\n0.5495\n0.5861\n0.8480\n0.1940\n0.9206\n0.5589\n0.2148\n0.1828\n0.0636\n0.2885\n0.9426\n0.6787\n0.0080\n0.7527\n\n\nx_17\n0.5032\n0.5585\n0.0789\n0.0409\n0.3918\n0.2908\n0.3802\n0.0407\n0.6447\n0.3241\n0.8544\n0.4245\n0.3987\n0.4367\n0.3384\n0.2285\n0.7890\n0.9094\n\n\n\n\n  $Q$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\nx_0\n1.4985\n0.1823\n0.8819\n1.1215\n0.8124\n1.0154\n0.4127\n0.4993\n0.3792\n1.1249\n0.3349\n0.5480\n1.1208\n1.0406\n0.0373\n0.0713\n0.0563\n0.0038\n\n\nx_1\n0.1823\n0.7474\n0.3388\n0.8779\n0.3284\n0.4377\n0.5683\n0.6806\n0.1517\n0.6860\n0.1334\n0.2869\n0.3903\n0.5594\n0.2687\n0.7078\n0.5301\n0.5990\n\n\nx_2\n0.8819\n0.3388\n1.5250\n1.3832\n1.4418\n0.7184\n0.4702\n1.2013\n1.1584\n1.3029\n1.0110\n1.0116\n1.4669\n1.5483\n0.6885\n0.7897\n1.0060\n0.8858\n\n\nx_3\n1.1215\n0.8779\n1.3832\n2.4360\n1.6422\n1.2901\n1.1545\n1.7087\n1.4626\n2.4015\n1.3411\n1.4348\n2.0194\n2.1839\n1.2870\n1.3528\n1.5829\n1.6857\n\n\nx_4\n0.8124\n0.3284\n1.4418\n1.6422\n3.2440\n1.8293\n0.6635\n2.2540\n1.7788\n2.2643\n1.5406\n1.9412\n2.5939\n2.3073\n2.1935\n1.1262\n1.5039\n1.7599\n\n\nx_5\n1.0154\n0.4377\n0.7184\n1.2901\n1.8293\n2.6367\n0.6717\n2.0377\n1.2394\n1.6564\n1.4530\n1.5842\n2.3638\n2.1887\n2.0828\n1.2967\n0.6836\n0.8444\n\n\nx_6\n0.4127\n0.5683\n0.4702\n1.1545\n0.6635\n0.6717\n1.8814\n1.4915\n0.6485\n1.3643\n0.6293\n1.5192\n1.1450\n1.7599\n1.6733\n1.4465\n0.7419\n1.1889\n\n\nx_7\n0.4993\n0.6806\n1.2013\n1.7087\n2.2540\n2.0377\n1.4915\n3.7419\n2.0936\n2.3554\n2.8167\n2.9808\n2.6624\n3.4940\n3.4288\n2.7951\n1.6056\n2.6631\n\n\nx_8\n0.3792\n0.1517\n1.1584\n1.4626\n1.7788\n1.2394\n0.6485\n2.0936\n3.2420\n2.6115\n2.9707\n2.0584\n3.2186\n2.4394\n2.7694\n2.0308\n2.6026\n2.2404\n\n\nx_9\n1.1249\n0.6860\n1.3029\n2.4015\n2.2643\n1.6564\n1.3643\n2.3554\n2.6115\n4.7867\n3.5267\n2.4691\n3.7434\n3.6431\n3.0501\n2.1870\n3.3714\n2.4697\n\n\nx_10\n0.3349\n0.1334\n1.0110\n1.3411\n1.5406\n1.4530\n0.6293\n2.8167\n2.9707\n3.5267\n4.8661\n2.7361\n3.9109\n3.7590\n4.1276\n3.2131\n3.3042\n2.8676\n\n\nx_11\n0.5480\n0.2869\n1.0116\n1.4348\n1.9412\n1.5842\n1.5192\n2.9808\n2.0584\n2.4691\n2.7361\n4.5057\n3.4122\n4.2709\n3.9642\n3.3446\n2.1979\n2.7754\n\n\nx_12\n1.1208\n0.3903\n1.4669\n2.0194\n2.5939\n2.3638\n1.1450\n2.6624\n3.2186\n3.7434\n3.9109\n3.4122\n6.2732\n5.0000\n5.0583\n4.1123\n3.9812\n3.5458\n\n\nx_13\n1.0406\n0.5594\n1.5483\n2.1839\n2.3073\n2.1887\n1.7599\n3.4940\n2.4394\n3.6431\n3.7590\n4.2709\n5.0000\n6.2369\n5.2005\n4.6903\n3.3753\n4.1217\n\n\nx_14\n0.0373\n0.2687\n0.6885\n1.2870\n2.1935\n2.0828\n1.6733\n3.4288\n2.7694\n3.0501\n4.1276\n3.9642\n5.0583\n5.2005\n7.8159\n5.5318\n4.1568\n5.4245\n\n\nx_15\n0.0713\n0.7078\n0.7897\n1.3528\n1.1262\n1.2967\n1.4465\n2.7951\n2.0308\n2.1870\n3.2131\n3.3446\n4.1123\n4.6903\n5.5318\n5.8308\n3.6118\n5.0658\n\n\nx_16\n0.0563\n0.5301\n1.0060\n1.5829\n1.5039\n0.6836\n0.7419\n1.6056\n2.6026\n3.3714\n3.3042\n2.1979\n3.9812\n3.3753\n4.1568\n3.6118\n5.2990\n4.3170\n\n\nx_17\n0.0038\n0.5990\n0.8858\n1.6857\n1.7599\n0.8444\n1.1889\n2.6631\n2.2404\n2.4697\n2.8676\n2.7754\n3.5458\n4.1217\n5.4245\n5.0658\n4.3170\n7.1195\n\n\n\n\n  $b$ \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.5371\n\n\nx_1\n0.6015\n\n\nx_2\n0.3190\n\n\nx_3\n0.9543\n\n\nx_4\n0.5112\n\n\nx_5\n0.0341\n\n\nx_6\n0.9601\n\n\nx_7\n0.1604\n\n\nx_8\n0.4499\n\n\nx_9\n0.8575\n\n\nx_10\n0.2647\n\n\nx_11\n0.4293\n\n\nx_12\n0.9210\n\n\nx_13\n0.5512\n\n\nx_14\n0.0890\n\n\nx_15\n0.4351\n\n\nx_16\n0.3804\n\n\nx_17\n0.4879\n\n\n\n\n  $H$ \n\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\ny_0\n0.5241\n0.2182\n0.7958\n0.7816\n0.3235\n0.8518\n0.4334\n0.7567\n0.5235\n0.2247\n0.2498\n0.6324\n0.0037\n0.8468\n0.7664\n0.0362\n0.2519\n0.5872\n\n\ny_1\n0.4556\n0.2781\n0.0315\n0.3598\n0.2876\n0.8363\n0.0685\n0.5543\n0.9194\n0.3232\n0.0243\n0.2689\n0.8404\n0.9788\n0.9912\n0.0846\n0.1129\n0.0503\n\n\ny_2\n0.8881\n0.6638\n0.5292\n0.3452\n0.4999\n0.6894\n0.7628\n0.4233\n0.4219\n0.3110\n0.1801\n0.5059\n0.2597\n0.9244\n0.6246\n0.8295\n0.5742\n0.7359\n\n\ny_3\n0.2917\n0.2912\n0.9906\n0.3964\n0.5851\n0.0647\n0.3191\n0.0659\n0.9295\n0.0189\n0.8553\n0.6701\n0.6306\n0.6152\n0.5295\n0.9469\n0.9927\n0.7433\n\n\ny_4\n0.5977\n0.7385\n0.9348\n0.8533\n0.6523\n0.7823\n0.7676\n0.4763\n0.6374\n0.8520\n0.4391\n0.5353\n0.9097\n0.7429\n0.2067\n0.4188\n0.0382\n0.9770\n\n\ny_5\n0.6669\n0.7935\n0.4501\n0.6770\n0.0361\n0.3082\n0.9436\n0.8420\n0.2966\n0.6996\n0.8092\n0.0206\n0.9509\n0.0499\n0.3504\n0.8491\n0.5674\n0.8691\n\n\ny_6\n0.4429\n0.2004\n0.3868\n0.9650\n0.0220\n0.4891\n0.0179\n0.3229\n0.1670\n0.6188\n0.6477\n0.0439\n0.3738\n0.3988\n0.6175\n0.9562\n0.6395\n0.7886\n\n\ny_7\n0.6403\n0.2487\n0.6137\n0.2387\n0.7919\n0.1610\n0.2259\n0.9336\n0.8569\n0.6710\n0.9067\n0.1028\n0.7898\n0.3126\n0.5972\n0.3078\n0.3259\n0.5631\n\n\ny_8\n0.5374\n0.9159\n0.0255\n0.7863\n0.0953\n0.7248\n0.3355\n0.1565\n0.2010\n0.3647\n0.3080\n0.8794\n0.2877\n0.2028\n0.8040\n0.8565\n0.2100\n0.2746\n\n\n\n\n  $R$ \n\n\n\n\nvariable\ny_0\ny_1\ny_2\ny_3\ny_4\ny_5\ny_6\ny_7\ny_8\n\n\n\n\ny_0\n0.5106\n0.3847\n0.4957\n0.2641\n0.0725\n0.3685\n0.7145\n0.0334\n0.6538\n\n\ny_1\n0.3847\n1.1931\n1.0307\n0.4238\n0.1569\n0.4712\n0.6801\n0.9019\n0.7750\n\n\ny_2\n0.4957\n1.0307\n1.9865\n0.5453\n0.3404\n0.8202\n1.0589\n0.7202\n1.1494\n\n\ny_3\n0.2641\n0.4238\n0.5453\n1.3356\n0.2462\n0.9538\n0.8949\n1.0863\n1.4202\n\n\ny_4\n0.0725\n0.1569\n0.3404\n0.2462\n0.9155\n0.8953\n0.5091\n0.3793\n0.3845\n\n\ny_5\n0.3685\n0.4712\n0.8202\n0.9538\n0.8953\n1.9467\n1.7410\n1.1834\n1.4452\n\n\ny_6\n0.7145\n0.6801\n1.0589\n0.8949\n0.5091\n1.7410\n2.9640\n1.6060\n1.6805\n\n\ny_7\n0.0334\n0.9019\n0.7202\n1.0863\n0.3793\n1.1834\n1.6060\n3.7560\n2.1550\n\n\ny_8\n0.6538\n0.7750\n1.1494\n1.4202\n0.3845\n1.4452\n1.6805\n2.1550\n3.0331\n\n\n\n\n  $d$ \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.4399\n\n\ny_1\n0.8723\n\n\ny_2\n0.2250\n\n\ny_3\n0.0971\n\n\ny_4\n0.6572\n\n\ny_5\n0.7544\n\n\ny_6\n0.5670\n\n\ny_7\n0.7409\n\n\ny_8\n0.7357\n\n\n\n\n  $B$ \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\nc_6\nc_7\nc_8\nc_9\nc_10\nc_11\nc_12\nc_13\n\n\n\n\nx_0\n0.0135\n0.9418\n0.6751\n0.3042\n0.0136\n0.7803\n0.2302\n0.5920\n0.7610\n0.8504\n0.2033\n0.5990\n0.8954\n0.0604\n\n\nx_1\n0.2530\n0.1406\n0.4280\n0.1165\n0.5817\n0.2896\n0.4509\n0.2735\n0.8425\n0.5123\n0.4176\n0.5773\n0.3222\n0.5276\n\n\nx_2\n0.4523\n0.6324\n0.4716\n0.0785\n0.9462\n0.5346\n0.9771\n0.4970\n0.5893\n0.5292\n0.6864\n0.5196\n0.8370\n0.3849\n\n\nx_3\n0.4922\n0.3260\n0.1990\n0.6217\n0.7287\n0.4589\n0.8500\n0.1375\n0.9457\n0.8397\n0.5203\n0.8416\n0.1371\n0.5186\n\n\nx_4\n0.4377\n0.2392\n0.4949\n0.4146\n0.3028\n0.6810\n0.1177\n0.1563\n0.2588\n0.8996\n0.9248\n0.5575\n0.2553\n0.0631\n\n\nx_5\n0.5413\n0.5853\n0.4166\n0.9482\n0.0665\n0.4683\n0.0348\n0.6635\n0.0501\n0.1221\n0.1268\n0.7322\n0.3311\n0.0151\n\n\nx_6\n0.1452\n0.5820\n0.8673\n0.3090\n0.8065\n0.7325\n0.1682\n0.5885\n0.1180\n0.4120\n0.2043\n0.8200\n0.5015\n0.3238\n\n\nx_7\n0.3842\n0.4433\n0.3052\n0.4963\n0.4459\n0.9266\n0.6286\n0.8807\n0.3252\n0.0861\n0.7891\n0.1666\n0.1766\n0.0318\n\n\nx_8\n0.8600\n0.8088\n0.8600\n0.5418\n0.4772\n0.9634\n0.3191\n0.1484\n0.6377\n0.0586\n0.5372\n0.8380\n0.8808\n0.2243\n\n\nx_9\n0.4277\n0.0173\n0.9436\n0.3526\n0.1852\n0.2433\n0.8409\n0.7467\n0.4969\n0.2585\n0.3466\n0.4240\n0.1253\n0.2661\n\n\nx_10\n0.9022\n0.0314\n0.0804\n0.7244\n0.3651\n0.0938\n0.8409\n0.0069\n0.3613\n0.6663\n0.1531\n0.9582\n0.1326\n0.9434\n\n\nx_11\n0.8686\n0.9671\n0.1879\n0.7194\n0.3153\n0.5075\n0.6469\n0.0551\n0.2449\n0.5830\n0.3328\n0.4071\n0.2686\n0.4456\n\n\nx_12\n0.5746\n0.1570\n0.5606\n0.7224\n0.6012\n0.4299\n0.0548\n0.3849\n0.0750\n0.4321\n0.9120\n0.4023\n0.5149\n0.5738\n\n\nx_13\n0.1813\n0.1437\n0.8099\n0.2174\n0.2784\n0.7365\n0.5066\n0.1417\n0.6935\n0.0812\n0.0792\n0.1286\n0.6698\n0.1731\n\n\nx_14\n0.3023\n0.8685\n0.0737\n0.2969\n0.0566\n0.7863\n0.9368\n0.2227\n0.0272\n0.9288\n0.2405\n0.8415\n0.4647\n0.5220\n\n\nx_15\n0.2359\n0.5393\n0.3662\n0.9737\n0.1073\n0.0926\n0.9738\n0.8049\n0.2272\n0.4266\n0.4965\n0.2811\n0.5143\n0.1134\n\n\nx_16\n0.8076\n0.4430\n0.9223\n0.0757\n0.7333\n0.1208\n0.4115\n0.5446\n0.8064\n0.5765\n0.2153\n0.4235\n0.2613\n0.2662\n\n\nx_17\n0.4906\n0.6666\n0.1782\n0.4631\n0.4471\n0.4886\n0.6511\n0.1357\n0.9547\n0.8251\n0.5739\n0.0537\n0.9671\n0.1413\n\n\n\n\n  $m_0$ \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.4748\n\n\nx_1\n0.0525\n\n\nx_2\n0.8524\n\n\nx_3\n0.5821\n\n\nx_4\n0.7281\n\n\nx_5\n0.9879\n\n\nx_6\n0.6011\n\n\nx_7\n0.4692\n\n\nx_8\n0.9031\n\n\nx_9\n0.9123\n\n\nx_10\n0.6185\n\n\nx_11\n0.8070\n\n\nx_12\n0.5830\n\n\nx_13\n0.5986\n\n\nx_14\n0.5898\n\n\nx_15\n0.8722\n\n\nx_16\n0.7868\n\n\nx_17\n0.8305\n\n\n\n\n  $P_0$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\nx_0\n0.7075\n0.4575\n0.1025\n0.7552\n0.4678\n0.4102\n0.6403\n0.2264\n0.0279\n0.6776\n0.8178\n0.4180\n0.7019\n0.4730\n0.7924\n0.5532\n0.6499\n0.0084\n\n\nx_1\n0.4575\n1.3349\n0.8975\n1.0693\n0.9878\n0.5854\n0.6734\n0.6813\n0.5162\n1.2483\n1.1523\n0.6240\n0.8392\n0.6469\n1.1093\n0.4286\n0.6817\n0.5511\n\n\nx_2\n0.1025\n0.8975\n1.7778\n1.4223\n1.0713\n0.5301\n0.4902\n0.9565\n0.7575\n1.1096\n0.6516\n0.6540\n1.3722\n0.9620\n0.6709\n1.0418\n0.7424\n0.5799\n\n\nx_3\n0.7552\n1.0693\n1.4223\n2.9851\n1.4541\n1.0645\n1.6541\n0.9605\n1.5344\n2.4493\n1.9649\n1.5684\n1.7126\n1.3214\n1.6471\n1.7876\n1.5110\n1.4273\n\n\nx_4\n0.4678\n0.9878\n1.0713\n1.4541\n1.6513\n1.2777\n1.3102\n0.7676\n1.0484\n1.4509\n1.6367\n1.4200\n1.9104\n0.8253\n1.1014\n0.9313\n1.6568\n1.2782\n\n\nx_5\n0.4102\n0.5854\n0.5301\n1.0645\n1.2777\n2.0910\n2.0046\n1.1160\n1.0262\n1.7268\n2.2526\n2.0053\n2.1983\n1.2724\n1.6174\n1.5978\n2.3012\n1.8972\n\n\nx_6\n0.6403\n0.6734\n0.4902\n1.6541\n1.3102\n2.0046\n3.2961\n1.3658\n2.1582\n2.6066\n2.7869\n2.4754\n2.3180\n1.8782\n1.9604\n2.2224\n3.0209\n2.9851\n\n\nx_7\n0.2264\n0.6813\n0.9565\n0.9605\n0.7676\n1.1160\n1.3658\n2.2370\n1.1397\n2.0217\n1.7425\n1.4582\n1.7065\n1.2660\n1.8500\n1.3857\n2.3256\n1.4360\n\n\nx_8\n0.0279\n0.5162\n0.7575\n1.5344\n1.0484\n1.0262\n2.1582\n1.1397\n3.1976\n2.4686\n2.6294\n2.0770\n2.1077\n1.4005\n1.6343\n2.0925\n2.2970\n2.9207\n\n\nx_9\n0.6776\n1.2483\n1.1096\n2.4493\n1.4509\n1.7268\n2.6066\n2.0217\n2.4686\n3.8815\n3.3607\n2.4745\n3.0405\n2.4247\n2.8433\n3.0134\n3.4315\n3.3814\n\n\nx_10\n0.8178\n1.1523\n0.6516\n1.9649\n1.6367\n2.2526\n2.7869\n1.7425\n2.6294\n3.3607\n4.7920\n3.0654\n3.5766\n2.7664\n3.4915\n3.4792\n4.0401\n3.2582\n\n\nx_11\n0.4180\n0.6240\n0.6540\n1.5684\n1.4200\n2.0053\n2.4754\n1.4582\n2.0770\n2.4745\n3.0654\n3.1407\n3.3492\n2.5045\n2.9033\n2.5216\n3.3064\n2.7858\n\n\nx_12\n0.7019\n0.8392\n1.3722\n1.7126\n1.9104\n2.1983\n2.3180\n1.7065\n2.1077\n3.0405\n3.5766\n3.3492\n6.7801\n4.6939\n3.9369\n4.9600\n4.8216\n3.7575\n\n\nx_13\n0.4730\n0.6469\n0.9620\n1.3214\n0.8253\n1.2724\n1.8782\n1.2660\n1.4005\n2.4247\n2.7664\n2.5045\n4.6939\n5.2116\n3.8491\n4.6837\n3.8005\n3.5731\n\n\nx_14\n0.7924\n1.1093\n0.6709\n1.6471\n1.1014\n1.6174\n1.9604\n1.8500\n1.6343\n2.8433\n3.4915\n2.9033\n3.9369\n3.8491\n5.2003\n4.0112\n3.8169\n3.0359\n\n\nx_15\n0.5532\n0.4286\n1.0418\n1.7876\n0.9313\n1.5978\n2.2224\n1.3857\n2.0925\n3.0134\n3.4792\n2.5216\n4.9600\n4.6837\n4.0112\n6.7694\n5.0850\n4.5844\n\n\nx_16\n0.6499\n0.6817\n0.7424\n1.5110\n1.6568\n2.3012\n3.0209\n2.3256\n2.2970\n3.4315\n4.0401\n3.3064\n4.8216\n3.8005\n3.8169\n5.0850\n6.7135\n5.3989\n\n\nx_17\n0.0084\n0.5511\n0.5799\n1.4273\n1.2782\n1.8972\n2.9851\n1.4360\n2.9207\n3.3814\n3.2582\n2.7858\n3.7575\n3.5731\n3.0359\n4.5844\n5.3989\n7.9999"
  },
  {
    "objectID": "results/model_training.html#fine-tuning",
    "href": "results/model_training.html#fine-tuning",
    "title": "Training Kalman Filter for Results",
    "section": "Fine tuning",
    "text": "Fine tuning\n\nFine tune Variable | gap only for one variable | gap len 6-336\nfine tune the model to only one variable\n\nfrom fastcore.basics import *\n\n\nfrom IPython.display import HTML\n\n\nvar_learning = {\n    'TA': [{'lr': 1e-3, 'n': 3}],  \n    'SW_IN': [{'lr': 1e-3, 'n': 4}],  \n    'SW_IN': [{'lr': 1e-3, 'n': 4}],  \n    'LW_IN': [{'lr': 1e-3, 'n': 3}],  \n    'VPD': [{'lr': 1e-3, 'n': 3}],  \n    'WS': [{'lr': 1e-3, 'n': 3}],  \n    'PA': [{'lr': 1e-3, 'n': 3}],  \n    # 'P': [{'lr': 1e-3, 'n': 3}], missing on purpose  \n    'SWC' : [{'lr': 1e-3, 'n': 5}, {'lr': 1e-5, 'n': 1}],\n    'TS' : [{'lr': 1e-3, 'n': 5}],\n\n\n}\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\ndef fine_tune(var_learning, learn):\n    spec_models = {}\n    spec_dls = {}\n    spec_learn = {}\n    spec_items = {}\n    for var in tqdm(var_learning.keys()):\n        display(HTML(f\"&lt;h4&gt; {var} | Gap len 6-336  finetune&lt;/h4&gt;\"))\n        spec_dls[var] = imp_dataloader(hai, hai_era, var_sel = var, block_len=100+336, gap_len=gen_gap_len(6, 336), bs=20, control_lags=[1], n_rep=3, shifts=gen_shifts(50)).cpu()\n        if show_metrics:\n            display(HTML(\"Metrics generic model\"))\n            display(metric_valid(learn, dls=spec_dls[var].valid))\n        for i, train in enumerate(var_learning[var]):\n            lr, n = train\n            display(HTML(f\"train {i}\"))\n            spec_models[var] = learn.model.copy()\n            spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], lr, n, base / f\"{var}_specialized_gap_6-336_v1_{i}\")\n            plt.show()\n    return spec_models, spec_dls, spec_learn, spec_items\n\n\nspec_models, spec_dls, spec_learn, spec_items = fine_tune(var_learning, learn_A1v)\n\n\n\n\n TA | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-73.308072\n-59.729135\n0.155103\n23:04\n\n\n1\n1\n-87.049737\n-73.262853\n0.139768\n20:37\n\n\n2\n2\n-92.947376\n-82.557740\n0.131374\n21:16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n SW_IN | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n53.445060\n43.464660\n0.286705\n23:25\n\n\n1\n1\n49.458273\n42.814378\n0.285042\n23:47\n\n\n2\n2\n48.186476\n43.087234\n0.283170\n22:43\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n LW_IN | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n104.665918\n106.041969\n0.414556\n22:34\n\n\n1\n1\n101.284686\n107.526856\n0.419593\n23:28\n\n\n2\n2\n99.767878\n108.885340\n0.420613\n23:43\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n VPD | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n52.165032\n50.852617\n0.295505\n23:31\n\n\n1\n1\n45.131744\n36.635116\n0.272213\n22:41\n\n\n2\n2\n41.990330\n32.914802\n0.264974\n23:06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n WS | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n175.891940\n246.176982\n0.761467\n22:58\n\n\n1\n1\n165.224262\n235.823659\n0.719208\n23:03\n\n\n2\n2\n159.586716\n246.720599\n0.727202\n21:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n PA | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-99.014311\n-97.738537\n0.127062\n21:53\n\n\n1\n1\n-123.031773\n-104.182228\n0.120468\n22:45\n\n\n2\n2\n-130.901483\n-133.160406\n0.104076\n24:11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n SWC | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n172.303569\n200.127139\n0.599423\n23:57\n\n\n1\n1\n132.754943\n76.512459\n0.305065\n22:01\n\n\n2\n2\n93.005439\n57.391193\n0.270830\n20:58\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 1\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n172.303569\n200.127139\n0.599423\n23:57\n\n\n1\n1\n132.754943\n76.512459\n0.305065\n22:01\n\n\n2\n2\n93.005439\n57.391193\n0.270830\n20:58\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n TS | Gap len 6-336  finetune\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n68.246805\n54.157492\n0.268725\n23:05\n\n\n1\n1\n57.107431\n39.662085\n0.247396\n21:34\n\n\n2\n2\n57.394308\n53.605346\n0.260903\n20:52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar_learning2 = {\n    'TA': [{'lr': 1e-3, 'n': 3}],  \n    'VPD': [{'lr': 1e-3, 'n': 2}],  \n    'PA': [{'lr': 1e-3, 'n': 2}],  \n    'SWC' : [{'lr': 1e-3, 'n': 3}, {'lr': 1e-5, 'n': 1}],\n    'TS' : [{'lr': 1e-3, 'n': 2}],\n}\n\n\ndef fine_tune2(var_learning, spec_dls, spec_learn, spec_items):\n    spec_learn = spec_learn.copy()\n    for var in tqdm(var_learning.keys()):\n        display(HTML(f\"&lt;h4&gt; {var} | Gap len 6-336  finetune 2 &lt;/h4&gt;\"))\n        for i, train in enumerate(var_learning[var]):\n            lr, n = train['lr'], train['n']\n            v = train.get('v', 2)\n            display(HTML(f\"train {i}\"))\n            spec_learn[var], _ = train_or_load(spec_learn[var].model, spec_dls[var], n, lr, path=base / f\"{var}_specialized_gap_6-336_v{v}_{i}\")\n            plt.show()\n    return spec_dls, spec_learn, spec_items\n\n\nspec_dls2, spec_learn2, spec_items2 = fine_tune2(var_learning2, spec_dls, spec_learn, spec_items)\n\n\n\n\n TA | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-97.718002\n-90.171141\n0.125761\n21:44\n\n\n1\n1\n-100.493585\n-88.338616\n0.126856\n20:23\n\n\n2\n2\n-103.769092\n-73.432097\n0.135440\n20:41\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n SW_IN | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n45.373127\n42.291928\n0.282366\n20:34\n\n\n1\n1\n46.487908\n41.780965\n0.283791\n20:26\n\n\n2\n2\n46.903155\n45.729009\n0.290985\n20:43\n\n\n3\n3\n46.917554\n39.832071\n0.278899\n20:29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n LW_IN | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n98.188426\n107.646729\n0.414824\n20:22\n\n\n1\n1\n95.761911\n108.694974\n0.416340\n20:30\n\n\n2\n2\n97.285924\n106.717304\n0.415862\n20:30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n VPD | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n42.370522\n31.185345\n0.254092\n20:27\n\n\n1\n1\n37.933048\n29.823322\n0.255506\n20:25\n\n\n2\n2\n35.904875\n25.101849\n0.245977\n20:28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n WS | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n158.971839\n247.131486\n0.730278\n20:37\n\n\n1\n1\n158.584698\n253.701959\n0.736434\n20:34\n\n\n2\n2\n156.970299\n263.529629\n0.749744\n20:22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n PA | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-131.680055\n-112.508634\n0.115626\n23:56\n\n\n1\n1\n-145.849450\n-95.348580\n0.114896\n24:38\n\n\n2\n2\n-145.281960\n-107.879192\n0.116886\n24:47\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n SWC | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n35.588624\n-20.680066\n0.174256\n24:56\n\n\n1\n1\n10.683595\n-23.630941\n0.167582\n24:32\n\n\n2\n2\n-34.885516\n-76.834793\n0.124991\n25:11\n\n\n3\n3\n-47.704883\n0.936394\n0.166095\n24:47\n\n\n4\n4\n-61.871826\n-75.499625\n0.120001\n24:45\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 1\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-82.762576\n-101.858651\n0.103941\n24:41\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n TS | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n40.478151\n29.605951\n0.230160\n25:47\n\n\n1\n1\n36.143091\n21.822619\n0.211396\n28:18\n\n\n2\n2\n30.493916\n-0.701642\n0.181059\n24:59\n\n\n3\n3\n26.646244\n17.138843\n0.201734\n27:48\n\n\n4\n4\n19.231903\n10.231487\n0.202251\n29:09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar_learning3 = {\n    'TA': [{'lr': 1e-5, 'n': 1, 'v': 3}],  \n    'PA': [{'lr': 1e-5, 'n': 1, 'v': 3}],  \n}\n\n\nspec_dls3, spec_learn3, spec_items3 = fine_tune2(var_learning3, spec_dls, spec_learn, spec_items)\n\n\n\n\n TA | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-98.172894\n-86.769272\n0.131181\n28:21\n\n\n\n\n\n\n\n\n\n\n\n\n PA | Gap len 6-336  finetune 2 \n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-155.452174\n-135.116148\n0.105047\n27:51"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#outline",
    "href": "presentations/presentation_bioclim_18_jan_23.html#outline",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\nKalman Filter\nPreliminary results\nNext steps"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#background",
    "href": "presentations/presentation_bioclim_18_jan_23.html#background",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Background",
    "text": "Background\n\nEC tower measures also meteorological variables (eg. air temperature, wind speed)\ntechnical issues (eg. broken sensor) result in meteo time series with gaps\nPresence of gaps is a problem in many EC data applications (eg. ecosystem modelling)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#dataset",
    "href": "presentations/presentation_bioclim_18_jan_23.html#dataset",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Dataset",
    "text": "Dataset\n\nFluxnet 2015 data from Hainich (more 20 years)\nGlobal meteo dataset (downscaled ERA-Interim from Fluxnet 2015)\nmeteorological measurements every 30 mins\nfocusing on 3 variables\n\nAir temperature: TA\nIncoming shortwave radiation: SW_IN\nVapour Pressure Deficit: VPD"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of gaps for all TA, SW_IN and VPD for all sites. 26 millions total obs"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of small gaps (&lt;1 week) for all TA, SW_IN and VPD for all sites. 26 millions total obs"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-fill-gaps",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-fill-gaps",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to fill gaps",
    "text": "How to fill gaps\n\nuse previous and following measurements for one variable and temporal auto-correlation (eg. diurnal cycles)\ncorrelation with other variables measures (eg. solar radiation and temperature)\nother measurements of meteo variables (eg. nearby station)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#state-of-the-art",
    "href": "presentations/presentation_bioclim_18_jan_23.html#state-of-the-art",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "State of the art",
    "text": "State of the art\nOneFlux pipeline (Fluxnet + ICOS + AmeriFlux)\n\nShort and medium gaps using Marginal Distribution Sampling (MDS)\nLong gaps filled with ERA data (global meteo dataset) using linear transformation to reduce site bias"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-mds-marginal-distribution-sampling-works",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-mds-marginal-distribution-sampling-works",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How MDS (Marginal Distribution Sampling) works",
    "text": "How MDS (Marginal Distribution Sampling) works\n\n\n\ntake a time window (7 days) around the gap\nuse 3 predictors variables (TA, SW_IN and VPD) and divide them in n discrete bins\nfor each bin (combination of conditions) find the average value of the missing variable\nfor each gap find the closest condition and fill with the average value\nif necessary increase the time window\nquality flag depends on the time window size"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-ta",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-ta",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling TA",
    "text": "MDS - gap filling TA"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-sw_in",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-sw_in",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling SW_IN",
    "text": "MDS - gap filling SW_IN"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-vpd",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-vpd",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling VPD",
    "text": "MDS - gap filling VPD"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#current-approaches-limitations",
    "href": "presentations/presentation_bioclim_18_jan_23.html#current-approaches-limitations",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Current approaches limitations",
    "text": "Current approaches limitations\n\ndon’t consider the observations before and after the gap\nEither MDS (variable correlation) or ERA data, don’t combine the information\nNo uncertainty for the predictions (only a quality flag)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#thesis-goal",
    "href": "presentations/presentation_bioclim_18_jan_23.html#thesis-goal",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Thesis goal",
    "text": "Thesis goal\n\ndevelop model to impute missing data in meteorological time series\ninclude all 3 imputation approaches\nprovide an uncertainty of the predictions"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-kalman-filter-works",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-kalman-filter-works",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How Kalman Filter works",
    "text": "How Kalman Filter works\n\n\nModels over time a latent variable (we are not observing it), the state of the system.\nThe current state \\(x_t\\) depends using:\n\nthe previous state \\(\\color{blue}{x_{t-1}}\\)\ncurrent observation \\(\\color{green}{y_t}\\)\ncontrol variable \\(\\color{purple}{c_t}\\) (ERA data)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#previous-state",
    "href": "presentations/presentation_bioclim_18_jan_23.html#previous-state",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "1. Previous state",
    "text": "1. Previous state\n\n\n\\[ x_t = A\\color{blue}{x_{t-1}} + \\varepsilon \\] where:\n\n\\(x_{t}\\) is the current state\n\\(\\color{blue}{x_{t-1}}\\) is the previous state\n\\(A\\) is a linear transformation of \\(\\color{blue}{x_{t-1}}\\)\n\\(\\varepsilon\\) is the “process” noise which is a random variable with a normal distribution with mean 0\n\n\n\n\n\n\nExample of Kalman Filter \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#current-observation",
    "href": "presentations/presentation_bioclim_18_jan_23.html#current-observation",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "2. Current Observation",
    "text": "2. Current Observation\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(\\color{green}{y_{t}}\\) is the current observation\n\\(H\\) is a linear transformation of \\(\\color{green}{y_{t}}\\)\n\\(\\nu\\) is the “observation” noise which is a random variable with a normal distribution with mean 0\n\nusing the rules of probabilistic inference if we observe \\(y_t\\) you can update the distribution of \\(x_t\\)\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gaps",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gaps",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gaps",
    "text": "Gaps\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#control-variable",
    "href": "presentations/presentation_bioclim_18_jan_23.html#control-variable",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "3. Control variable",
    "text": "3. Control variable\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + B\\color{purple}{c_t} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(B\\) is a linear transformation of \\(\\color{purple}{c_t}\\)\n\nUse the difference between current and previous value of control variable\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\) \\(B=[-1,1]\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#extra-variable-correlation",
    "href": "presentations/presentation_bioclim_18_jan_23.html#extra-variable-correlation",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Extra: Variable correlation",
    "text": "Extra: Variable correlation\n\n\nGap in two variables ::: {.cell} ::: {.cell-output-display}\n\n\n\n:::\n\nGap in only one variable ::: {.cell} ::: {.cell-output-display}\n\n\n::: :::\n::::"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-find-model-parameters",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-find-model-parameters",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to find model parameters",
    "text": "How to find model parameters\n\ncreate artificial gaps\npredicting gap in the model\ncompute the log likelihood of the predictions\nmaximise the log likelihood"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filter",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filter",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filter",
    "text": "Kalman Filter\npros:\n\nProbabilist model: the output of the model is a distribution of predictions, not a single value\nCombines all 3 approaches to gap filling in one model\ninterpretable paramters\ncomputationally efficient\n\ncons:\n\nkeeps tracks only of the local state"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #1",
    "text": "Kalman Filters gap #1"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-2",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-2",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #2",
    "text": "Kalman Filters gap #2"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-3",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-3",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #3",
    "text": "Kalman Filters gap #3"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#what-is-missing-in-the-model-development",
    "href": "presentations/presentation_bioclim_18_jan_23.html#what-is-missing-in-the-model-development",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "What is missing in the model development",
    "text": "What is missing in the model development\n\nimprove numerical stability of model (work in progress)\nfind optimal settings for training and inference\n\nn observations before after/gap\nhow to best generate artificial gaps\n\nHow to assess the model?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#use-of-gap-filling",
    "href": "presentations/presentation_bioclim_18_jan_23.html#use-of-gap-filling",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Use of gap filling",
    "text": "Use of gap filling\n\nwhat is the impact of better gap filling for data users?\n\nwhy better filling for short/medium gaps is useful\nhow can the uncertainty be used"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-settings",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-settings",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? settings",
    "text": "How to assess the model? settings\n\nhow to choose gap lengths?\nhow to choose number of variables missing?\nwhich variable to focus on?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-metrics",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-metrics",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Metrics",
    "text": "How to assess the model? Metrics\n\nRMSE - interpretation difficult as it’s relative to the variable\nr2 - gaps are often too short to interpret properly\n?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nTime series\n\n\n\n\n\n\n\n\n\n\nScatter plots"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nGap length / mean RMSE\n\n\n\n\n\n\n\n\n\n\nDistribution gaps vs filled"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#future-outlook",
    "href": "presentations/presentation_bioclim_18_jan_23.html#future-outlook",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Future outlook",
    "text": "Future outlook\n\noptimize performance model\nprovide pre-trained model on Fluxnet 2015 and then to fine-tune to local site\nprovide web-service for filling gaps\nreprocess Fluxnet 2015 dataset"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html",
    "href": "presentations/Hainich_results_pres18_jan.html",
    "title": "Hainich with ERA-Interim",
    "section": "",
    "text": "Manual fine tuning learning process for presentation 18 Jan 2023\n%load_ext autoreload\n%autoreload 2\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom sklearn.decomposition import PCA\nreset_seed()\nhai = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path64)\n# dls = imp_dataloader(hai64, hai_era64, var_sel = gen_var_sel(['TA', 'SW_IN', 'VPD']), block_len=200, gap_len=10, bs=20, control_lags=[1])\npc = PCA().fit(hai)\nd0 = hai.iloc[0:1]\ntr0 = pc.transform(d0)\ntr0\n\narray([[-121.11917652,   -7.06844313,    0.87975241]])\nt0 = np.random.randn(5,1) * 10\ntt = np.hstack([t0, 2 * t0 + np.random.randn(5,1), 3 * t0 + np.random.randn(5,1)])\ntt = np.vstack([tt, -tt])\ntt.mean(0)\n\narray([1.77635684e-16, 0.00000000e+00, 0.00000000e+00])\ndef pca(X):\n    # Data matrix X, assumes 0-centered\n    n, m = X.shape\n    assert np.allclose(X.mean(axis=0), np.zeros(m))\n    # Compute covariance matrix\n    C = np.dot(X.T, X) / (n-1)\n    # Eigen decomposition\n    eigen_vals, eigen_vecs = np.linalg.eig(C)\n    # Project X onto PC space\n    X_pca = np.dot(X, eigen_vecs)\n    return X_pca, eigen_vecs\npca(tt)\n\n(array([[-9.15947487e+00, -3.03773243e-01,  3.81626348e-01],\n        [ 2.00045215e+01, -1.99595583e-02,  1.53745860e+00],\n        [ 1.33744310e+01,  2.01395206e-02, -3.19819214e-02],\n        [-8.99057811e+00,  1.24208069e-01,  9.57988123e-01],\n        [ 3.14616875e+01, -4.88141095e-02, -5.79117681e-01],\n        [ 9.15947487e+00,  3.03773243e-01, -3.81626348e-01],\n        [-2.00045215e+01,  1.99595583e-02, -1.53745860e+00],\n        [-1.33744310e+01, -2.01395206e-02,  3.19819214e-02],\n        [ 8.99057811e+00, -1.24208069e-01, -9.57988123e-01],\n        [-3.14616875e+01,  4.88141095e-02,  5.79117681e-01]]),\n array([[-0.26545847, -0.95659856,  0.12021234],\n        [-0.53518386,  0.04249433, -0.84366608],\n        [-0.80194142,  0.28829401,  0.52323659]]))\nsk_pc = PCA(2).fit(tt)\ntt @ sk_pc.components_.T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\ntr = sk_pc.transform(tt)\ntr\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\nsk_pc.components_.T\n\narray([[-0.26545847,  0.12021234],\n       [-0.53518386, -0.84366608],\n       [-0.80194142,  0.52323659]])\n(sk_pc.components_ @ tt.T).T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\ntt[0, None]\n\narray([[2.76792539, 4.56712931, 7.45746711]])\nsk_pc.components_.shape\n\n(3, 3)\ntt[0].shape\n\n(3,)\n(sk_pc.components_ @ tt[0])\n\narray([-9.15947487,  0.38162635])\ntt[0]\n\narray([2.76792539, 4.56712931, 7.45746711])\nsk_pc.components_.T @ tr[0]\n\narray([2.47733634, 4.58003795, 7.54504311])\nsk_pc.inverse_transform(tr[0])\n\narray([2.47733634, 4.58003795, 7.54504311])\nd0.to_numpy()\n\narray([[-0.6  ,  0.   ,  0.222]])\nhai.iloc[0]\n\nTA      -0.600\nSW_IN    0.000\nVPD      0.222\nName: 2000-01-01 00:30:00, dtype: float64\n\\[ x = y\\Lambda \\]\npc.components_\n\narray([[ 0.01681572,  0.99979324,  0.01143269],\n       [ 0.93010891, -0.01983747,  0.36674772],\n       [-0.36689868, -0.00446652,  0.93025018]])\nnp.linalg.inv(pc.components_)\n\narray([[ 0.01681572,  0.93010891, -0.36689868],\n       [ 0.99979324, -0.01983747, -0.00446652],\n       [ 0.01143269,  0.36674772,  0.93025018]])\nassuming that the control has the same dimensions of the observations then if we are doing a local slope model we have \\(B \\in \\mathbb{R}^{state \\times contr}\\): \\[ B = \\begin{bmatrix} -I & I \\\\ 0 & 0 \\end{bmatrix}\\]\nfrom torch import hstack, eye, vstack, ones, zeros, tensor\nfrom functools import partial\ndef set_dtype(*args, dtype=torch.float64):\n    return [partial(arg, dtype=dtype) for arg in args]\neye, ones, zeros, tensor = set_dtype(eye, ones, zeros, tensor)\ndef init_smart(n_dim_obs, n_dim_state, df, pca=True):\n    # n_dim_obs == n_dim_contr\n    if pca:\n        comp = PCA(n_dim_state).fit(df).components_\n        obs_matrix = tensor(comp.T) # transform state -&gt; obs\n        contr_matrix = tensor(comp) # transform obs -&gt; state\n    else:\n        obs_matrix, contr_matrix = eye(n_dim_obs), eye(n_dim_obs)\n        \n    return KalmanFilter(\n        trans_matrix =     vstack([hstack([eye(n_dim_state),                eye(n_dim_state)]),\n                                   hstack([zeros(n_dim_state, n_dim_state), eye(n_dim_state)])]),\n        trans_off =        zeros(n_dim_state * 2),        \n        trans_cov =        eye(n_dim_state * 2)*.1,        \n        obs_matrix =       hstack([obs_matrix, zeros(n_dim_obs, n_dim_state)]),\n        obs_off =          zeros(n_dim_obs),          \n        obs_cov =          eye(n_dim_obs)*.01,            \n        contr_matrix =     vstack([hstack([-contr_matrix,                  contr_matrix]),\n                                   hstack([ zeros(n_dim_state,n_dim_obs), zeros(n_dim_state, n_dim_obs)])]),\n        init_state_mean =  zeros(n_dim_state * 2),        \n        init_state_cov =   eye(n_dim_state * 2) * 3,\n    )\nnp.hstack([np.eye(2), np.eye(2)])\n\narray([[1., 0., 1., 0.],\n       [0., 1., 0., 1.]])\ninit_smart(3,2, hai)\n\nKalman Filter (3 obs, 4 state, 6 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n1.0000\n0.0000\n1.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n1.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\n\n\n\n\ny_0\n0.0168\n0.9301\n0.0000\n0.0000\n\n\ny_1\n0.9998\n-0.0198\n0.0000\n0.0000\n\n\ny_2\n0.0114\n0.3667\n0.0000\n0.0000\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0168\n-0.9998\n-0.0114\n0.0168\n0.9998\n0.0114\n\n\nx_1\n-0.9301\n0.0198\n-0.3667\n0.9301\n-0.0198\n0.3667\n\n\nx_2\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\nclass PersistentRecorder(Callback):\n    order = 70\n    name = \"per_recorder\"\n    attrs = ['lrs', 'iters', 'losses', 'values']\n    def before_fit(self):\n        \"Prepare state for training\"\n        for attr in self.attrs:\n            if not hasattr(self.per_recorder, attr): setattr(self.per_recorder, attr, [])\n\n    def after_batch(self):\n        for attr in self.attrs:\n            setattr(self.per_recorder, attr, getattr(self.recorder, attr))\nmodels = []\ndls = imp_dataloader(hai, hai_era, var_sel = ['TA', 'SW_IN', 'VPD'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\nlen(dls.valid.items)\nitems = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\ndef train_show_save(learn, n_iter, lr):\n    learn.fit(n_iter, lr)\n    models.append(learn.model.state_dict().copy())\n    learn.recorder.plot_loss()\n    items = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n    return show_results(learn, items = items, control=hai_control)"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html#gaps-all-variables",
    "href": "presentations/Hainich_results_pres18_jan.html#gaps-all-variables",
    "title": "Hainich with ERA-Interim",
    "section": "Gaps all variables",
    "text": "Gaps all variables\n\nmodel = init_smart(3,3,3, hai, pca=False).cuda()\n\n\nloss = loss_func=KalmanLoss(only_gap=False)\nlearn = Learner(dls, model, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\nlearn.model.use_smooth = True\n\n\nshow_results(learn, items=items, control=hai_control)\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-87.370314\n-96.212915\n0.069196\n0.210351\n0.946861\n-250571814513532732431918956544.000000\n02:37\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1489\n0.0042\n0.0120\n0.8404\n-0.0794\n-0.1127\n\n\nx_1\n-0.0954\n1.1464\n-0.0546\n0.1177\n0.8560\n0.0599\n\n\nx_2\n-0.0225\n0.0420\n1.1357\n-0.0717\n-0.1190\n0.8475\n\n\nx_3\n0.1201\n0.0661\n0.1170\n1.1840\n-0.1150\n-0.1073\n\n\nx_4\n0.0770\n0.1299\n0.0068\n-0.1847\n1.0987\n-0.1500\n\n\nx_5\n0.1084\n0.0358\n0.1133\n-0.1809\n-0.0281\n1.1533\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0341\n0.0493\n0.0361\n-0.0008\n-0.0312\n0.0026\n\n\nx_1\n0.0493\n0.1125\n0.0531\n0.0475\n0.0047\n0.0509\n\n\nx_2\n0.0361\n0.0531\n0.0399\n0.0001\n-0.0327\n0.0058\n\n\nx_3\n-0.0008\n0.0475\n0.0001\n0.0608\n0.0614\n0.0585\n\n\nx_4\n-0.0312\n0.0047\n-0.0327\n0.0614\n0.0901\n0.0554\n\n\nx_5\n0.0026\n0.0509\n0.0058\n0.0585\n0.0554\n0.0594\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0008\n\n\nx_1\n-0.0083\n\n\nx_2\n0.0067\n\n\nx_3\n0.0023\n\n\nx_4\n-0.0007\n\n\nx_5\n0.0005\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.8238\n-0.1897\n-0.1014\n-0.0841\n0.1914\n0.1286\n\n\ny_1\n0.0814\n0.8353\n0.1048\n0.1040\n0.0127\n0.0689\n\n\ny_2\n-0.0742\n-0.1502\n0.8296\n0.1282\n0.1811\n-0.0746\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0084\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0084\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0084\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0017\n\n\ny_1\n-0.0046\n\n\ny_2\n0.0026\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0290\n-0.0485\n-0.0022\n0.9297\n0.0792\n-0.0247\n\n\nx_1\n-0.0267\n-0.8778\n-0.0233\n0.0113\n0.9099\n0.0261\n\n\nx_2\n-0.0018\n-0.0592\n-1.0044\n-0.0310\n0.0674\n0.9204\n\n\nx_3\n-0.0503\n-0.0312\n-0.0556\n-0.0440\n-0.0529\n-0.0587\n\n\nx_4\n-0.0375\n-0.0537\n-0.0246\n-0.0412\n-0.0352\n-0.0338\n\n\nx_5\n-0.0653\n-0.0179\n-0.0575\n-0.0655\n-0.0307\n-0.0449\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0393\n\n\nx_1\n0.0044\n\n\nx_2\n-0.0092\n\n\nx_3\n0.0207\n\n\nx_4\n-0.0094\n\n\nx_5\n-0.0021\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.5500\n-0.1402\n-0.0082\n0.5233\n0.3011\n0.0882\n\n\nx_1\n-0.1402\n2.4578\n0.1502\n0.0355\n0.5714\n-0.0459\n\n\nx_2\n-0.0082\n0.1502\n2.5045\n0.2739\n0.0634\n0.5195\n\n\nx_3\n0.5233\n0.0355\n0.2739\n2.3814\n-0.2201\n-0.1842\n\n\nx_4\n0.3011\n0.5714\n0.0634\n-0.2201\n2.3880\n-0.1162\n\n\nx_5\n0.0882\n-0.0459\n0.5195\n-0.1842\n-0.1162\n2.3874\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-108.023250\n-113.113034\n0.073622\n0.198386\n0.956828\n-105578612000578019271909572608.000000\n02:37\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1440\n-0.0057\n0.0107\n0.8262\n-0.0845\n-0.1205\n\n\nx_1\n-0.0534\n1.2089\n-0.0223\n0.0948\n0.7937\n0.0494\n\n\nx_2\n-0.0289\n0.0176\n1.1417\n-0.0702\n-0.1154\n0.8351\n\n\nx_3\n0.1277\n0.0589\n0.1074\n1.1787\n-0.1203\n-0.1024\n\n\nx_4\n0.0654\n0.1560\n0.0043\n-0.1781\n1.1087\n-0.1415\n\n\nx_5\n0.0959\n0.0439\n0.1197\n-0.1725\n-0.0340\n1.1496\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0207\n0.0254\n0.0210\n-0.0052\n-0.0232\n-0.0033\n\n\nx_1\n0.0254\n0.0548\n0.0255\n0.0259\n0.0034\n0.0281\n\n\nx_2\n0.0210\n0.0255\n0.0222\n-0.0056\n-0.0241\n-0.0023\n\n\nx_3\n-0.0052\n0.0259\n-0.0056\n0.0473\n0.0504\n0.0465\n\n\nx_4\n-0.0232\n0.0034\n-0.0241\n0.0504\n0.0695\n0.0473\n\n\nx_5\n-0.0033\n0.0281\n-0.0023\n0.0465\n0.0473\n0.0477\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0004\n\n\nx_1\n-0.0055\n\n\nx_2\n0.0048\n\n\nx_3\n0.0027\n\n\nx_4\n-0.0033\n\n\nx_5\n0.0021\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7902\n-0.2032\n-0.1217\n-0.0986\n0.2164\n0.1090\n\n\ny_1\n0.0160\n0.7417\n0.0555\n0.1016\n0.1041\n0.0374\n\n\ny_2\n-0.1103\n-0.1594\n0.7916\n0.1185\n0.2096\n-0.0949\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0077\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0077\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0077\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0009\n\n\ny_1\n-0.0053\n\n\ny_2\n-0.0021\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0277\n-0.0595\n-0.0019\n0.9181\n0.0838\n-0.0194\n\n\nx_1\n-0.0414\n-0.8661\n-0.0323\n0.0110\n0.8741\n0.0262\n\n\nx_2\n-0.0029\n-0.0647\n-1.0045\n-0.0319\n0.0757\n0.8957\n\n\nx_3\n-0.0678\n-0.0368\n-0.0564\n-0.0559\n-0.0610\n-0.0639\n\n\nx_4\n-0.0374\n-0.0690\n-0.0209\n-0.0485\n-0.0406\n-0.0354\n\n\nx_5\n-0.0591\n-0.0267\n-0.0753\n-0.0633\n-0.0407\n-0.0555\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0396\n\n\nx_1\n-0.0009\n\n\nx_2\n-0.0049\n\n\nx_3\n0.0153\n\n\nx_4\n-0.0139\n\n\nx_5\n-0.0094\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.6364\n-0.1456\n-0.0020\n0.5695\n0.3986\n0.1628\n\n\nx_1\n-0.1456\n2.3292\n0.1064\n0.2074\n0.7591\n0.1475\n\n\nx_2\n-0.0020\n0.1064\n2.4205\n0.3489\n0.1488\n0.6143\n\n\nx_3\n0.5695\n0.2074\n0.3489\n2.2351\n-0.4355\n-0.3277\n\n\nx_4\n0.3986\n0.7591\n0.1488\n-0.4355\n2.1625\n-0.3445\n\n\nx_5\n0.1628\n0.1475\n0.6143\n-0.3277\n-0.3445\n2.2518\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps\")\n\nPath('models/17_jan_all_gaps.pth')\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-122.620241\n-126.621236\n0.077944\n0.202366\n0.948695\n-115274990280631988956159803392.000000\n02:38\n\n\n1\n-134.455010\n-139.768441\n0.074152\n0.188695\n0.959962\n-103653315974387814635399020544.000000\n02:31\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1347\n-0.0147\n0.0100\n0.8021\n-0.1122\n-0.1444\n\n\nx_1\n-0.0388\n1.2480\n-0.0185\n0.0773\n0.7576\n0.0414\n\n\nx_2\n-0.0257\n-0.0018\n1.1500\n-0.0814\n-0.1272\n0.8203\n\n\nx_3\n0.1332\n0.0451\n0.0866\n1.1612\n-0.1103\n-0.0917\n\n\nx_4\n0.0559\n0.1720\n0.0056\n-0.1625\n1.1052\n-0.1248\n\n\nx_5\n0.0721\n0.0457\n0.1226\n-0.1523\n-0.0385\n1.1357\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0146\n0.0183\n0.0142\n-0.0081\n-0.0202\n-0.0067\n\n\nx_1\n0.0183\n0.0445\n0.0187\n0.0148\n-0.0014\n0.0168\n\n\nx_2\n0.0142\n0.0187\n0.0143\n-0.0065\n-0.0187\n-0.0044\n\n\nx_3\n-0.0081\n0.0148\n-0.0065\n0.0340\n0.0393\n0.0343\n\n\nx_4\n-0.0202\n-0.0014\n-0.0187\n0.0393\n0.0550\n0.0378\n\n\nx_5\n-0.0067\n0.0168\n-0.0044\n0.0343\n0.0378\n0.0361\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0014\n\n\nx_1\n-0.0059\n\n\nx_2\n0.0039\n\n\nx_3\n0.0016\n\n\nx_4\n-0.0036\n\n\nx_5\n0.0032\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7310\n-0.1642\n-0.0983\n-0.1030\n0.2236\n0.0907\n\n\ny_1\n0.0158\n0.6777\n0.0705\n0.1196\n0.1642\n0.0316\n\n\ny_2\n-0.1124\n-0.1249\n0.7399\n0.1211\n0.2193\n-0.0846\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0064\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0065\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0065\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0019\n\n\ny_1\n-0.0046\n\n\ny_2\n-0.0026\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0316\n-0.0708\n-0.0019\n0.8851\n0.0973\n-0.0094\n\n\nx_1\n-0.0727\n-0.8659\n-0.0584\n0.0075\n0.7964\n0.0142\n\n\nx_2\n0.0098\n-0.0734\n-1.0045\n-0.0156\n0.0845\n0.8489\n\n\nx_3\n-0.0977\n-0.0434\n-0.0574\n-0.0723\n-0.0760\n-0.0730\n\n\nx_4\n-0.0309\n-0.1114\n-0.0185\n-0.0559\n-0.0634\n-0.0420\n\n\nx_5\n-0.0503\n-0.0424\n-0.1056\n-0.0644\n-0.0571\n-0.0729\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0585\n\n\nx_1\n-0.0141\n\n\nx_2\n-0.0070\n\n\nx_3\n0.0239\n\n\nx_4\n-0.0043\n\n\nx_5\n-0.0069\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.9257\n0.1245\n0.1987\n0.4475\n0.2435\n0.0893\n\n\nx_1\n0.1245\n2.4393\n0.3641\n0.2107\n0.7802\n0.1895\n\n\nx_2\n0.1987\n0.3641\n2.4862\n0.2959\n0.0089\n0.6053\n\n\nx_3\n0.4475\n0.2107\n0.2959\n2.1058\n-0.5771\n-0.4644\n\n\nx_4\n0.2435\n0.7802\n0.0089\n-0.5771\n1.9818\n-0.5064\n\n\nx_5\n0.0893\n0.1895\n0.6053\n-0.4644\n-0.5064\n2.0952\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-149.288297\n-152.804403\n0.072760\n0.179190\n0.956827\n-61340364986143763995601928192.000000\n02:35\n\n\n1\n-160.099637\n-165.002097\n0.071263\n0.181697\n0.963587\n-48387608947773502947627892736.000000\n02:37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps_final\")\n\nPath('models/17_jan_all_gaps_final.pth')\n\n\n\np = show_results(learn, control=hai_control, items = [items[i] for i in [0,5,4]])"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html#gap-in-only-1-variable",
    "href": "presentations/Hainich_results_pres18_jan.html#gap-in-only-1-variable",
    "title": "Hainich with ERA-Interim",
    "section": "Gap in only 1 variable",
    "text": "Gap in only 1 variable\n\ndls2 = imp_dataloader(hai, hai_era, var_sel = ['TA'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nmodel2 = init_smart(3,3, hai, pca=True).cuda()\n\n\nloss2 = loss_func=KalmanLoss(only_gap=False)\nlearn2 = Learner(dls2, model2, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\ntrain_show_save(learn2, 1, 1e-3)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/91 00:00&lt;?]\n    \n    \n\n\n\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\nInput In [141], in &lt;cell line: 1&gt;()\n----&gt; 1 train_show_save(learn2, 1, 1e-3)\n\nInput In [123], in train_show_save(learn, n_iter, lr)\n      1 def train_show_save(learn, n_iter, lr):\n----&gt; 2     learn.fit(n_iter, lr)\n      3     models.append(learn.model.state_dict().copy())\n      4     learn.recorder.plot_loss()\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:256, in Learner.fit(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\n    254 self.opt.set_hypers(lr=self.lr if lr is None else lr)\n    255 self.n_epoch = n_epoch\n--&gt; 256 self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:193, in Learner._with_events(self, f, event_type, ex, final)\n    192 def _with_events(self, f, event_type, ex, final=noop):\n--&gt; 193     try: self(f'before_{event_type}');  f()\n    194     except ex: self(f'after_cancel_{event_type}')\n    195     self(f'after_{event_type}');  final()\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:245, in Learner._do_fit(self)\n    243 for epoch in range(self.n_epoch):\n    244     self.epoch=epoch\n--&gt; 245     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:193, in Learner._with_events(self, f, event_type, ex, final)\n    192 def _with_events(self, f, event_type, ex, final=noop):\n--&gt; 193     try: self(f'before_{event_type}');  f()\n    194     except ex: self(f'after_cancel_{event_type}')\n    195     self(f'after_{event_type}');  final()\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:239, in Learner._do_epoch(self)\n    238 def _do_epoch(self):\n--&gt; 239     self._do_epoch_train()\n    240     self._do_epoch_validate()\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:231, in Learner._do_epoch_train(self)\n    229 def _do_epoch_train(self):\n    230     self.dl = self.dls.train\n--&gt; 231     self._with_events(self.all_batches, 'train', CancelTrainException)\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:193, in Learner._with_events(self, f, event_type, ex, final)\n    192 def _with_events(self, f, event_type, ex, final=noop):\n--&gt; 193     try: self(f'before_{event_type}');  f()\n    194     except ex: self(f'after_cancel_{event_type}')\n    195     self(f'after_{event_type}');  final()\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:199, in Learner.all_batches(self)\n    197 def all_batches(self):\n    198     self.n_iter = len(self.dl)\n--&gt; 199     for o in enumerate(self.dl): self.one_batch(*o)\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:227, in Learner.one_batch(self, i, b)\n    225 b = self._set_device(b)\n    226 self._split(b)\n--&gt; 227 self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:193, in Learner._with_events(self, f, event_type, ex, final)\n    192 def _with_events(self, f, event_type, ex, final=noop):\n--&gt; 193     try: self(f'before_{event_type}');  f()\n    194     except ex: self(f'after_cancel_{event_type}')\n    195     self(f'after_{event_type}');  final()\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/learner.py:205, in Learner._do_one_batch(self)\n    204 def _do_one_batch(self):\n--&gt; 205     self.pred = self.model(*self.xb)\n    206     self('after_pred')\n    207     if len(self.yb):\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)\n   1190 # If we don't have any hooks, we want to skip the rest of the logic in\n   1191 # this function, and just call forward.\n   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1193         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1194     return forward_call(*input, **kwargs)\n   1195 # Do not call functions when jit is used\n   1196 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/nn/modules/module.py:246, in _forward_unimplemented(self, *input)\n    235 def _forward_unimplemented(self, *input: Any) -&gt; None:\n    236     r\"\"\"Defines the computation performed at every call.\n    237 \n    238     Should be overridden by all subclasses.\n   (...)\n    244         registered hooks while the latter silently ignores them.\n    245     \"\"\"\n--&gt; 246     raise NotImplementedError(f\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\")\n\nNotImplementedError: Module [KalmanFilter] is missing the required \"forward\" function\n\n\n\nDisable smoother\n\nmodel.use_smooth = False\n\n\ntrain_show_save(learn, 3, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n534.159578\n465.194522\n0.284379\n0.329570\n-1.919198\n-241825956681560181426503024640.000000\n02:24\n\n\n\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.6761\n0.1157\n-0.0095\n0.5895\n0.0728\n0.0850\n\n\nx_1\n0.0309\n0.6675\n0.1639\n0.2296\n0.5542\n-0.1129\n\n\nx_2\n0.1836\n-0.1476\n0.6498\n0.1684\n-0.2142\n0.5899\n\n\nx_3\n-0.1246\n0.0361\n0.0138\n0.6844\n-0.0830\n0.1501\n\n\nx_4\n-0.0167\n-0.0399\n0.0044\n0.0089\n0.7537\n0.0580\n\n\nx_5\n0.0341\n0.0300\n-0.0436\n-0.0362\n0.2878\n0.6487\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.5106\n0.3768\n0.4383\n-0.1720\n-0.2863\n-0.1749\n\n\nx_1\n0.3768\n0.5687\n0.1931\n-0.2781\n0.0461\n0.0798\n\n\nx_2\n0.4383\n0.1931\n0.4406\n-0.0734\n-0.3650\n-0.2384\n\n\nx_3\n-0.1720\n-0.2781\n-0.0734\n0.5150\n0.3608\n0.3418\n\n\nx_4\n-0.2863\n0.0461\n-0.3650\n0.3608\n0.8281\n0.6975\n\n\nx_5\n-0.1749\n0.0798\n-0.2384\n0.3418\n0.6975\n0.6156\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0166\n\n\nx_1\n-0.0196\n\n\nx_2\n-0.0446\n\n\nx_3\n0.0033\n\n\nx_4\n-0.0022\n\n\nx_5\n0.0107\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n-0.3096\n0.5414\n-0.0652\n0.4906\n-0.0643\n-0.3834\n\n\ny_1\n0.6288\n-0.3366\n-0.3681\n0.0321\n-0.2062\n0.3634\n\n\ny_2\n-0.2813\n0.0500\n0.4738\n-0.0601\n0.2872\n-0.1334\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.3678\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.3707\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.3693\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0197\n\n\ny_1\n-0.0452\n\n\ny_2\n-0.0451\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9410\n-0.5681\n-0.8722\n0.9998\n1.2958\n1.0828\n\n\nx_1\n-0.7019\n-0.7354\n-0.9277\n1.2348\n1.1344\n0.9620\n\n\nx_2\n-0.9322\n-0.7328\n-0.8307\n1.0310\n1.1112\n1.1017\n\n\nx_3\n0.0914\n0.2592\n0.0286\n0.0638\n0.1644\n-0.0007\n\n\nx_4\n0.2175\n-0.0398\n0.0226\n0.2072\n-0.1205\n-0.0117\n\n\nx_5\n-0.0578\n0.1485\n0.0908\n-0.0613\n0.0627\n0.0792\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0290\n\n\nx_1\n-0.0320\n\n\nx_2\n-0.0494\n\n\nx_3\n0.0296\n\n\nx_4\n0.0387\n\n\nx_5\n0.1010\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n3.7171\n2.7054\n2.8366\n2.0078\n1.3904\n2.3275\n\n\nx_1\n2.7054\n4.5189\n2.1010\n1.4436\n1.8804\n2.8057\n\n\nx_2\n2.8366\n2.1010\n3.3027\n1.3323\n0.9307\n2.0280\n\n\nx_3\n2.0078\n1.4436\n1.3323\n3.3867\n2.1461\n3.0821\n\n\nx_4\n1.3904\n1.8804\n0.9307\n2.1461\n2.5441\n2.6815\n\n\nx_5\n2.3275\n2.8057\n2.0280\n3.0821\n2.6815\n3.7653\n\n\n\n\n \n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nInput In [70], in &lt;cell line: 1&gt;()\n----&gt; 1 train_show_save(learn, 1, 1e-2)\n\nInput In [46], in train_show_save(learn, n_iter, lr)\n      3 models.append(learn.model.state_dict().copy())\n      4 display_as_row(learn.model.get_info())\n----&gt; 5 items = [learn.dls.valid.items[i] for i in [10, 100, 300, 500, 700, 1000]]\n      6 return show_results(learn, items = items, control=hai_controls)\n\nInput In [46], in &lt;listcomp&gt;(.0)\n      3 models.append(learn.model.state_dict().copy())\n      4 display_as_row(learn.model.get_info())\n----&gt; 5 items = [learn.dls.valid.items[i] for i in [10, 100, 300, 500, 700, 1000]]\n      6 return show_results(learn, items = items, control=hai_controls)\n\nIndexError: list index out of range\n\n\n\n\ntrain_show_save(learn, 1, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n272.526685\n207.314376\n0.279615\n0.290300\n-3.962859\n-113462721102480788295669252096.000000\n06:35\n\n\n\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5320\n0.0984\n0.2567\n\n\nx_1\n0.5386\n-0.1615\n-0.1019\n\n\nx_2\n0.4759\n0.4640\n0.4202\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.4043\n0.5126\n1.8864\n\n\nx_1\n0.5126\n0.1871\n0.6885\n\n\nx_2\n1.8864\n0.6885\n2.5339\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0972\n\n\nx_1\n-0.2735\n\n\nx_2\n0.0298\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1792\n0.7014\n-0.3164\n\n\ny_1\n-0.5045\n0.1311\n0.3455\n\n\ny_2\n-0.0962\n0.0705\n0.1246\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0190\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.1624\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0297\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1664\n\n\ny_1\n-0.0433\n\n\ny_2\n0.0232\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0331\n-0.4960\n0.2924\n0.5946\n-0.1127\n-0.1933\n\n\nx_1\n0.5991\n0.3053\n0.2248\n0.7649\n0.9446\n0.0394\n\n\nx_2\n0.0060\n0.0013\n-0.0667\n-0.1143\n0.7781\n0.6409\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0627\n\n\nx_1\n-0.6392\n\n\nx_2\n-1.2803\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n9.0224\n3.6789\n10.5469\n\n\nx_1\n3.6789\n2.3654\n5.9687\n\n\nx_2\n10.5469\n5.9687\n21.9915\n\n\n\n\n \n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nInput In [61], in &lt;cell line: 1&gt;()\n----&gt; 1 train_show_save(learn, 1, 1e-2)\n\nInput In [45], in train_show_save(learn, n_iter, lr)\n      3 models.append(learn.model.state_dict().copy())\n      4 display_as_row(learn.model.get_info())\n----&gt; 5 items = [learn1.dls.valid.items[i] for i in [10, 100, 300, 500, 700, 1000]]\n      6 return show_results(learn, items = items)\n\nInput In [45], in &lt;listcomp&gt;(.0)\n      3 models.append(learn.model.state_dict().copy())\n      4 display_as_row(learn.model.get_info())\n----&gt; 5 items = [learn1.dls.valid.items[i] for i in [10, 100, 300, 500, 700, 1000]]\n      6 return show_results(learn, items = items)\n\nNameError: name 'learn1' is not defined\n\n\n\n\ntrain_show_save(learn, 1, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n160.675568\n150.361449\n0.266633\n0.269324\n-1.959343\n-53151966368643636872563654656.000000\n05:30\n\n\n\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5363\n0.1070\n0.2723\n\n\nx_1\n0.5375\n-0.1534\n-0.0948\n\n\nx_2\n0.4909\n0.4638\n0.4301\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.3408\n0.4970\n1.7922\n\n\nx_1\n0.4970\n0.1842\n0.6643\n\n\nx_2\n1.7922\n0.6643\n2.3955\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0948\n\n\nx_1\n-0.2568\n\n\nx_2\n0.0393\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1742\n0.6954\n-0.3174\n\n\ny_1\n-0.5094\n0.1551\n0.3429\n\n\ny_2\n-0.1147\n0.0583\n0.1163\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0121\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.1100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0190\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1697\n\n\ny_1\n-0.0123\n\n\ny_2\n0.0387\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0363\n-0.5225\n0.3103\n0.5948\n-0.0692\n-0.1628\n\n\nx_1\n0.5955\n0.3523\n0.1332\n0.7587\n0.9820\n-0.0509\n\n\nx_2\n-0.0159\n0.0573\n-0.0471\n-0.1409\n0.7802\n0.6457\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0500\n\n\nx_1\n-0.6014\n\n\nx_2\n-1.2993\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n10.9221\n3.8061\n12.4742\n\n\nx_1\n3.8061\n1.7574\n5.6427\n\n\nx_2\n12.4742\n5.6427\n24.9669\n\n\n\n\n \n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n115.738528\n104.632719\n0.257947\n0.259673\n-1.679687\n-24519279833493970234084687872.000000\n05:09\n\n\n\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5303\n0.1124\n0.2795\n\n\nx_1\n0.5333\n-0.1470\n-0.0908\n\n\nx_2\n0.4963\n0.4602\n0.4322\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.2399\n0.4645\n1.6526\n\n\nx_1\n0.4645\n0.1740\n0.6191\n\n\nx_2\n1.6526\n0.6191\n2.2026\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0883\n\n\nx_1\n-0.2505\n\n\nx_2\n0.0440\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1715\n0.6898\n-0.3162\n\n\ny_1\n-0.5142\n0.1782\n0.3403\n\n\ny_2\n-0.1173\n0.0543\n0.1176\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0079\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0732\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0124\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1699\n\n\ny_1\n-0.0058\n\n\ny_2\n0.0383\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0449\n-0.5621\n0.3391\n0.5937\n0.0285\n-0.1154\n\n\nx_1\n0.5860\n0.3917\n0.0584\n0.7468\n1.0129\n-0.1247\n\n\nx_2\n-0.0479\n0.1664\n-0.0280\n-0.1814\n0.7807\n0.6422\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0488\n\n\nx_1\n-0.5958\n\n\nx_2\n-1.3038\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n12.1503\n4.1501\n13.5248\n\n\nx_1\n4.1501\n1.7544\n5.9730\n\n\nx_2\n13.5248\n5.9730\n27.6423\n\n\n\n\n \n\n\n\n\n\n\n\n\nlearn.model.use_smooth = True\nshow_results(learn, items = [learn.dls.valid.items[i] for i in [10, 100, 300, 500, 700, 1000]])\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n30.829145\n26.059960\n0.201186\n0.215770\n-0.034140\n-48287381455002891037683220480.000000\n06:32\n\n\n\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5309\n0.1159\n0.2797\n\n\nx_1\n0.5313\n-0.1385\n-0.0881\n\n\nx_2\n0.4961\n0.4560\n0.4324\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.1473\n0.4769\n1.5832\n\n\nx_1\n0.4769\n0.2008\n0.6682\n\n\nx_2\n1.5832\n0.6682\n2.2249\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0894\n\n\nx_1\n-0.2426\n\n\nx_2\n0.0426\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1711\n0.6883\n-0.3146\n\n\ny_1\n-0.5151\n0.1887\n0.3414\n\n\ny_2\n-0.1155\n0.0454\n0.1145\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0063\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0583\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0097\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1747\n\n\ny_1\n0.0044\n\n\ny_2\n0.0287\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0475\n-0.5920\n0.3650\n0.5948\n0.0563\n-0.0801\n\n\nx_1\n0.5812\n0.4322\n-0.0088\n0.7377\n1.0076\n-0.1962\n\n\nx_2\n-0.0461\n0.1875\n-0.0228\n-0.1820\n0.7675\n0.6408\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0477\n\n\nx_1\n-0.5959\n\n\nx_2\n-1.2990\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n12.4282\n4.5316\n13.6935\n\n\nx_1\n4.5316\n2.1406\n6.4461\n\n\nx_2\n13.6935\n6.4461\n27.6478\n\n\n\n\n \n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n2.643850\n-2.886783\n0.193556\n0.212050\n0.165402\n-29540765662186335322106757120.000000\n06:27\n\n\n\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5339\n0.1210\n0.2783\n\n\nx_1\n0.5270\n-0.1285\n-0.0862\n\n\nx_2\n0.4976\n0.4502\n0.4350\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.0913\n0.4682\n1.5174\n\n\nx_1\n0.4682\n0.2037\n0.6628\n\n\nx_2\n1.5174\n0.6628\n2.1595\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0873\n\n\nx_1\n-0.2397\n\n\nx_2\n0.0401\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1690\n0.6841\n-0.3145\n\n\ny_1\n-0.5175\n0.1944\n0.3405\n\n\ny_2\n-0.1099\n0.0372\n0.1142\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0051\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0467\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0078\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1766\n\n\ny_1\n0.0070\n\n\ny_2\n0.0210\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0614\n-0.6233\n0.4026\n0.5869\n0.0964\n-0.0301\n\n\nx_1\n0.5842\n0.4624\n-0.0654\n0.7358\n0.9837\n-0.2582\n\n\nx_2\n-0.0390\n0.2132\n-0.0130\n-0.1775\n0.7580\n0.6438\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0447\n\n\nx_1\n-0.5957\n\n\nx_2\n-1.2961\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n13.1380\n4.8428\n14.4531\n\n\nx_1\n4.8428\n2.3677\n6.8288\n\n\nx_2\n14.4531\n6.8288\n28.0978"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html#loss-only-gap",
    "href": "presentations/Hainich_results_pres18_jan.html#loss-only-gap",
    "title": "Hainich with ERA-Interim",
    "section": "Loss only gap",
    "text": "Loss only gap\n\nlearn.model.use_smooth = True\nlearn.loss_func = KalmanLoss(only_gap=True)\n\n\ntrain_show_save(learn, 1,  5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-3.892116\n-2.872949\n0.120294\n0.204105\n-2.923830\n-85436991145187843135533744128.000000\n04:26\n\n\n\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.2027\n0.3761\n0.2969\n\n\nx_1\n0.3594\n0.3686\n0.3606\n\n\nx_2\n0.4679\n0.2295\n0.2852\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0220\n0.0113\n-0.0224\n\n\nx_1\n0.0113\n0.0064\n-0.0097\n\n\nx_2\n-0.0224\n-0.0097\n0.0290\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.3025\n\n\nx_1\n-0.5231\n\n\nx_2\n0.2064\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.1489\n0.3517\n-0.0053\n\n\ny_1\n0.2922\n0.1890\n-0.4105\n\n\ny_2\n0.3920\n-0.0638\n0.2473\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0001\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0054\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0003\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.2038\n\n\ny_1\n0.1323\n\n\ny_2\n-0.1988\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9856\n0.6178\n0.4694\n-0.5368\n0.5367\n-0.0159\n\n\nx_1\n0.6667\n0.5327\n-0.3882\n0.6907\n-0.0209\n-0.2365\n\n\nx_2\n0.0330\n-0.7175\n-0.1714\n0.2048\n-0.6491\n0.1349\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0922\n\n\nx_1\n-0.6262\n\n\nx_2\n0.2368\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n8.1062\n12.7070\n6.0908\n\n\nx_1\n12.7070\n21.6312\n10.2860\n\n\nx_2\n6.0908\n10.2860\n5.5679"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html#smoother",
    "href": "presentations/Hainich_results_pres18_jan.html#smoother",
    "title": "Hainich with ERA-Interim",
    "section": "Smoother",
    "text": "Smoother\n\nlearn1.model.use_smooth = True\n\n\ntrain_show_save(learn1, 1,  5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-415.543654\n-449.643886\n0.055635\n0.200575\n0.759219\n-45104963579554009935054372864.000000\n07:43\n\n\n\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.1815\n0.3765\n0.3138\n\n\nx_1\n0.3401\n0.3500\n0.3764\n\n\nx_2\n0.4904\n0.2297\n0.2438\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0491\n0.0170\n-0.0614\n\n\nx_1\n0.0170\n0.0127\n-0.0243\n\n\nx_2\n-0.0614\n-0.0243\n0.0889\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.3080\n\n\nx_1\n-0.5210\n\n\nx_2\n0.2044\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.1630\n0.3552\n-0.0202\n\n\ny_1\n0.3143\n0.1963\n-0.4623\n\n\ny_2\n0.4022\n-0.0653\n0.2466\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0001\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0054\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0003\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.2096\n\n\ny_1\n0.1459\n\n\ny_2\n-0.2079\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0006\n0.5988\n0.4673\n-0.5447\n0.5317\n-0.0122\n\n\nx_1\n0.6825\n0.5156\n-0.3620\n0.7188\n-0.0186\n-0.1805\n\n\nx_2\n0.0303\n-0.7130\n-0.1672\n0.2100\n-0.6278\n0.1642\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0933\n\n\nx_1\n-0.6332\n\n\nx_2\n0.2392\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n8.2929\n13.0004\n6.2311\n\n\nx_1\n13.0004\n22.1310\n10.5205\n\n\nx_2\n6.2311\n10.5205\n5.6921\n\n\n\n\n \n\n\n\n\n\n\n\n\n# learn1.save(\"model_16_jan1\")\n\nPath('models/model_16_jan1.pth')"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html",
    "href": "presentations/plots_for_presentation_18_jan_23.html",
    "title": "Code for presentation of 18th January",
    "section": "",
    "text": "::: {#b87acfa4-b43e-4cbe-9bc2-103a084ec5a2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n:::\nfrom meteo_imp.kalman.filter import *\nimport torch\nimport numpy as np\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.fastai import plot_variable\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\nfrom pyprojroot import here\nn_obs = 20\nbase_dir = here(\"presentations/plots_18_jan\")\nbase_dir.mkdir(exist_ok=True)\ndef save_plot(p, path):\n    f_name = base_dir / (path + \".vl.json\")\n    with open(f_name, 'w') as f:\n        f.write(p.to_json())\n    return f_name\nplt_props = {'width': 460, 'height': 400}"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#only-state",
    "href": "presentations/plots_for_presentation_18_jan_23.html#only-state",
    "title": "Code for presentation of 18th January",
    "section": "Only state",
    "text": "Only state\n\\[ x_t = A\\color{blue}{x_{-1}} + \\varepsilon\\]\n\ntorch.tensor([[0.]]).shape\n\ntorch.Size([1, 1])\n\n\n\nk0 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([0.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.3]]),\n    obs_cov = torch.tensor([[0.1]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.01]]),\n)\n\n\npred0 = k0.smooth(torch.ones(1,n_obs,1), torch.zeros(1,n_obs,1, dtype=bool), torch.zeros(1,n_obs,1))\n\n\ntime = pd.Index(range(n_obs), name=\"time\")\n\n\npred0.cov.shape\n\ntorch.Size([1, 20, 1, 1])\n\n\n\npred0_df = NormalsDf(pd.DataFrame(pred0.mean.squeeze(0), index=time, columns=[\"state\"]),\n                     pd.DataFrame(pred0.cov.squeeze(0).squeeze(-1), index=time, columns=[\"state\"]))\n\n\np0 = facet_variable(pred0_df.tidy(), ys=[\"mean\", \"mean\"], error=True, point=False, gap_area=False, props=plt_props)\np0\n\n\n\n\n\n\n\nsave_plot(p0, \"only_state\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/only_state.vl.json')"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#observations",
    "href": "presentations/plots_for_presentation_18_jan_23.html#observations",
    "title": "Code for presentation of 18th January",
    "section": "Observations",
    "text": "Observations\n\ntorch.tensor([[0.]]).shape\n\ntorch.Size([1, 1])\n\n\n\nk1 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nobs = 2 * torch.sin(torch.arange(n_obs) * 2 * torch.pi / (n_obs-2))\n\n\nplt.plot(obs)\n\n\n\n\n\n\n\n\n\nmask1 = torch.ones(1,n_obs,1, dtype=bool)\n\n\npred1 = k1.smooth(obs.unsqueeze(0).unsqueeze(-1), mask1, torch.zeros(1,n_obs,1))\n\n\ntime = pd.Index(range(n_obs), name=\"time\")\n\n\npred1.cov.shape\n\ntorch.Size([1, 20, 1, 1])\n\n\n\npred_df1 = NormalsDf(pd.DataFrame(pred1.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred1.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df1 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask1[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df1 = pd.merge(obs_df1, pred_df1, on=['time', 'variable'])\n\n\np1 = facet_variable(plot_df1, ys=[\"value\", \"mean\"], error=True, gap_area=False, props=plt_props)\np1\n\n\n\n\n\n\n\nsave_plot(p1, \"obs\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/obs.vl.json')"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#gaps",
    "href": "presentations/plots_for_presentation_18_jan_23.html#gaps",
    "title": "Code for presentation of 18th January",
    "section": "Gaps",
    "text": "Gaps\n\nk2 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nmask2 = torch.ones(1,n_obs,1, dtype=bool)\nmask2[0,11:16,0] = False\nmask2\n\ntensor([[[ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False],\n         [ True],\n         [ True],\n         [ True],\n         [ True]]])\n\n\n\npred2 = k2.smooth(obs.unsqueeze(0).unsqueeze(-1), mask2, torch.zeros(1,n_obs,1))\n\n\npred_df2 = NormalsDf(pd.DataFrame(pred2.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred2.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df2 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask2[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df2 = pd.merge(obs_df2, pred_df2, on=['time', 'variable'])\n\n\np2 = facet_variable(plot_df2, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 750, 'height': 500})\np2\n\n\n\n\n\n\n\nsave_plot(p2, \"gaps\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/gaps.vl.json')"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#control",
    "href": "presentations/plots_for_presentation_18_jan_23.html#control",
    "title": "Code for presentation of 18th January",
    "section": "Control",
    "text": "Control\n\nk3 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([-1., 1]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nmask3 = torch.ones(1,n_obs,1, dtype=bool)\nmask3[0,11:16,0] = False\nmask3\n\ntensor([[[ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False],\n         [ True],\n         [ True],\n         [ True],\n         [ True]]])\n\n\n\ncontr = torch.stack([(obs + 0.5)*1.2, ((obs + 0.5)*1.2).roll(-1)], dim=-1)\n\n\ncontr.shape\n\ntorch.Size([20, 2])\n\n\n\nplt.plot(contr[:,0])\nplt.plot(contr[:,1])\n\n\n\n\n\n\n\n\n\npred3 = k3.smooth(obs.unsqueeze(0).unsqueeze(-1), mask3, contr.unsqueeze(0))\n\n\npred_df3 = NormalsDf(pd.DataFrame(pred3.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred3.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df3 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask3[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df3 = pd.merge(obs_df3, pred_df3, on=['time', 'variable'])\n\n\np3 = (plot_variable(plot_df3, variable=\"var\", ys=[\"value\", \"mean\"], error=True, gap_area=False, props=plt_props) +\nalt.Chart(pd.DataFrame({'contr':contr[:,0], 'time': time, 'col': 'control'})).mark_line(strokeDash=[6,3], color='orange').encode(x='time', y='contr'))\np3\n\n\n\n\n\n\n\nsave_plot(p3, \"control\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/control.vl.json')"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html#variables-correlation",
    "href": "presentations/plots_for_presentation_18_jan_23.html#variables-correlation",
    "title": "Code for presentation of 18th January",
    "section": "Variables correlation",
    "text": "Variables correlation\n\nk4 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([[.7], [.3]]),        \n    contr_matrix = torch.tensor([[0.]]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.diag(torch.tensor([0.1, 0.1])),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0., 0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.01]]),\n)\n\n\nk4\n\nKalman Filter (2 obs, 1 state, 1 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n1.0000\n\n\n\n\n  trans cov (Q) \n\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n0.5000\n\n\n\n\n  trans off \n\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\n\n\n  obs matrix (H) \n\n\n\n\nvariable\nx_0\n\n\n\n\ny_0\n0.7000\n\n\ny_1\n0.3000\n\n\n\n\n  obs cov (R) \n\n\n\n\nvariable\ny_0\ny_1\n\n\n\n\ny_0\n0.1000\n0.0000\n\n\ny_1\n0.0000\n0.1000\n\n\n\n\n  obs off \n\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\n\n\n  contr matrix (B) \n\n\n\n\nstate\nc_0\n\n\n\n\nx_0\n0.0000\n\n\n\n\n  init state mean \n\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\n\n\n  init state cov \n\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n0.0100\n\n\n\n\n \n\n\n\nmask4 = torch.ones(1,n_obs,2, dtype=bool)\nmask4[0,11:16,0] = False\nmask4\n\ntensor([[[ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True]]])\n\n\n\nobs4 = torch.stack([\n    obs *.7 + torch.randn_like(obs) * .1,\n    obs *.3 + torch.randn_like(obs) * .1\n], dim=-1)\n\n\nplt.plot(obs4)\nplt.plot(obs)\n\n\n\n\n\n\n\n\n\nobs4.shape\n\ntorch.Size([20, 2])\n\n\n\npred4 = k4.predict(obs4.unsqueeze(0), mask4, torch.zeros(1,n_obs,1))\n\n\npred_df4 = NormalsDf(pd.DataFrame(pred4.mean.squeeze(0), index=time, columns=[\"var1\", \"var2\"]),\n                     pd.DataFrame(pred4.std.squeeze(0).squeeze(-1), index=time, columns=[\"var1\", \"var2\"])).tidy()\n\n\nobs_df4_0 = pd.DataFrame({'time': time, 'var1': obs4[:,0].numpy(),\n                        'is_present': mask4[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \nobs_df4_1 = pd.DataFrame({'time': time, 'var2': obs4[:,1].numpy(),\n                        'is_present': mask4[0,:,1].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nobs_df4 = pd.concat([obs_df4_0, obs_df4_1])\n\n\nplot_df4 = pd.merge(obs_df4, pred_df4, on=['time', 'variable'])\n\n\np4 = facet_variable(plot_df4, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 350, 'height': 220}, n_cols=1)\np4\n\n\n\n\n\n\n\nsave_plot(p4, \"var_corr\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/var_corr.vl.json')\n\n\n\nmask5 = torch.ones(1,n_obs,2, dtype=bool)\nmask5[0,11:16,:] = False\nmask5\n\ntensor([[[ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [False, False],\n         [False, False],\n         [False, False],\n         [False, False],\n         [False, False],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True]]])\n\n\n\npred5 = k4.predict(obs4.unsqueeze(0), mask5, torch.zeros(1,n_obs,1))\n\n\npred_df5 = NormalsDf(pd.DataFrame(pred5.mean.squeeze(0), index=time, columns=[\"var1\", \"var2\"]),\n                     pd.DataFrame(pred5.std.squeeze(0).squeeze(-1), index=time, columns=[\"var1\", \"var2\"])).tidy()\n\n\nobs_df5_0 = pd.DataFrame({'time': time, 'var1': obs4[:,0].numpy(),\n                        'is_present': mask5[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \nobs_df5_1 = pd.DataFrame({'time': time, 'var2': obs4[:,1].numpy(),\n                        'is_present': mask5[0,:,1].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nobs_df5 = pd.concat([obs_df5_0, obs_df5_1])\n\n\nplot_df5 = pd.merge(obs_df4, pred_df5, on=['time', 'variable'])\n\n\np5 = facet_variable(plot_df5, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 350, 'height': 220}, n_cols=1)\np5\n\n\n\n\n\n\n\nsave_plot(p5, \"var_no_corr\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/var_no_corr.vl.json')"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#outline",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#outline",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\nKalman Filter\nPreliminary results\nNext steps"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#about-me",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#about-me",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "About me",
    "text": "About me\n\njust finished master in Forest and Ecosystem Sciences at university of Göttingen\nmaster thesis"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#background",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#background",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Background",
    "text": "Background\n\n\n\nEddy Covariance (EC) tower measures also meteorological variables (eg. air temperature, wind speed) - technical issues (eg. broken sensor) result in meteo time series with gaps - Presence of gaps is a problem in many EC data applications (eg. ecosystem modelling) - over 98% f gaps are shorter than a week"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-impute-gaps",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-impute-gaps",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "How to impute gaps",
    "text": "How to impute gaps\nin multivariate time series:\n\nuse other observations before and after the gap and exploit the variable temporal autocorrelation\nuse statistical dependency with other (non missing) variables\nuse other timeseries that measure similar variables"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#state-of-the-art",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#state-of-the-art",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "State of the art",
    "text": "State of the art\n\nMarginal Distribution Sampling (MDS)\n\n\nuse mean of all other observations from similar meteorological conditions (close in time and in values of other variables)\nmost commonly used method and default for short gaps\n\n\nGlobal meteorological dataset (ERA-I)\n\n\nglobal dataset using weather models\nlimited spatial/temporal resolution\nnot all variables are available"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#example-imputation",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#example-imputation",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Example imputation",
    "text": "Example imputation"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#potentail-for-improvements",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#potentail-for-improvements",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Potentail for Improvements",
    "text": "Potentail for Improvements\n\ntemporal autocorrelation: observations closer to the gap should have a stronger weight in the prediction\ncombine imputation approaches: use information both from the local time series and the global dataset\nUncertainty: provide a quantifiable and interpretable uncertainty in the predictions"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#contributions-of-this-work",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#contributions-of-this-work",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Contributions of this work",
    "text": "Contributions of this work\ndevelop a new imputation methods that uses Kalman Filter as it allows to: - use variable temporal autocorrelation - use inter-variable correlation - use global dataset - uncertainty of predictions - computationally efficient\n\nassess the KF imputation performance and the aspect\n\nexpect limited performance for long gaps"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-assumptions",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-assumptions",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Kalman Filter assumptions",
    "text": "Kalman Filter assumptions\n\nthe observed variables (\\(y_t\\)) depend on a latent state of system\nthe latent states (\\(x_t\\)) are connected by a Markov Chain \\(p(x_t|x_{t-1}) = p(x_t|x_{t-1}, x_{t-2}, \\hdots, x_0)\\)\nall distributions are Gaussian and relationship are linear"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-for-imputation",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-for-imputation",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Kalman Filter for imputation",
    "text": "Kalman Filter for imputation\n\npredict time at next time state (\\(x_{t+1}\\)) using current state (\\(x_{t+1}\\)) and potentially a control variable\ncorrect the state distribution using the observed variable (\\(y_t\\)) if available\nif the"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-with-equations",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-with-equations",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Kalman Filter with equations",
    "text": "Kalman Filter with equations\n\\[\n\\begin{align}\np(x_t | x_{t-1}) &= \\mathcal{N}(x_t; Ax_{t-1} + d + Bc_t, Q) \\\\\np(y_t | x_t) &= \\mathcal{N}(y_t; Hx_t + b, R)\n\\end{align}\n\\]"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-with-images",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-with-images",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Kalman Filter with images",
    "text": "Kalman Filter with images"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-implementation",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-implementation",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Kalman Filter implementation",
    "text": "Kalman Filter implementation\n\nKF equations derived using Bayesian inference + properties of Gaussian distributions\nnaive equations are numerically unstable\ncustom implementation in PyTorch"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#model-training",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#model-training",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Model training",
    "text": "Model training\n\nartificial gaps\nfind optimal parameters using gradient descend"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kf-vs-sota",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kf-vs-sota",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "KF vs SoTA",
    "text": "KF vs SoTA"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#conclusions",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#conclusions",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Conclusions",
    "text": "Conclusions\n\nKF consistently outperform state-of-the-art methods\nKF provides uncertainty predictions\nimputation performance changes a lot between variables\nnumerical stability issues limit applications of current implementation for long gaps in all variables\ndifficult to train the KF: parameters initialisation and training data"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#future-outlook",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#future-outlook",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Future outlook",
    "text": "Future outlook"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#more-details",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#more-details",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "More details",
    "text": "More details\n\nMaster Thesis\ncode\nthis presentation:"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#gap-len-distribution-in-fluxnet",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#gap-len-distribution-in-fluxnet",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of gaps for all TA, SW_IN and VPD for all sites. 26 millions total obs"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#gap-len-distribution-in-fluxnet-1",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#gap-len-distribution-in-fluxnet-1",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of small gaps (&lt;1 week) for all TA, SW_IN and VPD for all sites. 26 millions total obs"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-fill-gaps",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-fill-gaps",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "How to fill gaps",
    "text": "How to fill gaps\n\nuse previous and following measurements for one variable and temporal auto-correlation (eg. diurnal cycles)\ncorrelation with other variables measures (eg. solar radiation and temperature)\nother measurements of meteo variables (eg. nearby station)"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#state-of-the-art-1",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#state-of-the-art-1",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "State of the art",
    "text": "State of the art\nOneFlux pipeline (Fluxnet + ICOS + AmeriFlux)\n\nShort and medium gaps using Marginal Distribution Sampling (MDS)\nLong gaps filled with ERA data (global meteo dataset) using linear transformation to reduce site bias"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-mds-marginal-distribution-sampling-works",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-mds-marginal-distribution-sampling-works",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "How MDS (Marginal Distribution Sampling) works",
    "text": "How MDS (Marginal Distribution Sampling) works\n\n\n\ntake a time window (7 days) around the gap\nuse 3 predictors variables (TA, SW_IN and VPD) and divide them in n discrete bins\nfor each bin (combination of conditions) find the average value of the missing variable\nfor each gap find the closest condition and fill with the average value\nif necessary increase the time window\nquality flag depends on the time window size"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#mds---gap-filling-ta",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#mds---gap-filling-ta",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "MDS - gap filling TA",
    "text": "MDS - gap filling TA"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#mds---gap-filling-sw_in",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#mds---gap-filling-sw_in",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "MDS - gap filling SW_IN",
    "text": "MDS - gap filling SW_IN"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#mds---gap-filling-vpd",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#mds---gap-filling-vpd",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "MDS - gap filling VPD",
    "text": "MDS - gap filling VPD"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#current-approaches-limitations",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#current-approaches-limitations",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Current approaches limitations",
    "text": "Current approaches limitations\n\ndon’t consider the observations before and after the gap\nEither MDS (variable correlation) or ERA data, don’t combine the information\nNo uncertainty for the predictions (only a quality flag)"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#thesis-goal",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#thesis-goal",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Thesis goal",
    "text": "Thesis goal\n\ndevelop model to impute missing data in meteorological time series\ninclude all 3 imputation approaches\nprovide an uncertainty of the predictions"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-kalman-filter-works",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-kalman-filter-works",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "How Kalman Filter works",
    "text": "How Kalman Filter works\n\n\nModels over time a latent variable (we are not observing it), the state of the system.\nThe current state \\(x_t\\) depends using:\n\nthe previous state \\(\\color{blue}{x_{t-1}}\\)\ncurrent observation \\(\\color{green}{y_t}\\)\ncontrol variable \\(\\color{purple}{c_t}\\) (ERA data)"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#previous-state",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#previous-state",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "1. Previous state",
    "text": "1. Previous state\n\n\n\\[ x_t = A\\color{blue}{x_{t-1}} + \\varepsilon \\] where:\n\n\\(x_{t}\\) is the current state\n\\(\\color{blue}{x_{t-1}}\\) is the previous state\n\\(A\\) is a linear transformation of \\(\\color{blue}{x_{t-1}}\\)\n\\(\\varepsilon\\) is the “process” noise which is a random variable with a normal distribution with mean 0\n\n\n\n\n\n\nExample of Kalman Filter \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#current-observation",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#current-observation",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "2. Current Observation",
    "text": "2. Current Observation\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(\\color{green}{y_{t}}\\) is the current observation\n\\(H\\) is a linear transformation of \\(\\color{green}{y_{t}}\\)\n\\(\\nu\\) is the “observation” noise which is a random variable with a normal distribution with mean 0\n\nusing the rules of probabilistic inference if we observe \\(y_t\\) you can update the distribution of \\(x_t\\)\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#gaps",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#gaps",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Gaps",
    "text": "Gaps\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#control-variable",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#control-variable",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "3. Control variable",
    "text": "3. Control variable\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + B\\color{purple}{c_t} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(B\\) is a linear transformation of \\(\\color{purple}{c_t}\\)\n\nUse the difference between current and previous value of control variable\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\) \\(B=[-1,1]\\)"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#extra-variable-correlation",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#extra-variable-correlation",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Extra: Variable correlation",
    "text": "Extra: Variable correlation\n\n\nGap in two variables ::: {.cell} ::: {.cell-output-display}\n\n\n\n:::\n\nGap in only one variable ::: {.cell} ::: {.cell-output-display}\n\n\n::: :::\n::::"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-find-model-parameters",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-find-model-parameters",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "How to find model parameters",
    "text": "How to find model parameters\n\ncreate artificial gaps\npredicting gap in the model\ncompute the log likelihood of the predictions\nmaximise the log likelihood"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-1",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filter-1",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Kalman Filter",
    "text": "Kalman Filter\npros:\n\nProbabilist model: the output of the model is a distribution of predictions, not a single value\nCombines all 3 approaches to gap filling in one model\ninterpretable paramters\ncomputationally efficient\n\ncons:\n\nkeeps tracks only of the local state"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filters-gap-1",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filters-gap-1",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Kalman Filters gap #1",
    "text": "Kalman Filters gap #1"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filters-gap-2",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filters-gap-2",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Kalman Filters gap #2",
    "text": "Kalman Filters gap #2"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filters-gap-3",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#kalman-filters-gap-3",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Kalman Filters gap #3",
    "text": "Kalman Filters gap #3"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#what-is-missing-in-the-model-development",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#what-is-missing-in-the-model-development",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "What is missing in the model development",
    "text": "What is missing in the model development\n\nimprove numerical stability of model (work in progress)\nfind optimal settings for training and inference\n\nn observations before after/gap\nhow to best generate artificial gaps\n\nHow to assess the model?"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#use-of-gap-filling",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#use-of-gap-filling",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Use of gap filling",
    "text": "Use of gap filling\n\nwhat is the impact of better gap filling for data users?\n\nwhy better filling for short/medium gaps is useful\nhow can the uncertainty be used"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-assess-the-model-settings",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-assess-the-model-settings",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "How to assess the model? settings",
    "text": "How to assess the model? settings\n\nhow to choose gap lengths?\nhow to choose number of variables missing?\nwhich variable to focus on?"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-assess-the-model-metrics",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-assess-the-model-metrics",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "How to assess the model? Metrics",
    "text": "How to assess the model? Metrics\n\nRMSE - interpretation difficult as it’s relative to the variable\nr2 - gaps are often too short to interpret properly\n?"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-assess-the-model-figures",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-assess-the-model-figures",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nTime series\n\n\n\n\n\n\n\n\n\n\nScatter plots"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-assess-the-model-figures-1",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#how-to-assess-the-model-figures-1",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nGap length / mean RMSE\n\n\n\n\n\n\n\n\n\n\nDistribution gaps vs filled"
  },
  {
    "objectID": "presentations/presentation_sinz_lab_meeting_9_may_23.html#future-outlook-1",
    "href": "presentations/presentation_sinz_lab_meeting_9_may_23.html#future-outlook-1",
    "title": "Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications",
    "section": "Future outlook",
    "text": "Future outlook\n\noptimize performance model\nprovide pre-trained model on Fluxnet 2015 and then to fine-tune to local site\nprovide web-service for filling gaps\nreprocess Fluxnet 2015 dataset"
  },
  {
    "objectID": "results/extra plot.html",
    "href": "results/extra plot.html",
    "title": "Meteo Imp Analsyis",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom meteo_imp.kalman.results import *\nfrom meteo_imp.data import *\nfrom meteo_imp.utils import *\nimport pandas as pd\nimport numpy as np\nfrom pyprojroot import here\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG, Image\nimport altair as alt\nfrom functools import partial\n\nimport io\nfrom contextlib import redirect_stderr\nfrom fastai.vision.data import get_grid\nimport vl_convert as vlc\nfrom pyprojroot import here\nbase_path_img = here(\"manuscript/Master Thesis - Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications - Simone Massaro/images/\")\nbase_path_tbl = here(\"manuscript/Master Thesis - Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications - Simone Massaro/tables/\")\n\nbase_path_img.mkdir(exist_ok=True), base_path_tbl.mkdir(exist_ok=True)\n\ndef save_show_plot(plot, path):\n    with redirect_stderr(io.StringIO()):\n        plot.save(base_path_img / (path + \".pdf\"))\n    # render to image for displaying in notebook\n    png_data = vlc.vegalite_to_png(vl_spec=plot.to_json(), scale=1)\n    return Image(png_data)\nplotting additional figure for introduction\nhai = pd.read_parquet(hai_big_path).reindex(columns=var_type.categories)\nhai_era = pd.read_parquet(hai_era_big_path)\nmodels_var = pd.DataFrame.from_records([\n    {'var': 'TA',    'model': l_model(\"TA_specialized_gap_6-336_v3_0.pickle\",base_path)},\n    {'var': 'SW_IN', 'model': l_model(\"SW_IN_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'LW_IN', 'model': l_model(\"LW_IN_specialized_gap_6-336_v1.pickle\",base_path)},\n    {'var': 'VPD',   'model': l_model(\"VPD_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'WS',    'model': l_model(\"WS_specialized_gap_6-336_v1.pickle\",base_path)},\n    {'var': 'PA',    'model': l_model(\"PA_specialized_gap_6-336_v3_0.pickle\",base_path)},\n    {'var': 'P',     'model': l_model(\"1_gap_varying_6-336_v3.pickle\",base_path)},\n    {'var': 'TS',    'model': l_model(\"TS_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'SWC',   'model': l_model(\"SWC_specialized_gap_6-336_v2_1.pickle\",base_path)},\n])\nreset_seed()\ncomp_Av = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 48+100, time_series=True, rmse=False)\nresults_ts = comp_Av.compare(gap_len = [48], var=list(hai.columns), n_rep=1)\nres_ts = results_ts.query(\"method != 'Kalman Filter'\")\nres_ts_plot = pd.concat([unnest_predictions(row, ctx_len=72) for _,row in res_ts.iterrows()])\nfrom meteo_imp.kalman.results import _plot_timeseries, _get_labels\nscale_sota = alt.Scale(domain=[\"ERA-I\", \"MDS\"], range=list(sns.color_palette('Dark2', 3).as_hex())[1:])\nfrom fastcore.basics import *\nfrom typing import Callable\n@patch\ndef pipe(self: alt.Chart, f: Callable): return f(self)\nclass\nfont_size = 20\nlegend_font_size = 22\ntitle_font_size = 22\np = (facet_wrap(res_ts_plot, partial(_plot_timeseries, scale_color=scale_sota, err_band = False), col=\"var\",\n                y_labels = _get_labels(res_ts_plot, 'mean', None),\n               )\n            .configure_legend(orient=\"bottom\", labelFontSize=font_size, titleFontSize=legend_font_size)\n            .configure_axis(labelFontSize=font_size, titleFontSize=font_size )\n            .configure_title(fontSize=title_font_size)\n    )\nsave_show_plot(p, \"timeseries_sota\")"
  },
  {
    "objectID": "results/extra plot.html#correlation",
    "href": "results/extra plot.html#correlation",
    "title": "Meteo Imp Analsyis",
    "section": "Correlation",
    "text": "Correlation\n\nimport matplotlib.pyplot as plt\n\n\nimport statsmodels.api as sm\n\n\ndef auto_corr_df(data, nlags=96):\n    autocorr = {}\n    for col in data.columns:\n        autocorr[col] = sm.tsa.acf(data[col], nlags=nlags)\n    return pd.DataFrame(autocorr)\n\n\nauto_corr = auto_corr_df(hai).reset_index(names=\"gap_len\").melt(id_vars=\"gap_len\")\nauto_corr.gap_len = auto_corr.gap_len / 2\n\n\nauto_corr\n\n\n\n\n\n\n\n\ngap_len\nvariable\nvalue\n\n\n\n\n0\n0.0\nTA\n1.000000\n\n\n1\n0.5\nTA\n0.998595\n\n\n2\n1.0\nTA\n0.995814\n\n\n3\n1.5\nTA\n0.992141\n\n\n4\n2.0\nTA\n0.987630\n\n\n...\n...\n...\n...\n\n\n868\n46.0\nTS\n0.959680\n\n\n869\n46.5\nTS\n0.961116\n\n\n870\n47.0\nTS\n0.962085\n\n\n871\n47.5\nTS\n0.962551\n\n\n872\n48.0\nTS\n0.962480\n\n\n\n\n873 rows × 3 columns\n\n\n\n\np = (alt.Chart(auto_corr).mark_line().encode(\n    x = alt.X('gap_len', title=\"Gap length [h]\", axis = alt.Axis(values= [12, 24, 36, 48])),\n    y = alt.Y(\"value\", title=\"correlation\"),\n    color=alt.Color(\"variable\", scale=meteo_scale, title=\"Variable\"),\n    facet =alt.Facet('variable', columns=3, sort = meteo_scale.domain, title=None,\n                     header = alt.Header(labelFontWeight=\"bold\", labelFontSize=14))\n)\n    .properties(height=140, width=150)\n    .resolve_scale(y='independent', x = 'independent')\n    .configure_legend(orient=\"bottom\", labelFontSize=13, titleFontSize=13)\n    .configure_axis(labelFontSize=13, titleFontSize=13 )\n    .configure_title(fontSize=16))\n\nsave_show_plot(p, \"temporal_autocorrelation\")\n\n\n\n\n\n\n\n\n\naxes = get_grid(1,1,1, figsize=(10,8))\nsns.heatmap(hai.corr(), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), ax=axes[0], square=True, cbar=True)\n# axes[0].set(xlabel=\"Variable\", ylabel=\"Variable\", title=\"Inter-variable Correlation\");\nplt.tight_layout()\nplt.savefig(base_path_img / \"correlation.pdf\")"
  },
  {
    "objectID": "results/result_plots.html",
    "href": "results/result_plots.html",
    "title": "Plotting for results",
    "section": "",
    "text": "This notebook produces all results plots. It generates some gap in the data, fill with a method (filter, MDS …), compute metrics and then makes all relevant plots\n%load_ext autoreload\n%autoreload 2\nimport altair as alt\nfrom meteo_imp.kalman.results import *\nfrom meteo_imp.data import *\nfrom meteo_imp.utils import *\nimport pandas as pd\nimport numpy as np\nfrom pyprojroot import here\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom IPython.display import SVG, Image\n\nfrom meteo_imp.kalman.results import _plot_timeseries, _get_labels\nfrom functools import partial\nfrom contextlib import redirect_stderr\nimport io\n\nimport polars as pl\nfrom fastai.vision.data import get_grid\nimport cairosvg\nthe generation of a proper pdf is complex as altair_render doesn’t support XOffset, so plotsare first renderder to svg using vl-convert and then to pdf using cairosvg. However this last methods doesn’t support negative numbers …\nDue to the high number of samples also cannot use the browser render in the notebook so using vl-convert to a png for the visualization in the notebook\nimport vl_convert as vlc\nfrom pyprojroot import here\nbase_path_img = here(\"manuscript/Master Thesis - Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications - Simone Massaro/images/\")\nbase_path_tbl = here(\"manuscript/Master Thesis - Evaluation of Kalman filter for meteorological time series imputation for Eddy Covariance applications - Simone Massaro/tables/\")\n\nbase_path_img.mkdir(exist_ok=True), base_path_tbl.mkdir(exist_ok=True)\n\ndef save_show_plot(plot,\n                   path,\n                   altair_render=False # use altair render for pdf?\n                  ):\n    plt_json = plot.to_json()\n    if not altair_render:\n        svg_data = vlc.vegalite_to_svg(vl_spec=plt_json)\n        with open(base_path_img / (path + \".svg\"), 'w') as f:\n            f.write(svg_data)\n\n        cairosvg.svg2pdf(file_obj=open(base_path_img / (path + \".svg\")), write_to=str(base_path_img / (path + \".pdf\")))\n    else:\n        #save svg version anyway\n        svg_data = vlc.vegalite_to_svg(vl_spec=plt_json)\n        with open(base_path_img / (path + \".svg\"), 'w') as f:\n            f.write(svg_data)\n        #convert to pdf using altair\n        with redirect_stderr(io.StringIO()):\n            plot.save(base_path_img / (path + \".pdf\"))\n\n    # render to image for displaying in notebook\n    png_data = vlc.vegalite_to_png(vl_spec=plot.to_json(), scale=1)\n    return Image(png_data)\nreset_seed()\nn_rep = 500\nhai = pd.read_parquet(hai_big_path).reindex(columns=var_type.categories)\nhai_era = pd.read_parquet(hai_era_big_path)\nalt.data_transformers.disable_max_rows() # it is safe to do so as the plots are rendered using vl-convert and then showed as images\n\nDataTransformerRegistry.enable('default')"
  },
  {
    "objectID": "results/result_plots.html#correlation",
    "href": "results/result_plots.html#correlation",
    "title": "Plotting for results",
    "section": "Correlation",
    "text": "Correlation\n\nimport matplotlib.pyplot as plt\n\n\nimport statsmodels.api as sm\n\n\ndef auto_corr_df(data, nlags=96):\n    autocorr = {}\n    for col in data.columns:\n        autocorr[col] = sm.tsa.acf(data[col], nlags=nlags)\n    return pd.DataFrame(autocorr)\n\n\nauto_corr = auto_corr_df(hai).reset_index(names=\"gap_len\").melt(id_vars=\"gap_len\")\nauto_corr.gap_len = auto_corr.gap_len / 2\n\n\nauto_corr\n\n\n\n\n\n\n\n\ngap_len\nvariable\nvalue\n\n\n\n\n0\n0.0\nTA\n1.000000\n\n\n1\n0.5\nTA\n0.998595\n\n\n2\n1.0\nTA\n0.995814\n\n\n3\n1.5\nTA\n0.992141\n\n\n4\n2.0\nTA\n0.987630\n\n\n...\n...\n...\n...\n\n\n868\n46.0\nTS\n0.959680\n\n\n869\n46.5\nTS\n0.961116\n\n\n870\n47.0\nTS\n0.962085\n\n\n871\n47.5\nTS\n0.962551\n\n\n872\n48.0\nTS\n0.962480\n\n\n\n\n873 rows × 3 columns\n\n\n\n\np = (alt.Chart(auto_corr).mark_line().encode(\n    x = alt.X('gap_len', title=\"Gap length [h]\", axis = alt.Axis(values= [12, 24, 36, 48])),\n    y = alt.Y(\"value\", title=\"correlation\"),\n    color=alt.Color(\"variable\", scale=meteo_scale, title=\"Variable\"),\n    facet =alt.Facet('variable', columns=3, sort = meteo_scale.domain, title=None,\n                     header = alt.Header(labelFontWeight=\"bold\", labelFontSize=20))\n)\n    .properties(height=120, width=250)\n    .resolve_scale(y='independent', x = 'independent')\n    .pipe(plot_formatter))\n\nsave_show_plot(p, \"temporal_autocorrelation\")\n\n\n\n\n\n\n\n\n\naxes = get_grid(1,1,1, figsize=(10,8))\nsns.set(font_scale=1.25)\nsns.heatmap(hai.corr(), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), ax=axes[0], square=True, cbar=True)\n# axes[0].set(xlabel=\"Variable\", ylabel=\"Variable\", title=\"Inter-variable Correlation\");\n# size_old = plt.rcParams[\"axes.labelsize\"]\n# w_old = plt.rcParams[\"axes.labelweight\"]\n# plt.rcParams[\"axes.labelsize\"] = 30\n# plt.rcParams[\"axes.labelweight\"] = 'bold'\nplt.tight_layout()\nplt.xticks(weight = 'bold')\nplt.yticks(weight = 'bold')\n\nwith matplotlib.rc_context({\"axes.labelsize\": 30}):\n    plt.savefig(base_path_img / \"correlation.pdf\")\n    plt.show()\n\n# plt.rcParams[\"axes.labelsize\"] = size_old\n# plt.rcParams[\"axes.labelweight\"] = w_old"
  },
  {
    "objectID": "results/result_plots.html#comparison-imputation-methods",
    "href": "results/result_plots.html#comparison-imputation-methods",
    "title": "Plotting for results",
    "section": "Comparison Imputation methods",
    "text": "Comparison Imputation methods\n\nbase_path = here(\"analysis/results/trained_models\")\n\n\ndef l_model(x, base_path=base_path): return torch.load(base_path / x)\n\n\nmodels_var = pd.DataFrame.from_records([\n    {'var': 'TA',    'model': l_model(\"TA_specialized_gap_6-336_v3_0.pickle\",base_path)},\n    {'var': 'SW_IN', 'model': l_model(\"SW_IN_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'LW_IN', 'model': l_model(\"LW_IN_specialized_gap_6-336_v1.pickle\",base_path)},\n    {'var': 'VPD',   'model': l_model(\"VPD_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'WS',    'model': l_model(\"WS_specialized_gap_6-336_v1.pickle\",base_path)},\n    {'var': 'PA',    'model': l_model(\"PA_specialized_gap_6-336_v3_0.pickle\",base_path)},\n    {'var': 'P',     'model': l_model(\"1_gap_varying_6-336_v3.pickle\",base_path)},\n    {'var': 'TS',    'model': l_model(\"TS_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'SWC',   'model': l_model(\"SWC_specialized_gap_6-336_v2_1.pickle\",base_path)},\n])\n\n\n@cache_disk(cache_dir / \"the_results\")\ndef get_the_results(n_rep=20):\n    reset_seed()\n    comp_Av = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 446, time_series=False)\n    results_Av = comp_Av.compare(gap_len = [12,24, 48, 336], var=list(hai.columns), n_rep=n_rep) \n    return results_Av\n\nresults_Av = get_the_results(n_rep)\n\n\nState of the art\nthe first plot is a time series using only state-of-the-art methods\n\nreset_seed()\ncomp_ts = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 48+100, time_series=True, rmse=False)\nresults_ts = comp_ts.compare(gap_len = [48], var=list(hai.columns), n_rep=1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nres_ts = results_ts.query(\"method != 'Kalman Filter'\")\nres_ts_plot = pd.concat([unnest_predictions(row, ctx_len=72) for _,row in res_ts.iterrows()])\n\n\nscale_sota = alt.Scale(domain=[\"ERA-I\", \"MDS\"], range=list(sns.color_palette('Dark2', 3).as_hex())[1:])\n\n\np = (facet_wrap(res_ts_plot, partial(_plot_timeseries, scale_color=scale_sota, err_band = False), col=\"var\",\n                y_labels = _get_labels(res_ts_plot, 'mean', None),\n               )\n            .pipe(plot_formatter)\n    )\nsave_show_plot(p, \"timeseries_sota\", altair_render=True)\n\n\n\n\n\n\n\n\n\n\nPercentage improvement\n\nresults_Av.method.unique()\n\n['Kalman Filter', 'ERA-I', 'MDS']\nCategories (3, object): ['Kalman Filter' &lt; 'ERA-I' &lt; 'MDS']\n\n\n\nall_res = results_Av.query('var != \"P\"').groupby(['method']).agg({'rmse_stand': 'mean'}).T\n\n\nall_res\n\n\n\n\n\n\n\nmethod\nKalman Filter\nERA-I\nMDS\n\n\n\n\nrmse_stand\n0.204628\n0.307361\n0.482837\n\n\n\n\n\n\n\npercentage of improvement across all variables\n\n(all_res[\"ERA-I\"] - all_res[\"Kalman Filter\"]) / all_res[\"ERA-I\"] * 100 \n\nrmse_stand    33.42398\ndtype: float64\n\n\n\n(all_res[\"MDS\"] - all_res[\"Kalman Filter\"]) / all_res[\"MDS\"] * 100 \n\nrmse_stand    57.619542\ndtype: float64\n\n\n\nres_var = results_Av.groupby(['method', 'var']).agg({'rmse_stand': 'mean'}) \n\n\nres_var = res_var.reset_index().pivot(columns='method', values='rmse_stand', index='var')\n\n\npd.DataFrame({'ERA': (res_var[\"ERA-I\"] - res_var[\"Kalman Filter\"]) / res_var[\"ERA-I\"] * 100, 'MDS': (res_var[\"MDS\"] - res_var[\"Kalman Filter\"]) / res_var[\"MDS\"] * 100 })\n\n\n\n\n\n\n\n\nERA\nMDS\n\n\nvar\n\n\n\n\n\n\nTA\n54.540802\n77.713711\n\n\nSW_IN\n12.004508\n35.516142\n\n\nLW_IN\n5.166063\n52.289627\n\n\nVPD\n44.402821\n65.407769\n\n\nWS\n21.064305\n40.321732\n\n\nPA\n28.784191\n90.751559\n\n\nP\n-18.544370\n-22.084360\n\n\nSWC\nNaN\n41.543006\n\n\nTS\nNaN\n25.772326\n\n\n\n\n\n\n\n\nres_var2 = results_Av.groupby(['method', 'var', 'gap_len']).agg({'rmse_stand': 'mean'}) \n\n\nres_var2 = res_var2.reset_index().pivot(columns='method', values='rmse_stand', index=['var', 'gap_len'])\n\n\npd.DataFrame({'ERA': (res_var2[\"ERA-I\"] - res_var2[\"Kalman Filter\"]) / res_var2[\"ERA-I\"] * 100, 'MDS': (res_var2[\"MDS\"] - res_var2[\"Kalman Filter\"]) / res_var2[\"MDS\"] * 100 })\n\n\n\n\n\n\n\n\n\nERA\nMDS\n\n\nvar\ngap_len\n\n\n\n\n\n\nTA\n6\n69.897582\n85.052698\n\n\n12\n58.766166\n79.376385\n\n\n24\n51.538443\n75.395970\n\n\n168\n41.823614\n73.000401\n\n\nSW_IN\n6\n9.519984\n29.746651\n\n\n12\n11.165399\n30.639223\n\n\n24\n14.232051\n34.811941\n\n\n168\n12.305658\n42.651906\n\n\nLW_IN\n6\n21.023524\n59.136518\n\n\n12\n9.110040\n52.211404\n\n\n24\n-3.553292\n50.720632\n\n\n168\n-4.260023\n48.223005\n\n\nVPD\n6\n66.980942\n79.449579\n\n\n12\n47.785633\n69.081018\n\n\n24\n33.663749\n56.728120\n\n\n168\n32.272332\n57.702579\n\n\nWS\n6\n32.402977\n45.724043\n\n\n12\n25.209162\n43.275430\n\n\n24\n15.543672\n37.142502\n\n\n168\n12.735569\n36.436106\n\n\nPA\n6\n39.823585\n91.511486\n\n\n12\n30.995845\n90.532461\n\n\n24\n24.727301\n89.319180\n\n\n168\n20.691181\n91.421434\n\n\nP\n6\n-18.485009\n-13.917879\n\n\n12\n-28.935358\n-37.127331\n\n\n24\n-24.423076\n-29.998707\n\n\n168\n-7.725322\n-11.587796\n\n\nSWC\n6\nNaN\n61.302664\n\n\n12\nNaN\n47.976950\n\n\n24\nNaN\n42.535719\n\n\n168\nNaN\n23.301469\n\n\nTS\n6\nNaN\n64.264901\n\n\n12\nNaN\n46.699870\n\n\n24\nNaN\n27.050291\n\n\n168\nNaN\n-15.268479\n\n\n\n\n\n\n\n\n\nMain plot\n\nfrom itertools import product\nimport altair as alt\n\n\np = the_plot(results_Av)\nsave_show_plot(p, \"the_plot\")\n\n\n\n\n\n\n\n\n\np = the_plot_stand(results_Av)\nsave_show_plot(p, \"the_plot_stand\")\n\n\n\n\n\n\n\n\n\n\nTable\n\nt = the_table(results_Av)\nthe_table_latex(t, base_path_tbl / \"the_table.tex\", label=\"tbl:the_table\",\n                caption=\"\\\\CapTheTable\")\nt\n\n\n\n\n\n\n\n\n\nKalman Filter\nERA-I\nMDS\n\n\n\nRMSE\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nVariable\nGap\n\n\n\n\n\n\n\n\n\n\nTA\n6 h\n0.405453\n0.258301\n1.346910\n0.997843\n2.712546\n1.896914\n\n\n12 h\n0.606836\n0.400849\n1.471695\n0.900611\n2.942435\n1.748131\n\n\n1 day (24 h)\n0.741275\n0.368468\n1.529614\n0.800256\n3.012819\n1.611311\n\n\n1 week (168 h)\n1.020608\n0.444591\n1.754334\n0.643160\n3.780087\n1.315472\n\n\nSW_IN\n6 h\n44.636609\n40.464629\n49.333113\n66.241975\n63.536627\n85.401585\n\n\n12 h\n48.155186\n33.868178\n54.207691\n49.769296\n69.427115\n68.936352\n\n\n1 day (24 h)\n56.564277\n30.042752\n65.950367\n40.930505\n86.770917\n59.603564\n\n\n1 week (168 h)\n61.582820\n25.740161\n70.224393\n34.883199\n107.384249\n53.606111\n\n\nLW_IN\n6 h\n10.902409\n7.736087\n13.804628\n12.987987\n26.680077\n15.022366\n\n\n12 h\n13.421656\n7.734502\n14.766929\n12.584725\n28.085478\n13.457335\n\n\n1 day (24 h)\n14.593819\n7.840046\n14.093052\n12.227900\n29.614461\n12.416763\n\n\n1 week (168 h)\n17.062880\n6.425136\n16.365697\n11.129569\n32.954558\n8.833972\n\n\nVPD\n6 h\n0.428187\n0.363168\n1.296787\n1.547397\n2.083592\n2.149288\n\n\n12 h\n0.660623\n0.504761\n1.265213\n1.288794\n2.136626\n2.095549\n\n\n1 day (24 h)\n0.827563\n0.501975\n1.247527\n1.032319\n1.912472\n1.605013\n\n\n1 week (168 h)\n1.125680\n0.633392\n1.662069\n1.127314\n2.661345\n1.965431\n\n\nWS\n6 h\n0.616774\n0.316972\n0.912428\n0.508295\n1.136367\n0.783146\n\n\n12 h\n0.715412\n0.350974\n0.956550\n0.524247\n1.261203\n0.796744\n\n\n1 day (24 h)\n0.801851\n0.343378\n0.949427\n0.446912\n1.275665\n0.608630\n\n\n1 week (168 h)\n0.950211\n0.363124\n1.088887\n0.348541\n1.494891\n0.615371\n\n\nPA\n6 h\n0.045046\n0.034294\n0.074856\n0.061726\n0.530665\n0.441476\n\n\n12 h\n0.053359\n0.041613\n0.077328\n0.058476\n0.563603\n0.427426\n\n\n1 day (24 h)\n0.059481\n0.038666\n0.079021\n0.051491\n0.556899\n0.404451\n\n\n1 week (168 h)\n0.066325\n0.047544\n0.083628\n0.053654\n0.773143\n0.384029\n\n\nP\n6 h\n0.134093\n0.274033\n0.113173\n0.315504\n0.117710\n0.305539\n\n\n12 h\n0.178871\n0.295419\n0.138729\n0.297227\n0.130442\n0.281377\n\n\n1 day (24 h)\n0.206231\n0.253588\n0.165750\n0.288432\n0.158641\n0.265257\n\n\n1 week (168 h)\n0.239885\n0.173820\n0.222682\n0.201782\n0.214975\n0.197499\n\n\nSWC\n6 h\n0.508379\n0.487342\nNaN\nNaN\n1.313730\n1.556829\n\n\n12 h\n0.664855\n0.471849\nNaN\nNaN\n1.278001\n1.323011\n\n\n1 day (24 h)\n0.779066\n0.640996\nNaN\nNaN\n1.355740\n1.472185\n\n\n1 week (168 h)\n1.493784\n0.947799\nNaN\nNaN\n1.947605\n1.488284\n\n\nTS\n6 h\n0.341080\n0.431992\nNaN\nNaN\n0.954469\n0.889126\n\n\n12 h\n0.534363\n0.783787\nNaN\nNaN\n1.002555\n0.876784\n\n\n1 day (24 h)\n0.786670\n0.851931\nNaN\nNaN\n1.078373\n0.856964\n\n\n1 week (168 h)\n1.659875\n1.077782\nNaN\nNaN\n1.440008\n0.764040\n\n\n\n\n\n\n\n\nt = the_table(results_Av, 'rmse_stand', y_name=\"Stand. RMSE\")\nthe_table_latex(t, base_path_tbl / \"the_table_stand.tex\", stand = True, label=\"tbl:the_table_stand\", \n                caption = \"\\\\CapTheTableStand\")\nt\n\n\n\n\n\n\n\n\n\nKalman Filter\nERA-I\nMDS\n\n\n\nStand. RMSE\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nVariable\nGap\n\n\n\n\n\n\n\n\n\n\nTA\n6 h\n0.051164\n0.032595\n0.169965\n0.125917\n0.342294\n0.239370\n\n\n12 h\n0.076576\n0.050583\n0.185712\n0.113647\n0.371303\n0.220595\n\n\n1 day (24 h)\n0.093541\n0.046497\n0.193021\n0.100984\n0.380185\n0.203330\n\n\n1 week (168 h)\n0.128790\n0.056103\n0.221378\n0.081160\n0.477006\n0.165998\n\n\nSW_IN\n6 h\n0.218804\n0.198354\n0.241826\n0.324711\n0.311450\n0.418630\n\n\n12 h\n0.236052\n0.166018\n0.265721\n0.243964\n0.340325\n0.337919\n\n\n1 day (24 h)\n0.277272\n0.147267\n0.323282\n0.200637\n0.425342\n0.292171\n\n\n1 week (168 h)\n0.301873\n0.126176\n0.344233\n0.170994\n0.526387\n0.262772\n\n\nLW_IN\n6 h\n0.259855\n0.184387\n0.329028\n0.309564\n0.635910\n0.358053\n\n\n12 h\n0.319900\n0.184349\n0.351964\n0.299952\n0.669407\n0.320751\n\n\n1 day (24 h)\n0.347838\n0.186865\n0.335903\n0.291448\n0.705850\n0.295949\n\n\n1 week (168 h)\n0.406688\n0.153141\n0.390071\n0.265269\n0.785460\n0.210555\n\n\nVPD\n6 h\n0.098019\n0.083135\n0.296855\n0.354224\n0.476967\n0.492006\n\n\n12 h\n0.151227\n0.115548\n0.289627\n0.295025\n0.489108\n0.479704\n\n\n1 day (24 h)\n0.189442\n0.114910\n0.285579\n0.236314\n0.437795\n0.367413\n\n\n1 week (168 h)\n0.257686\n0.144994\n0.380474\n0.258060\n0.609224\n0.449918\n\n\nWS\n6 h\n0.379454\n0.195008\n0.561347\n0.312715\n0.699120\n0.481810\n\n\n12 h\n0.440138\n0.215927\n0.588492\n0.322529\n0.775922\n0.490176\n\n\n1 day (24 h)\n0.493318\n0.211254\n0.584110\n0.274951\n0.784819\n0.374443\n\n\n1 week (168 h)\n0.584592\n0.223403\n0.669909\n0.214431\n0.919692\n0.378591\n\n\nPA\n6 h\n0.052675\n0.040103\n0.087534\n0.072180\n0.620545\n0.516250\n\n\n12 h\n0.062397\n0.048661\n0.090425\n0.068381\n0.659061\n0.499820\n\n\n1 day (24 h)\n0.069556\n0.045215\n0.092405\n0.060212\n0.651223\n0.472953\n\n\n1 week (168 h)\n0.077558\n0.055597\n0.097793\n0.062741\n0.904092\n0.449073\n\n\nP\n6 h\n0.478431\n0.977725\n0.403790\n1.125691\n0.419979\n1.090136\n\n\n12 h\n0.638197\n1.054031\n0.494974\n1.060481\n0.465404\n1.003928\n\n\n1 day (24 h)\n0.735816\n0.904779\n0.591382\n1.029100\n0.566018\n0.946414\n\n\n1 week (168 h)\n0.855891\n0.620173\n0.794512\n0.719941\n0.767011\n0.704660\n\n\nSWC\n6 h\n0.057037\n0.054677\nNaN\nNaN\n0.147393\n0.174667\n\n\n12 h\n0.074593\n0.052939\nNaN\nNaN\n0.143384\n0.148434\n\n\n1 day (24 h)\n0.087407\n0.071916\nNaN\nNaN\n0.152106\n0.165171\n\n\n1 week (168 h)\n0.167594\n0.106338\nNaN\nNaN\n0.218510\n0.166977\n\n\nTS\n6 h\n0.060276\n0.076342\nNaN\nNaN\n0.168674\n0.157127\n\n\n12 h\n0.094433\n0.138512\nNaN\nNaN\n0.177172\n0.154946\n\n\n1 day (24 h)\n0.139021\n0.150554\nNaN\nNaN\n0.190571\n0.151443\n\n\n1 week (168 h)\n0.293335\n0.190466\nNaN\nNaN\n0.254479\n0.135022\n\n\n\n\n\n\n\n\n\nTimeseries\n\n@cache_disk(cache_dir / \"the_results_ts\")\ndef get_the_results_ts():\n    reset_seed()\n    comp_Av = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 446, time_series=True, rmse=False)\n    results_Av = comp_Av.compare(gap_len = [12,24, 336], var=list(hai.columns), n_rep=4) \n    return results_Av\n\nresults_ts = get_the_results_ts()\n\n\nts = plot_timeseries(results_ts.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=0)\nsave_show_plot(ts, \"timeseries_1\", altair_render=True)\n\n\n\n\n\n\n\n\n\n%time ts = plot_timeseries(results_ts.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=0)\n%time save_show_plot(ts, \"timeseries_2\", altair_render=True)\n\nCPU times: user 2.82 s, sys: 765 µs, total: 2.82 s\nWall time: 2.84 s\nCPU times: user 9.53 s, sys: 127 ms, total: 9.66 s\nWall time: 12.8 s\n\n\n\n\n\n\n\n\n\n\nfrom tqdm.auto import tqdm\n\n\n# @cache_disk(cache_dir / \"ts_plot\", rm_cache=True)\ndef plot_additional_ts():\n    for idx in tqdm(results_ts.idx_rep.unique()):\n        if idx == 0: continue # skip first plot as is done above\n        ts1 = plot_timeseries(results_ts.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=idx)\n        save_show_plot(ts1, f\"timeseries_1_{idx}\", altair_render=True)\n        ts2 = plot_timeseries(results_ts.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=idx)\n        save_show_plot(ts2, f\"timeseries_2_{idx}\", altair_render=True)        \n\n\nplot_additional_ts()"
  },
  {
    "objectID": "results/result_plots.html#kalman-filter-analysis",
    "href": "results/result_plots.html#kalman-filter-analysis",
    "title": "Plotting for results",
    "section": "Kalman Filter analysis",
    "text": "Kalman Filter analysis\n\nGap len\n\n@cache_disk(cache_dir / \"gap_len\")\ndef get_g_len(n_rep=n_rep):\n    reset_seed()\n    return KalmanImpComparison(models_var, hai, hai_era, block_len=48*7+100).compare(gap_len = [2,6,12,24,48,48*2, 48*3, 48*7], var=list(hai.columns), n_rep=n_rep)\n\n\ngap_len = get_g_len(n_rep)\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nInput In [46], in &lt;cell line: 1&gt;()\n----&gt; 1 gap_len = get_g_len(n_rep)\n\nFile ~/Documents/uni/Thesis/meteo_imp/meteo_imp/utils.py:44, in cache_disk.&lt;locals&gt;.decorator.&lt;locals&gt;.new_func(*args)\n     42 def new_func(*args):\n     43     if tuple(args) not in cache:\n---&gt; 44         cache[tuple(args)] = original_func(*args)\n     45         save_data()\n     46     return cache[args]\n\nInput In [45], in get_g_len(n_rep)\n      1 @cache_disk(cache_dir / \"gap_len\")\n      2 def get_g_len(n_rep=n_rep):\n      3     reset_seed()\n----&gt; 4     return KalmanImpComparison(models_var, hai, hai_era, block_len=48*7+100).compare(gap_len = [2,6,12,24,48,48*2, 48*3, 48*7], var=list(hai.columns), n_rep=n_rep)\n\nFile ~/Documents/uni/Thesis/meteo_imp/meteo_imp/kalman/results.py:458, in KalmanImpComparison.compare(self, n_rep, gap_len, var)\n    456 out = []\n    457 for arg_set in tqdm(arg_sets):\n--&gt; 458     out.append(self._compare_single(**arg_set, n_rep=n_rep))\n    459 return prep_df(pd.concat(out))\n\nFile ~/Documents/uni/Thesis/meteo_imp/meteo_imp/kalman/results.py:439, in KalmanImpComparison._compare_single(self, n_rep, gap_len, var)\n    437 pred, targ, metric = imp.imp.preds_all_metrics(items = [items[i]], dls=dls, metrics=metrics_fn)\n    438 pred, targ = pred[0], targ[0]\n--&gt; 439 pred = pred.mean.iloc[:, [var_idx]]\n    440 targ = MeteoImpDf(targ.data.iloc[:, [var_idx]], targ.mask.iloc[:, [var_idx]], targ.control.iloc[:, [var_idx]])\n    441 out = {\n    442     'var': var,\n    443     'loss': metric['loss'][0].item(),\n   (...)\n    446     'idx_rep': i,\n    447 } | imp.drop(index=[\"model\", \"imp\"]).to_dict()\n\nFile ~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:140, in IndexingMixin.iloc(self)\n    135 class IndexingMixin:\n    136     \"\"\"\n    137     Mixin for adding .loc/.iloc/.at/.iat to Dataframes and Series.\n    138     \"\"\"\n--&gt; 140     @property\n    141     def iloc(self) -&gt; _iLocIndexer:\n    142         \"\"\"\n    143         Purely integer-location based indexing for selection by position.\n    144 \n   (...)\n    275         2  1000  3000\n    276         \"\"\"\n    277         return _iLocIndexer(\"iloc\", self)\n\nKeyboardInterrupt: \n\n\n\n\np = plot_gap_len(gap_len, hai, hai_era)\nsave_show_plot(p, \"gap_len\")\n\n\nt = table_gap_len(gap_len)\ntable_gap_len_latex(t, base_path_tbl / \"gap_len.tex\", label=\"gap_len\",\n                caption=\"\\\\CapGapLen\")\nt\n\n\ng_len_agg = gap_len.groupby('gap_len').agg({'rmse_stand': 'mean'})\n(g_len_agg.iloc[0])/g_len_agg.iloc[-1]\n\n\ng_len_agg = gap_len.groupby(['gap_len', 'var']).agg({'rmse_stand': 'mean'})\n(g_len_agg.loc[1.])/g_len_agg.loc[168.]\n\n\ng_len_agg\n\n\ng_len_agg_std = gap_len.groupby('gap_len').agg({'rmse_stand': 'std'})\n(g_len_agg_std.iloc[0])/g_len_agg_std.iloc[-1]\n\n\n(gap_len.groupby(['gap_len', 'var']).agg({'rmse_stand': 'std'})\n    .unstack(\"var\")\n    .droplevel(0, 1) \n    .plot(subplots=True, layout=(3,3), figsize=(10,10)))\n\n\n# with open(base_path_tbl / \"gap_len.tex\") as f:\n    # print(f.readlines())\n\n\n\nControl\n\nmodels_nc = pd.DataFrame({'model': [ l_model(\"1_gap_varying_336_no_control_v1.pickle\"), l_model(\"1_gap_varying_6-336_v3.pickle\")],\n                          'type':   [ 'No Control',                                       'Use Control'                         ]})                                        \n\n\n@cache_disk(cache_dir / \"use_control\")\ndef get_control(n_rep=n_rep):\n    reset_seed()\n    \n    kcomp_control = KalmanImpComparison(models_nc, hai, hai_era, block_len=100+48*7)\n\n    k_results_control = kcomp_control.compare(n_rep =n_rep, gap_len = [12, 24, 48, 48*7], var = list(hai.columns))\n    \n    return k_results_control\n\n\nfrom time import sleep\n\n\nk_results_control = get_control(n_rep)\n\n\nk_results_control\n\n\np = plot_compare(k_results_control, 'type', y = 'rmse', scale_domain=[\"Use Control\", \"No Control\"])\nsave_show_plot(p, \"use_control\")\np\n\n\nfrom functools import partial\n\n\nt = table_compare(k_results_control, 'type')\ntable_compare_latex(t, base_path_tbl / \"control.tex\", label=\"tbl:control\",\n                caption=\"\\\\CapControl\")\nt\n\n\n\nGap in Other variables\n\nmodels_gap_single = pd.DataFrame.from_records([\n    {'Gap':  'All variables', 'gap_single_var': False, 'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")},\n    {'Gap':  'Only one var',  'gap_single_var': True,  'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")},\n])\n\n\n@cache_disk(cache_dir / \"gap_single\")\ndef get_gap_single(n_rep):\n    kcomp_single = KalmanImpComparison(models_gap_single, hai, hai_era, block_len=130)\n\n    return kcomp_single.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nres_single = get_gap_single(n_rep)\n\n\np = plot_compare(res_single, \"Gap\", y = 'rmse', scale_domain=[\"Only one var\", \"All variables\"])\nsave_show_plot(p, \"gap_single_var\")\n\n\nt = table_compare(res_single, 'Gap')\ntable_compare_latex(t, base_path_tbl / \"gap_single_var.tex\", caption=\"\\\\CapGapSingle\", label=\"tbl:gap_single_var\")\nt\n\n\nres_singl_perc = res_single.groupby(['Gap', 'var', 'gap_len']).agg({'rmse_stand': 'mean'}).reset_index().pivot(columns = 'Gap', values='rmse_stand', index=['var', 'gap_len'])\n\n\npd.DataFrame({'Only one var': (res_singl_perc[\"All variables\"] - res_singl_perc[\"Only one var\"]) / res_singl_perc[\"All variables\"] * 100})\n\n\nres_singl_perc = res_single.groupby(['Gap', 'var']).agg({'rmse_stand': 'mean'}).reset_index().pivot(columns = 'Gap', values='rmse_stand', index=['var'])\n\n\npd.DataFrame({'Only one var': (res_singl_perc[\"All variables\"] - res_singl_perc[\"Only one var\"]) / res_singl_perc[\"All variables\"] * 100})\n\n\n\nGeneric vs Specialized\n\nmodels_generic = models_var.copy()\n\n\nmodels_generic.model = l_model(\"1_gap_varying_6-336_v3.pickle\") \nmodels_generic['type'] = 'Generic'\n\n\nmodels_generic\n\n\nmodels_var['type'] = 'Fine-tuned one var'\n\n\nmodels_gen_vs_spec = pd.concat([models_generic, models_var])\n\n\nmodels_gen_vs_spec\n\n\n@cache_disk(cache_dir / \"generic\")\ndef get_generic(n_rep=n_rep):\n    reset_seed()\n\n    comp_generic = KalmanImpComparison(models_gen_vs_spec, hai, hai_era, block_len=100+48*7)\n\n    return comp_generic.compare(n_rep =n_rep, gap_len = [12, 24, 48, 48*7], var = list(hai.columns))\nk_results_generic = get_generic(n_rep)\n\n\nplot_formatter.legend_symbol_size = 300\n\n\np = plot_compare(k_results_generic, 'type', y = 'rmse', scale_domain=[\"Fine-tuned one var\", \"Generic\"])\nsave_show_plot(p, \"generic\")\np\n\n\nt = table_compare(k_results_generic, 'type')\ntable_compare_latex(t, base_path_tbl / \"generic.tex\", label='tbl:generic', caption=\"\\\\CapGeneric\")\nt\n\n\nres_singl_perc = k_results_generic.groupby(['type', 'var']).agg({'rmse_stand': 'mean'}).reset_index().pivot(columns = 'type', values='rmse_stand', index=['var'])\n\n\n(res_singl_perc[\"Generic\"] - res_singl_perc[\"Fine-tuned one var\"]) / res_singl_perc[\"Generic\"] * 100\n\n\n\nTraining\n\nmodels_train = pd.DataFrame.from_records([\n    # {'Train':  'All variables',  'model': l_model(\"All_gap_all_30_v1.pickle\")  },\n    {'Train':  'Only one var',   'model': l_model(\"1_gap_varying_6-336_v3.pickle\")  },\n    {'Train':  'Multi vars',     'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")  },\n    {'Train':  'Random params',  'model': l_model(\"rand_all_varying_gap_varying_len_6-30_v4.pickle\")  }\n])\n\n\nmodels_train\n\n\n@cache_disk(cache_dir / \"train\")\ndef get_train(n_rep):\n    reset_seed()\n    kcomp = KalmanImpComparison(models_train, hai, hai_era, block_len=130)\n\n    return kcomp.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nres_train = get_train(n_rep)\n\n\nres_train_agg = res_train.groupby(['Train', 'gap_len']).agg({'rmse_stand': 'mean'}).reset_index()\n\n\nres_train_agg\n\n\np = plot_compare(res_train, \"Train\", y='rmse', scale_domain=[\"Multi vars\", \"Only one var\", \"Random params\"])\nsave_show_plot(p, \"train_compare\")\n\n\nt = table_compare3(res_train, 'Train')\ntable_compare3_latex(t, base_path_tbl / \"train.tex\", label=\"tbl:train_compare\", caption=\"\\\\CapTrain\")\nt"
  },
  {
    "objectID": "results/result_plots.html#extra-results",
    "href": "results/result_plots.html#extra-results",
    "title": "Plotting for results",
    "section": "Extra results",
    "text": "Extra results\n\nStandard deviations\n\nhai_std = hai.std().to_frame(name='std')\nhai_std.index.name = \"Variable\"\nhai_std = hai_std.reset_index().assign(unit=[f\"\\\\si{{{unit}}}\" for unit in units_big.values()])\n\n\nhai_std\n\n\nlatex = hai_std.style.hide(axis=\"index\").format(precision=3).to_latex(hrules=True, caption=\"\\\\CapStd\", label=\"tbl:hai_std\", position_float=\"centering\")\n\nwith open(base_path_tbl / \"hai_std.tex\", 'w') as f:\n    f.write(latex)\n\n\n\nGap distribution\n\nout_dir = here(\"../fluxnet/gap_stat\")\n\n\nsite_info = pl.read_parquet(out_dir / \"../site_info.parquet\").select([\n    pl.col(\"start\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"end\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"site\").cast(pl.Categorical).sort()\n])\n\n\ndef duration_n_obs(duration):\n    \"converts a duration into a n of fluxnet observations\"\n    return abs(int(duration.total_seconds() / (30 * 60)))\n\n\nfiles = out_dir.ls()\nfiles.sort() # need to sort to match the site_info\nsites = []\nfor i, path in enumerate(files):\n    sites.append(pl.scan_parquet(path).with_columns([\n        pl.lit(site_info[i, \"site\"]).alias(\"site\"),\n        pl.lit(duration_n_obs(site_info[i, \"start\"] -  site_info[i, \"end\"])).alias(\"total_obs\"),\n        pl.col(\"TIMESTAMP_END\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\").alias(\"end\"),\n    ]).drop(\"TIMESTAMP_END\"))\n\ngap_stat = pl.concat(sites)\n\n\npl.read_parquet(files[0])\n\n\ngap_stat.head().collect()\n\n\ndef plot_var_dist(var, small=False, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).with_column(pl.col(\"gap_len\") / (24 *2 * 7)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    if not small: ax.set_yscale('log')\n    ax.set_xlabel(\"gap length (weeks)\")\n    ax.set_ylabel(f\"{'Log' if not small else ''} n gaps\")\n    # plt.xscale('log') \n\n\nplot_var_dist('TA_F_QC')\n\n\ncolor_map = dict(zip(scale_meteo.domain, list(sns.color_palette('Set2', n_colors=len(hai.columns)).as_hex())))\n\n\nqc_map = {\n    'TA': 'TA_F_QC',\n    'SW_IN': 'SW_IN_F_QC',\n    'LW_IN': 'LW_IN_F_QC',\n    'VPD': 'VPD_F_QC',\n    'WS': 'WS_F_QC',\n    'PA': 'PA_F_QC',\n    'P': 'P_F_QC',\n    'TS': 'TS_F_MDS_1_QC',\n    'SWC': 'SWC_F_MDS_1_QC',\n}\n\n\ndef pl_in(col, values):\n    expr = False\n    for val in values:\n        expr |= pl.col(col) == val\n    return expr\n\n\ngap_stat.filter(pl_in('variable', qc_map.values())\n               ).with_columns([\n    pl.when(pl.col(\"gap_len\") &lt; 48*7).then(True).otherwise(False).alias(\"short\"),\n    pl.count().alias(\"total\"),\n    pl.count().alias(\"total len\"),\n]).groupby(\"short\").agg([\n    (pl.col(\"gap_len\").count() / pl.col(\"total\")).alias(\"frac_num\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total len\")).alias(\"frac_len\")\n]).collect()\n\n\ngap_stat.filter(pl_in('variable', qc_map.values())\n               ).with_columns([\n    pl.when(pl.col(\"gap_len\") &lt; 48*7).then(True).otherwise(False).alias(\"short\"),\n    pl.count().alias(\"total\"),\n    pl.count().alias(\"total len\"),\n]).groupby(\"short\").agg([\n    (pl.col(\"gap_len\").count() / pl.col(\"total\")).alias(\"frac_num\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total len\")).alias(\"frac_len\")\n]).collect()\n\n\nfrac_miss = gap_stat.filter(\n    pl_in('variable', qc_map.values())\n).groupby([\"site\", \"variable\"]).agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()).alias(\"frac_gap\")\n])\n\n\nfrac_miss.groupby('variable').agg([\n    pl.col(\"frac_gap\").max().alias(\"max\"),\n    pl.col(\"frac_gap\").min().alias(\"min\"),\n    pl.col(\"frac_gap\").std().alias(\"std\"),\n    pl.col(\"frac_gap\").mean().alias(\"mean\"),\n]).collect()\n\n\nfrac_miss.sort(\"frac_gap\", reverse=True).collect()\n\n\nsite_info.filter((pl.col(\"site\") == \"US-LWW\"))\n\n\ngap_stat.filter((pl.col(\"site\") == \"US-LWW\") & (pl.col(\"variable\") == \"LW_IN_F_QC\" )).collect()\n\n\nimport matplotlib\n\n\nmatplotlib.rcParams.update({'font.size': 22})\nsns.set_style(\"whitegrid\")\n\n\ndef plot_var_dist(var, ax=None, small=True):\n    if ax is None: ax = get_grid(1)[0]\n    \n    color = color_map[var]\n    var_qc = qc_map[var]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var_qc)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) if small else True \n    ).with_column(pl.col(\"gap_len\") / (2 if small else 48 * 7)\n                 ).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax, edgecolor=\"white\", color=color)\n    ax.set_title(f\"{var} - { 'gap length &lt;  1 week' if small else 'all gaps'}\")\n    ax.set_xlabel(f\"Gap length ({ 'hour' if small else 'week'})\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n    plt.tight_layout()\n\n\nvars = gap_stat.select(pl.col(\"variable\").unique()).collect()\n\n\nvars.filter(pl.col(\"variable\").str.contains(\"TA\"))\n\n\nfor ax, var in zip(get_grid(9,3,3, figsize=(15,12), sharey=False), list(var_type.categories)):\n    plot_var_dist(var, ax=ax)\nplt.savefig(base_path_img / \"gap_len_dist_small.pdf\")\n\n\nfor ax, var in zip(get_grid(9,3,3, figsize=(15,12), sharey=False), list(var_type.categories)):\n    plot_var_dist(var, ax=ax, small=False)\nplt.savefig(base_path_img / \"gap_len_dist.pdf\")\n\n\n\nSquare Root Filter\n\nNumerical Stability\n\nfrom meteo_imp.kalman.performance import *\n\n\nerr = cache_disk(cache_dir / \"fuzz_sr\")(fuzz_filter_SR)(100, 110) # this is already handling the random seed\n\n\np = plot_err_sr_filter(err)\nsave_show_plot(p, \"numerical_stability\")\n\n\n\nPerformance\n\n@cache_disk(cache_dir / \"perf_sr\")\ndef get_perf_sr():\n    reset_seed()\n    return perf_comb_params('filter', use_sr_filter=[True, False], rep=range(100),\n                           n_obs = 100,\n                           n_dim_obs=9,\n                           n_dim_state=18,\n                           n_dim_contr=14,\n                           p_missing=0,\n                           bs=20 ,\n                           init_method = 'local_slope'\n                           ) \n\n\nperf1 = (get_perf_sr()\n    .groupby('use_sr_filter')\n    .agg(pl.col(\"time\").mean())\n    .with_column(\n        pl.when(pl.col(\"use_sr_filter\"))\n        .then(pl.lit(\"Square Root Filter\"))\n        .otherwise(pl.lit(\"Standard Filter\"))\n        .alias(\"Filter type\")\n    ))\n\n\nperf1\n\n\n(perf1[0, 'time'] - perf1[1, 'time']) / perf1[1, 'time'] * 100\n\n\nplot_perf_sr = alt.Chart(perf1.to_pandas()).mark_bar(size = 50).encode(\n    x=alt.X('Filter type', axis=alt.Axis(labelAngle=0)),\n    y=alt.Y('time', scale=alt.Scale(zero=False), title=\"time [s]\"),\n    color=alt.Color('Filter type',\n                    scale = alt.Scale(scheme = 'accent'))\n).properties(width=300)\n\n\nsave_show_plot(plot_perf_sr, \"perf_sr\")"
  },
  {
    "objectID": "exploration/ERA.html",
    "href": "exploration/ERA.html",
    "title": "Meteo Imp Analsyis",
    "section": "",
    "text": "import pandas as pd\nfrom meteo_imp.data import *\n\n\nhai = pd.read_parquet(hai_big_path)\nhai_era = pd.read_parquet(hai_era_big_path)\n\n\npd.to_datetime('2000-04-01 00:30:00')\n\nTimestamp('2000-04-01 00:30:00')\n\n\n\napril = pd.date_range('2000-04-01 00:30:00', '2000-04-30 00:30:00')\n\n\nhai.loc[april]\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\nPA\nP\nWS\nLW_IN\n\n\n\n\n2000-04-01 00:30:00\n3.54\n0.0\n0.339\n94.89\n0.00\n3.54\n303.324\n\n\n2000-04-02 00:30:00\n3.05\n0.0\n0.964\n95.04\n0.00\n2.42\n283.856\n\n\n2000-04-03 00:30:00\n7.29\n0.0\n4.169\n94.63\n0.00\n2.91\n270.567\n\n\n2000-04-04 00:30:00\n10.52\n0.0\n4.751\n93.65\n0.00\n0.50\n290.341\n\n\n2000-04-05 00:30:00\n8.64\n0.0\n3.537\n94.10\n0.00\n3.03\n285.369\n\n\n2000-04-06 00:30:00\n-1.03\n0.0\n2.317\n96.10\n0.00\n3.49\n245.078\n\n\n2000-04-07 00:30:00\n3.10\n0.0\n1.738\n97.31\n0.00\n2.72\n252.078\n\n\n2000-04-08 00:30:00\n4.51\n0.0\n1.810\n97.11\n0.00\n1.91\n276.303\n\n\n2000-04-09 00:30:00\n5.09\n0.0\n2.824\n96.22\n0.00\n1.19\n306.269\n\n\n2000-04-10 00:30:00\n2.56\n0.0\n1.905\n95.73\n0.00\n3.33\n262.740\n\n\n2000-04-11 00:30:00\n3.57\n0.0\n2.115\n94.95\n0.00\n2.16\n288.538\n\n\n2000-04-12 00:30:00\n6.41\n0.0\n1.671\n93.78\n0.00\n2.32\n284.162\n\n\n2000-04-13 00:30:00\n3.28\n0.0\n2.211\n94.08\n0.00\n2.92\n253.648\n\n\n2000-04-14 00:30:00\n4.90\n0.0\n1.476\n94.28\n0.00\n4.18\n286.252\n\n\n2000-04-15 00:30:00\n5.52\n0.0\n1.807\n94.59\n0.14\n2.03\n275.038\n\n\n2000-04-16 00:30:00\n5.85\n0.0\n1.916\n94.60\n0.00\n3.44\n274.303\n\n\n2000-04-17 00:30:00\n10.27\n0.0\n6.081\n94.43\n0.00\n2.60\n310.984\n\n\n2000-04-18 00:30:00\n7.95\n0.0\n2.976\n95.35\n0.00\n3.02\n308.451\n\n\n2000-04-19 00:30:00\n7.98\n0.0\n0.533\n95.38\n0.14\n2.75\n349.121\n\n\n2000-04-20 00:30:00\n10.23\n0.0\n3.256\n95.95\n0.00\n3.79\n280.670\n\n\n2000-04-21 00:30:00\n13.03\n0.0\n5.223\n95.96\n0.00\n3.23\n298.943\n\n\n2000-04-22 00:30:00\n15.93\n0.0\n8.690\n96.23\n0.00\n1.56\n320.440\n\n\n2000-04-23 00:30:00\n20.81\n0.0\n16.393\n95.47\n0.00\n2.90\n313.058\n\n\n2000-04-24 00:30:00\n14.63\n0.0\n4.436\n95.20\n0.00\n2.75\n349.007\n\n\n2000-04-25 00:30:00\n6.73\n0.0\n0.905\n95.90\n0.00\n2.50\n316.985\n\n\n2000-04-26 00:30:00\n9.38\n0.0\n1.877\n95.85\n0.00\n2.44\n286.490\n\n\n2000-04-27 00:30:00\n18.74\n0.0\n11.533\n95.75\n0.00\n2.23\n309.960\n\n\n2000-04-28 00:30:00\n14.76\n0.0\n4.916\n95.36\n0.00\n2.71\n320.407\n\n\n2000-04-29 00:30:00\n15.12\n0.0\n3.507\n95.44\n0.00\n1.82\n331.168\n\n\n2000-04-30 00:30:00\n13.54\n0.0\n1.086\n96.32\n0.00\n2.07\n356.716\n\n\n\n\n\n\n\n\nhai[]\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\nPA\nP\nWS\nLW_IN\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n96.630\n0.0\n2.05\n302.475\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n96.580\n0.0\n2.53\n302.475\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n96.560\n0.0\n3.15\n301.677\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n96.560\n0.0\n3.12\n301.677\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n96.570\n0.0\n3.04\n301.677\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2012-12-31 22:00:00\n4.75\n0.0\n2.249\n95.212\n0.0\n4.28\n268.160\n\n\n2012-12-31 22:30:00\n4.48\n0.0\n2.154\n95.183\n0.0\n4.47\n278.250\n\n\n2012-12-31 23:00:00\n4.32\n0.0\n2.108\n95.164\n0.0\n3.73\n263.720\n\n\n2012-12-31 23:30:00\n4.02\n0.0\n1.996\n95.142\n0.0\n3.41\n269.440\n\n\n2013-01-01 00:00:00\n3.99\n0.0\n2.000\n95.106\n0.0\n3.96\n282.740\n\n\n\n\n227952 rows × 7 columns"
  },
  {
    "objectID": "exploration/pytorch performance.html",
    "href": "exploration/pytorch performance.html",
    "title": "Meteo Imp Analsyis",
    "section": "",
    "text": "the idea is to bechmark the performance of a loop similar to a kalman filter one and see if there are possibilities to improve the performance\ntorch jit is just useless …\n@torch.jit.script\ndef loop_mult_jit(A, B):\n    n = 0\n    while n&lt;1000:\n        A = A @ B @ A\n        n += 1\ndef loop_mult(A, B):\n    n = 0\n    while n&lt;1000:\n        A = A @ B @ A\n        n += 1\n%timeit loop_mult(A,A)\n\n67.2 ms ± 2.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n%timeit loop_mult_jit(A,A)\n\n64.8 ms ± 3.25 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "exploration/pytorch performance.html#pytorch-loop",
    "href": "exploration/pytorch performance.html#pytorch-loop",
    "title": "Meteo Imp Analsyis",
    "section": "pytorch loop",
    "text": "pytorch loop\n\nimport torch\n\n\nout = torch.zeros(10_000, 20)\nobs = torch.ones(10_000, 20)\n\n\ndef test_loop_torch(n=10_000):\n    out = torch.zeros(10_000, 20)\n    obs = torch.ones(10_000, 20)\n\n    m = torch.ones(1, 20)\n    for i in range(1, len(obs)):\n        out[i] = (out[i-1] + obs[i]) * .3\n    return out\n\n\n%timeit test_loop\n\n14.9 ns ± 0.177 ns per loop (mean ± std. dev. of 7 runs, 100,000,000 loops each)"
  },
  {
    "objectID": "exploration/pytorch performance.html#torch-jit",
    "href": "exploration/pytorch performance.html#torch-jit",
    "title": "Meteo Imp Analsyis",
    "section": "torch jit",
    "text": "torch jit\n\ntest_loop_jit = torch.jit.script(test_loop_torch)\n\n\n%timeit test_loop_jit\n\n14.9 ns ± 0.193 ns per loop (mean ± std. dev. of 7 runs, 100,000,000 loops each)\n\n\n\ntest_loop_torch()\n\ntensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.3000, 0.3000, 0.3000,  ..., 0.3000, 0.3000, 0.3000],\n        [0.3900, 0.3900, 0.3900,  ..., 0.3900, 0.3900, 0.3900],\n        ...,\n        [0.4286, 0.4286, 0.4286,  ..., 0.4286, 0.4286, 0.4286],\n        [0.4286, 0.4286, 0.4286,  ..., 0.4286, 0.4286, 0.4286],\n        [0.4286, 0.4286, 0.4286,  ..., 0.4286, 0.4286, 0.4286]])"
  },
  {
    "objectID": "exploration/pytorch performance.html#numpy-loop",
    "href": "exploration/pytorch performance.html#numpy-loop",
    "title": "Meteo Imp Analsyis",
    "section": "Numpy loop",
    "text": "Numpy loop\n\nimport numpy as np\n\n\ndef test_loop_np(n=10_000):\n    out = np.zeros((n, 20))\n    obs = torch.ones((n, 20))\n\n    for i in range(1, len(obs)):\n        out[i] = (out[i-1] + obs[i]) * .3\n    return out\n\n\n%timeit test_loop\n\n14.8 ns ± 0.0836 ns per loop (mean ± std. dev. of 7 runs, 100,000,000 loops each)"
  },
  {
    "objectID": "exploration/pytorch performance.html#jax",
    "href": "exploration/pytorch performance.html#jax",
    "title": "Meteo Imp Analsyis",
    "section": "Jax",
    "text": "Jax\n\nimport jax.numpy as jnp\nfrom jax import grad, jit\n\n\ndef test_loop_jax_1(n=10_000):\n    out = jnp.zeros((10_000, 20))\n    obs = jnp.ones((10_000, 20))\n\n    for i in range(1, len(obs)):\n        out.at[i, :].set((out[i-1] + obs[i]) * .3)\n    return out\n\n\n%timeit test_loop_jax_1()\n\n16.8 s ± 2.3 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\ndef test_loop_jax_2(n=10_000):\n    out = np.zeros((10_000, 20))\n    obs = jnp.ones((10_000, 20))\n\n    m = jnp.ones((1, 20))\n    for i in range(1, len(obs)):\n        out[i, :] = (out[i-1] + obs[i]) * 1.01\n    return out\n\n\n%timeit test_loop_jax_2()\n\n4.36 s ± 235 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nJulia comparison\n\nimport torch\n\n\ndef loop_add(A, B, max=100):\n    for _ in range(int(max)):\n        A = A + B\n    return A\n\n\n%timeit loop_add(1,2, max=1e7)\n\n428 ms ± 22.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%timeit loop_add(torch.tensor(1),torch.tensor(2), max=1e7)\n\n22.6 s ± 2.13 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nA = torch.rand(100,100, dtype=torch.float64)\nB = torch.rand(100,100, dtype=torch.float64)\n\n\n%timeit loop_add(A, B, max=1e5)\n\n686 ms ± 33.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "exploration/Untitled.html",
    "href": "exploration/Untitled.html",
    "title": "Meteo Imp Analsyis",
    "section": "",
    "text": "import altair\n\n\nimport altair as alt\nimport vegafusion as vf\nimport pandas as pd\nfrom itertools import product\nfrom itertools import zip_longest\nfrom fastcore.basics import listify\n\n\nvf.enable()\n\nvegafusion.enable(mimetype='html', row_limit=10000, embed_options=None)\n\n\n\nvf.__version__\n\n'1.0.3'\n\n\n\nalt.__version__\n\n'4.2.2'\n\n\n\nalt.__file__\n\n'/home/simone/anaconda3/envs/vegafusion/lib/python3.11/site-packages/altair/__init__.py'\n\n\n\nvf.__file__\n\n'/home/simone/anaconda3/envs/vegafusion/lib/python3.11/site-packages/vegafusion/__init__.py'\n\n\n\ntest_data = pd.DataFrame(list(product(['0','1'], ['a', 'b'])), columns = ['row', 'col'])\ntest_data['text'] = test_data.row + test_data.col\n\n\ndef test_plot(data, *args): return alt.Chart(data).mark_text().encode(text='text')\n\n\ntest_plot(test_data)\n\n\n\n\n\n\n\naltair.__version__\n\n'4.2.2'\n\n\n::: {#8a245735-22a1-49a3-bdfb-8fd7626fc399 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef facet_wrap(data: pd.DataFrame, # full dataset\n               plot_fn,# function that makes the plot, takes 2 arguments: data and y_label\n               col: str, # column to facet\n               y_labels: list[str]|None =None, # custom labels y axis \n               n_cols=3,\n               y_resolve='independent'\n              ):\n    col_vals = data[col].unique()\n    plot_list = [alt.hconcat() for _ in range(0, len(col_vals), n_cols)]\n    for i, col_v in enumerate(col_vals):\n        plot = plot_fn(data[data[col]==col_v].copy(),\n                       y_labels[i] if y_labels is not None else col_v\n                      ).properties(title=str(col_v))\n        plot_list[i // n_cols] |= plot\n    return alt.vconcat(*plot_list)#.resolve_scale(\n        #y=y_resolve\n    #)\n    \n:::\n::: {#628a9ef0-28ed-4a19-ada3-94dd55ff2f28 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef facet_grid(data: pd.DataFrame, # full dataset\n               plot_fn,# function that makes the plot, takes 2 arguments: data and y_label\n               col: str, # column to facet,\n               row: str,\n               y_labels: list[str]|None = None, # custom labels y axis\n              ):\n    row_vals = data[row].unique()\n    n_cols = len(data[col].unique())\n    plots = []\n    for row_val, y_label in zip_longest(row_vals, listify(y_labels)):\n        plot = facet_wrap(data[data[row]==row_val].copy(), plot_fn, col, y_label, n_cols=n_cols).properties(title=row_val)\n        plots.append(plot)\n    return alt.vconcat(*plots)    \n:::\n\nfrom itertools import product\n\n\ntest_data = pd.DataFrame(list(product(['0','1'], ['a', 'b'])), columns = ['row', 'col'])\ntest_data['text'] = test_data.row + test_data.col\n\n\ndef test_plot(data, *args): return alt.Chart(data).mark_text().encode(text='text')\n\n\nfacet_wrap(test_data, test_plot, col = 'row')\n\n\n\n\n\n\n\nfacet_grid(test_data, test_plot, col = 'col', row='row')"
  },
  {
    "objectID": "exploration/performance_julia.html",
    "href": "exploration/performance_julia.html",
    "title": "Meteo Imp Analsyis",
    "section": "",
    "text": "using BenchmarkTools\n\n\nfunction loop_add(A, B, max=100)\n    for _ = 1:max\n        A = A + B\n    end\n    return A\nend\n\nloop_add (generic function with 2 methods)\n\n\n\nloop_add(1,2)\n\n201\n\n\n\n@code_lowered loop_add(1,2)\n\n\nCodeInfo(\n1 ─ %1 = (#self#)(A, B, 100)\n└──      return %1\n)\n\n\n\n\n@benchmark loop_add(1.,.7, 1e7)\n\n\nBenchmarkTools.Trial: 333 samples with 1 evaluation.\n Range (min … max):  10.129 ms … 22.728 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     15.112 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   14.990 ms ±  1.617 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n                           ▁ ▁  ▃▁ ▁▃▁▄█▄▁▆▃▄ ▄▅▃▄▂▃▃          \n  ▄▁▃▁▁▄▁▃▁▃▁▅▄▄▁▆▃▅▆█▆▆▆▄▄█▇██▆██▆██████████▆███████▇▆▆▇▅▅▆▄ ▅\n  10.1 ms         Histogram: frequency by time        17.8 ms &lt;\n Memory estimate: 0 bytes, allocs estimate: 0.\n\n\n\n\n@benchmark loop_mult(1.,.7, 1e5)\n\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):   89.096 μs … 396.304 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     129.356 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   138.852 μs ±  46.353 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  █ ▂                                                            \n  █▆█▅▅▃▂▂▃▃▄▂▄▅▂▂▃▂▅▄▂▄▂▃▃▂▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁ ▂\n  89.1 μs          Histogram: frequency by time          272 μs &lt;\n Memory estimate: 0 bytes, allocs estimate: 0.\n\n\n\nThe benchmark seems is working\n\nusing Distributions\n\n\nA = rand(Uniform(0,1), 100, 100)\nB = rand(Uniform(0,1), 100, 100);\n\n\ntypeof(A)\n\n\nMatrix{Float64} (alias for Array{Float64, 2})\n\n\n\n\n@benchmark loop_add(A,B, 1e5)\n\n\nBenchmarkTools.Trial: 5 samples with 1 evaluation.\n Range (min … max):  1.025 s …   1.056 s  ┊ GC (min … max): 17.27% … 17.53%\n Time  (median):     1.032 s              ┊ GC (median):    17.43%\n Time  (mean ± σ):   1.038 s ± 13.007 ms  ┊ GC (mean ± σ):  17.34% ±  0.15%\n  █     █    █                           █                █  \n  █▁▁▁▁▁█▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  1.03 s         Histogram: frequency by time        1.06 s &lt;\n Memory estimate: 7.46 GiB, allocs estimate: 200000."
  },
  {
    "objectID": "fluxnet/analyze_gaps_fluxnet.html",
    "href": "fluxnet/analyze_gaps_fluxnet.html",
    "title": "Analyze gaps fluxnet",
    "section": "",
    "text": "from IPython.display import display\nfrom ipywidgets import widgets, interact\nfrom pathlib import Path\nimport polars as pl\nfrom datetime import datetime\nfrom fastcore.utils import * # support of ls for paths\nimport matplotlib.pyplot as plt\nimport altair as alt\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\nsite_info = pl.read_parquet(out_dir / \"../site_info.parquet\").select([\n    pl.col(\"start\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"end\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"site\").cast(pl.Categorical).sort()\n])\nsite_info.head()\n\n\n\nshape: (5, 3)\n\n\n\n\nstart\n\n\nend\n\n\nsite\n\n\n\n\ndatetime[μs]\n\n\ndatetime[μs]\n\n\ncat\n\n\n\n\n\n\n2009-01-01 00:30:00\n\n\n2012-01-01 00:00:00\n\n\n\"AR-SLu\"\n\n\n\n\n2009-01-01 00:30:00\n\n\n2013-01-01 00:00:00\n\n\n\"AR-Vir\"\n\n\n\n\n2002-01-01 00:30:00\n\n\n2013-01-01 00:00:00\n\n\n\"AT-Neu\"\n\n\n\n\n2007-01-01 00:30:00\n\n\n2010-01-01 00:00:00\n\n\n\"AU-Ade\"\n\n\n\n\n2010-01-01 00:30:00\n\n\n2015-01-01 00:00:00\n\n\n\"AU-ASM\"\ndef duration_n_obs(duration):\n    \"converts a duration into a n of fluxnet observations\"\n    return abs(int(duration.total_seconds() / (30 * 60)))\nduration_n_obs(site_info[1, \"start\"] - site_info[1, \"end\"])\n\n70127\nduration_n_obs(site_info[0, \"start\"] - site_info[0, \"end\"])\n\n52559\nsite_info.select((pl.col(\"end\")-pl.col(\"start\")).dt.minutes() // 30).sum()\n\n\n\nshape: (1, 1)\n\n\n\n\nend\n\n\n\n\ni64\n\n\n\n\n\n\n26175287\nsp = site_info.to_pandas()\n((sp.end - sp.start).dt.total_seconds() / (30*60)).astype(int).sum()\n\n26175287\n# maybe this code should actually go in 20_gap_finding\nfiles = out_dir.ls()\nfiles.sort() # need to sort to match the site_info\nsites = []\nfor i, path in enumerate(files):\n    sites.append(pl.scan_parquet(path).with_columns([\n        pl.lit(site_info[i, \"site\"]).alias(\"site\"),\n        pl.lit(duration_n_obs(site_info[i, \"start\"] -  site_info[i, \"end\"])).alias(\"total_obs\"),\n        pl.col(\"TIMESTAMP_END\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\").alias(\"end\"),\n    ]).drop(\"TIMESTAMP_END\"))\ngap_stat = pl.concat(sites)\ngap_stat.head().fetch(5)\n\n\n\nshape: (5, 5)\n\n\n\n\ngap_len\n\n\nvariable\n\n\nsite\n\n\ntotal_obs\n\n\nend\n\n\n\n\nu32\n\n\nstr\n\n\nstr\n\n\ni32\n\n\ndatetime[μs]\n\n\n\n\n\n\n16992\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2009-01-01 00:30:00\n\n\n\n\n5\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2009-12-21 11:00:00\n\n\n\n\n1\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2009-12-21 17:00:00\n\n\n\n\n1\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2010-01-06 13:00:00\n\n\n\n\n3\n\n\n\"TA_F_MDS_QC\"\n\n\n\"AR-SLu\"\n\n\n52559\n\n\n2010-01-07 13:00:00\ndef filter_variables(variables = [\"TA_F_QC\", \"SW_IN_QC\", \"LW_IN_QC\", \"VPD_F_QC\"]):\n    expr = False\n    for var in variables:\n        expr |= pl.col(\"variable\") == var\n    return expr\ndef pl_in(col, values):\n    expr = False\n    for val in values:\n        expr |= pl.col(col) == val\n    return expr\nsome sites have a lot of data missing, with the avg gap length of several years, so is seems that the year can have an impact\nImportant! here the 3 possibles gap value of a QC variable are considered as one (null, 1, 2) we should co\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).groupby(\"site\").agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()).alias(\"frac_gap\")\n]).collect().describe()\n\n\n\nshape: (7, 4)\n\n\n\n\ndescribe\n\n\nsite\n\n\nmean\n\n\nfrac_gap\n\n\n\n\nstr\n\n\nstr\n\n\nf64\n\n\nf64\n\n\n\n\n\n\n\"count\"\n\n\n\"205\"\n\n\n205.0\n\n\n205.0\n\n\n\n\n\"null_count\"\n\n\n\"0\"\n\n\n0.0\n\n\n0.0\n\n\n\n\n\"mean\"\n\n\nnull\n\n\n1327.107552\n\n\n0.198846\n\n\n\n\n\"std\"\n\n\nnull\n\n\n4421.109085\n\n\n0.256043\n\n\n\n\n\"min\"\n\n\n\"AR-SLu\"\n\n\n1.0\n\n\n0.000011\n\n\n\n\n\"max\"\n\n\n\"ZM-Mon\"\n\n\n52608.0\n\n\n2.299027\n\n\n\n\n\"median\"\n\n\nnull\n\n\n214.365854\n\n\n0.141903"
  },
  {
    "objectID": "fluxnet/analyze_gaps_fluxnet.html#interactive-histograms",
    "href": "fluxnet/analyze_gaps_fluxnet.html#interactive-histograms",
    "title": "Analyze gaps fluxnet",
    "section": "Interactive histograms",
    "text": "Interactive histograms\n\nall_vars = gap_stat.select(pl.col(\"variable\").unique().sort()).collect()[\"variable\"]\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var):\n    ta_gaps = gap_stat.filter(\n        pl.col(\"variable\") == var \n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n\n\n\nall_sites = gap_stat.select(pl.col(\"site\").unique().sort()).collect()[\"site\"]\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site, small=False):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.title(f\"{site}: {var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n\n\n\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var, small=False):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).collect().to_pandas().hist(\"gap_len\", bins=30)\n    plt.title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    plt.yscale('log')\n    plt.xscale('log') \n\n\n\n\n\nfrom fastai.vision.data import get_grid\nfrom pyprojroot import here\n\n\ndef plot_var_dist(var, small=False, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).with_column(pl.col(\"gap_len\") / (24 *2 * 7)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    if not small: ax.set_yscale('log')\n    ax.set_xlabel(\"gap length (weeks)\")\n    ax.set_ylabel(f\"{'Log' if not small else ''} n gaps\")\n    # plt.xscale('log') \n\n\nfor ax, var in zip(get_grid(3,1,3, figsize=(15,8), sharey=True), ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC']):\n    plot_var_dist(var, ax=ax)\nplt.savefig(here(\"analysis/presentations/plots_18_jan/gap_len_dist.png\", warn=False))\n\n\n\n\n\n\n\n\n\ndef plot_var_dist_small(var, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) \n    ).with_column(pl.col(\"gap_len\") / (24 *2)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - gap len &lt; 1 week\")\n    ax.set_xlabel(\"gap length (days)\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n\n\nfor ax, var in zip(get_grid(3,1,3, figsize=(15,8), sharey=True), ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC']):\n    plot_var_dist_small(var, ax=ax)\nplt.savefig(here(\"analysis/presentations/plots_18_jan/gap_len_dist_small.png\", warn=False))\n\n\n\n\n\n\n\n\n\ndef plot_var_dist_cum(var, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    \n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) \n    ).collect() #.to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    bins = pl.cut(ta_gaps[\"gap_len\"], bins = pl.arange(0, 24 * 2 * 7, (24 * 2 * 7) // 50, eager=True))\n    return ta_gaps\n    ax.set_title(f\"{var} - gap len &lt; 1 week\")\n    ax.set_xlabel(\"gap length (days)\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log')"
  },
  {
    "objectID": "fluxnet/analyze_gaps_fluxnet.html#difference-sites",
    "href": "fluxnet/analyze_gaps_fluxnet.html#difference-sites",
    "title": "Analyze gaps fluxnet",
    "section": "Difference sites",
    "text": "Difference sites\n\nvar_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == 'TA_F_QC')\n    ).filter(\n        pl.col(\"gap_len\") &lt; 20000\n    ).sort(pl.col(\"gap_len\")\n        \n    ).collect().to_pandas()\n\n\nalt.data_transformers.enable('data_server')\n\n\n \nalt.Chart(var_gaps).mark_boxplot().encode(\n    y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n    x='gap_len'\n)\n\n\nwidgets.IntSlider?\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var, max_len=widgets.IntSlider(1000, 100, 20_000, 100)):\n    var_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; max_len\n    ).collect().to_pandas()\n    \n    display(alt.Chart(var_gaps).mark_boxplot().encode(\n        y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n        x='gap_len'\n    ))\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(pl.col(\"gap_len\").sum()).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).groupby(\"variable\").agg(pl.col(\"gap_len\").mean()).collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).filter(pl.col(\"variable\") == \"TA_F_QC\").collect()"
  }
]