[
  {
    "objectID": "Init_parameters_effect.html",
    "href": "Init_parameters_effect.html",
    "title": "Init Lambda",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\nfrom meteo_imp.results import _display_as_row\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\nfrom IPython.display import HTML\n\n\ndef reset_seed():\n    torch.manual_seed(27);\n    np.random.seed(27);\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\ncache = True\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n302.475\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n\n\n...\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n330.202\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n330.202\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n330.202\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n\n\n\n\n200 rows × 4 columns\n\n\n\n\nreset_seed()\ndata = GPFADataTest(hai[:150]).add_random_missing()\n\n\nimp = GPFAImputationExplorer(hai[:20], latent_dims=2)\n\n\nimp\n\nGPFA Imputation Explorer:\n    N obs: 20\n    N features 4 (TA, SW_IN, LW_IN, VPD)\n    N missing observations 0\n    N latent: 2\n\n\n\nimp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.ones(4,2))\n\nimp.fit()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nGPFA Imputation Explorer:\n    N obs: 20\n    N features 4 (TA, SW_IN, LW_IN, VPD)\n    N missing observations 0\n    N latent: 2\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n\npca = PCA(2).fit(data.data_complete)\n\n\nPCA(2).fit(data.data_complete).components_\n\narray([[ 2.64510596e-03,  9.72190612e-01, -2.34163954e-01,\n         2.37925628e-03],\n       [-8.47181130e-03, -2.34134422e-01, -9.72167258e-01,\n        -3.50525961e-04]])\n\n\n\nimp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.tensor(PCA(2).fit(data.data_complete).components_, dtype=torch.float).T)\n\n\nimp.fit()\n\n\n\n\nGPFA Imputation Explorer:\n    N obs: 20\n    N features 4 (TA, SW_IN, LW_IN, VPD)\n    N missing observations 0\n    N latent: 2\n\n\n\n\n\n\ncache = here() / \".cache\" / \"hai_test_init_values.pickle\"\n# cache.unlink()\n\n\n@cache_disk(cache)\ndef compute():\n    out = {}\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    out[\"normal\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.ones(4,2))\n    out[\"ones\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.zeros(4,2))\n    out[\"zeros\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.rand(4,2))\n    out[\"rand1\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.rand(4,2))\n    out[\"rand2\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.rand(4,2))\n    out[\"rand3\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.tensor(PCA(2).fit(data.data_complete).components_, dtype=torch.float).T)\n    out[\"pca\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    return out\n\n\nresults = compute()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor key, result in results.items():\n    display(HTML(f\"&lt;h4&gt;{key}&lt;/h4&gt;\"))\n    _display_as_row({'r2': result.r2(), **result.model_info})\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1858\n\n\nLW_IN\n0.9682\n\n\nVPD\n0.5829\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.5849\n0.8590\n\n\nSW_IN\n0.3494\n-0.0086\n\n\nLW_IN\n-0.6155\n0.4553\n\n\nVPD\n0.5600\n0.5320\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.3226\n\n\nz1\n7.1896\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.7995\n\n\nLW_IN\n0.0192\n\n\nVPD\n0.4386\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0053\n\n\n\n\n\n \n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6200\n\n\nSW_IN\n0.1293\n\n\nLW_IN\n0.0110\n\n\nVPD\n0.9652\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.5621\n0.5621\n\n\nSW_IN\n0.2967\n0.2967\n\n\nLW_IN\n-0.1298\n-0.1298\n\n\nVPD\n0.7420\n0.7420\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.5764\n\n\nz1\n5.5764\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4161\n\n\nSW_IN\n0.8069\n\n\nLW_IN\n0.9339\n\n\nVPD\n0.0106\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0226\n\n\n\n\n\n \n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-0.0176\n\n\nSW_IN\n-0.0140\n\n\nLW_IN\n-0.0032\n\n\nVPD\n-0.0225\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.0000\n0.0000\n\n\nSW_IN\n0.0000\n0.0000\n\n\nLW_IN\n0.0000\n0.0000\n\n\nVPD\n0.0000\n0.0000\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n0.6931\n\n\nz1\n0.6931\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4942\n\n\nSW_IN\n0.4942\n\n\nLW_IN\n0.4942\n\n\nVPD\n0.4942\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.4943\n\n\n\n\n\n \n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9916\n\n\nSW_IN\n0.9688\n\n\nLW_IN\n0.1685\n\n\nVPD\n0.6162\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.1113\n0.8822\n\n\nSW_IN\n-1.2775\n0.4726\n\n\nLW_IN\n0.5127\n-0.0920\n\n\nVPD\n-0.2842\n0.7279\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.2701\n\n\nz1\n6.5753\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0011\n\n\nSW_IN\n0.0177\n\n\nLW_IN\n0.8398\n\n\nVPD\n0.4049\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n \n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1856\n\n\nLW_IN\n0.9680\n\n\nVPD\n0.5827\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.9612\n0.3015\n\n\nSW_IN\n0.0887\n0.3397\n\n\nLW_IN\n0.2557\n-0.7323\n\n\nVPD\n0.6497\n0.3773\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.0300\n\n\nz1\n7.5588\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.8078\n\n\nLW_IN\n0.0191\n\n\nVPD\n0.4389\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n \n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.1840\n\n\nLW_IN\n0.9684\n\n\nVPD\n0.5822\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.2080\n0.9447\n\n\nSW_IN\n-0.2592\n0.2189\n\n\nLW_IN\n0.7847\n-0.0903\n\n\nVPD\n-0.0126\n0.7127\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.8303\n\n\nz1\n6.8435\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.7976\n\n\nLW_IN\n0.0190\n\n\nVPD\n0.4412\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n \n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.1845\n\n\nLW_IN\n0.9683\n\n\nVPD\n0.5823\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.9431\n-0.1357\n\n\nSW_IN\n0.2026\n0.2780\n\n\nLW_IN\n-0.0436\n-0.7956\n\n\nVPD\n0.7012\n0.0679\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.8095\n\n\nz1\n7.8881\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0008\n\n\nSW_IN\n0.7966\n\n\nLW_IN\n0.0195\n\n\nVPD\n0.4391\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0052\n\n\n\n\n\n \n\n\n\n\n\n\nfor key, result in results.items():\n    display(HTML(f\"&lt;h4&gt;{key}&lt;/h4&gt;\"))\n    result.display_results()\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1858\n\n\nLW_IN\n0.9682\n\n\nVPD\n0.5829\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0863\n°C\n\n\nSW_IN\n33.5182\nW m-2\n\n\nLW_IN\n3.3613\nW m-2\n\n\nVPD\n0.1651\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9819\n\n\nSW_IN\n0.1773\n\n\nLW_IN\n0.9495\n\n\nVPD\n0.5814\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1205\n°C\n\n\nSW_IN\n43.8162\nW m-2\n\n\nLW_IN\n4.6838\nW m-2\n\n\nVPD\n0.1803\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.5849\n0.8590\n\n\nSW_IN\n0.3494\n-0.0086\n\n\nLW_IN\n-0.6155\n0.4553\n\n\nVPD\n0.5600\n0.5320\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.3226\n\n\nz1\n7.1896\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.7995\n\n\nLW_IN\n0.0192\n\n\nVPD\n0.4386\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0053\n\n\n\n\n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6200\n\n\nSW_IN\n0.1293\n\n\nLW_IN\n0.0110\n\n\nVPD\n0.9652\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.5693\n°C\n\n\nSW_IN\n34.6616\nW m-2\n\n\nLW_IN\n18.7311\nW m-2\n\n\nVPD\n0.0477\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6607\n\n\nSW_IN\n0.0861\n\n\nLW_IN\n-0.0338\n\n\nVPD\n0.9568\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.5220\n°C\n\n\nSW_IN\n46.1813\nW m-2\n\n\nLW_IN\n21.2015\nW m-2\n\n\nVPD\n0.0579\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.5621\n0.5621\n\n\nSW_IN\n0.2967\n0.2967\n\n\nLW_IN\n-0.1298\n-0.1298\n\n\nVPD\n0.7420\n0.7420\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.5764\n\n\nz1\n5.5764\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4161\n\n\nSW_IN\n0.8069\n\n\nLW_IN\n0.9339\n\n\nVPD\n0.0106\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0226\n\n\n\n\n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-0.0176\n\n\nSW_IN\n-0.0140\n\n\nLW_IN\n-0.0032\n\n\nVPD\n-0.0225\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.9315\n°C\n\n\nSW_IN\n37.4040\nW m-2\n\n\nLW_IN\n18.8651\nW m-2\n\n\nVPD\n0.2585\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-0.1121\n\n\nSW_IN\n-0.0709\n\n\nLW_IN\n-0.0378\n\n\nVPD\n-0.1479\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.9450\n°C\n\n\nSW_IN\n49.9906\nW m-2\n\n\nLW_IN\n21.2424\nW m-2\n\n\nVPD\n0.2985\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.0000\n0.0000\n\n\nSW_IN\n0.0000\n0.0000\n\n\nLW_IN\n0.0000\n0.0000\n\n\nVPD\n0.0000\n0.0000\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n0.6931\n\n\nz1\n0.6931\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4942\n\n\nSW_IN\n0.4942\n\n\nLW_IN\n0.4942\n\n\nVPD\n0.4942\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.4943\n\n\n\n\n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9916\n\n\nSW_IN\n0.9688\n\n\nLW_IN\n0.1685\n\n\nVPD\n0.6162\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0847\n°C\n\n\nSW_IN\n6.5572\nW m-2\n\n\nLW_IN\n17.1756\nW m-2\n\n\nVPD\n0.1584\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9825\n\n\nSW_IN\n0.9670\n\n\nLW_IN\n0.2287\n\n\nVPD\n0.6275\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1185\n°C\n\n\nSW_IN\n8.7722\nW m-2\n\n\nLW_IN\n18.3136\nW m-2\n\n\nVPD\n0.1700\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.1113\n0.8822\n\n\nSW_IN\n-1.2775\n0.4726\n\n\nLW_IN\n0.5127\n-0.0920\n\n\nVPD\n-0.2842\n0.7279\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.2701\n\n\nz1\n6.5753\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0011\n\n\nSW_IN\n0.0177\n\n\nLW_IN\n0.8398\n\n\nVPD\n0.4049\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1856\n\n\nLW_IN\n0.9680\n\n\nVPD\n0.5827\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0859\n°C\n\n\nSW_IN\n33.5210\nW m-2\n\n\nLW_IN\n3.3695\nW m-2\n\n\nVPD\n0.1651\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9820\n\n\nSW_IN\n0.1771\n\n\nLW_IN\n0.9495\n\n\nVPD\n0.5812\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1203\n°C\n\n\nSW_IN\n43.8197\nW m-2\n\n\nLW_IN\n4.6882\nW m-2\n\n\nVPD\n0.1803\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.9612\n0.3015\n\n\nSW_IN\n0.0887\n0.3397\n\n\nLW_IN\n0.2557\n-0.7323\n\n\nVPD\n0.6497\n0.3773\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.0300\n\n\nz1\n7.5588\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.8078\n\n\nLW_IN\n0.0191\n\n\nVPD\n0.4389\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.1840\n\n\nLW_IN\n0.9684\n\n\nVPD\n0.5822\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0852\n°C\n\n\nSW_IN\n33.5551\nW m-2\n\n\nLW_IN\n3.3494\nW m-2\n\n\nVPD\n0.1652\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9822\n\n\nSW_IN\n0.1742\n\n\nLW_IN\n0.9509\n\n\nVPD\n0.5801\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1194\n°C\n\n\nSW_IN\n43.8981\nW m-2\n\n\nLW_IN\n4.6204\nW m-2\n\n\nVPD\n0.1805\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.2080\n0.9447\n\n\nSW_IN\n-0.2592\n0.2189\n\n\nLW_IN\n0.7847\n-0.0903\n\n\nVPD\n-0.0126\n0.7127\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.8303\n\n\nz1\n6.8435\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.7976\n\n\nLW_IN\n0.0190\n\n\nVPD\n0.4412\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n \n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.1845\n\n\nLW_IN\n0.9683\n\n\nVPD\n0.5823\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0852\n°C\n\n\nSW_IN\n33.5447\nW m-2\n\n\nLW_IN\n3.3548\nW m-2\n\n\nVPD\n0.1652\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9822\n\n\nSW_IN\n0.1750\n\n\nLW_IN\n0.9508\n\n\nVPD\n0.5801\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1195\n°C\n\n\nSW_IN\n43.8775\nW m-2\n\n\nLW_IN\n4.6273\nW m-2\n\n\nVPD\n0.1805\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.9431\n-0.1357\n\n\nSW_IN\n0.2026\n0.2780\n\n\nLW_IN\n-0.0436\n-0.7956\n\n\nVPD\n0.7012\n0.0679\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.8095\n\n\nz1\n7.8881\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0008\n\n\nSW_IN\n0.7966\n\n\nLW_IN\n0.0195\n\n\nVPD\n0.4391\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0052"
  },
  {
    "objectID": "Log transform - Multi latent - Imputation GPFA - Hainich.html",
    "href": "Log transform - Multi latent - Imputation GPFA - Hainich.html",
    "title": "Log Transform",
    "section": "",
    "text": "Trying to lo transform\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import *\n\n\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'log_SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'log_VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n1.636\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).log_transform(['SW_IN', 'VPD']).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).log_transform(['SW_IN', 'VPD']).add_gap(15, units.keys())\n\n\ndata_r_gaps.data\n\n\n\n\n\n\n\n\nTA\nlog_SW_IN\nlog_VPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.200489\n\n\n2000-01-01 01:00:00\nNaN\nNaN\nNaN\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.086178\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.104360\n\n\n2000-01-01 02:30:00\nNaN\n0.0\n0.097127\n\n\n...\n...\n...\n...\n\n\n2000-01-04 01:00:00\nNaN\nNaN\nNaN\n\n\n2000-01-04 01:30:00\n2.13\nNaN\n0.530628\n\n\n2000-01-04 02:00:00\n2.10\n0.0\n0.594983\n\n\n2000-01-04 02:30:00\n2.19\n0.0\n0.668854\n\n\n2000-01-04 03:00:00\n2.27\n0.0\nNaN\n\n\n\n\n150 rows × 3 columns\n\n\n\n\ncache_file_gaps = cache_path / \"hai_lo_transform.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\n@cache_disk(cache_file_gaps)\ndef compute_multiple_latent():\n    hai_r_gaps = [GPFAImputationExplorer(\n        data_r_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_r_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    hai_c_gaps = [GPFAImputationExplorer(\n        data_c_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_c_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    return hai_r_gaps, hai_c_gaps # cc\n\n\nhai_r_gaps, hai_c_gaps = compute_multiple_latent()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhai_r_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9929\n\n\nlog_SW_IN\n0.9888\n\n\nlog_VPD\n0.9838\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0778\n°C\n\n\nlog_SW_IN\n0.1859\nW m-2\n\n\nlog_VPD\n0.0244\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9858\n\n\nlog_SW_IN\n0.9867\n\n\nlog_VPD\n0.9841\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1064\n°C\n\n\nlog_SW_IN\n0.2308\nW m-2\n\n\nlog_VPD\n0.0255\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\nTA\n-0.0231\n0.1967\n0.9051\n\n\nlog_SW_IN\n0.8899\n0.3733\n0.0379\n\n\nlog_VPD\n-0.1494\n0.7971\n0.5105\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n4.2271\n\n\nz1\n2.9433\n\n\nz2\n6.6462\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0013\n\n\nlog_SW_IN\n0.0102\n\n\nlog_VPD\n0.0141\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0041\n\n\n\n\n\n \n\n\n\nhai_r_gaps[1].units = units\nhai_r_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9919\n\n\nlog_SW_IN\n0.9906\n\n\nlog_VPD\n0.5973\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0832\n°C\n\n\nlog_SW_IN\n0.1700\nW m-2\n\n\nlog_VPD\n0.1216\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9844\n\n\nlog_SW_IN\n0.9890\n\n\nlog_VPD\n0.6195\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1115\n°C\n\n\nlog_SW_IN\n0.2099\nW m-2\n\n\nlog_VPD\n0.1251\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.0841\n-0.9069\n\n\nlog_SW_IN\n0.9343\n0.0371\n\n\nlog_VPD\n0.2455\n-0.6557\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n3.7979\n\n\nz1\n6.6354\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0013\n\n\nlog_SW_IN\n0.0098\n\n\nlog_VPD\n0.4257\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0053\n\n\n\n\n\n \n\n\n\nhai_r_gaps[0].units = units\nhai_r_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9917\n\n\nlog_SW_IN\n-0.0027\n\n\nlog_VPD\n0.5397\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0844\n°C\n\n\nlog_SW_IN\n1.7565\nW m-2\n\n\nlog_VPD\n0.1300\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9834\n\n\nlog_SW_IN\n-0.0389\n\n\nlog_VPD\n0.5384\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1151\n°C\n\n\nlog_SW_IN\n2.0372\nW m-2\n\n\nlog_VPD\n0.1378\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\n\n\n\n\nTA\n0.9099\n\n\nlog_SW_IN\n0.0142\n\n\nlog_VPD\n0.6696\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.5268\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0021\n\n\nlog_SW_IN\n0.9885\n\n\nlog_VPD\n0.4629\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0043\n\n\n\n\n\n \n\n\n\n\n\n\nhai_c_gaps[2].units = units\nhai_c_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9357\n\n\nlog_SW_IN\n0.9046\n\n\nlog_VPD\n0.9007\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.2342\n°C\n\n\nlog_SW_IN\n0.5418\nW m-2\n\n\nlog_VPD\n0.0604\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-11.2260\n\n\nlog_SW_IN\n-2.0597\n\n\nlog_VPD\n-1027.6723\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.7308\n°C\n\n\nlog_SW_IN\n1.6771\nW m-2\n\n\nlog_VPD\n0.1880\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\nTA\n0.2127\n-0.4678\n0.6172\n\n\nlog_SW_IN\n-0.0392\n0.5399\n0.7212\n\n\nlog_VPD\n0.6975\n-0.0231\n0.5030\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n2.3790\n\n\nz1\n3.0309\n\n\nz2\n4.2982\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nlog_SW_IN\n0.0038\n\n\nlog_VPD\n0.0044\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0022\n\n\n\n\n\n \n\n\n\nhai_c_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9919\n\n\nlog_SW_IN\n0.9906\n\n\nlog_VPD\n0.5973\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0832\n°C\n\n\nlog_SW_IN\n0.1700\nW m-2\n\n\nlog_VPD\n0.1216\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9844\n\n\nlog_SW_IN\n0.9890\n\n\nlog_VPD\n0.6195\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1115\n°C\n\n\nlog_SW_IN\n0.2099\nW m-2\n\n\nlog_VPD\n0.1251\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.0841\n-0.9069\n\n\nlog_SW_IN\n0.9343\n0.0371\n\n\nlog_VPD\n0.2455\n-0.6557\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n3.7979\n\n\nz1\n6.6354\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0013\n\n\nlog_SW_IN\n0.0098\n\n\nlog_VPD\n0.4257\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0053\n\n\n\n\n\n \n\n\n\nhai_c_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9917\n\n\nlog_SW_IN\n-0.0027\n\n\nlog_VPD\n0.5397\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0844\n°C\n\n\nlog_SW_IN\n1.7565\nW m-2\n\n\nlog_VPD\n0.1300\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9834\n\n\nlog_SW_IN\n-0.0389\n\n\nlog_VPD\n0.5384\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1151\n°C\n\n\nlog_SW_IN\n2.0372\nW m-2\n\n\nlog_VPD\n0.1378\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\nvariable\nz0\n\n\n\n\nTA\n0.9099\n\n\nlog_SW_IN\n0.0142\n\n\nlog_VPD\n0.6696\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.5268\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0021\n\n\nlog_SW_IN\n0.9885\n\n\nlog_VPD\n0.4629\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0043"
  },
  {
    "objectID": "gap length variation.html",
    "href": "gap length variation.html",
    "title": "Gap Length Variation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\nfrom meteo_imp.results import _display_as_row \n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\nfrom itertools import combinations, repeat, zip_longest\n\nfrom ipywidgets import interact\nfrom tqdm.auto import tqdm\n\nfrom multiprocessing import Pool\n\nimport pickle\n\nModuleNotFoundError: No module named 'meteo_imp.gpfa.data_preparation'\n\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    #\"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nn_obs = 200\nn_latent = 1\ntotal_iter = 100\n\n\nmodel_save_dir = here() / \"analysis/trained_models\"\n\nmodel_path = model_save_dir / f\"GPFA_l_{n_latent}_train_{total_iter}_1ker_{n_obs}_obs.pickle\"\n\n\ndata = GPFADataTest(hai[:n_obs])\n\n\n# inspired from https://datagy.io/python-combinations-of-a-list/\ndef all_comb(l):\n    list_combinations = []\n    for n in range(1, len(l) + 1):\n        list_combinations += list(combinations(l, n))\n    return list_combinations\n\n\nall_comb(meteo_vars.values())\n\n\ndef to_result_pretrained(gap_len, n_latent, var_sel, gap_start=None):\n    data = GPFADataTest(hai[:n_obs]).add_gap(gap_len, var_sel, gap_start)\n    imp = GPFAImputationExplorer(data.data, latent_dims = n_latent)\n    model_path = model_save_dir / f\"GPFA_l_{n_latent}_train_{total_iter}_1ker_{n_obs}_obs.pickle\"\n    imp.learner.load(model_path)\n    return imp.to_result(data.data_compl_tidy, units=units)\n\n\n# to_result_pretrained(10, 1, ['TA'])\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# @cache_disk(here() / \".cache/diff_gaps\")\n# def diff_gaps(gap_start=30):\n#     return {n_lat:\n#             {var_sel:{ \n#                 gap_len: to_result_pretrained(GPFADataTest(hai[:n_obs]).add_gap(gap_len, ['TA'], gap_start), n_lat)\n#                 for gap_len in [2, 4, 5, 7, 10 , 15, 20, 30, 50, 100]\n#                 }\n#                 for var_sel in all_comb(meteo_vars.values())}\n#             for n_lat in range(1,4)}\n:::\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\npath_base = here() / \".cache/diff_gap_partial\"\n# path_base.rmdir()\n\n\ndef process_var_sel(args, path_base=path_base):\n    var_sel, n_lat = args # limitations in python map...\n    f_name = path_base / f\"{'-'.join(var_sel)}__l_{n_lat}.pickle\"\n    if f_name.exists(): return\n    out = {}\n    for gap_len in gaps:\n        out[gap_len] = {}\n        for gap_start in gap_starts:\n            out[gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start) \n    with open(f_name, \"wb\") as f:\n        pickle.dump(out, f)    \n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# # this is going to run on the process\n# # @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\n# def process_n_lat(n_lat):\n#     out = {}\n#     for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n#         out[var_sel] = {}\n#         for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n#             out[var_sel][gap_len] = {}\n#             for gap_start in tqdm(gap_starts, desc=f\"gap len: {gap_len}\"):\n#                 out[var_sel][gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n#     return out\n:::\n\n# @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\ndef compute_diff_gaps(gap_start=30):\n    for n_lat in tqdm(range(1,4)):\n        with Pool(processes=4) as pool:\n            list(pool.imap(process_var_sel, zip(all_comb(meteo_vars.values()), repeat(n_lat,))))\n\n\n\n\nthis is memory intensive! (maybe there is a leak to fix somewhere …)\n\n# compute_diff_gaps()\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\n# def diff_gaps(gap_start=30):\n#     with Pool(processes=4) as pool:\n#         out = pool.map(res, range(1,4)\n#         for n_lat in tqdm(range(1,4)):\n#             out[n_lat] = {}\n#             for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n#                 out[n_lat][var_sel] = {}\n#                 for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n#                     out[n_lat][var_sel][gap_len] = {}\n#                     for gap_start in tqdm(gap_starts, desc=f\"gap len: {gap_len}\"):\n#                         out[n_lat][var_sel][gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n#     return out\n:::\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# @cache_disk(here() / \".cache/diff_gaps\")\n# def diff_gaps(gap_start=30):\n#     with Pool(processes=4) as pool:\n#         out = {}  \n#         for n_lat in tqdm(range(1,4)):\n#             out[n_lat] = {}\n#             for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n#                 out[n_lat][var_sel] = {}\n#                 for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n#                     out[n_lat][var_sel][gap_len] = {}\n#                     f = lambda gap_start: to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n#                     results = pool.map(f, gap_starts)\n#                     for gap_start, res in zip(gap_starts, results):\n#                         out[n_lat][var_sel][gap_len][gap_start] = res\n#     return out\n:::\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n# diff_gaps_res = diff_gaps()\n:::\n\n# loads computations from disk\ndef load_diff_gaps():\n    out = {}\n    for n_lat in tqdm(range(1,4)):\n        out[n_lat] = {}\n        for var_sel in all_comb(meteo_vars.values()):\n            f_name = path_base / f\"{'-'.join(var_sel)}__l_{n_lat}.pickle\"\n            with open(f_name, \"rb\") as f:\n                out[n_lat][var_sel] = pickle.load(f)  \n    return out\n\n\ndiff_gaps_res = load_diff_gaps()\n\n\n\n\n\nWhat I am doing here:\n\ntake a dataset with 200 obs and 3 variables\ndistribution and correlation between vars\nfit the kernel parameters using gradient descend on whole dataset and save trained model notebook\ncreate a dataset with all combinations of gap_len, gap_start, n latents and variable missing\npredict the model for all 200 Obs also when there are no gaps!\nNote: in case there is a gap in not all variable, the variable with the gap have the (correct) prediction conditioned on the other variables, but the variables with no gap have the base model prediction (which is often bad), which should not be considered\n\n\n\nWhat we can see from this result:\n\n\n\n1 latent the Lambda is almost 1 for TA, 0 for SW_IN and .4 for VPD. Hence is good for TA, horrible for SW_IN and somehow okayish forVPD`\n2 latent2: good for TA and SW_IN, still limited for VPD\n3 latents: quite good fit for all 3 models\n\ncomments\n\ncorrelation between SW_IN and the others variables is pretty low\ntherefore with 1-2 latents the model cannot model accurately more then 1 variable\n\n\n\n\n\nwhen gaps are short &lt;~10 the model works kind of well, but there are issues in some locations (eg: TA, len: 10 start: 60)\n\ncomments\nthe lengthscale of the kernel is quite small (3 latents):\n\n5.2 z0\n1.8 z1\n4.0 z2\n\nso for longer gaps (in only one var) the main driver for the predictions are observations from the other variables, otherwise the models predictions are contastant as there is no way to use more information (eg. gap_len: 50, gap_start: 30, gaps in all vars)\nnotes - when SW_IN and VPD are close to 0 the gap filling is not that great also for shorter gaps (eg. SW_IN, len: 7, start: 30, n_lat: 3)\n\n\n\nthe interesting aspect is when there is a long gap, but in only 1-2 variables\n\nfor gaps only in TA with len up to 50 the models manages to follow the variations in the measurements, but with an error\n\nthis is pretty similar if there are gaps also in SW_IN, but not if there are gaps in VPD\nwith gap len over 100 it get way worse\n\n\nfor gaps only in VP with len up to 50 the models overall manages to follow the variations in the measurements, but with a considerable error (measurements are still in error bar) and the models has a lot of variations which are not present in the data\n\nthe predictions for SW_IN are bad (underestimates a lot the values) during the day for long gaps\n\n\n\n\n\n\n\n\n\n\n\n\nmore kernels -&gt; can have different timescales. However with 150 obs both kernels have the same timescale, should use more data but then there are computation issues (with 1500 it would take more then 20hours to do the training)\nlog transform\nmore variables\n\n\n\n\nmodel performance: at the moment it takes ~8 minutes to train with 200 obs and ~20 seconds for inference\n\nprofile current model\nuse SparseGP\nCUDA support\n\nparameters init\nlearning rate and stability of parameters over training\nvariable transformation:\n\nall vars are now normalized (0 mean, 1 std)\ntime is enconded as integer increasing at steps of 1. Maybe not a good idea?\n\n\n\n\n\n\n\nuse ERA5-Land (world-wide dataset with complete meteo vars, but a coarse spatial-temporal scale)\ncompare performance with state of art models\nmodel where relation between variables changes over time\nunderstand gap distribution in real world:\n\naverage gap len (tentative results are: a lot of short gaps(&lt;10) and some pretty long gap (&gt;10.0000)\ncorrelation between variable gaps\nsite distributions of gaps?\n\n\n\n\n\n\n\n%time r = to_result_pretrained(12, 3, ['SW_IN', 'TA'], gap_start=60)\n\n\nr.display_results()\n\n\ndata = GPFADataTest(hai[:n_obs]).add_gap(50, ['TA'], 30)\n\n\nimp = GPFAImputationExplorer(data.data, latent_dims = n_latent)\nimp.learner.load(model_path)\n\n\nresult_pretrained(data)"
  },
  {
    "objectID": "analyze_gaps_fluxnet.html",
    "href": "analyze_gaps_fluxnet.html",
    "title": "Analyze gaps fluxnet",
    "section": "",
    "text": "from IPython.display import display\nfrom ipywidgets import widgets, interact\n\n\nfrom pathlib import Path\nimport polars as pl\nfrom datetime import datetime\nfrom fastcore.utils import * # support of ls for paths\nimport matplotlib.pyplot as plt\nimport altair as alt\n\n\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\n\n\nsite_info = pl.read_parquet(out_dir / \"../site_info.parquet\").select([\n    pl.col(\"start\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"end\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"site\").cast(pl.Categorical).sort()\n])\n\n\nsite_info.head()\n\n\n\n\nshape: (5, 3)\nstart\nend\nsite\ndatetime[μs]\ndatetime[μs]\ncat\n2009-01-01 00:30:00\n2012-01-01 00:00:00\n\"AR-SLu\"\n2009-01-01 00:30:00\n2013-01-01 00:00:00\n\"AR-Vir\"\n2002-01-01 00:30:00\n2013-01-01 00:00:00\n\"AT-Neu\"\n2007-01-01 00:30:00\n2010-01-01 00:00:00\n\"AU-Ade\"\n2010-01-01 00:30:00\n2015-01-01 00:00:00\n\"AU-ASM\"\n\n\n\n\n\ndef duration_n_obs(duration):\n    \"converts a duration into a n of fluxnet observations\"\n    return abs(int(duration.total_seconds() / (30 * 60)))\n\n\nduration_n_obs(site_info[1, \"start\"] - site_info[1, \"end\"])\n\n70127\n\n\n\nduration_n_obs(site_info[0, \"start\"] - site_info[0, \"end\"])\n\n52559\n\n\n\nsite_info.select((pl.col(\"end\")-pl.col(\"start\")).dt.minutes() // 30).sum()\n\n\n\n\nshape: (1, 1)\nend\ni64\n26175287\n\n\n\n\n\nsp = site_info.to_pandas()\n\n\n((sp.end - sp.start).dt.total_seconds() / (30*60)).astype(int).sum()\n\n26175287\n\n\n\n# maybe this code should actually go in 20_gap_finding\nfiles = out_dir.ls()\nfiles.sort() # need to sort to match the site_info\nsites = []\nfor i, path in enumerate(files):\n    sites.append(pl.scan_parquet(path).with_columns([\n        pl.lit(site_info[i, \"site\"]).alias(\"site\"),\n        pl.lit(duration_n_obs(site_info[i, \"start\"] -  site_info[i, \"end\"])).alias(\"total_obs\"),\n        pl.col(\"TIMESTAMP_END\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\").alias(\"end\"),\n    ]).drop(\"TIMESTAMP_END\"))\n\n\ngap_stat = pl.concat(sites)\n\n\ngap_stat.head().fetch(5)\n\n\n\n\nshape: (5, 5)\ngap_len\nvariable\nsite\ntotal_obs\nend\nu32\nstr\nstr\ni32\ndatetime[μs]\n16992\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-01-01 00:30:00\n5\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-12-21 11:00:00\n1\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-12-21 17:00:00\n1\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2010-01-06 13:00:00\n3\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2010-01-07 13:00:00\n\n\n\n\n\ndef filter_variables(variables = [\"TA_F_QC\", \"SW_IN_QC\", \"LW_IN_QC\", \"VPD_F_QC\"]):\n    expr = False\n    for var in variables:\n        expr |= pl.col(\"variable\") == var\n    return expr\n\n\ndef pl_in(col, values):\n    expr = False\n    for val in values:\n        expr |= pl.col(col) == val\n    return expr\n\nsome sites have a lot of data missing, with the avg gap length of several years, so is seems that the year can have an impact\nImportant! here the 3 possibles gap value of a QC variable are considered as one (null, 1, 2) we should co\n\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).groupby(\"site\").agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()).alias(\"frac_gap\")\n]).collect().describe()\n\n\n\n\nshape: (7, 4)\ndescribe\nsite\nmean\nfrac_gap\nstr\nstr\nf64\nf64\n\"count\"\n\"205\"\n205.0\n205.0\n\"null_count\"\n\"0\"\n0.0\n0.0\n\"mean\"\nnull\n1327.107552\n0.198846\n\"std\"\nnull\n4421.109085\n0.256043\n\"min\"\n\"AR-SLu\"\n1.0\n0.000011\n\"max\"\n\"ZM-Mon\"\n52608.0\n2.299027\n\"median\"\nnull\n214.365854\n0.141903\n\n\n\n\n\n\n\nshort_gaps = gap_stat.filter(pl_in('variable', ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC'])).filter(pl.col('gap_len') &lt; 200).groupby('variable').agg(pl.col('gap_len').mean()).collect()\n\n\nshort_gaps\n\n\n\n\nshape: (3, 2)\nvariable\ngap_len\nstr\nf64\n\"VPD_F_QC\"\n9.642444\n\"TA_F_QC\"\n11.33915\n\"SW_IN_F_QC\"\n5.726891\n\n\n\n\n\nprint(short_gaps.to_pandas().to_markdown())\n\n|    | variable   |   gap_len |\n|---:|:-----------|----------:|\n|  0 | VPD_F_QC   |   9.64244 |\n|  1 | TA_F_QC    |  11.3392  |\n|  2 | SW_IN_F_QC |   5.72689 |\n\n\n\ngaps_year_site_ta = gap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).with_column(\n  pl.col(\"end\").dt.year().alias(\"year\")  \n).groupby([\"site\", \"year\"]).agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / (48 * 365)).alias(\"frac_gap\")\n]).sort([\"site\", \"year\"]).collect()\n\n\ngaps_year_site_ta.describe()\n\n\n\n\nshape: (7, 5)\ndescribe\nsite\nyear\nmean\nfrac_gap\nstr\nstr\nf64\nf64\nf64\n\"count\"\n\"1232\"\n1232.0\n1232.0\n1232.0\n\"null_count\"\n\"0\"\n0.0\n0.0\n0.0\n\"mean\"\nnull\n2007.18263\n826.696712\n0.156607\n\"std\"\nnull\n4.633427\n3339.649175\n0.350794\n\"min\"\n\"AR-SLu\"\n1991.0\n1.0\n0.000057\n\"max\"\n\"ZM-Mon\"\n2015.0\n52608.0\n7.686073\n\"median\"\nnull\n2008.0\n42.166667\n0.030822\n\n\n\n\n\ngaps_year_site_ta.filter(pl.col(\"frac_gap\") &gt;=1)\n\n\n\n\nshape: (15, 4)\nsite\nyear\nmean\nfrac_gap\nstr\ni32\nf64\nf64\n\"BE-Bra\"\n2003\n17738.0\n1.012443\n\"BR-Sa1\"\n2006\n4167.4\n1.189326\n\"CA-NS1\"\n2001\n1219.941176\n1.183733\n\"CA-NS3\"\n2002\n4580.8\n1.307306\n\"DE-Lnf\"\n2007\n52608.0\n3.00274\n\"IT-Cpz\"\n1998\n228.990566\n1.385445\n\"IT-Ro2\"\n2008\n1464.5\n1.504623\n\"RU-Cok\"\n2003\n411.729167\n1.128025\n\"SJ-Adv\"\n2011\n7235.0\n1.23887\n\"US-Blo\"\n1997\n292.784615\n1.086244\n\"US-CRT\"\n2004\n17545.0\n1.001427\n\"US-LWW\"\n2011\n52608.0\n3.00274\n\"US-Syv\"\n2009\n52560.0\n3.0\n\"US-WCr\"\n2007\n17520.0\n1.0\n\"ZM-Mon\"\n2000\n22443.333333\n7.686073\n\n\n\n\n\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).filter(pl_in(\"site\", [\"ZM-Mon\"])).collect()\n\n\n\n\nshape: (8, 5)\ngap_len\nvariable\nsite\ntotal_obs\nend\nu32\nstr\nstr\ni32\ndatetime[μs]\n2913\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-01-01 00:30:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-02 17:30:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-04 04:30:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-04 09:00:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-10 11:30:00\n131743\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-11 02:00:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2008-01-30 12:00:00\n7753\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2009-07-23 12:00:00\n\n\n\n\n\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).filter(pl_in(\"site\", [\"DE-Lnf\"])).collect()\n\n\n\n\nshape: (6, 5)\ngap_len\nvariable\nsite\ntotal_obs\nend\nu32\nstr\nstr\ni32\ndatetime[μs]\n5116\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2002-01-01 00:30:00\n52608\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2007-01-01 00:30:00\n1\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2010-04-15 10:00:00\n20\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2012-08-14 13:00:00\n45\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2012-10-09 14:00:00\n2\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2012-10-16 13:30:00\n\n\n\n\n\ndef visualize_by_site(df):\n    sites = df[\"site\"].unique()\n    for site in sites:\n        yield df.filter(pl.col(\"site\") == site)\n\n\nby_site = list(visualize_by_site(gaps_year_site_ta))\n\n\nbutton_next = widgets.Button(description=\"Next\", icon=\"arrow-right\")\nbutton_prev = widgets.Button(description=\"Previous\", icon=\"arrow-left\")\noutput = widgets.Output()\n\ndisplay(button_next, button_prev, output)\n\ni = 0\n\ndef update_view():\n    with output:\n        print(f\"{i} of {len(by_site)}\")\n        display(by_site[i])\n    output.clear_output(wait=True)\n\ndef on_next(b):\n    global i\n    if i &lt; len(by_site):\n        i +=1\n    else:\n        button_next.disabled = True\n    update_view()\n\ndef on_prev(b):\n    global i\n    if i &gt; 0:\n        i -=1\n    else:\n        button_prev.disabled = True\n    update_view()\n    \n\nbutton_next.on_click(on_next)\nbutton_prev.on_click(on_prev)\n\n\n\n\n\n\n\n\n\n\n\nta_gaps = gap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).collect()\n\n\nta_gaps.to_pandas().hist(column=\"gap_len\")\nplt.yscale('log')\n\n\n\n\n\n\n\n\nall_vars = gap_stat.select(pl.col(\"variable\").unique().sort()).collect()[\"variable\"]\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var):\n    ta_gaps = gap_stat.filter(\n        pl.col(\"variable\") == var \n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n\n\n\nall_sites = gap_stat.select(pl.col(\"site\").unique().sort()).collect()[\"site\"]\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site, small=False):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.title(f\"{site}: {var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n\n\n\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var, small=False):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).collect().to_pandas().hist(\"gap_len\", bins=30)\n    plt.title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    plt.yscale('log')\n    plt.xscale('log') \n\n\n\n\n\nfrom fastai.vision.data import get_grid\nfrom pyprojroot import here\n\n\ndef plot_var_dist(var, small=False, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).with_column(pl.col(\"gap_len\") / (24 *2 * 7)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    if not small: ax.set_yscale('log')\n    ax.set_xlabel(\"gap length (weeks)\")\n    ax.set_ylabel(f\"{'Log' if not small else ''} n gaps\")\n    # plt.xscale('log') \n\n\nfor ax, var in zip(get_grid(3,1,3, figsize=(15,8), sharey=True), ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC']):\n    plot_var_dist(var, ax=ax)\nplt.savefig(here(\"analysis/presentations/plots_18_jan/gap_len_dist.png\", warn=False))\n\n\n\n\n\ndef plot_var_dist_small(var, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) \n    ).with_column(pl.col(\"gap_len\") / (24 *2)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - gap len &lt; 1 week\")\n    ax.set_xlabel(\"gap length (days)\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n\n\nfor ax, var in zip(get_grid(3,1,3, figsize=(15,8), sharey=True), ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC']):\n    plot_var_dist_small(var, ax=ax)\nplt.savefig(here(\"analysis/presentations/plots_18_jan/gap_len_dist_small.png\", warn=False))\n\n\n\n\n\ndef plot_var_dist_cum(var, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    \n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) \n    ).collect() #.to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    bins = pl.cut(ta_gaps[\"gap_len\"], bins = pl.arange(0, 24 * 2 * 7, (24 * 2 * 7) // 50, eager=True))\n    return ta_gaps\n    ax.set_title(f\"{var} - gap len &lt; 1 week\")\n    ax.set_xlabel(\"gap length (days)\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n\n\n\n\n\nvar_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == 'TA_F_QC')\n    ).filter(\n        pl.col(\"gap_len\") &lt; 20000\n    ).sort(pl.col(\"gap_len\")\n        \n    ).collect().to_pandas()\n\n\nalt.data_transformers.enable('data_server')\n\n\n \nalt.Chart(var_gaps).mark_boxplot().encode(\n    y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n    x='gap_len'\n)\n\n\nwidgets.IntSlider?\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var, max_len=widgets.IntSlider(1000, 100, 20_000, 100)):\n    var_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; max_len\n    ).collect().to_pandas()\n    \n    display(alt.Chart(var_gaps).mark_boxplot().encode(\n        y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n        x='gap_len'\n    ))\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(pl.col(\"gap_len\").sum()).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).groupby(\"variable\").agg(pl.col(\"gap_len\").mean()).collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).filter(pl.col(\"variable\") == \"TA_F_QC\").collect()"
  },
  {
    "objectID": "Simple GP Hainich.html",
    "href": "Simple GP Hainich.html",
    "title": "Simple GP Hainich",
    "section": "",
    "text": "imputation using simple GP\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.simple_gp_imputation import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import *\n\n\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n1.636\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n\n\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(20, meteo_vars.values(), 60)\n\n\ndata = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values(), 60)\nres = SimpleGPImputationExplorer(data.data).fit().to_result(data.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres.display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9186\n\n\nSW_IN\n0.9267\n\n\nVPD\n0.9880\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.2635\n°C\n\n\nSW_IN\n10.0560\nW m-2\n\n\nVPD\n0.0280\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-9.1010\n\n\nSW_IN\n0.7756\n\n\nVPD\n0.5680\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.8120\n°C\n\n\nSW_IN\n30.2152\nW m-2\n\n\nVPD\n0.0796\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  lengthscale \n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n4.7362\n\n\nSW_IN\n3.5688\n\n\nVPD\n2.6062\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.6055\n\n\nSW_IN\n1.4438\n\n\nVPD\n0.7713\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0057\n\n\nSW_IN\n0.0237\n\n\nVPD\n0.0046\n\n\n\n\n\n \n\n\n\ndata = GPFADataTest(hai[:150]).add_gap(20, meteo_vars.values(), 60)\nres = SimpleGPImputationExplorer(data.data).fit().to_result(data.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres.display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9600\n\n\nSW_IN\n0.3226\n\n\nVPD\n0.9833\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1846\n°C\n\n\nSW_IN\n30.5714\nW m-2\n\n\nVPD\n0.0330\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-0.9281\n\n\nSW_IN\n-0.9669\n\n\nVPD\n0.5433\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.4883\n°C\n\n\nSW_IN\n83.2934\nW m-2\n\n\nVPD\n0.0841\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  lengthscale \n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n4.5675\n\n\nSW_IN\n3.7809\n\n\nVPD\n2.6307\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.6024\n\n\nSW_IN\n0.7026\n\n\nVPD\n0.7619\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0040\n\n\nSW_IN\n0.0249\n\n\nVPD\n0.0045\n\n\n\n\n\n \n\n\n\n\n\nres_r_gaps = SimpleGPImputationExplorer(data_r_gaps.data).fit().to_result(data_r_gaps.data_compl_tidy, units=units)\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n\n\n\n\n\n\n\n\n\nres_r_gaps.display_results(plot_args = {'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9911\n\n\nSW_IN\n0.9661\n\n\nVPD\n0.9675\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0873\n°C\n\n\nSW_IN\n6.8369\nW m-2\n\n\nVPD\n0.0461\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9813\n\n\nSW_IN\n0.9574\n\n\nVPD\n0.9504\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1222\n°C\n\n\nSW_IN\n10.1436\nW m-2\n\n\nVPD\n0.0613\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  lengthscale \n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n6.4697\n\n\nSW_IN\n5.0863\n\n\nVPD\n5.3424\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.7881\n\n\nSW_IN\n1.6644\n\n\nVPD\n1.1420\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0068\n\n\nSW_IN\n0.0376\n\n\nVPD\n0.0305\n\n\n\n\n\n \n\n\n\n\n\n\nres_c_gaps = SimpleGPImputationExplorer(data_c_gaps.data).fit().to_result(data_c_gaps.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres_c_gaps.display_results(plot_args = {'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9500\n\n\nSW_IN\n0.9746\n\n\nVPD\n0.9401\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.2064\n°C\n\n\nSW_IN\n5.9171\nW m-2\n\n\nVPD\n0.0626\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-8.4622\n\n\nSW_IN\n0.1705\n\n\nVPD\n-1065.0841\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.6429\n°C\n\n\nSW_IN\n10.5049\nW m-2\n\n\nVPD\n0.1929\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  lengthscale \n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n3.0968\n\n\nSW_IN\n3.9848\n\n\nVPD\n2.5715\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.6266\n\n\nSW_IN\n0.7329\n\n\nVPD\n0.7777\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0028\n\n\nSW_IN\n0.0239\n\n\nVPD\n0.0064\n\n\n\n\n\n \n\n\n\n\n\n\ntry with a different random seed\n\nreset_seed(101)\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values())\n\n\n\n\nres_r_gaps = SimpleGPImputationExplorer(data_r_gaps.data).fit().to_result(data_r_gaps.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres_r_gaps.display_results(plot_args = {'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.9671\n\n\nVPD\n0.9646\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0849\n°C\n\n\nSW_IN\n6.7353\nW m-2\n\n\nVPD\n0.0481\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.9149\n\n\nVPD\n0.9433\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0848\n°C\n\n\nSW_IN\n9.2910\nW m-2\n\n\nVPD\n0.0680\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  lengthscale \n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n6.4669\n\n\nSW_IN\n4.6214\n\n\nVPD\n4.9073\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.7311\n\n\nSW_IN\n0.9371\n\n\nVPD\n0.8954\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0094\n\n\nSW_IN\n0.0193\n\n\nVPD\n0.0215\n\n\n\n\n\n \n\n\n\n\n\n\nres_c_gaps = SimpleGPImputationExplorer(data_c_gaps.data).fit().to_result(data_c_gaps.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres_c_gaps.display_results(plot_args = {'bind_interaction': False, 'properties': {}})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9849\n\n\nSW_IN\n0.9654\n\n\nVPD\n0.9291\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1135\n°C\n\n\nSW_IN\n6.9104\nW m-2\n\n\nVPD\n0.0680\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.2640\n\n\nSW_IN\n0.0000\n\n\nVPD\n-9.7505\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.3417\n°C\n\n\nSW_IN\n14.0756\nW m-2\n\n\nVPD\n0.2107\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  lengthscale \n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n3.0464\n\n\nSW_IN\n3.9805\n\n\nVPD\n2.5759\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.6008\n\n\nSW_IN\n0.7517\n\n\nVPD\n0.7584\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0025\n\n\nSW_IN\n0.0284\n\n\nVPD\n0.0061"
  },
  {
    "objectID": "GPFA Hainich.html",
    "href": "GPFA Hainich.html",
    "title": "Single latent",
    "section": "",
    "text": "First analysis of the Hainich data using GPFA for filling the gaps\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n302.475\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n\n\n...\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n330.202\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n330.202\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n330.202\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n\n\n\n\n200 rows × 4 columns\n\n\n\n\n\n\n\ngpfa_data = GPFADataTest(hai).add_random_missing()\n\n\ngpfa_hai = GPFAImputation(gpfa_data.data, gpfa_data.tidy_df(complete=True, is_missing=True))\n\nTypeError: rand(): argument 'size' must be tuple of ints, but found element of type DataFrame at pos 2\n\n\n\ngpfa_hai\n\n\n%time imputed = gpfa_hai.impute()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nCPU times: user 5min 57s, sys: 485 ms, total: 5min 57s\nWall time: 5min 59s\n\n\n\nimputed\n\n\n\n\n\n\n\n\ntime\nvariable\nmean\nstd\n\n\n\n\n0\n0.0\nTA\n-0.600000\nNaN\n\n\n1\n2.0\nTA\n-0.580000\nNaN\n\n\n2\n3.0\nTA\n-0.510000\nNaN\n\n\n3\n4.0\nTA\n-0.490000\nNaN\n\n\n4\n11.0\nTA\n-0.230000\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n403\n189.0\nVPD\n0.826632\n0.252326\n\n\n404\n190.0\nVPD\n0.827371\n0.252322\n\n\n405\n192.0\nVPD\n1.213000\n0.000000\n\n\n406\n193.0\nVPD\n0.826446\n0.252319\n\n\n407\n197.0\nVPD\n0.820434\n0.252332\n\n\n\n\n800 rows × 4 columns\n\n\n\n\nhai_plot = gpfa_hai.plot_pred(units=units, properties =  {'height': 190 , 'width': 380})\n\nhai_plot.save(\"plots/plot_hai_winter_4_var_200_obs_random_gaps_row_20_value_10.vl.json\")\nhai_plot\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n#gpfa_hai.plot_pred(complete= gpfa_data.tidy_df(complete=True, is_missing=True) )\n\n\ngpfa_hai.rmse()\n\nAttributeError: module 'sklearn' has no attribute 'metrics'\n\n\n\ngpfa_hai.r2()\n\n\nlosses = pd.DataFrame(gpfa_hai.learner.losses.cpu().numpy(), columns=['loss'])\n\np = losses.plot()\nplt.savefig(here('analysis/plots/loss_plot_hai_winter_4_var_200_obs_random_gaps_row_20_value_10.png'))\np\n\nLambda parameter, the latent variable is very similar to the\n\ngpfa_hai.data.corr()\n\nNameError: name 'gpfa_hai' is not defined\n\n\n\ngpfa_hai.learner.model.covar_module.Lambda.detach()\n\nthis is the value of the length scale of the RBF latent kernel\n\ngpfa_hai.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\ngpfa_hai.learner.model.covar_module.psi.detach()\n\n\n\nThe low correlation between SW_IN and TA is likely due to cloud cover, which is hard to predict with a dialy cycle. Hence we are looking at summer days and there is a much better correlation\n\nhai_raw2 = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows= 7 * 30 * 24 * 2)\n\nNameError: name 'pd' is not defined\n\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai2 = (hai_raw2\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai2\n\n\nhai2[-800:-500].SW_IN.plot()\n\n\nhai2[-800:-500].corr()\n\n\ngpdata2 = GPFADataTest(hai2[-800:-500].copy()).add_random_missing()\n\n\ngp_imp2 = GPFAImputation(gpdata2.data, gpdata2.tidy_df(complete=True, is_missing=True))\n\n\n%time data_imp2 = gp_imp2.impute()\n\n\ngp_imp2.plot_pred(units=units)\n\n\ndata_imp2\n\n\ngp_imp2.rmse()\n\n\ngpdata2.data.corr()\n\n\ngp_imp2.learner.model.covar_module.Lambda.detach()\n\n\ngp_imp2.learner.model.covar_module.psi.detach()\n\nthis is the value of the length scale of the RBF latent kernel\n\ngp_imp2.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\n\n\ngpdata3 = GPFADataTest(hai2[-800:-500].loc[:, [\"TA\", \"SW_IN\"]].copy()).add_random_missing()\n\n\ngp_imp3 = GPFAImputation(gpdata3.data, gpdata3.tidy_df(complete=True, is_missing=True))\n\n\n%time data_imp3 = gp_imp3.impute()\n\n\ngp_imp3.plot_pred(units=units, bind_interaction=False)\n\n\ndata_imp3\n\n\ngpdata3.data.corr()\n\n\ngp_imp3.learner.model.covar_module.Lambda.detach()\n\nthis is the value of the length scale of the RBF latent kernel\n\ngp_imp3.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\ngp_imp3.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\n\n\n\n\nTrying to see how the model works with a continous gap of 10% the length of the dataset for all variables\n\ngpd_gap = GPFADataTest(hai).add_gap(20, variables = ['TA', 'SW_IN', 'LW_IN', 'VPD'])\n\n\ngp_gap = GPFAImputation(gpd_gap.data, gpd_gap.tidy_df(complete=True, is_missing=True))\n\n\ngp_gap\n\n\n%time gp_gap.impute()\n\n\ngap_plot= gp_gap.plot_pred(units=units, properties =  {'height': 190 , 'width': 380})\n\ngap_plot.save(here(\"analysis/plots\") /\" plot_hai_winter_4_var_200_obs_gap_20.vl.json\")\ngap_plot\n\n\nprint(gp_gap.rmse().to_markdown(index=False))\n\n\nprint(pd.DataFrame(gp_gap.learner.model.covar_module.Lambda.detach().numpy()).to_markdown(index=False))\n\n\npsi = pd.DataFrame(gp_gap.learner.model.covar_module.psi.detach().numpy())\npsi.insert(0, \"variable\", meteo_vars.values())\nprint(psi.to_markdown(index=False))\n\n\ngp_gap.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\nlosses = pd.DataFrame(gp_gap.learner.losses.cpu().numpy(), columns=['loss'])\n\np = losses.plot()\nplt.savefig(here('analysis/plots/') /'loss_plot_hai_winter_4_var_200_obs_gap_20.png')\np"
  },
  {
    "objectID": "results/result_plotting.html",
    "href": "results/result_plotting.html",
    "title": "Plotting for results",
    "section": "",
    "text": "This notebook produces all results plots. It generates some gap in the data, fill with a method (filter, MDS …), compute metrics and then makes all relevant plots\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport altair as alt\n\n\nfrom meteo_imp.kalman.results import *\nfrom meteo_imp.data import *\nfrom meteo_imp.utils import *\nimport pandas as pd\nimport numpy as np\nfrom pyprojroot import here\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG, Image\n\n\nimport vl_convert as vlc\nfrom pyprojroot import here\nbase_path_img = here(\"manuscript/Master Thesis - Meteorological time series imputation using Kalman filters - Simone Massaro/images/\")\nbase_path_tbl = here(\"manuscript/Master Thesis - Meteorological time series imputation using Kalman filters - Simone Massaro/tables/\")\n\ndef save_plot(plot, path):\n    png_data = vlc.vegalite_to_png(vl_spec=plot.to_json(), scale=3)\n    with open(base_path_img / (path + \".png\"), \"wb\") as f:\n        f.write(png_data)\n\ndef show_plot(path): return Image(filename=base_path_img / (path + \".png\"))\n\n\nreset_seed()\nn_rep = 500\n\n\nhai = pd.read_parquet(hai_big_path).reindex(columns=var_type.categories)\nhai_era = pd.read_parquet(hai_era_big_path)\n\n\n\n\n\nThe gap is a only in the variable that is gap-filled\n\nhai.columns\n\nIndex(['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], dtype='object')\n\n\n\nbase_path = here(\"analysis/results/trained_14feb\")\n\n\ndef l_model(x): return torch.load(base_path / x)\n\n\nmodels_var = [\n    {'var': 'TA',    'model': l_model(\"TA_specialized_gap_12-336_v2.pickle.pickle\")},\n    {'var': 'SW_IN', 'model': l_model(\"SW_IN_specialized_gap_12-336_v2.pickle.pickle\")},\n    {'var': 'LW_IN', 'model': l_model(\"LW_IN_specialized_gap_12-336_v1.pickle\")},\n    {'var': 'VPD',   'model': l_model(\"VPD_specialized_gap_12-336_v1.pickle\")},\n    {'var': 'WS',    'model': l_model(\"WS_specialized_gap_12-336_v3.pickle\")},\n    {'var': 'PA',    'model': l_model(\"PA_specialized_gap_12-336_v3.pickle\")},\n    {'var': 'P',     'model': l_model(\"P_specialized_gap_12-336_v1.pickle\")},\n    {'var': 'TS',    'model': l_model(\"TS_specialized_gap_12-336_v3.pickle\")},\n    {'var': 'SWC',   'model': l_model(\"SWC_specialized_gap_12-336_v3_4.pickle\")},\n]\nmodels_var = pd.DataFrame.from_records(models_var)\n\n\n@cache_disk(cache_dir / \"the_results\")\ndef get_the_results(n_rep=20):\n    comp_Av = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 446, time_series=False)\n    results_Av = comp_Av.compare(gap_len = [12,24, 48, 336], var=list(hai.columns), n_rep=n_rep) \n    return results_Av\n\nresults_Av = get_the_results(n_rep)\n\n\nfrom itertools import product\nimport altair as alt\n\n\nall_res = results_Av.groupby('method').agg({'rmse_stand': 'mean'}).T\n\n\nall_res\n\n\n\n\n\n\n\nmethod\nKalmanFilter\nERA\nMDS\n\n\n\n\nrmse_stand\n0.245708\n0.345047\n0.490811\n\n\n\n\n\n\n\npercentage of improvement across all variables\n\n(all_res[\"ERA\"] - all_res[\"KalmanFilter\"]) / all_res[\"ERA\"] * 100 \n\nrmse_stand    28.790004\ndtype: float64\n\n\n\n(all_res[\"MDS\"] - all_res[\"KalmanFilter\"]) / all_res[\"MDS\"] * 100 \n\nrmse_stand    49.938377\ndtype: float64\n\n\n\nall_res[]\n\n\nalt.data_transformers.disable_max_rows() # it is safe to do so as the plots are rendered using vl-convert and then showed as images\n\nDataTransformerRegistry.enable('default')\n\n\n\np = the_plot(results_Av)\nsave_plot(p, \"the_plot\")\nshow_plot(\"the_plot\")\n\n\n\n\n\np = the_plot_stand(results_Av)\nsave_plot(p, \"the_plot_stand\")\nshow_plot(\"the_plot_stand\")\n\n\n\n\n\np = the_plot_stand2(results_Av)\nsave_plot(p, \"the_plot_stand2\")\nshow_plot(\"the_plot_stand2\")\n\n\n\n\n\np = the_plot_stand3(results_Av)\nsave_plot(p, \"the_plot_stand3\")\nshow_plot(\"the_plot_stand3\")\n\n\n\n\n\nt = the_table(results_Av)\nthe_table_latex(t, base_path_tbl / \"the_table.tex\", label=\"the_table\",\n                caption=\"RMSE Comparison imputation methods. The best method for each gap length is highligthed in bold\")\nt\n\n\n\n\n\n\n\n\n\nKalmanFilter\nERA\nMDS\n\n\n\nRMSE\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\n\nTA\n6.0\n0.489414\n0.286708\n1.346910\n0.997843\n2.712546\n1.896914\n\n\n12.0\n0.717705\n0.450045\n1.471695\n0.900611\n2.942435\n1.748131\n\n\n24.0\n0.846503\n0.424666\n1.529614\n0.800256\n3.012819\n1.611311\n\n\n168.0\n1.074202\n0.465658\n1.754334\n0.643160\n3.780087\n1.315472\n\n\nSW_IN\n6.0\n44.406835\n41.185820\n49.333113\n66.241975\n63.536627\n85.401585\n\n\n12.0\n49.386211\n34.587600\n54.207691\n49.769296\n69.427115\n68.936352\n\n\n24.0\n57.888189\n29.905625\n65.950367\n40.930505\n86.770917\n59.603564\n\n\n168.0\n62.929765\n25.382879\n70.224393\n34.883199\n107.384249\n53.606111\n\n\nLW_IN\n6.0\n10.933181\n8.117911\n13.804628\n12.987987\n26.680077\n15.022366\n\n\n12.0\n12.976946\n8.420187\n14.766929\n12.584725\n28.085478\n13.457335\n\n\n24.0\n14.311389\n8.301348\n14.093052\n12.227900\n29.614461\n12.416763\n\n\n168.0\n16.982041\n6.724037\n16.365697\n11.129569\n32.954558\n8.833972\n\n\nVPD\n6.0\n0.579836\n0.454156\n1.296787\n1.547397\n2.083592\n2.149288\n\n\n12.0\n0.830825\n0.620231\n1.265213\n1.288794\n2.136626\n2.095549\n\n\n24.0\n0.957941\n0.616140\n1.247527\n1.032319\n1.912472\n1.605013\n\n\n168.0\n1.282954\n0.735500\n1.662069\n1.127314\n2.661345\n1.965431\n\n\nWS\n6.0\n0.604378\n0.309739\n0.912428\n0.508295\n1.136367\n0.783146\n\n\n12.0\n0.701851\n0.335770\n0.956550\n0.524247\n1.261203\n0.796744\n\n\n24.0\n0.786750\n0.333186\n0.949427\n0.446912\n1.275665\n0.608630\n\n\n168.0\n0.946239\n0.337541\n1.088887\n0.348541\n1.494891\n0.615371\n\n\nPA\n6.0\n0.048172\n0.039004\n0.074856\n0.061726\n0.530665\n0.441476\n\n\n12.0\n0.054868\n0.044319\n0.077328\n0.058476\n0.563603\n0.427426\n\n\n24.0\n0.061694\n0.040415\n0.079021\n0.051491\n0.556899\n0.404451\n\n\n168.0\n0.067441\n0.047825\n0.083628\n0.053654\n0.773143\n0.384029\n\n\nP\n6.0\n0.111547\n0.253684\n0.113173\n0.315504\n0.117710\n0.305539\n\n\n12.0\n0.134660\n0.248484\n0.138729\n0.297227\n0.130442\n0.281377\n\n\n24.0\n0.156399\n0.242352\n0.165750\n0.288432\n0.158641\n0.265257\n\n\n168.0\n0.195263\n0.175225\n0.222682\n0.201782\n0.214975\n0.197499\n\n\nSWC\n6.0\n0.509564\n0.363343\nNaN\nNaN\n1.313730\n1.556829\n\n\n12.0\n0.610184\n0.439400\nNaN\nNaN\n1.278001\n1.323011\n\n\n24.0\n0.755764\n0.582507\nNaN\nNaN\n1.355740\n1.472185\n\n\n168.0\n1.592585\n1.258741\nNaN\nNaN\n1.947605\n1.488284\n\n\nTS\n6.0\n0.372636\n0.333516\nNaN\nNaN\n0.954469\n0.889126\n\n\n12.0\n0.611555\n0.645607\nNaN\nNaN\n1.002555\n0.876784\n\n\n24.0\n0.916664\n0.733149\nNaN\nNaN\n1.078373\n0.856964\n\n\n168.0\n1.401425\n0.800632\nNaN\nNaN\n1.440008\n0.764040\n\n\n\n\n\n\n\n\nt = the_table(results_Av, 'rmse_stand')\nthe_table_latex(t, base_path_tbl / \"the_table_stand.tex\", stand = True, label=\"the_table_stand\", \n                caption = \"Comparison of imputation methods using Standardized RMSE. The best method for each gap length is highligthed in bold\")\nt\n\n\n\n\n\n\n\n\n\nKalmanFilter\nERA\nMDS\n\n\n\nRMSE\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\n\nTA\n6.0\n0.061759\n0.036179\n0.169965\n0.125917\n0.342294\n0.239370\n\n\n12.0\n0.090567\n0.056791\n0.185712\n0.113647\n0.371303\n0.220595\n\n\n24.0\n0.106819\n0.053588\n0.193021\n0.100984\n0.380185\n0.203330\n\n\n168.0\n0.135553\n0.058761\n0.221378\n0.081160\n0.477006\n0.165998\n\n\nSW_IN\n6.0\n0.217678\n0.201889\n0.241826\n0.324711\n0.311450\n0.418630\n\n\n12.0\n0.242086\n0.169545\n0.265721\n0.243964\n0.340325\n0.337919\n\n\n24.0\n0.283762\n0.146594\n0.323282\n0.200637\n0.425342\n0.292171\n\n\n168.0\n0.308475\n0.124424\n0.344233\n0.170994\n0.526387\n0.262772\n\n\nLW_IN\n6.0\n0.260588\n0.193487\n0.329028\n0.309564\n0.635910\n0.358053\n\n\n12.0\n0.309301\n0.200692\n0.351964\n0.299952\n0.669407\n0.320751\n\n\n24.0\n0.341107\n0.197860\n0.335903\n0.291448\n0.705850\n0.295949\n\n\n168.0\n0.404761\n0.160265\n0.390071\n0.265269\n0.785460\n0.210555\n\n\nVPD\n6.0\n0.132734\n0.103964\n0.296855\n0.354224\n0.476967\n0.492006\n\n\n12.0\n0.190189\n0.141981\n0.289627\n0.295025\n0.489108\n0.479704\n\n\n24.0\n0.219288\n0.141044\n0.285579\n0.236314\n0.437795\n0.367413\n\n\n168.0\n0.293689\n0.168368\n0.380474\n0.258060\n0.609224\n0.449918\n\n\nWS\n6.0\n0.371828\n0.190559\n0.561347\n0.312715\n0.699120\n0.481810\n\n\n12.0\n0.431795\n0.206574\n0.588492\n0.322529\n0.775922\n0.490176\n\n\n24.0\n0.484027\n0.204984\n0.584110\n0.274951\n0.784819\n0.374443\n\n\n168.0\n0.582148\n0.207663\n0.669909\n0.214431\n0.919692\n0.378591\n\n\nPA\n6.0\n0.056331\n0.045611\n0.087534\n0.072180\n0.620545\n0.516250\n\n\n12.0\n0.064161\n0.051826\n0.090425\n0.068381\n0.659061\n0.499820\n\n\n24.0\n0.072144\n0.047260\n0.092405\n0.060212\n0.651223\n0.472953\n\n\n168.0\n0.078864\n0.055925\n0.097793\n0.062741\n0.904092\n0.449073\n\n\nP\n6.0\n0.397991\n0.905123\n0.403790\n1.125691\n0.419979\n1.090136\n\n\n12.0\n0.480457\n0.886571\n0.494974\n1.060481\n0.465404\n1.003928\n\n\n24.0\n0.558019\n0.864691\n0.591382\n1.029100\n0.566018\n0.946414\n\n\n168.0\n0.696682\n0.625189\n0.794512\n0.719941\n0.767011\n0.704660\n\n\nSWC\n6.0\n0.057170\n0.040765\nNaN\nNaN\n0.147393\n0.174667\n\n\n12.0\n0.068459\n0.049298\nNaN\nNaN\n0.143384\n0.148434\n\n\n24.0\n0.084792\n0.065354\nNaN\nNaN\n0.152106\n0.165171\n\n\n168.0\n0.178679\n0.141223\nNaN\nNaN\n0.218510\n0.166977\n\n\nTS\n6.0\n0.065853\n0.058939\nNaN\nNaN\n0.168674\n0.157127\n\n\n12.0\n0.108074\n0.114092\nNaN\nNaN\n0.177172\n0.154946\n\n\n24.0\n0.161994\n0.129563\nNaN\nNaN\n0.190571\n0.151443\n\n\n168.0\n0.247661\n0.141488\nNaN\nNaN\n0.254479\n0.135022\n\n\n\n\n\n\n\n\n\n\n\n@cache_disk(cache_dir / \"the_results_ts\")\ndef get_the_results_ts():\n    comp_Av = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 446, time_series=True, rmse=False)\n    results_Av = comp_Av.compare(gap_len = [12,24, 336], var=list(hai.columns), n_rep=4) \n    return results_Av\n\n# results_ts = get_the_results_ts()\n\n\n# ts = plot_timeseries(results_ts.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=0)\n# save_plot(ts, \"timeseries_1\")\nshow_plot(\"timeseries_1\")\n\n\n\n\n\n# %time ts = plot_timeseries(results_ts.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=0)\n# %time save_plot(ts, \"timeseries_2\")\nshow_plot(\"timeseries_2\")\n\n\nfrom tqdm.auto import tqdm\n\n\nresults_ts.idx_rep.unique()\n\n\n# @cache_disk(cache_dir / \"ts_plot\")\ndef plot_additional_ts():\n    for idx in tqdm(results_ts.idx_rep.unique()):\n        if idx == 0: continue # skip first plot as is done above\n        ts1 = plot_timeseries(results_ts.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=idx)\n        save_plot(ts1, f\"timeseries_1_{idx}\")\n        ts2 = plot_timeseries(results_ts.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=idx)\n        save_plot(ts2, f\"timeseries_2_{idx}\")        \n\n\n# plot_additional_ts()\n\n\n\n\n\n\n\n\n@cache_disk(\"gap_len\")\ndef get_g_len(n_rep=n_rep):\n    return KalmanImpComparison(models_var, hai, hai_era, block_len=48*7+100).compare(gap_len = [2,6,12,24,48,48*2, 48*3, 48*7], var=list(hai.columns), n_rep=n_rep)\n\n\ngap_len = get_g_len(n_rep)\n\n\n\n\n\np = plot_gap_len(gap_len, hai, hai_era)\nsave_plot(p, \"gap_len\")\np\n\n\n\n\n\n\n\nt = table_gap_len(gap_len)\ntable_gap_len_latex(t, base_path_tbl / \"gap_len.tex\", label=\"gap_len\",\n                caption=\"RMSE Comparison Kalman filter for different gap lengths\")\nt\n\n\n\n\n\n\n\n\n\n1\n3\n6\n12\n24\n48\n72\n168\n\n\nVariable\nRMSE\n\n\n\n\n\n\n\n\n\n\n\n\nTA\nmean\n0.249874\n0.369584\n0.544200\n0.721894\n0.911514\n0.996244\n1.033084\n1.062626\n\n\nSW_IN\nmean\n27.218983\n39.195194\n46.166472\n50.616872\n57.511795\n57.831439\n59.806769\n60.254291\n\n\nLW_IN\nmean\n5.760089\n8.424112\n12.038032\n14.447153\n16.137752\n16.569009\n16.380740\n17.414271\n\n\nVPD\nmean\n0.283951\n0.430531\n0.615616\n0.826526\n1.098688\n1.148027\n1.219952\n1.236631\n\n\nWS\nmean\n0.349414\n0.481933\n0.589038\n0.705312\n0.785513\n0.842590\n0.919232\n0.921488\n\n\nPA\nmean\n0.023761\n0.035799\n0.047033\n0.054201\n0.060738\n0.069416\n0.070363\n0.068169\n\n\nP\nmean\n0.061754\n0.086840\n0.111877\n0.145961\n0.171795\n0.163256\n0.169645\n0.187402\n\n\nSWC\nmean\n0.250153\n0.388866\n0.482214\n0.585416\n0.718911\n0.930457\n1.038485\n1.533061\n\n\nTS\nmean\n0.214781\n0.291131\n0.394747\n0.617634\n0.899105\n1.084576\n1.238317\n1.458591\n\n\n\n\n\n\n\n\n# with open(base_path_tbl / \"gap_len.tex\") as f:\n    # print(f.readlines())\n\n\nfrom meteo_imp.kalman.results import _get_era_rmse, _plot_gap_len\n\n\ndata = prep_df(agg_gap_len(gap_len))\ndata = pd.merge(data, _get_era_rmse(hai, hai_era), on='var', how='left')\ndata = data[data.method == 'KalmanFilter']\ndata = data.astype({'gap_len': float})\n\n\ndata\n\n\ndata.era_rmse = 3\n\n\ndata.query(\"var == 'SWC'\")\n\n\n_plot_gap_len(data.query(\"var == 'SWC'\"), \"\")\n\n\np\n\n\n\n\n\nmodels_nc = pd.DataFrame({'model': [ l_model(\"1_gap_varying_336_no_control_v1.pickle\"), l_model(\"1_gap_varying_12-336_v1.pickle\")],\n                          'type':   [ 'No Control',                                       'Use Control'                         ]})                                        \n\n\n@cache_disk(\"use_control\")\ndef get_control(n_rep=n_rep):\n    \n    kcomp_control = KalmanImpComparison(models_nc, hai, hai_era, block_len=100+48*7)\n\n    k_results_control = kcomp_control.compare(n_rep =n_rep, gap_len = [12, 24, 48, 48*7], var = list(hai.columns))\n    \n    return k_results_control\n\n\nk_results_control = get_control(n_rep)\n\n\n\n\nKeyboardInterrupt: \n\n\n\nk_results_control\n\n\np = plot_compare(k_results_control, 'type', scale_domain=[\"Use Control\", \"No Control\"])\nsave_plot(p, \"use_control\")\np\n\n\nfrom functools import partial\n\n\nt = table_compare(k_results_control, 'type')\ntable_compare_latex(t, base_path_tbl / \"control.tex\", label=\"control\",\n                caption=\"Comparison between generic model with control variable (Use Control) and generic model without control variable (No Control). 50 samples for each variable and each gap length. The best result for each for each gap length is highligthed in bold\")\nt\n\n\n\n\n\nmodels_gap_all = models_var.copy()\n\n\nmodels_gap_all.model = l_model(\"All_gap_all_30_v1.pickle\") \nmodels_gap_all['Gap'] = 'All variables'\nmodels_gap_all['gap_single_var'] = False\n\nmodels_gap_gen = models_var.copy() \nmodels_gap_gen['model'] = l_model(\"1_gap_varying_12-336_v1.pickle\") \nmodels_gap_all['Gap'] = 'Only one var | generic'\nmodels_gap_all['gap_single_var'] = True\n\n\nmodels_gap_all = pd.concat([models_gap_all, models_gap_gen, models_var.assign(Gap = 'Only one var | fine tuned', gap_single_var = True)])\n\n\n@cache_disk(\"multi_gap\")\ndef get_multi_gap(n_rep):\n    kcomp_single = KalmanImpComparison(models_gap_all, hai, hai_era, block_len=120)\n\n    return kcomp_single.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nk_results_single = get_multi_gap(n_rep)\n\n\np = plot_compare(k_results_single, \"Gap\", scale_domain=[\"Only one var | fine tuned\", \"Only one var | generic\", \"All variables\"])\nsave_plot(p, \"gap_single_var\")\np\n\n\nt = table_compare(k_results_single, 'Gap')\ntable_compare_latex(t, base_path_tbl / \"gap_single_var.tex\")\nt\n\n\n\n\n\nmodels_generic = models_var.copy()\n\n\nmodels_generic.model = l_model(\"1_gap_varying_12-336_v1.pickle\") \nmodels_generic['type'] = 'Generic'\n\n\nmodels_generic\n\n\nmodels_var['type'] = 'Finetuned one var'\n\n\nmodels_gen_vs_spec = pd.concat([models_generic, models_var])\n\n\nmodels_gen_vs_spec\n\n\n@cache_disk(\"generic\")\ndef get_generic(n_rep=n_rep):\n\n    comp_generic = KalmanImpComparison(models_gen_vs_spec, hai, hai_era, block_len=100+48*7)\n\n    return comp_generic.compare(n_rep =n_rep, gap_len = [12, 24, 48, 48*7], var = list(hai.columns))\nk_results_generic = get_generic(n_rep)\n\n\np = plot_compare(k_results_generic, 'type', scale_domain=[\"Finetuned one var\", \"Generic\"])\nsave_plot(p, \"generic\")\np\n\n\nt = table_compare(k_results_generic, 'type')\ntable_compare_latex(t, base_path_tbl / \"generic.tex\")\nt\n\n\n\n\n\n\nimport polars as pl\nfrom fastai.vision.data import get_grid\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nimport statsmodels.api as sm\n\n\ndef auto_corr_df(data, nlags=48):\n    autocorr = {}\n    for col in data.columns:\n        autocorr[col] = sm.tsa.acf(data[col], nlags=nlags)\n    return pd.DataFrame(autocorr)\n\n\nauto_corr = auto_corr_df(hai).T[[1,3,6,12,24,48]]\nauto_corr.columns = auto_corr.columns /2\n\n\naxes = get_grid(2,1,2, figsize=(15,8))\nsns.heatmap(hai.corr(), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), ax=axes[0], square=True, cbar=False)\nsns.heatmap(auto_corr, annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, ax=axes[1])\naxes[1].set(ylabel=\"Variable\", xlabel=\"Shift [h]\", title=\"Temporal Autocorrelation\")\naxes[0].set(xlabel=\"Variable\", ylabel=\"Variable\", title=\"Correlation\");\nplt.tight_layout()\nplt.savefig(base_path_img / \"correlation.png\")\n\n\n\n\n\nout_dir = here(\"../fluxnet/gap_stat\")\n\n\nsite_info = pl.read_parquet(out_dir / \"../site_info.parquet\").select([\n    pl.col(\"start\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"end\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"site\").cast(pl.Categorical).sort()\n])\n\n\ndef duration_n_obs(duration):\n    \"converts a duration into a n of fluxnet observations\"\n    return abs(int(duration.total_seconds() / (30 * 60)))\n\n\nfiles = out_dir.ls()\nfiles.sort() # need to sort to match the site_info\nsites = []\nfor i, path in enumerate(files):\n    sites.append(pl.scan_parquet(path).with_columns([\n        pl.lit(site_info[i, \"site\"]).alias(\"site\"),\n        pl.lit(duration_n_obs(site_info[i, \"start\"] -  site_info[i, \"end\"])).alias(\"total_obs\"),\n        pl.col(\"TIMESTAMP_END\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\").alias(\"end\"),\n    ]).drop(\"TIMESTAMP_END\"))\n\ngap_stat = pl.concat(sites)\n\n\ndef plot_var_dist(var, small=False, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).with_column(pl.col(\"gap_len\") / (24 *2 * 7)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    if not small: ax.set_yscale('log')\n    ax.set_xlabel(\"gap length (weeks)\")\n    ax.set_ylabel(f\"{'Log' if not small else ''} n gaps\")\n    # plt.xscale('log') \n\n\nplot_var_dist('TA_F_QC')\n\n\ncolor_map = dict(zip(scale_meteo.domain, list(sns.color_palette('Set2', n_colors=len(hai.columns)).as_hex())))\n\n\nqc_map = {\n    'TA': 'TA_F_QC',\n    'SW_IN': 'SW_IN_F_QC',\n    'LW_IN': 'LW_IN_F_QC',\n    'VPD': 'VPD_F_QC',\n    'WS': 'WS_F_QC',\n    'PA': 'PA_F_QC',\n    'P': 'P_F_QC',\n    'TS': 'TS_F_MDS_1_QC',\n    'SWC': 'SWC_F_MDS_1_QC',\n}\n\n\ndef pl_in(col, values):\n    expr = False\n    for val in values:\n        expr |= pl.col(col) == val\n    return expr\n\n\ngap_stat.filter(pl_in('variable', qc_map.values())\n               ).with_columns([\n    pl.when(pl.col(\"gap_len\") &lt; 48*7).then(True).otherwise(False).alias(\"short\"),\n    pl.count().alias(\"total\"),\n    pl.count().alias(\"total len\"),\n]).groupby(\"short\").agg([\n    (pl.col(\"gap_len\").count() / pl.col(\"total\")).alias(\"frac_num\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total len\")).alias(\"frac_len\")\n]).collect()\n\n\ndef plot_var_dist_small(var, ax=None, small=True):\n    if ax is None: ax = get_grid(1)[0]\n    \n    color = color_map[var]\n    var_qc = qc_map[var]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var_qc)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) if small else True \n    ).with_column(pl.col(\"gap_len\") / (2 if small else 48 * 7)\n                 ).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax, edgecolor=\"white\", color=color)\n    ax.set_title(f\"{var} - { 'gap length &lt;  1 week' if small else 'all gaps'}\")\n    ax.set_xlabel(f\"gap length ({ 'hour' if small else 'week'})\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n\n\nvars = gap_stat.select(pl.col(\"variable\").unique()).collect()\n\n\nvars.filter(pl.col(\"variable\").str.contains(\"TA\"))\n\n\nfor ax, var in zip(get_grid(9,3,3, figsize=(15,12), sharey=False), list(var_type.categories)):\n    plot_var_dist_small(var, ax=ax)\nplt.savefig(base_path_img / \"gap_len_dist_small.png\")\n\n\nfor ax, var in zip(get_grid(9,3,3, figsize=(15,12), sharey=False), list(var_type.categories)):\n    plot_var_dist_small(var, ax=ax, small=False)\nplt.savefig(base_path_img / \"gap_len_dist.png\")\n\nmethods colors\n\nprint(sns.color_palette('Dark2').as_hex())"
  },
  {
    "objectID": "results/train_4_feb_results.html",
    "href": "results/train_4_feb_results.html",
    "title": "Training Kalman Filter for Results",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\nfrom meteo_imp.gaussian import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\n\nfrom sklearn.decomposition import PCA\n\n\nreset_seed()\n\n\nhaiB = pd.read_parquet(hai_big_path)\nhai_eraB = pd.read_parquet(hai_era_big_path)\n\n\nlist(haiB.columns)\n\n['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN']\n\n\n\n\n\ndls_TA = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=50, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_TA = KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = haiB, pred_only_gap=True, use_conditional=True)\n\n\nsave_models_TA = SaveModelsBatch(times_epoch=10)\n\n\nitems_TA = random.choices(dls_TA.valid.items, k=4)\n\n\nlearn_TA = Learner(dls_TA, model_TA, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_TA], metrics=rmse_gap)\n\n\nlearn_TA.fit(1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-17.669978\n-18.692597\n0.040505\n11:47\n\n\n\n\n\n\nlearn_TA.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_TA.model, \"trained_4_feb_TA_gap_12_v1.pickle\")\n\n\nlearn_TA\n\n\nKalman Filter (7 obs, 14 state, 14 contr)  $A$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\n\n\n\n\nx_0\n1.0540\n0.0263\n-0.1801\n0.1166\n-0.0537\n0.0176\n-0.0122\n0.9143\n0.0358\n0.2069\n-0.1760\n0.0973\n0.0417\n-0.0181\n\n\nx_1\n0.0331\n1.0263\n-0.1854\n0.1730\n-0.0476\n0.0026\n-0.0084\n0.0203\n1.0045\n0.2047\n-0.2114\n0.0382\n0.1549\n-0.0265\n\n\nx_2\n-0.0073\n-0.0518\n1.1940\n-0.1817\n0.0451\n0.0350\n-0.0043\n-0.0032\n0.0095\n0.7757\n0.1996\n-0.0706\n-0.0505\n0.0121\n\n\nx_3\n0.0022\n0.0489\n-0.2066\n1.1895\n-0.0348\n-0.0384\n0.0039\n0.0109\n-0.0043\n0.2244\n0.7726\n0.0558\n0.0648\n-0.0132\n\n\nx_4\n-0.0512\n-0.0249\n0.2067\n-0.1689\n1.0340\n-0.0274\n0.0076\n0.0279\n-0.0143\n-0.2281\n0.2212\n0.9361\n-0.0667\n0.0287\n\n\nx_5\n-0.0319\n-0.0366\n0.1644\n-0.1662\n0.0304\n0.9821\n0.0050\n-0.0164\n0.0291\n-0.2027\n0.2111\n-0.0685\n0.8243\n0.0220\n\n\nx_6\n-0.0558\n0.0097\n0.0369\n-0.0054\n0.0365\n-0.0207\n1.0128\n-0.0391\n0.0098\n-0.0321\n0.0340\n-0.0274\n-0.0362\n1.1283\n\n\nx_7\n0.0175\n-0.0001\n-0.0490\n0.0358\n-0.0187\n0.0404\n-0.0147\n1.0056\n-0.0110\n-0.1325\n0.0999\n0.0109\n0.0650\n0.0072\n\n\nx_8\n0.0198\n0.0140\n-0.0681\n0.0669\n-0.0049\n0.0069\n-0.0147\n-0.0261\n0.9871\n-0.0827\n0.0552\n0.0196\n0.0729\n0.0038\n\n\nx_9\n-0.0124\n-0.0219\n0.0507\n-0.0514\n0.0192\n-0.0041\n0.0071\n0.0212\n0.0110\n1.0889\n-0.0715\n-0.0237\n-0.0894\n-0.0072\n\n\nx_10\n0.0022\n0.0194\n-0.0439\n0.0492\n-0.0057\n-0.0017\n-0.0091\n-0.0213\n-0.0032\n-0.1142\n1.0904\n0.0206\n0.0681\n0.0026\n\n\nx_11\n-0.0269\n-0.0079\n0.0741\n-0.0596\n0.0211\n-0.0365\n0.0127\n0.0027\n0.0129\n0.0879\n-0.0527\n0.9728\n-0.0868\n-0.0051\n\n\nx_12\n-0.0138\n-0.0181\n0.0449\n-0.0477\n-0.0084\n-0.0033\n0.0205\n0.0170\n0.0028\n0.1311\n-0.1018\n-0.0136\n0.9346\n-0.0122\n\n\nx_13\n-0.0303\n-0.0016\n0.0642\n-0.0774\n0.0129\n-0.0009\n0.0150\n0.0006\n0.0311\n-0.0401\n0.0387\n-0.0224\n-0.0663\n1.0180\n\n\n\n\n$Q$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\n\n\n\n\nx_0\n0.0603\n0.0128\n0.0115\n0.0061\n0.0150\n-0.0126\n0.0103\n0.0076\n-0.0200\n-0.0013\n-0.0074\n0.0004\n0.0211\n0.0006\n\n\nx_1\n0.0128\n0.1126\n0.0136\n-0.0148\n-0.0008\n-0.0037\n-0.0113\n0.0081\n-0.0175\n0.0001\n0.0029\n0.0102\n-0.0094\n0.0078\n\n\nx_2\n0.0115\n0.0136\n0.0798\n0.0384\n-0.0597\n-0.0427\n-0.0099\n-0.0391\n0.0153\n0.0081\n-0.0137\n-0.0111\n0.0390\n-0.0067\n\n\nx_3\n0.0061\n-0.0148\n0.0384\n0.0868\n0.0370\n0.0523\n-0.0017\n0.0122\n-0.0219\n0.0202\n-0.0043\n0.0327\n-0.0257\n0.0129\n\n\nx_4\n0.0150\n-0.0008\n-0.0597\n0.0370\n0.1871\n0.0655\n0.0114\n0.0448\n-0.0234\n0.0061\n0.0197\n0.0317\n-0.0447\n0.0082\n\n\nx_5\n-0.0126\n-0.0037\n-0.0427\n0.0523\n0.0655\n0.1900\n0.0022\n0.0155\n-0.0198\n-0.0082\n-0.0155\n0.0184\n-0.0427\n0.0062\n\n\nx_6\n0.0103\n-0.0113\n-0.0099\n-0.0017\n0.0114\n0.0022\n0.1411\n0.0030\n-0.0052\n-0.0030\n0.0024\n0.0027\n-0.0025\n-0.0092\n\n\nx_7\n0.0076\n0.0081\n-0.0391\n0.0122\n0.0448\n0.0155\n0.0030\n0.1724\n-0.0748\n0.0415\n0.0064\n0.0843\n-0.0691\n0.0358\n\n\nx_8\n-0.0200\n-0.0175\n0.0153\n-0.0219\n-0.0234\n-0.0198\n-0.0052\n-0.0748\n0.1626\n-0.0003\n-0.0328\n-0.0405\n0.1111\n-0.0152\n\n\nx_9\n-0.0013\n0.0001\n0.0081\n0.0202\n0.0061\n-0.0082\n-0.0030\n0.0415\n-0.0003\n0.1007\n0.0491\n-0.0237\n-0.0595\n-0.0133\n\n\nx_10\n-0.0074\n0.0029\n-0.0137\n-0.0043\n0.0197\n-0.0155\n0.0024\n0.0064\n-0.0328\n0.0491\n0.1143\n0.0264\n-0.0264\n0.0155\n\n\nx_11\n0.0004\n0.0102\n-0.0111\n0.0327\n0.0317\n0.0184\n0.0027\n0.0843\n-0.0405\n-0.0237\n0.0264\n0.2406\n-0.0565\n0.0455\n\n\nx_12\n0.0211\n-0.0094\n0.0390\n-0.0257\n-0.0447\n-0.0427\n-0.0025\n-0.0691\n0.1111\n-0.0595\n-0.0264\n-0.0565\n0.2944\n-0.0166\n\n\nx_13\n0.0006\n0.0078\n-0.0067\n0.0129\n0.0082\n0.0062\n-0.0092\n0.0358\n-0.0152\n-0.0133\n0.0155\n0.0455\n-0.0166\n0.2207\n\n\n\n\n\n$b$\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0051\n\n\nx_1\n0.0013\n\n\nx_2\n-0.0071\n\n\nx_3\n0.0069\n\n\nx_4\n0.0000\n\n\nx_5\n0.0048\n\n\nx_6\n0.0097\n\n\nx_7\n-0.0066\n\n\nx_8\n-0.0045\n\n\nx_9\n0.0044\n\n\nx_10\n-0.0051\n\n\nx_11\n0.0065\n\n\nx_12\n0.0051\n\n\nx_13\n-0.0005\n\n\n\n\n\n$H$\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\n\n\n\n\ny_0\n0.0096\n-0.1188\n0.6133\n-0.2500\n0.0814\n0.0259\n0.0046\n-0.0204\n-0.0681\n0.1127\n-0.0955\n0.0484\n-0.0042\n0.0207\n\n\ny_1\n0.9850\n0.1105\n0.2278\n-0.0157\n0.0148\n-0.1788\n0.0102\n0.1131\n-0.0796\n0.1846\n-0.1719\n0.0587\n0.1843\n-0.0333\n\n\ny_2\n0.0415\n-0.0219\n0.7257\n0.6141\n-0.0440\n0.0144\n-0.0002\n0.0166\n0.0696\n-0.1107\n0.0991\n-0.0417\n-0.0550\n-0.0219\n\n\ny_3\n0.0665\n0.0416\n0.2722\n-0.2680\n-0.2304\n-0.9431\n0.0098\n0.0517\n0.0827\n-0.0350\n0.0336\n-0.0461\n0.0551\n-0.0349\n\n\ny_4\n-0.0238\n-0.0257\n-0.0034\n-0.0257\n0.0291\n0.0403\n1.2371\n-0.0236\n0.0283\n-0.0672\n0.0391\n-0.0402\n-0.0049\n-0.1147\n\n\ny_5\n0.1026\n-0.1011\n-0.2424\n0.0548\n1.0465\n0.0073\n0.0193\n-0.0573\n-0.0184\n-0.1912\n0.2090\n0.0438\n-0.0392\n0.0262\n\n\ny_6\n-0.1024\n-1.0913\n-0.1905\n0.2058\n0.0868\n-0.1214\n0.0166\n-0.0516\n0.0307\n0.0977\n-0.0751\n-0.0070\n-0.0340\n0.0021\n\n\n\n\n\n$R$\n\n\n\nvariable\ny_0\ny_1\ny_2\ny_3\ny_4\ny_5\ny_6\n\n\n\n\ny_0\n0.0024\n-0.0171\n0.0077\n0.0078\n-0.0003\n0.0176\n-0.0061\n\n\ny_1\n-0.0171\n0.1305\n-0.0550\n-0.0581\n0.0045\n-0.1186\n0.0311\n\n\ny_2\n0.0077\n-0.0550\n0.0310\n0.0102\n0.0010\n0.0652\n-0.0293\n\n\ny_3\n0.0078\n-0.0581\n0.0102\n0.0699\n-0.0053\n0.0594\n0.0204\n\n\ny_4\n-0.0003\n0.0045\n0.0010\n-0.0053\n0.0145\n0.0066\n-0.0025\n\n\ny_5\n0.0176\n-0.1186\n0.0652\n0.0594\n0.0066\n0.2732\n0.0217\n\n\ny_6\n-0.0061\n0.0311\n-0.0293\n0.0204\n-0.0025\n0.0217\n0.1247\n\n\n\n\n\n$d$\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0038\n\n\ny_1\n-0.0016\n\n\ny_2\n0.0033\n\n\ny_3\n-0.0048\n\n\ny_4\n0.0000\n\n\ny_5\n-0.0174\n\n\ny_6\n-0.0078\n\n\n\n\n\n$B$\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\nc_6\nc_7\nc_8\nc_9\nc_10\nc_11\nc_12\nc_13\n\n\n\n\nx_0\n0.1167\n-0.9769\n-0.0553\n0.0013\n0.0001\n-0.0145\n-0.0575\n0.1969\n0.9707\n-0.0257\n-0.0092\n0.0141\n0.0467\n0.0306\n\n\nx_1\n0.1976\n0.0029\n-0.0212\n0.0082\n-0.0222\n-0.0687\n0.9226\n0.0571\n0.0154\n-0.0553\n-0.0020\n-0.0017\n0.0169\n-0.9740\n\n\nx_2\n-0.8669\n-0.0072\n-0.4643\n-0.0086\n0.0123\n0.1030\n0.1404\n0.7364\n0.0033\n0.4928\n0.0606\n-0.0111\n-0.0839\n-0.1544\n\n\nx_3\n0.5027\n0.0291\n-0.8831\n-0.0053\n-0.0012\n-0.0808\n-0.0618\n-0.3491\n-0.0220\n0.8552\n-0.0398\n0.0016\n0.0779\n0.0841\n\n\nx_4\n-0.1806\n-0.0226\n0.0442\n0.1440\n0.0033\n-0.9289\n0.0550\n-0.1573\n0.0184\n0.0263\n-0.1280\n0.0084\n0.9363\n-0.0234\n\n\nx_5\n-0.1248\n-0.0215\n0.0321\n0.9747\n-0.0141\n0.2016\n0.0474\n-0.1575\n0.0181\n0.0381\n-0.9735\n0.0186\n-0.1620\n-0.0088\n\n\nx_6\n-0.0989\n-0.0234\n0.0083\n-0.0287\n-0.9298\n0.0117\n0.0258\n-0.1111\n-0.0121\n0.0174\n0.0214\n0.9692\n-0.0285\n0.0169\n\n\nx_7\n0.0672\n-0.0093\n-0.0187\n0.0061\n-0.0021\n0.0007\n-0.0170\n0.0515\n-0.0079\n-0.0154\n0.0087\n-0.0062\n-0.0037\n-0.0194\n\n\nx_8\n0.0901\n-0.0058\n-0.0231\n0.0070\n0.0006\n0.0007\n-0.0120\n0.0943\n-0.0096\n-0.0184\n0.0063\n0.0007\n0.0153\n-0.0104\n\n\nx_9\n-0.0884\n0.0108\n0.0269\n-0.0104\n-0.0066\n-0.0051\n0.0058\n-0.0851\n0.0131\n0.0219\n-0.0113\n-0.0029\n-0.0140\n0.0054\n\n\nx_10\n0.0883\n-0.0110\n-0.0225\n0.0082\n0.0050\n0.0123\n-0.0054\n0.0796\n-0.0118\n-0.0177\n0.0095\n0.0018\n0.0159\n-0.0074\n\n\nx_11\n-0.0780\n0.0025\n0.0207\n-0.0054\n0.0053\n0.0003\n0.0141\n-0.0827\n0.0086\n0.0160\n-0.0047\n0.0042\n-0.0142\n0.0131\n\n\nx_12\n-0.0863\n0.0055\n0.0210\n-0.0079\n0.0008\n-0.0018\n0.0159\n-0.0798\n0.0033\n0.0176\n-0.0084\n-0.0008\n-0.0056\n0.0178\n\n\nx_13\n-0.0636\n-0.0051\n0.0192\n-0.0140\n-0.0009\n0.0355\n0.0006\n-0.0683\n0.0025\n0.0169\n-0.0139\n-0.0022\n0.0233\n-0.0019\n\n\n\n\n\n$m_0$\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0011\n\n\nx_1\n-0.0008\n\n\nx_2\n-0.0004\n\n\nx_3\n-0.0001\n\n\nx_4\n-0.0002\n\n\nx_5\n0.0003\n\n\nx_6\n-0.0000\n\n\nx_7\n-0.0008\n\n\nx_8\n0.0006\n\n\nx_9\n0.0003\n\n\nx_10\n-0.0005\n\n\nx_11\n-0.0003\n\n\nx_12\n-0.0012\n\n\nx_13\n-0.0000\n\n\n\n\n\n$P_0$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\n\n\n\n\nx_0\n2.9565\n-0.0008\n0.0011\n0.0001\n-0.0007\n-0.0009\n0.0002\n-0.0001\n0.0011\n-0.0006\n0.0005\n-0.0011\n-0.0014\n0.0004\n\n\nx_1\n-0.0008\n2.9605\n-0.0000\n0.0004\n0.0015\n-0.0006\n0.0000\n0.0005\n-0.0009\n0.0003\n-0.0002\n0.0002\n0.0015\n-0.0003\n\n\nx_2\n0.0011\n-0.0000\n2.9587\n-0.0004\n-0.0004\n0.0008\n-0.0002\n0.0003\n-0.0002\n-0.0003\n0.0003\n0.0003\n-0.0002\n-0.0002\n\n\nx_3\n0.0001\n0.0004\n-0.0004\n2.9591\n0.0011\n-0.0002\n-0.0003\n0.0007\n-0.0005\n-0.0001\n-0.0006\n-0.0005\n0.0007\n0.0000\n\n\nx_4\n-0.0007\n0.0015\n-0.0004\n0.0011\n2.9663\n-0.0018\n-0.0008\n-0.0026\n0.0005\n0.0010\n-0.0003\n0.0005\n0.0002\n-0.0005\n\n\nx_5\n-0.0009\n-0.0006\n0.0008\n-0.0002\n-0.0018\n2.9595\n0.0005\n0.0017\n0.0001\n-0.0009\n0.0002\n-0.0014\n-0.0003\n0.0000\n\n\nx_6\n0.0002\n0.0000\n-0.0002\n-0.0003\n-0.0008\n0.0005\n2.9590\n0.0010\n-0.0003\n-0.0002\n-0.0001\n-0.0002\n0.0005\n0.0001\n\n\nx_7\n-0.0001\n0.0005\n0.0003\n0.0007\n-0.0026\n0.0017\n0.0010\n2.9589\n-0.0008\n0.0004\n-0.0001\n0.0009\n0.0007\n-0.0010\n\n\nx_8\n0.0011\n-0.0009\n-0.0002\n-0.0005\n0.0005\n0.0001\n-0.0003\n-0.0008\n2.9602\n-0.0002\n0.0002\n-0.0001\n-0.0015\n0.0003\n\n\nx_9\n-0.0006\n0.0003\n-0.0003\n-0.0001\n0.0010\n-0.0009\n-0.0002\n0.0004\n-0.0002\n2.9588\n-0.0002\n-0.0003\n0.0005\n0.0003\n\n\nx_10\n0.0005\n-0.0002\n0.0003\n-0.0006\n-0.0003\n0.0002\n-0.0001\n-0.0001\n0.0002\n-0.0002\n2.9597\n0.0003\n-0.0004\n0.0001\n\n\nx_11\n-0.0011\n0.0002\n0.0003\n-0.0005\n0.0005\n-0.0014\n-0.0002\n0.0009\n-0.0001\n-0.0003\n0.0003\n2.9583\n0.0005\n0.0004\n\n\nx_12\n-0.0014\n0.0015\n-0.0002\n0.0007\n0.0002\n-0.0003\n0.0005\n0.0007\n-0.0015\n0.0005\n-0.0004\n0.0005\n2.9619\n-0.0007\n\n\nx_13\n0.0004\n-0.0003\n-0.0002\n0.0000\n-0.0005\n0.0000\n0.0001\n-0.0010\n0.0003\n0.0003\n0.0001\n0.0004\n-0.0007\n2.9587\n\n\n\n\n\n \n\n\n\nshow_results(learn_TA, items = items_TA, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=4518, shift=15, var_sel=['TA'], gap_len=12), MeteoImpItem(i=3825, shift=5, var_sel=['TA'], gap_len=12), MeteoImpItem(i=3706, shift=-25, var_sel=['TA'], gap_len=12), MeteoImpItem(i=4397, shift=15, var_sel=['TA'], gap_len=12)]\n\n\n\n\n\n\n\n\n\nafter the first trainig fine tune the model for longer gaps\n\ndls_TA96 = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=300, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_TA96 = model_TA.copy()\n\n\nsave_models_TA96 = SaveModelsBatch(times_epoch=5)\n\n\nitems_TA96 = random.choices(dls_TA96.valid.items, k=4)\n\n\nlearn_TA96 = Learner(dls_TA96, model_TA96, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_TA96], metrics=rmse_gap)\n\n\nshow_results(learn_TA96, items = items_TA96, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=723, shift=90, var_sel=['TA'], gap_len=96), MeteoImpItem(i=718, shift=30, var_sel=['TA'], gap_len=96), MeteoImpItem(i=706, shift=-150, var_sel=['TA'], gap_len=96), MeteoImpItem(i=756, shift=90, var_sel=['TA'], gap_len=96)]\n\n\n\n\n\n\n\n\nlearn_TA96.fit(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-61.745678\n-63.454943\n0.116816\n22:50\n\n\n\n\n\n\nlearn_TA96.recorder.plot_loss()\n\n\n\n\n\nshow_results(learn_TA96, items = items_TA96, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=723, shift=90, var_sel=['TA'], gap_len=96), MeteoImpItem(i=718, shift=30, var_sel=['TA'], gap_len=96), MeteoImpItem(i=706, shift=-150, var_sel=['TA'], gap_len=96), MeteoImpItem(i=756, shift=90, var_sel=['TA'], gap_len=96)]\n\n\n\n\n\n\n\n\ntorch.save(learn_TA96.model, \"trained_4_feb_TA_gap_96_v1.pickle\")\n\n\n\n\n\nModel Av (All varying) all variables with a varing numbers of variables\n\ndls_Av = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Av = model_TA.copy() \n\n\nsave_models_Av = SaveModelsBatch(times_epoch=1) # save once per repetition\n\n\nitems_Av = random.choices(dls_Av.valid.items, k=4)\n\n\nlearn_Av = Learner(dls_Av, model_Av, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_Av], metrics=rmse_gap)\n\n\nlearn_Av.fit(2, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n27.495375\n25.322661\n0.489185\n26:10\n\n\n1\n14.247212\n13.797795\n0.408078\n26:31\n\n\n\n\n\n\nlearn_Av.recorder.plot_loss()\n\n\n\n\n\nwith with_settings(learn_Av.model, use_conditional =False): #speed up training\n    learn_Av.fit(2, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n7.901138\n7.892225\n0.360798\n23:14\n\n\n1\n2.295464\n4.088155\n0.336661\n23:39\n\n\n\n\n\n\nlearn_Av.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_Av.model, \"trained_4_feb_All_gap_varying_12_v1.pickle\")\n\n\nshow_results(learn_Av, items=items_Av, control_map=control_map)\n\n[MeteoImpItem(i=1867, shift=0, var_sel=['TA'], gap_len=12), MeteoImpItem(i=2001, shift=-50, var_sel=['PA', 'VPD', 'LW_IN', 'TA', 'WS'], gap_len=12), MeteoImpItem(i=1960, shift=-40, var_sel=['TA', 'PA', 'P', 'SW_IN', 'VPD', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=2163, shift=30, var_sel=['TA'], gap_len=12)]\n\n\n\n\n\n\n\n\nwith with_settings(learn_Av.model, use_conditional =False): #speed up training\n    learn_Av.fit(2, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n3.646232\n1.610648\n0.321632\n24:10\n\n\n1\n-1.735119\n-0.024182\n0.312380\n24:34\n\n\n\n\n\n\nlearn_Av.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_Av.model, \"trained_4_feb_All_gap_varying_12_v2.pickle\")\n\n\nshow_results(learn_Av, items=items_Av, control_map=control_map)\n\n[MeteoImpItem(i=1867, shift=0, var_sel=['TA'], gap_len=12), MeteoImpItem(i=2001, shift=-50, var_sel=['PA', 'VPD', 'LW_IN', 'TA', 'WS'], gap_len=12), MeteoImpItem(i=1960, shift=-40, var_sel=['TA', 'PA', 'P', 'SW_IN', 'VPD', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=2163, shift=30, var_sel=['TA'], gap_len=12)]\n\n\n\n\n\n\n\n\n\n\n\ndls_Aa = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=70, gap_len=12, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa = model_Av.copy() \nmodel_Aa.use_conditional = False\n\n\nsave_models_Aa = SaveModelsBatch(times_epoch=1) # save once per repetition\n\n\nitems_Aa = random.choices(dls_Aa.valid.items, k=4)\n\n\nlearn_Aa = Learner(dls_Aa, model_Aa, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_Aa], metrics=rmse_gap)\n\n\nlearn_Aa.fit(2, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-3.860496\n-2.163738\n0.297889\n32:06\n\n\n1\n-3.991504\n-3.813984\n0.290560\n32:34\n\n\n\n\n\n\nlearn_Aa.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_Aa.model, \"trained_4_feb_All_gap_all_12_v1.pickle\")\n\n\nshow_results(learn_Aa, items=items_Aa, control_map=control_map)\n\n[MeteoImpItem(i=3064, shift=7, var_sel=['P', 'VPD', 'WS', 'SW_IN', 'PA', 'TA'], gap_len=12), MeteoImpItem(i=2776, shift=7, var_sel=['VPD', 'PA', 'SW_IN', 'WS'], gap_len=12), MeteoImpItem(i=3253, shift=-21, var_sel=['VPD', 'SW_IN'], gap_len=12), MeteoImpItem(i=2716, shift=14, var_sel=['SW_IN'], gap_len=12)]\n\n\n\n\n\n\n\n\n\n\n\ndls_SW_IN = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN', block_len=50, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN = model_Av.copy()\n\n\nsave_models_SW_IN = SaveModelsBatch(times_epoch=10)\n\n\nitems_SW_IN = random.choices(dls_SW_IN.valid.items, k=4)\n\n\nlearn_SW_IN = Learner(dls_SW_IN, model_SW_IN, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_SW_IN], metrics=rmse_gap)\n\n\nlearn_SW_IN.fit(3, 5e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n1.571930\n2.161097\n0.229693\n12:20\n\n\n1\n1.701283\n1.642652\n0.216893\n12:26\n\n\n2\n1.033681\n1.752522\n0.214100\n12:50\n\n\n\n\n\n\nlearn_SW_IN.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_SW_IN.model, \"trained_4_feb_SW_IN_gap_12_v1.pickle\")\n\n\nlearn_SW_IN\n\n\nKalman Filter (7 obs, 14 state, 14 contr)  $A$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\n\n\n\n\nx_0\n1.2092\n0.0099\n-0.1905\n0.0169\n-0.0984\n-0.0086\n-0.0375\n0.8756\n0.0942\n0.2282\n-0.1515\n-0.0565\n-0.0419\n-0.0731\n\n\nx_1\n0.0250\n1.1628\n-0.1911\n0.1139\n-0.0933\n-0.0061\n-0.0202\n0.0043\n0.8123\n0.1668\n-0.1492\n0.0625\n0.0224\n0.0082\n\n\nx_2\n0.0058\n-0.0328\n1.2095\n-0.1337\n0.0424\n0.0783\n0.0067\n-0.0326\n-0.0048\n0.6848\n0.1042\n-0.0030\n-0.0034\n0.0172\n\n\nx_3\n0.0506\n0.0430\n-0.1128\n1.1909\n-0.0675\n-0.0662\n0.0139\n0.0205\n0.0210\n0.1381\n0.6624\n0.0020\n0.0031\n-0.0198\n\n\nx_4\n-0.1150\n-0.0565\n0.1214\n-0.1329\n1.1548\n0.0006\n0.0438\n0.0248\n0.0160\n-0.1514\n0.2205\n0.7190\n-0.0158\n-0.0161\n\n\nx_5\n-0.0051\n-0.0034\n0.1459\n-0.1069\n0.0482\n1.0111\n0.0273\n0.1421\n-0.0900\n-0.1165\n0.1743\n-0.0579\n0.6217\n0.0555\n\n\nx_6\n-0.1867\n0.0374\n0.0283\n0.0998\n0.1161\n-0.1061\n1.0440\n0.0212\n-0.1477\n0.0734\n-0.0844\n0.0142\n0.1911\n1.1859\n\n\nx_7\n0.1577\n-0.0190\n-0.0549\n0.0844\n0.0527\n0.0865\n0.0427\n0.9606\n-0.0319\n-0.1238\n0.0597\n0.0510\n0.0823\n0.0125\n\n\nx_8\n0.0562\n0.1505\n-0.0321\n0.0300\n-0.0300\n0.0235\n-0.0623\n-0.0047\n0.9781\n-0.0897\n0.0459\n0.0068\n0.0778\n0.0129\n\n\nx_9\n0.0766\n-0.0076\n0.1531\n-0.0059\n-0.0445\n-0.0611\n0.0210\n-0.0223\n-0.0109\n1.1227\n-0.0934\n0.0140\n-0.0822\n-0.0101\n\n\nx_10\n-0.0910\n-0.0241\n-0.0252\n0.1167\n0.0297\n0.0981\n-0.0182\n-0.0112\n0.0018\n-0.1045\n1.1444\n0.0188\n0.0485\n0.0639\n\n\nx_11\n0.0049\n-0.0412\n0.0115\n-0.0502\n0.1451\n-0.0527\n-0.0148\n0.0156\n-0.0171\n0.0801\n-0.0582\n1.0207\n-0.0824\n0.0098\n\n\nx_12\n-0.1013\n0.0220\n-0.0262\n-0.0522\n0.0147\n0.1882\n0.0488\n-0.0134\n0.0225\n0.0985\n-0.0930\n-0.0503\n0.9256\n-0.0362\n\n\nx_13\n-0.0310\n0.0349\n0.0375\n-0.0787\n0.0667\n-0.0509\n0.1694\n-0.0421\n0.0158\n-0.0043\n0.0215\n0.0122\n-0.1242\n0.8931\n\n\n\n\n$Q$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\n\n\n\n\nx_0\n0.0535\n-0.0058\n0.0094\n-0.0022\n0.0197\n-0.0093\n0.0291\n0.0008\n-0.0093\n0.0033\n0.0175\n-0.0064\n0.0123\n-0.0169\n\n\nx_1\n-0.0058\n0.0853\n0.0153\n-0.0184\n0.0206\n-0.0001\n-0.0288\n0.0110\n0.0101\n0.0152\n-0.0061\n-0.0041\n0.0050\n0.0180\n\n\nx_2\n0.0094\n0.0153\n0.0469\n0.0356\n-0.0255\n-0.0048\n-0.0373\n-0.0236\n-0.0104\n0.0321\n0.0009\n0.0050\n0.0260\n0.0117\n\n\nx_3\n-0.0022\n-0.0184\n0.0356\n0.0919\n0.0083\n-0.0059\n-0.0550\n-0.0075\n-0.0501\n0.0344\n0.0013\n0.0492\n0.0305\n0.0328\n\n\nx_4\n0.0197\n0.0206\n-0.0255\n0.0083\n0.1291\n-0.0189\n0.0220\n0.0355\n-0.0023\n-0.0047\n-0.0040\n0.0122\n-0.0185\n-0.0110\n\n\nx_5\n-0.0093\n-0.0001\n-0.0048\n-0.0059\n-0.0189\n0.0369\n-0.0015\n-0.0240\n0.0130\n-0.0187\n-0.0059\n-0.0153\n0.0204\n-0.0223\n\n\nx_6\n0.0291\n-0.0288\n-0.0373\n-0.0550\n0.0220\n-0.0015\n0.2657\n0.0163\n0.0369\n-0.0117\n0.0199\n-0.0298\n-0.0415\n-0.0983\n\n\nx_7\n0.0008\n0.0110\n-0.0236\n-0.0075\n0.0355\n-0.0240\n0.0163\n0.0855\n-0.0254\n0.0138\n-0.0050\n0.0298\n-0.0230\n0.0170\n\n\nx_8\n-0.0093\n0.0101\n-0.0104\n-0.0501\n-0.0023\n0.0130\n0.0369\n-0.0254\n0.1099\n-0.0127\n-0.0089\n-0.0276\n-0.0180\n-0.0152\n\n\nx_9\n0.0033\n0.0152\n0.0321\n0.0344\n-0.0047\n-0.0187\n-0.0117\n0.0138\n-0.0127\n0.0855\n0.0023\n0.0067\n0.0010\n-0.0180\n\n\nx_10\n0.0175\n-0.0061\n0.0009\n0.0013\n-0.0040\n-0.0059\n0.0199\n-0.0050\n-0.0089\n0.0023\n0.0590\n0.0247\n-0.0185\n0.0288\n\n\nx_11\n-0.0064\n-0.0041\n0.0050\n0.0492\n0.0122\n-0.0153\n-0.0298\n0.0298\n-0.0276\n0.0067\n0.0247\n0.1196\n-0.0086\n0.0207\n\n\nx_12\n0.0123\n0.0050\n0.0260\n0.0305\n-0.0185\n0.0204\n-0.0415\n-0.0230\n-0.0180\n0.0010\n-0.0185\n-0.0086\n0.0850\n0.0112\n\n\nx_13\n-0.0169\n0.0180\n0.0117\n0.0328\n-0.0110\n-0.0223\n-0.0983\n0.0170\n-0.0152\n-0.0180\n0.0288\n0.0207\n0.0112\n0.3036\n\n\n\n\n\n$b$\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0035\n\n\nx_1\n-0.0264\n\n\nx_2\n0.0016\n\n\nx_3\n-0.0055\n\n\nx_4\n-0.0095\n\n\nx_5\n-0.0025\n\n\nx_6\n-0.0167\n\n\nx_7\n0.0090\n\n\nx_8\n0.0001\n\n\nx_9\n0.0003\n\n\nx_10\n0.0088\n\n\nx_11\n-0.0035\n\n\nx_12\n0.0114\n\n\nx_13\n-0.0019\n\n\n\n\n\n$H$\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\n\n\n\n\ny_0\n0.1119\n-0.1229\n0.5709\n-0.2683\n0.0774\n-0.0004\n0.0066\n0.0550\n-0.0632\n0.0880\n-0.0444\n0.0467\n-0.0558\n0.0246\n\n\ny_1\n0.7579\n0.1629\n0.2327\n-0.0197\n-0.0384\n-0.2112\n-0.0065\n0.1209\n-0.0729\n0.2284\n-0.0980\n0.0492\n0.1636\n-0.0476\n\n\ny_2\n0.0352\n0.0115\n0.5271\n0.4768\n-0.0234\n0.0289\n-0.0082\n0.0849\n0.0510\n-0.0186\n0.0404\n-0.0603\n-0.1070\n-0.0790\n\n\ny_3\n0.0269\n0.0600\n0.1747\n-0.1470\n-0.0931\n-0.6539\n0.0019\n-0.0266\n0.0604\n-0.0557\n-0.0043\n-0.0811\n-0.0164\n-0.0517\n\n\ny_4\n0.0068\n-0.1011\n-0.0548\n-0.1172\n-0.0165\n0.0706\n1.3534\n-0.0528\n0.0761\n-0.1036\n0.1065\n-0.1210\n0.0232\n-0.3115\n\n\ny_5\n0.0833\n-0.0442\n-0.1637\n0.0456\n0.9350\n0.1287\n-0.0171\n-0.0895\n-0.0563\n-0.1668\n0.1400\n0.1260\n-0.0767\n0.0628\n\n\ny_6\n-0.1897\n-0.8226\n-0.0693\n0.0926\n0.0038\n-0.1236\n0.0132\n-0.0346\n-0.1286\n0.1014\n-0.0264\n0.0526\n-0.0744\n0.0379\n\n\n\n\n\n$R$\n\n\n\nvariable\ny_0\ny_1\ny_2\ny_3\ny_4\ny_5\ny_6\n\n\n\n\ny_0\n0.0014\n-0.0038\n0.0011\n0.0035\n0.0017\n0.0021\n-0.0001\n\n\ny_1\n-0.0038\n0.0176\n-0.0059\n-0.0111\n0.0069\n0.0018\n0.0015\n\n\ny_2\n0.0011\n-0.0059\n0.0058\n0.0040\n0.0065\n-0.0014\n-0.0039\n\n\ny_3\n0.0035\n-0.0111\n0.0040\n0.0107\n0.0027\n0.0077\n-0.0039\n\n\ny_4\n0.0017\n0.0069\n0.0065\n0.0027\n0.0722\n-0.0017\n-0.0053\n\n\ny_5\n0.0021\n0.0018\n-0.0014\n0.0077\n-0.0017\n0.0376\n-0.0054\n\n\ny_6\n-0.0001\n0.0015\n-0.0039\n-0.0039\n-0.0053\n-0.0054\n0.0136\n\n\n\n\n\n$d$\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0007\n\n\ny_1\n-0.0063\n\n\ny_2\n-0.0204\n\n\ny_3\n0.0145\n\n\ny_4\n-0.0090\n\n\ny_5\n-0.0468\n\n\ny_6\n-0.0133\n\n\n\n\n\n$B$\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\nc_6\nc_7\nc_8\nc_9\nc_10\nc_11\nc_12\nc_13\n\n\n\n\nx_0\n0.1186\n-0.8485\n-0.0630\n0.1522\n0.0082\n-0.0564\n-0.0824\n0.1909\n0.8190\n-0.0354\n0.1066\n-0.0523\n0.0139\n-0.0462\n\n\nx_1\n0.2492\n0.0849\n-0.0074\n0.0056\n-0.0049\n-0.0478\n0.8928\n0.1398\n0.0167\n-0.0199\n-0.0437\n0.0060\n0.0587\n-0.7312\n\n\nx_2\n-0.8873\n-0.0521\n-0.4452\n0.0263\n0.0363\n0.1346\n0.1307\n0.6552\n0.0144\n0.4837\n0.1016\n0.0027\n-0.0666\n-0.1407\n\n\nx_3\n0.5261\n-0.0119\n-0.8469\n-0.0277\n-0.0336\n-0.1063\n-0.0656\n-0.2830\n-0.1705\n0.8257\n-0.0865\n0.0056\n0.0958\n0.0383\n\n\nx_4\n-0.1738\n0.0272\n0.0501\n0.1181\n-0.0083\n-0.9154\n0.0459\n-0.1750\n0.1252\n0.0355\n-0.1135\n0.0329\n0.8490\n-0.0108\n\n\nx_5\n-0.0502\n-0.1978\n0.0494\n0.9221\n-0.0033\n0.2204\n0.0062\n-0.1387\n-0.0034\n-0.0207\n-0.9759\n0.0176\n-0.1051\n-0.0576\n\n\nx_6\n-0.0261\n0.0876\n-0.0362\n-0.0741\n-0.6339\n0.0092\n0.0529\n-0.0338\n0.0857\n-0.0185\n-0.0145\n0.6687\n-0.0498\n0.0243\n\n\nx_7\n0.0388\n-0.1206\n-0.0035\n0.1071\n-0.0304\n-0.0273\n-0.0187\n0.0207\n-0.1097\n0.0004\n0.1018\n-0.0148\n-0.0261\n-0.0148\n\n\nx_8\n0.0774\n-0.0308\n-0.0022\n0.0180\n0.0222\n0.0056\n0.0632\n0.0798\n-0.0283\n-0.0045\n0.0180\n-0.0022\n0.0181\n0.0522\n\n\nx_9\n-0.0570\n-0.0036\n-0.0621\n-0.0331\n0.0118\n0.0193\n0.0259\n-0.0540\n-0.0366\n-0.0568\n-0.0314\n0.0155\n0.0005\n0.0092\n\n\nx_10\n0.0497\n0.0343\n-0.0569\n0.0741\n0.0015\n-0.0078\n-0.0098\n0.0436\n0.0464\n-0.0525\n0.0730\n-0.0180\n-0.0003\n0.0007\n\n\nx_11\n-0.0501\n0.0310\n0.0116\n-0.0328\n0.0160\n-0.0563\n-0.0090\n-0.0572\n0.0090\n0.0115\n-0.0320\n0.0180\n-0.0631\n-0.0271\n\n\nx_12\n-0.0648\n0.0881\n0.0226\n0.0939\n-0.0059\n-0.0011\n0.0477\n-0.0567\n0.0641\n0.0235\n0.0896\n-0.0028\n-0.0068\n0.0513\n\n\nx_13\n-0.0057\n0.0274\n-0.0023\n-0.0298\n-0.1657\n0.0082\n0.0236\n-0.0124\n0.0238\n0.0026\n-0.0266\n0.0452\n-0.0011\n0.0150\n\n\n\n\n\n$m_0$\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0011\n\n\nx_1\n-0.0008\n\n\nx_2\n-0.0004\n\n\nx_3\n-0.0001\n\n\nx_4\n-0.0002\n\n\nx_5\n0.0003\n\n\nx_6\n-0.0000\n\n\nx_7\n-0.0008\n\n\nx_8\n0.0006\n\n\nx_9\n0.0003\n\n\nx_10\n-0.0005\n\n\nx_11\n-0.0003\n\n\nx_12\n-0.0012\n\n\nx_13\n-0.0000\n\n\n\n\n\n$P_0$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\n\n\n\n\nx_0\n2.8754\n-0.0007\n0.0011\n0.0001\n-0.0005\n-0.0010\n0.0001\n-0.0001\n0.0010\n-0.0006\n0.0007\n-0.0014\n-0.0013\n0.0003\n\n\nx_1\n-0.0007\n2.8798\n0.0000\n0.0004\n0.0015\n-0.0006\n-0.0000\n0.0004\n-0.0009\n0.0003\n-0.0001\n0.0001\n0.0014\n-0.0003\n\n\nx_2\n0.0011\n0.0000\n2.8779\n-0.0005\n-0.0004\n0.0008\n-0.0002\n0.0004\n-0.0002\n-0.0003\n0.0003\n0.0003\n-0.0002\n-0.0001\n\n\nx_3\n0.0001\n0.0004\n-0.0005\n2.8785\n0.0010\n-0.0002\n-0.0002\n0.0008\n-0.0004\n-0.0001\n-0.0009\n-0.0003\n0.0007\n0.0001\n\n\nx_4\n-0.0005\n0.0015\n-0.0004\n0.0010\n2.8854\n-0.0017\n-0.0008\n-0.0026\n0.0005\n0.0011\n-0.0001\n0.0004\n0.0002\n-0.0005\n\n\nx_5\n-0.0010\n-0.0006\n0.0008\n-0.0002\n-0.0017\n2.8786\n0.0005\n0.0016\n0.0001\n-0.0009\n0.0002\n-0.0014\n-0.0003\n0.0000\n\n\nx_6\n0.0001\n-0.0000\n-0.0002\n-0.0002\n-0.0008\n0.0005\n2.8783\n0.0010\n-0.0002\n-0.0002\n-0.0002\n-0.0002\n0.0005\n0.0001\n\n\nx_7\n-0.0001\n0.0004\n0.0004\n0.0008\n-0.0026\n0.0016\n0.0010\n2.8781\n-0.0007\n0.0004\n-0.0001\n0.0009\n0.0007\n-0.0009\n\n\nx_8\n0.0010\n-0.0009\n-0.0002\n-0.0004\n0.0005\n0.0001\n-0.0002\n-0.0007\n2.8794\n-0.0002\n0.0001\n0.0000\n-0.0015\n0.0003\n\n\nx_9\n-0.0006\n0.0003\n-0.0003\n-0.0001\n0.0011\n-0.0009\n-0.0002\n0.0004\n-0.0002\n2.8781\n-0.0001\n-0.0003\n0.0005\n0.0003\n\n\nx_10\n0.0007\n-0.0001\n0.0003\n-0.0009\n-0.0001\n0.0002\n-0.0002\n-0.0001\n0.0001\n-0.0001\n2.8795\n0.0001\n-0.0004\n-0.0000\n\n\nx_11\n-0.0014\n0.0001\n0.0003\n-0.0003\n0.0004\n-0.0014\n-0.0002\n0.0009\n0.0000\n-0.0003\n0.0001\n2.8775\n0.0004\n0.0004\n\n\nx_12\n-0.0013\n0.0014\n-0.0002\n0.0007\n0.0002\n-0.0003\n0.0005\n0.0007\n-0.0015\n0.0005\n-0.0004\n0.0004\n2.8811\n-0.0007\n\n\nx_13\n0.0003\n-0.0003\n-0.0001\n0.0001\n-0.0005\n0.0000\n0.0001\n-0.0009\n0.0003\n0.0003\n-0.0000\n0.0004\n-0.0007\n2.8780\n\n\n\n\n\n \n\n\n\nshow_results(learn_SW_IN, items = items_SW_IN, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=3927, shift=-15, var_sel=['SW_IN'], gap_len=12), MeteoImpItem(i=4045, shift=-5, var_sel=['SW_IN'], gap_len=12), MeteoImpItem(i=4390, shift=-25, var_sel=['SW_IN'], gap_len=12), MeteoImpItem(i=3968, shift=-25, var_sel=['SW_IN'], gap_len=12)]\n\n\n\n\n\n\n\n\n\nafter the first trainig fine tune the model for longer gaps\n\ndls_SW_IN96 = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN', block_len=300, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN96 = model_SW_IN.copy()\n\n\nsave_models_SW_IN96 = SaveModelsBatch(times_epoch=5)\n\n\nitems_SW_IN96 = random.choices(dls_SW_IN96.valid.items, k=4)\n\n\nlearn_SW_IN96 = Learner(dls_SW_IN96, model_SW_IN96, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_SW_IN96], metrics=rmse_gap)\n\n\nshow_results(learn_SW_IN96, items = items_SW_IN96, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=660, shift=30, var_sel=['SW_IN'], gap_len=96), MeteoImpItem(i=684, shift=30, var_sel=['SW_IN'], gap_len=96), MeteoImpItem(i=684, shift=90, var_sel=['SW_IN'], gap_len=96), MeteoImpItem(i=632, shift=-90, var_sel=['SW_IN'], gap_len=96)]\n\n\n\n\n\n\n\n\nlearn_SW_IN96.fit(2, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n23.111818\n21.649970\n0.277776\n22:20\n\n\n1\n23.287213\n21.537204\n0.277468\n22:06\n\n\n\n\n\n\nlearn_SW_IN96.recorder.plot_loss()\n\n\n\n\n\nshow_results(learn_SW_IN96, items = items_SW_IN96, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=660, shift=30, var_sel=['SW_IN'], gap_len=96), MeteoImpItem(i=684, shift=30, var_sel=['SW_IN'], gap_len=96), MeteoImpItem(i=684, shift=90, var_sel=['SW_IN'], gap_len=96), MeteoImpItem(i=632, shift=-90, var_sel=['SW_IN'], gap_len=96)]\n\n\n\n\n\n\n\n\ntorch.save(learn_SW_IN96.model, \"trained_4_feb_SW_IN_gap_96_v1.pickle\")\n\n\n\n\n\n\n\n\ndls_TA_nc = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=50, gap_len=12, bs=20, control_lags=[1], n_rep=1).cpu()\n\n\nmodel_TA_nc = model_TA96.copy()\nmodel_TA_nc.use_control = False\n\n\nsave_models_TA_nc = SaveModelsBatch(times_epoch=2)\n\n\nitems_TA_nc = random.choices(dls_TA_nc.valid.items, k=4)\n\n\nlearn_TA_nc = Learner(dls_TA_nc, model_TA_nc, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_TA_nc], metrics=rmse_gap)\n\n\nlearn_TA_nc.fit(1, 5e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-18.464700\n-18.834046\n0.040957\n02:33\n\n\n\n\n\n\nlearn_TA_nc.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_TA_nc.model, \"trained_4_feb_TA_gap_12_no_control_v1.pickle\")\n\n\nshow_results(learn_TA_nc, items = items_TA_nc, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=3919, shift=15, var_sel=['TA'], gap_len=12), MeteoImpItem(i=4068, shift=15, var_sel=['TA'], gap_len=12), MeteoImpItem(i=4114, shift=-15, var_sel=['TA'], gap_len=12), MeteoImpItem(i=4180, shift=-5, var_sel=['TA'], gap_len=12)]\n\n\n\n\n\n\n\n\n\n\n\ndls_TA96_nc = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=300, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_TA96_nc = model_TA_nc.copy()\nmodel_TA96_nc.use_control = False\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\nsave_models_TA96_nc = SaveModelsBatch(times_epoch=2)\n\n\nitems_TA96_nc = random.choices(dls_TA96_nc.valid.items, k=4)\n\n\nlearn_TA96_nc = Learner(dls_TA96_nc, model_TA96_nc, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_TA96_nc], metrics=rmse_gap)\n\n\nshow_results(learn_TA96_nc, items = items_TA96_nc, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=757, shift=-90, var_sel=['TA'], gap_len=96), MeteoImpItem(i=711, shift=-90, var_sel=['TA'], gap_len=96), MeteoImpItem(i=743, shift=-150, var_sel=['TA'], gap_len=96), MeteoImpItem(i=625, shift=-30, var_sel=['TA'], gap_len=96)]\n\n\n\n\n\n\n\n\nlearn_TA96_nc.fit(2, 5e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-16.875112\n-38.792134\n0.158970\n24:20\n\n\n1\n-36.028650\n-42.915542\n0.153475\n22:35\n\n\n\n\n\n\nlearn_TA96_nc.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_TA96_nc.model, \"trained_4_feb_TA_gap_96_no_control_v1.pickle\")\n\n\nshow_results(learn_TA96_nc, items = items_TA96_nc, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=757, shift=-90, var_sel=['TA'], gap_len=96), MeteoImpItem(i=711, shift=-90, var_sel=['TA'], gap_len=96), MeteoImpItem(i=743, shift=-150, var_sel=['TA'], gap_len=96), MeteoImpItem(i=625, shift=-30, var_sel=['TA'], gap_len=96)]\n\n\n\n\n\n\n\n\n\n\n\ndls_Aa96_nc = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=70, gap_len=12, bs=20, control_lags=[1], n_rep=2).cpu()\n\n\nmodel_Aa96_nc = model_Aa96.copy() \nmodel_Aa96_nc.use_control = False\n\n\nsave_models_Aa96_nc= SaveModelsBatch(times_epoch=1) # save once per repetition\n\n\nitems_Aa96_nc  = random.choices(dls_Aa96_nc.valid.items, k=4)\n\n\nlearn_Aa96_nc = Learner(dls_Aa96_nc, model_Aa96_nc, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_Aa96_nc], metrics=rmse_gap)\n\n\nshow_results(learn_Aa96_nc, items=items_Aa96_nc, control_map=control_map)\n\n[MeteoImpItem(i=3064, shift=7, var_sel=['P', 'VPD', 'WS', 'SW_IN', 'PA', 'TA'], gap_len=12), MeteoImpItem(i=2776, shift=7, var_sel=['VPD', 'PA', 'SW_IN', 'WS'], gap_len=12), MeteoImpItem(i=3253, shift=-21, var_sel=['VPD', 'SW_IN'], gap_len=12), MeteoImpItem(i=2716, shift=14, var_sel=['SW_IN'], gap_len=12)]\n\n\n\n\n\n\n\n\nlearn_Aa96_nc.fit(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n38.744036\n30.468859\n0.436350\n03:39\n\n\n\n\n\n\nlearn_Aa96_nc.recorder.plot_loss()\n\n\n\n\n\nlearn_Aa96_nc.fit(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n12.341979\n12.002963\n0.392356\n03:51\n\n\n\n\n\n\nlearn_Aa96_nc.fit(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n5.277773\n4.664958\n0.367391\n37:31\n\n\n\n\n\n\nlearn_Aa96_nc.fit(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n1.912588\n0.122255\n0.352851\n04:10\n\n\n\n\n\n\nlearn_Aa96_nc.fit(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-2.542222\n-3.259210\n0.344657\n03:51\n\n\n\n\n\n\nlearn_Aa96_nc.fit(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-5.897630\n-5.467389\n0.339895\n04:01\n\n\n\n\n\n\nlearn_Aa96_nc.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_Aa96_nc.model, \"trained_4_feb_All_gap_all_12_no_control_v1.pickle\")\n\n\nshow_results(learn_Aa96_nc, items=items_Aa96_nc, control_map=control_map)\n\n[MeteoImpItem(i=3077, shift=-35, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=3195, shift=-35, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=3230, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=2910, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12)]\n\n\n\n\n\n\n\n\n\n\n\ndls_Aa24_nc = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=150, gap_len=24, bs=20, control_lags=[1], n_rep=2).cpu()\n\n\nmodel_Aa24_nc = model_Aa_nc.copy() \nmodel_Aa24_nc.use_control = False\n\n\nsave_models_Aa24_nc= SaveModelsBatch(times_epoch=1) # save once per repetition\n\n\nitems_Aa24_nc  = random.choices(dls_Aa24_nc.valid.items, k=4)\n\n\nlearn_Aa24_nc = Learner(dls_Aa24_nc, model_Aa24_nc, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_Aa24_nc], metrics=rmse_gap)\n\n\nshow_results(learn_Aa24_nc, items=items_Aa24_nc, control_map=control_map)\n\n[MeteoImpItem(i=1439, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=24), MeteoImpItem(i=1222, shift=-75, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=24), MeteoImpItem(i=1376, shift=-75, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=24), MeteoImpItem(i=1364, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=24)]\n\n\n\n\n\n\n\n\nlearn_Aa24_nc.fit(5, 3e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n41.673347\n42.895707\n0.457301\n04:47\n\n\n1\n42.247850\n40.339583\n0.451268\n04:37\n\n\n2\n34.196943\n38.366525\n0.445817\n04:47\n\n\n3\n32.270157\n36.991478\n0.443363\n04:42\n\n\n4\n33.566557\n35.243634\n0.436804\n04:54\n\n\n\n\n\n\nlearn_Aa24_nc.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_Aa24_nc.model, \"trained_4_feb_All_gap_all_24_no_control_v1.pickle\")\n\n\nshow_results(learn_Aa24_nc, items=items_Aa24_nc, control_map=control_map)\n\n[MeteoImpItem(i=1439, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=24), MeteoImpItem(i=1222, shift=-75, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=24), MeteoImpItem(i=1376, shift=-75, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=24), MeteoImpItem(i=1364, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=24)]\n\n\n\n\n\n\n\n\n\n\n\ndls_Av24_nc = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=150, gap_len=24, bs=20, control_lags=[1], n_rep=2).cpu()\n\n\nmodel_Av24_nc = model_Aa24_nc.copy() \nmodel_Av24_nc.use_control = False\n\n\nsave_models_Av24_nc= SaveModelsBatch(times_epoch=1) # save once per repetition\n\n\nitems_Av24_nc  = random.choices(dls_Av24_nc.valid.items, k=4)\n\n\nlearn_Av24_nc = Learner(dls_Av24_nc, model_Av24_nc, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_Av24_nc], metrics=rmse_gap)\n\n\nshow_results(learn_Av24_nc, items=items_Av24_nc, control_map=control_map)\n\n[MeteoImpItem(i=1438, shift=0, var_sel=['SW_IN', 'TA', 'PA', 'LW_IN', 'VPD', 'WS', 'P'], gap_len=24), MeteoImpItem(i=1275, shift=0, var_sel=['WS', 'SW_IN', 'TA', 'LW_IN'], gap_len=24), MeteoImpItem(i=1316, shift=-75, var_sel=['VPD', 'WS', 'TA', 'P', 'LW_IN', 'PA', 'SW_IN'], gap_len=24), MeteoImpItem(i=1273, shift=-75, var_sel=['P', 'TA', 'SW_IN', 'LW_IN', 'WS'], gap_len=24)]\n\n\n\n\n\n\n\n\nlearn_Av24_nc.fit(2, 3e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n18.450633\n16.309334\n0.376543\n06:58\n\n\n1\n15.670756\n16.164595\n0.374336\n06:24\n\n\n\n\n\n\nlearn_Av24_nc.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_Av24_nc.model, \"trained_4_feb_All_gap_varying_24_no_control_v1.pickle\")\n\n\nshow_results(learn_Av24_nc, items=items_Av24_nc, control_map=control_map)\n\n\n\n\n\ndls_Aa36_nc = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=150, gap_len=36, bs=20, control_lags=[1], n_rep=2).cpu()\n\n\nmodel_Aa36_nc = model_Aa24_nc.copy() \nmodel_Aa36_nc.use_control = False\n\n\nsave_models_Aa36_nc= SaveModelsBatch(times_epoch=1) # save once per repetition\n\n\nitems_Aa36_nc  = random.choices(dls_Aa36_nc.valid.items, k=4)\n\n\nlearn_Aa36_nc = Learner(dls_Aa36_nc, model_Aa36_nc, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_Aa36_nc], metrics=rmse_gap)\n\n\nshow_results(learn_Aa36_nc, items=items_Aa36_nc, control_map=control_map)\n\n[MeteoImpItem(i=1384, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=36), MeteoImpItem(i=1362, shift=-75, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=36), MeteoImpItem(i=1410, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=36), MeteoImpItem(i=1505, shift=-75, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=36)]\n\n\n\n\n\n\n\n\nlearn_Aa36_nc.fit(2, 3e-4)\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/121 00:00&lt;?]\n    \n    \n\n\nValueError: Expected parameter covariance_matrix (Tensor of shape (7, 7)) of distribution MultivariateNormal(loc: torch.Size([7]), covariance_matrix: torch.Size([7, 7])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[ 1.6642e-02,  4.3281e-03,  9.5652e-03,  2.6394e-03,  2.6164e-02,\n          1.5804e-02,  3.9479e-02],\n        [ 4.3281e-03,  5.1357e-02,  3.1779e-03,  4.4854e-03, -1.5141e-02,\n          3.6567e-03, -3.5843e-02],\n        [ 9.5652e-03,  3.1779e-03,  1.6632e-02, -4.0737e-04,  1.4566e-02,\n          1.5618e-02,  1.2989e-02],\n        [ 2.6394e-03,  4.4854e-03, -4.0737e-04,  4.9380e-03, -9.9333e-03,\n         -1.2459e-03, -7.2656e-03],\n        [ 2.6164e-02, -1.5141e-02,  1.4566e-02, -9.9333e-03,  7.9823e-01,\n          2.5349e-02,  5.7989e-02],\n        [ 1.5804e-02,  3.6567e-03,  1.5618e-02, -1.2459e-03,  2.5349e-02,\n          1.0991e-01,  6.7019e-02],\n        [ 3.9479e-02, -3.5843e-02,  1.2989e-02, -7.2656e-03,  5.7989e-02,\n          6.7019e-02,  1.3826e-01]], dtype=torch.float64,\n       grad_fn=&lt;ExpandBackward0&gt;)\n\n\n\nlearn_Aa36_nc.recorder.plot_loss()\n\n\ntorch.save(learn_Aa36_nc.model, \"trained_4_feb_All_gap_all_36_no_control_v1.pickle\")\n\n\nshow_results(learn_Aa36_nc, items=items_Aa36_nc, control_map=control_map)\n\n\n\n\n\ndls_Aa48_nc = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=350, gap_len=48, bs=20, control_lags=[1], n_rep=2).cpu()\n\n\nmodel_Aa48_nc = model_Aa_nc.copy() \nmodel_Aa48_nc.use_control = False\n\n\nsave_models_Aa48_nc= SaveModelsBatch(times_epoch=1) # save once per repetition\n\n\nitems_Aa48_nc  = random.choices(dls_Aa48_nc.valid.items, k=4)\n\n\nlearn_Aa48_nc = Learner(dls_Aa48_nc, model_Aa48_nc, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_Aa48_nc], metrics=rmse_gap)\n\n\nshow_results(learn_Aa48_nc, items=items_Aa48_nc, control_map=control_map)\n\n[MeteoImpItem(i=640, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=48), MeteoImpItem(i=534, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=48), MeteoImpItem(i=621, shift=-175, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=48), MeteoImpItem(i=559, shift=-175, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=48)]\n\n\n\n\n\n\n\n\nlearn_Aa48_nc.fit(5, 3e-4)\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/51 00:00&lt;?]\n    \n    \n\n\nValueError: Expected parameter covariance_matrix (Tensor of shape (7, 7)) of distribution MultivariateNormal(loc: torch.Size([7]), covariance_matrix: torch.Size([7, 7])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[-2.1252e+31, -7.2463e+31, -5.0732e+29, -7.1944e+29,  1.8079e+31,\n          7.5784e+30, -4.2668e+31],\n        [-7.2463e+31, -2.4719e+32, -1.8432e+30, -2.4103e+30,  6.1574e+31,\n          2.6057e+31, -1.4537e+32],\n        [-5.0732e+29, -1.8432e+30, -1.2298e+29,  2.4634e+28,  3.6487e+29,\n          3.9303e+29, -9.0798e+29],\n        [-7.1944e+29, -2.4103e+30,  2.4634e+28, -4.0120e+28,  6.3716e+29,\n          1.7656e+29, -1.4861e+30],\n        [ 1.8079e+31,  6.1574e+31,  3.6487e+29,  6.3716e+29, -1.5419e+31,\n         -6.3192e+30,  3.6363e+31],\n        [ 7.5784e+30,  2.6057e+31,  3.9303e+29,  1.7656e+29, -6.3192e+30,\n         -3.1082e+30,  1.5004e+31],\n        [-4.2668e+31, -1.4537e+32, -9.0798e+29, -1.4861e+30,  3.6363e+31,\n          1.5004e+31, -8.5775e+31]], dtype=torch.float64,\n       grad_fn=&lt;ExpandBackward0&gt;)\n\n\n\nlearn_Aa48_nc.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_Aa48_nc.model, \"trained_4_feb_All_gap_all_48_no_control_v1.pickle\")\n\n\nshow_results(learn_Aa48_nc, items=items_Aa48_nc, control_map=control_map)\n\n[MeteoImpItem(i=3077, shift=-35, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=3195, shift=-35, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=3230, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=2910, shift=0, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12)]\n\n\n\n\n\n\n\n\n\n\n\n\ndls_LW_IN = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN', block_len=50, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN = model_Av.copy()\n\n\nsave_models_LW_IN = SaveModelsBatch(times_epoch=10)\n\n\nitems_LW_IN = random.choices(dls_LW_IN.valid.items, k=4)\n\n\nlearn_LW_IN = Learner(dls_LW_IN, model_LW_IN, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_LW_IN], metrics=rmse_gap)\n\n\nlearn_LW_IN.fit(1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n3.196418\n3.997740\n0.280681\n11:39\n\n\n\n\n\n\nlearn_LW_IN.recorder.plot_loss()\n\n\n\n\n\ntorch.save(learn_LW_IN.model, \"trained_4_feb_LW_IN_gap_12_v1.pickle\")\n\n\nshow_results(learn_LW_IN, items = items_LW_IN, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=4462, shift=15, var_sel=['LW_IN'], gap_len=12), MeteoImpItem(i=4415, shift=5, var_sel=['LW_IN'], gap_len=12), MeteoImpItem(i=4322, shift=15, var_sel=['LW_IN'], gap_len=12), MeteoImpItem(i=4212, shift=-5, var_sel=['LW_IN'], gap_len=12)]\n\n\n\n\n\n\n\n\n\nafter the first trainig fine tune the model for longer gaps\n\ndls_LW_IN96 = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN', block_len=300, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN96 = model_LW_IN.copy()\n\n\nsave_models_LW_IN96 = SaveModelsBatch(times_epoch=5)\n\n\nitems_LW_IN96 = random.choices(dls_LW_IN96.valid.items, k=4)\n\n\nlearn_LW_IN96 = Learner(dls_LW_IN96, model_LW_IN96, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models_LW_IN96], metrics=rmse_gap)\n\n\nshow_results(learn_LW_IN96, items = items_LW_IN96, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=634, shift=-90, var_sel=['LW_IN'], gap_len=96), MeteoImpItem(i=735, shift=-30, var_sel=['LW_IN'], gap_len=96), MeteoImpItem(i=651, shift=-150, var_sel=['LW_IN'], gap_len=96), MeteoImpItem(i=627, shift=-90, var_sel=['LW_IN'], gap_len=96)]\n\n\n\n\n\n\n\n\nlearn_LW_IN96.fit(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n51.693777\n57.928376\n0.419843\n21:42\n\n\n\n\n\n\nlearn_LW_IN96.recorder.plot_loss()\n\n\n\n\n\nshow_results(learn_LW_IN96, items = items_LW_IN96, control_map = control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=634, shift=-90, var_sel=['LW_IN'], gap_len=96), MeteoImpItem(i=735, shift=-30, var_sel=['LW_IN'], gap_len=96), MeteoImpItem(i=651, shift=-150, var_sel=['LW_IN'], gap_len=96), MeteoImpItem(i=627, shift=-90, var_sel=['LW_IN'], gap_len=96)]\n\n\n\n\n\n\n\n\ntorch.save(learn_LW_IN96.model, \"trained_4_feb_LW_IN_gap_96_v1.pickle\")\n\n\n\n\n\n\ndls3 = imp_dataloader(haiB, haiB, var_sel = gen_var_sel(list(haiB.columns)), block_len=100, gap_len=10, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel3 = KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = None, pred_only_gap=True, use_conditional=True)\n\n\nsave_models3 = SaveModelsBatch(times_epoch=1) # save once per repetition\n\n\nitems = random.choices(dls3.valid.items, k=4)\n\n\nlearn3 = Learner(dls3, model3, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models2], metrics=rmse_gap)\n\n\nlearn3.fit(20, 1e-4)\n\n\nlearn3.recorder.plot_loss()\n\n\ntorch.save(learn3.model, \"trained_2_feb_gap_partial_var_cond_v1.pickle\")\n\n\n# learnB.export(\"trained_2_feb_gap_all_var_v1\")\n\n\nshow_results(learn3, items=items)"
  },
  {
    "objectID": "results/train_21_feb_extra.html",
    "href": "results/train_21_feb_extra.html",
    "title": "Training Kalman Filter for Results - 21 Feb",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\nfrom meteo_imp.gaussian import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\nfrom fastcore.foundation import patch\nfrom pathlib import Path, PosixPath\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image, HTML\n\nfrom tqdm.auto import tqdm\n\n\nfrom fastcore.basics import *\n\n\nshow_metrics = False\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_big_path)\nhai_era = pd.read_parquet(hai_era_big_path)\n\n\nbase = here(\"analysis/results/trained_21feb\")\n\n\nbase.mkdir(exist_ok=True)\n\n\n@patch\ndef add_end(self: PosixPath, end): return self.parent / (self.name + end)\n\n\ndef train_or_load(model, dls, lr, n, path, keep=True):\n    save_models = SaveModelsBatch(times_epoch=5)\n    csv_logger = CSVLogger(path.add_end(\"log.csv\"))\n    learn = Learner(dls, model, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models, csv_logger], metrics=rmse_gap) \n    items = random.choices(dls.valid.items, k=4) \n    if path.add_end(\".pickle\").exists() and keep:\n        learn.model = torch.load(path.add_end(\".pickle\"))\n        display(csv_logger.read_log())\n        plot = Image(filename=path.add_end(\"_loss_plot.png\"))\n        display(plot)\n    else:\n        learn.fit(lr, n)\n        \n        torch.save(learn.model, path.add_end(\".pickle\"))\n        learn.recorder.plot_loss()\n        plt.savefig(path.add_end(\"_loss_plot.png\"))\n    return learn, items                           \n\n\ndef metric_valid(learn, dls=None):\n    nrmse = []\n    losses = []\n    dls = ifnone(dls, learn.dls.valid)\n    for input, target in tqdm(dls, leave=False):\n        pred = learn.model(input)\n        nrmse.append(learn.metrics[0](pred, target))\n        losses.append(learn.loss_func(pred, target).item())\n    metric = pd.DataFrame({'loss': losses, 'rmse': nrmse})\n    return metric.agg(['mean', 'std'])\n\n\nhai.columns\n\nIndex(['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], dtype='object')\n\n\n\n\n\ndls_A1v = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns), n_var=1),\n    block_len=100+336,\n    gap_len=gen_gap_len(6, 336),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5).cpu()\n\n\nlen(hai)\n\n227952\n\n\n\nlen(dls_A1v.train)*20, len(dls_A1v.valid)*20\n\n(2080, 520)\n\n\n\nmodel_A1v = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca = None, \n    pred_only_gap=True)\n\n\nmodel_A1v.B.shape\n\ntorch.Size([1, 18, 14])\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 3, 1e-3, base / \"1_gap_varying_6-336_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n196.765350\n163.493486\n0.579074\n48:33\n\n\n1\n1\n138.298704\n123.299909\n0.490741\n48:14\n\n\n2\n2\n113.640141\n116.746793\n0.488059\n39:00\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 1, 1e-4, base / \"1_gap_varying_6-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n94.869328\n112.046392\n0.471249\n43:59\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 1, 1e-6, base / \"1_gap_varying_6-336_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n95.239438\n104.268073\n0.467282\n40:19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_A1v_nc = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca=None,\n    pred_only_gap=True,\n    use_control=False\n)\n\n\nlearn_A1v_nc, items_A1v_nc = train_or_load(model_A1v_nc, dls_A1v, 3, 1e-3, base / \"1_gap_varying_336_no_control_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n219.686355\n178.506325\n0.658579\n37:14\n\n\n1\n1\n176.039201\n160.979378\n0.583213\n37:00\n\n\n2\n2\n166.012525\n158.206468\n0.574111\n36:47\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndls_Aa = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = list(hai.columns),\n    block_len=120,\n    gap_len=gen_gap_len(6, 30),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5\n).cpu()\n\n\ndls_Aa = imp_dataloader(hai, hai_era, var_sel = list(hai.columns), block_len=120, gap_len=gen_gap_len(6,30), bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa = learn_A1v.model.copy()\n\n\nif show_metrics: display(metric_valid(learn_A1v, dls=dls_Aa.valid))\n\n\ndls_A1v30 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns), n_var=1),\n    block_len=100+30,\n    gap_len=gen_gap_len(6, 30),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5).cpu()\n\n\nif show_metrics: display(metric_valid(learn_A1v, dls=dls_A1v30.valid))\n\n\nlearn_Aa, items_Aa = train_or_load(model_Aa, dls_Aa, 3, 3e-4, base / \"All_gap_all_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n24.688308\n21.049544\n0.365108\n28:05\n\n\n1\n1\n-7.512621\n-4.152000\n0.342344\n27:26\n\n\n2\n2\n-18.230698\n-19.744404\n0.327594\n26:15\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v30, items_A1v30 = train_or_load(learn_A1v.model.copy(), dls_A1v30, 3, 3e-4, base / \"1_gap_varying_tuned_6-30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n4.763339\n3.134267\n0.276432\n15:02\n\n\n1\n1\n2.390429\n1.721823\n0.267772\n16:12\n\n\n2\n2\n0.780169\n0.607745\n0.255524\n16:05\n\n\n\n\n\n\n\n\n\n\nso this is not working …\n\n\n\n\ndls_Vv30 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns)),\n    block_len=100+30,\n    gap_len=gen_gap_len(6, 30),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=20).cpu()\n\n\nlearn_Vv30, items_Vv30 = train_or_load(learn_Aa.model, dls_Vv30, 3, 5e-4, base / \"all_varying_gap_varying_len_6-30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-5.794561\n-4.508800\n0.213908\n1:00:55\n\n\n1\n1\n-3.717697\n-5.165062\n0.205841\n1:00:26\n\n\n2\n2\n-1.928287\n-6.012112\n0.202048\n1:00:21\n\n\n\n\n\n\n\n\n\n\n\nlearn_Vv30, items_Vv30 = train_or_load(learn_Aa.model, dls_Vv30, 1, 1e-5, base / \"all_varying_gap_varying_len_6-30_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.061014\n-6.663726\n0.192194\n57:55\n\n\n\n\n\n\n\n\n\n\n\nlearn_Vv30, items_Vv30 = train_or_load(learn_Aa.model, dls_Vv30, 1, 1e-5, base / \"all_varying_gap_varying_len_6-30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.417934\n-6.799482\n0.19076\n1:00:23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_Vv_rand = KalmanFilterSR.init_random(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=2*len(hai.columns),\n    n_dim_contr = 2*len(hai_era.columns),\n    seed=27,\n    pred_only_gap=True)\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(model_Vv_rand, dls_Vv30, 3, 1e-3, base / \"rand_all_varying_gap_varying_len_6-30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n13.508053\n14.374478\n0.448185\n1:00:35\n\n\n1\n9.766153\n11.389963\n0.395332\n58:51\n\n\n2\n6.503961\n6.754238\n0.305433\n54:16\n\n\n\n\n\n\n\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(learn_Vv_rand.model, dls_Vv30, 3, 1e-4, base / \"rand_all_varying_gap_varying_len_6-30_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n5.101230\n5.345336\n0.285398\n1:02:15\n\n\n1\n4.838514\n4.907970\n0.281667\n1:02:44\n\n\n2\n4.571287\n4.295109\n0.275344\n1:04:47\n\n\n\n\n\n\n\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(learn_Vv_rand.model, dls_Vv30, 3, 1e-5, base / \"rand_all_varying_gap_varying_len_6-30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n4.075190\n4.282635\n0.275272\n2:36:25\n\n\n1\n5.134852\n4.244826\n0.274858\n1:17:58\n\n\n2\n3.923739\n4.185355\n0.274057\n1:11:31\n\n\n\n\n\n\n\n\n\nlearn_Vv_rand, items_Vv_rand = train_or_load(learn_Vv_rand.model, dls_Vv30, 3, 1e-5, base / \"rand_all_varying_gap_varying_len_6-30_v4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n4.100049\n4.142156\n0.273928\n56:48\n\n\n1\n3.712223\n4.112651\n0.273566\n56:59\n\n\n2\n3.942678\n4.073334\n0.273349\n57:03\n\n\n\n\n\n\n\n\n\nmodel_Vv_rand\n\n\nKalman Filter (9 obs, 18 state, 14 contr)  $A$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\nx_0\n0.8775\n0.2675\n0.0937\n0.6706\n0.1638\n0.9272\n0.2620\n0.4967\n0.2630\n0.1175\n0.1694\n0.2100\n0.4890\n0.0564\n0.4760\n0.7606\n0.7759\n0.5243\n\n\nx_1\n0.3714\n0.0426\n0.2343\n0.9991\n0.1775\n0.6319\n0.6734\n0.7937\n0.6468\n0.5825\n0.4599\n0.7960\n0.9038\n0.9735\n0.6428\n0.3725\n0.2052\n0.0507\n\n\nx_2\n0.4448\n0.5775\n0.7237\n0.5927\n0.3217\n0.6441\n0.2801\n0.9132\n0.0329\n0.4856\n0.9927\n0.5895\n0.2611\n0.9413\n0.1371\n0.8726\n0.5590\n0.8451\n\n\nx_3\n0.1253\n0.9434\n0.0462\n0.2360\n0.0239\n0.8950\n0.7419\n0.9471\n0.6690\n0.1554\n0.0821\n0.7309\n0.7764\n0.9769\n0.0196\n0.0384\n0.4294\n0.3438\n\n\nx_4\n0.5494\n0.8238\n0.9845\n0.6826\n0.9001\n0.3022\n0.7509\n0.0926\n0.0328\n0.4798\n0.5335\n0.0434\n0.3530\n0.4157\n0.7495\n0.1716\n0.1980\n0.4298\n\n\nx_5\n0.9201\n0.6883\n0.5342\n0.7847\n0.3137\n0.1778\n0.5838\n0.9799\n0.3611\n0.3155\n0.7475\n0.5450\n0.5641\n0.2493\n0.8323\n0.9723\n0.1883\n0.3605\n\n\nx_6\n0.5344\n0.3443\n0.7696\n0.3410\n0.7553\n0.3177\n0.0315\n0.5209\n0.6514\n0.3131\n0.4510\n0.3550\n0.4790\n0.0676\n0.3606\n0.7299\n0.6713\n0.3134\n\n\nx_7\n0.7460\n0.1291\n0.4653\n0.5693\n0.9906\n0.8288\n0.9039\n0.5240\n0.6277\n0.3574\n0.0076\n0.6530\n0.8667\n0.9368\n0.8667\n0.6749\n0.3526\n0.6618\n\n\nx_8\n0.0837\n0.7188\n0.7247\n0.3211\n0.4898\n0.9030\n0.0358\n0.1662\n0.7741\n0.7937\n0.7183\n0.5141\n0.4918\n0.2773\n0.6901\n0.8565\n0.3723\n0.3410\n\n\nx_9\n0.4035\n0.0591\n0.6836\n0.8306\n0.4312\n0.0210\n0.0032\n0.9010\n0.6741\n0.3875\n0.3683\n0.5337\n0.0706\n0.8516\n0.7304\n0.8507\n0.6829\n0.6900\n\n\nx_10\n0.1059\n0.0500\n0.5736\n0.9595\n0.8101\n0.7397\n0.5282\n0.1294\n0.2746\n0.5556\n0.6463\n0.0023\n0.1761\n0.3391\n0.3346\n0.4655\n0.8172\n0.4176\n\n\nx_11\n0.1349\n0.0519\n0.1180\n0.9767\n0.1679\n0.8635\n0.3753\n0.9760\n0.2125\n0.8049\n0.2124\n0.6794\n0.0037\n0.9711\n0.5679\n0.9474\n0.8593\n0.6385\n\n\nx_12\n0.8770\n0.0469\n0.1582\n0.6694\n0.5670\n0.9794\n0.6498\n0.3257\n0.8462\n0.7727\n0.3213\n0.7318\n0.3665\n0.9550\n0.7188\n0.2660\n0.5867\n0.1134\n\n\nx_13\n0.7401\n0.1982\n0.4165\n0.3814\n0.5263\n0.6516\n0.9604\n0.8996\n0.8318\n0.7448\n0.6912\n0.5938\n0.0929\n0.5298\n0.2637\n0.8722\n0.5430\n0.2217\n\n\nx_14\n0.3495\n0.3756\n0.1251\n0.4052\n0.0638\n0.0588\n0.4379\n0.4891\n0.2796\n0.0740\n0.2123\n0.1370\n0.4477\n0.3628\n0.9125\n0.4047\n0.8130\n0.2332\n\n\nx_15\n0.8424\n0.0816\n0.8791\n0.3892\n0.2923\n0.8603\n0.1172\n0.6212\n0.6087\n0.6072\n0.8778\n0.6758\n0.5495\n0.8240\n0.7461\n0.1555\n0.2950\n0.0365\n\n\nx_16\n0.8060\n0.8602\n0.9453\n0.7811\n0.5495\n0.5861\n0.8480\n0.1940\n0.9206\n0.5589\n0.2148\n0.1828\n0.0636\n0.2885\n0.9426\n0.6787\n0.0080\n0.7527\n\n\nx_17\n0.5032\n0.5585\n0.0789\n0.0409\n0.3918\n0.2908\n0.3802\n0.0407\n0.6447\n0.3241\n0.8544\n0.4245\n0.3987\n0.4367\n0.3384\n0.2285\n0.7890\n0.9094\n\n\n\n\n$Q$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\nx_0\n1.4985\n0.1823\n0.8819\n1.1215\n0.8124\n1.0154\n0.4127\n0.4993\n0.3792\n1.1249\n0.3349\n0.5480\n1.1208\n1.0406\n0.0373\n0.0713\n0.0563\n0.0038\n\n\nx_1\n0.1823\n0.7474\n0.3388\n0.8779\n0.3284\n0.4377\n0.5683\n0.6806\n0.1517\n0.6860\n0.1334\n0.2869\n0.3903\n0.5594\n0.2687\n0.7078\n0.5301\n0.5990\n\n\nx_2\n0.8819\n0.3388\n1.5250\n1.3832\n1.4418\n0.7184\n0.4702\n1.2013\n1.1584\n1.3029\n1.0110\n1.0116\n1.4669\n1.5483\n0.6885\n0.7897\n1.0060\n0.8858\n\n\nx_3\n1.1215\n0.8779\n1.3832\n2.4360\n1.6422\n1.2901\n1.1545\n1.7087\n1.4626\n2.4015\n1.3411\n1.4348\n2.0194\n2.1839\n1.2870\n1.3528\n1.5829\n1.6857\n\n\nx_4\n0.8124\n0.3284\n1.4418\n1.6422\n3.2440\n1.8293\n0.6635\n2.2540\n1.7788\n2.2643\n1.5406\n1.9412\n2.5939\n2.3073\n2.1935\n1.1262\n1.5039\n1.7599\n\n\nx_5\n1.0154\n0.4377\n0.7184\n1.2901\n1.8293\n2.6367\n0.6717\n2.0377\n1.2394\n1.6564\n1.4530\n1.5842\n2.3638\n2.1887\n2.0828\n1.2967\n0.6836\n0.8444\n\n\nx_6\n0.4127\n0.5683\n0.4702\n1.1545\n0.6635\n0.6717\n1.8814\n1.4915\n0.6485\n1.3643\n0.6293\n1.5192\n1.1450\n1.7599\n1.6733\n1.4465\n0.7419\n1.1889\n\n\nx_7\n0.4993\n0.6806\n1.2013\n1.7087\n2.2540\n2.0377\n1.4915\n3.7419\n2.0936\n2.3554\n2.8167\n2.9808\n2.6624\n3.4940\n3.4288\n2.7951\n1.6056\n2.6631\n\n\nx_8\n0.3792\n0.1517\n1.1584\n1.4626\n1.7788\n1.2394\n0.6485\n2.0936\n3.2420\n2.6115\n2.9707\n2.0584\n3.2186\n2.4394\n2.7694\n2.0308\n2.6026\n2.2404\n\n\nx_9\n1.1249\n0.6860\n1.3029\n2.4015\n2.2643\n1.6564\n1.3643\n2.3554\n2.6115\n4.7867\n3.5267\n2.4691\n3.7434\n3.6431\n3.0501\n2.1870\n3.3714\n2.4697\n\n\nx_10\n0.3349\n0.1334\n1.0110\n1.3411\n1.5406\n1.4530\n0.6293\n2.8167\n2.9707\n3.5267\n4.8661\n2.7361\n3.9109\n3.7590\n4.1276\n3.2131\n3.3042\n2.8676\n\n\nx_11\n0.5480\n0.2869\n1.0116\n1.4348\n1.9412\n1.5842\n1.5192\n2.9808\n2.0584\n2.4691\n2.7361\n4.5057\n3.4122\n4.2709\n3.9642\n3.3446\n2.1979\n2.7754\n\n\nx_12\n1.1208\n0.3903\n1.4669\n2.0194\n2.5939\n2.3638\n1.1450\n2.6624\n3.2186\n3.7434\n3.9109\n3.4122\n6.2732\n5.0000\n5.0583\n4.1123\n3.9812\n3.5458\n\n\nx_13\n1.0406\n0.5594\n1.5483\n2.1839\n2.3073\n2.1887\n1.7599\n3.4940\n2.4394\n3.6431\n3.7590\n4.2709\n5.0000\n6.2369\n5.2005\n4.6903\n3.3753\n4.1217\n\n\nx_14\n0.0373\n0.2687\n0.6885\n1.2870\n2.1935\n2.0828\n1.6733\n3.4288\n2.7694\n3.0501\n4.1276\n3.9642\n5.0583\n5.2005\n7.8159\n5.5318\n4.1568\n5.4245\n\n\nx_15\n0.0713\n0.7078\n0.7897\n1.3528\n1.1262\n1.2967\n1.4465\n2.7951\n2.0308\n2.1870\n3.2131\n3.3446\n4.1123\n4.6903\n5.5318\n5.8308\n3.6118\n5.0658\n\n\nx_16\n0.0563\n0.5301\n1.0060\n1.5829\n1.5039\n0.6836\n0.7419\n1.6056\n2.6026\n3.3714\n3.3042\n2.1979\n3.9812\n3.3753\n4.1568\n3.6118\n5.2990\n4.3170\n\n\nx_17\n0.0038\n0.5990\n0.8858\n1.6857\n1.7599\n0.8444\n1.1889\n2.6631\n2.2404\n2.4697\n2.8676\n2.7754\n3.5458\n4.1217\n5.4245\n5.0658\n4.3170\n7.1195\n\n\n\n\n\n$b$\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.5371\n\n\nx_1\n0.6015\n\n\nx_2\n0.3190\n\n\nx_3\n0.9543\n\n\nx_4\n0.5112\n\n\nx_5\n0.0341\n\n\nx_6\n0.9601\n\n\nx_7\n0.1604\n\n\nx_8\n0.4499\n\n\nx_9\n0.8575\n\n\nx_10\n0.2647\n\n\nx_11\n0.4293\n\n\nx_12\n0.9210\n\n\nx_13\n0.5512\n\n\nx_14\n0.0890\n\n\nx_15\n0.4351\n\n\nx_16\n0.3804\n\n\nx_17\n0.4879\n\n\n\n\n\n$H$\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\ny_0\n0.5241\n0.2182\n0.7958\n0.7816\n0.3235\n0.8518\n0.4334\n0.7567\n0.5235\n0.2247\n0.2498\n0.6324\n0.0037\n0.8468\n0.7664\n0.0362\n0.2519\n0.5872\n\n\ny_1\n0.4556\n0.2781\n0.0315\n0.3598\n0.2876\n0.8363\n0.0685\n0.5543\n0.9194\n0.3232\n0.0243\n0.2689\n0.8404\n0.9788\n0.9912\n0.0846\n0.1129\n0.0503\n\n\ny_2\n0.8881\n0.6638\n0.5292\n0.3452\n0.4999\n0.6894\n0.7628\n0.4233\n0.4219\n0.3110\n0.1801\n0.5059\n0.2597\n0.9244\n0.6246\n0.8295\n0.5742\n0.7359\n\n\ny_3\n0.2917\n0.2912\n0.9906\n0.3964\n0.5851\n0.0647\n0.3191\n0.0659\n0.9295\n0.0189\n0.8553\n0.6701\n0.6306\n0.6152\n0.5295\n0.9469\n0.9927\n0.7433\n\n\ny_4\n0.5977\n0.7385\n0.9348\n0.8533\n0.6523\n0.7823\n0.7676\n0.4763\n0.6374\n0.8520\n0.4391\n0.5353\n0.9097\n0.7429\n0.2067\n0.4188\n0.0382\n0.9770\n\n\ny_5\n0.6669\n0.7935\n0.4501\n0.6770\n0.0361\n0.3082\n0.9436\n0.8420\n0.2966\n0.6996\n0.8092\n0.0206\n0.9509\n0.0499\n0.3504\n0.8491\n0.5674\n0.8691\n\n\ny_6\n0.4429\n0.2004\n0.3868\n0.9650\n0.0220\n0.4891\n0.0179\n0.3229\n0.1670\n0.6188\n0.6477\n0.0439\n0.3738\n0.3988\n0.6175\n0.9562\n0.6395\n0.7886\n\n\ny_7\n0.6403\n0.2487\n0.6137\n0.2387\n0.7919\n0.1610\n0.2259\n0.9336\n0.8569\n0.6710\n0.9067\n0.1028\n0.7898\n0.3126\n0.5972\n0.3078\n0.3259\n0.5631\n\n\ny_8\n0.5374\n0.9159\n0.0255\n0.7863\n0.0953\n0.7248\n0.3355\n0.1565\n0.2010\n0.3647\n0.3080\n0.8794\n0.2877\n0.2028\n0.8040\n0.8565\n0.2100\n0.2746\n\n\n\n\n\n$R$\n\n\n\nvariable\ny_0\ny_1\ny_2\ny_3\ny_4\ny_5\ny_6\ny_7\ny_8\n\n\n\n\ny_0\n0.5106\n0.3847\n0.4957\n0.2641\n0.0725\n0.3685\n0.7145\n0.0334\n0.6538\n\n\ny_1\n0.3847\n1.1931\n1.0307\n0.4238\n0.1569\n0.4712\n0.6801\n0.9019\n0.7750\n\n\ny_2\n0.4957\n1.0307\n1.9865\n0.5453\n0.3404\n0.8202\n1.0589\n0.7202\n1.1494\n\n\ny_3\n0.2641\n0.4238\n0.5453\n1.3356\n0.2462\n0.9538\n0.8949\n1.0863\n1.4202\n\n\ny_4\n0.0725\n0.1569\n0.3404\n0.2462\n0.9155\n0.8953\n0.5091\n0.3793\n0.3845\n\n\ny_5\n0.3685\n0.4712\n0.8202\n0.9538\n0.8953\n1.9467\n1.7410\n1.1834\n1.4452\n\n\ny_6\n0.7145\n0.6801\n1.0589\n0.8949\n0.5091\n1.7410\n2.9640\n1.6060\n1.6805\n\n\ny_7\n0.0334\n0.9019\n0.7202\n1.0863\n0.3793\n1.1834\n1.6060\n3.7560\n2.1550\n\n\ny_8\n0.6538\n0.7750\n1.1494\n1.4202\n0.3845\n1.4452\n1.6805\n2.1550\n3.0331\n\n\n\n\n\n$d$\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.4399\n\n\ny_1\n0.8723\n\n\ny_2\n0.2250\n\n\ny_3\n0.0971\n\n\ny_4\n0.6572\n\n\ny_5\n0.7544\n\n\ny_6\n0.5670\n\n\ny_7\n0.7409\n\n\ny_8\n0.7357\n\n\n\n\n\n$B$\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\nc_6\nc_7\nc_8\nc_9\nc_10\nc_11\nc_12\nc_13\n\n\n\n\nx_0\n0.0135\n0.9418\n0.6751\n0.3042\n0.0136\n0.7803\n0.2302\n0.5920\n0.7610\n0.8504\n0.2033\n0.5990\n0.8954\n0.0604\n\n\nx_1\n0.2530\n0.1406\n0.4280\n0.1165\n0.5817\n0.2896\n0.4509\n0.2735\n0.8425\n0.5123\n0.4176\n0.5773\n0.3222\n0.5276\n\n\nx_2\n0.4523\n0.6324\n0.4716\n0.0785\n0.9462\n0.5346\n0.9771\n0.4970\n0.5893\n0.5292\n0.6864\n0.5196\n0.8370\n0.3849\n\n\nx_3\n0.4922\n0.3260\n0.1990\n0.6217\n0.7287\n0.4589\n0.8500\n0.1375\n0.9457\n0.8397\n0.5203\n0.8416\n0.1371\n0.5186\n\n\nx_4\n0.4377\n0.2392\n0.4949\n0.4146\n0.3028\n0.6810\n0.1177\n0.1563\n0.2588\n0.8996\n0.9248\n0.5575\n0.2553\n0.0631\n\n\nx_5\n0.5413\n0.5853\n0.4166\n0.9482\n0.0665\n0.4683\n0.0348\n0.6635\n0.0501\n0.1221\n0.1268\n0.7322\n0.3311\n0.0151\n\n\nx_6\n0.1452\n0.5820\n0.8673\n0.3090\n0.8065\n0.7325\n0.1682\n0.5885\n0.1180\n0.4120\n0.2043\n0.8200\n0.5015\n0.3238\n\n\nx_7\n0.3842\n0.4433\n0.3052\n0.4963\n0.4459\n0.9266\n0.6286\n0.8807\n0.3252\n0.0861\n0.7891\n0.1666\n0.1766\n0.0318\n\n\nx_8\n0.8600\n0.8088\n0.8600\n0.5418\n0.4772\n0.9634\n0.3191\n0.1484\n0.6377\n0.0586\n0.5372\n0.8380\n0.8808\n0.2243\n\n\nx_9\n0.4277\n0.0173\n0.9436\n0.3526\n0.1852\n0.2433\n0.8409\n0.7467\n0.4969\n0.2585\n0.3466\n0.4240\n0.1253\n0.2661\n\n\nx_10\n0.9022\n0.0314\n0.0804\n0.7244\n0.3651\n0.0938\n0.8409\n0.0069\n0.3613\n0.6663\n0.1531\n0.9582\n0.1326\n0.9434\n\n\nx_11\n0.8686\n0.9671\n0.1879\n0.7194\n0.3153\n0.5075\n0.6469\n0.0551\n0.2449\n0.5830\n0.3328\n0.4071\n0.2686\n0.4456\n\n\nx_12\n0.5746\n0.1570\n0.5606\n0.7224\n0.6012\n0.4299\n0.0548\n0.3849\n0.0750\n0.4321\n0.9120\n0.4023\n0.5149\n0.5738\n\n\nx_13\n0.1813\n0.1437\n0.8099\n0.2174\n0.2784\n0.7365\n0.5066\n0.1417\n0.6935\n0.0812\n0.0792\n0.1286\n0.6698\n0.1731\n\n\nx_14\n0.3023\n0.8685\n0.0737\n0.2969\n0.0566\n0.7863\n0.9368\n0.2227\n0.0272\n0.9288\n0.2405\n0.8415\n0.4647\n0.5220\n\n\nx_15\n0.2359\n0.5393\n0.3662\n0.9737\n0.1073\n0.0926\n0.9738\n0.8049\n0.2272\n0.4266\n0.4965\n0.2811\n0.5143\n0.1134\n\n\nx_16\n0.8076\n0.4430\n0.9223\n0.0757\n0.7333\n0.1208\n0.4115\n0.5446\n0.8064\n0.5765\n0.2153\n0.4235\n0.2613\n0.2662\n\n\nx_17\n0.4906\n0.6666\n0.1782\n0.4631\n0.4471\n0.4886\n0.6511\n0.1357\n0.9547\n0.8251\n0.5739\n0.0537\n0.9671\n0.1413\n\n\n\n\n\n$m_0$\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.4748\n\n\nx_1\n0.0525\n\n\nx_2\n0.8524\n\n\nx_3\n0.5821\n\n\nx_4\n0.7281\n\n\nx_5\n0.9879\n\n\nx_6\n0.6011\n\n\nx_7\n0.4692\n\n\nx_8\n0.9031\n\n\nx_9\n0.9123\n\n\nx_10\n0.6185\n\n\nx_11\n0.8070\n\n\nx_12\n0.5830\n\n\nx_13\n0.5986\n\n\nx_14\n0.5898\n\n\nx_15\n0.8722\n\n\nx_16\n0.7868\n\n\nx_17\n0.8305\n\n\n\n\n\n$P_0$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\nx_16\nx_17\n\n\n\n\nx_0\n0.7075\n0.4575\n0.1025\n0.7552\n0.4678\n0.4102\n0.6403\n0.2264\n0.0279\n0.6776\n0.8178\n0.4180\n0.7019\n0.4730\n0.7924\n0.5532\n0.6499\n0.0084\n\n\nx_1\n0.4575\n1.3349\n0.8975\n1.0693\n0.9878\n0.5854\n0.6734\n0.6813\n0.5162\n1.2483\n1.1523\n0.6240\n0.8392\n0.6469\n1.1093\n0.4286\n0.6817\n0.5511\n\n\nx_2\n0.1025\n0.8975\n1.7778\n1.4223\n1.0713\n0.5301\n0.4902\n0.9565\n0.7575\n1.1096\n0.6516\n0.6540\n1.3722\n0.9620\n0.6709\n1.0418\n0.7424\n0.5799\n\n\nx_3\n0.7552\n1.0693\n1.4223\n2.9851\n1.4541\n1.0645\n1.6541\n0.9605\n1.5344\n2.4493\n1.9649\n1.5684\n1.7126\n1.3214\n1.6471\n1.7876\n1.5110\n1.4273\n\n\nx_4\n0.4678\n0.9878\n1.0713\n1.4541\n1.6513\n1.2777\n1.3102\n0.7676\n1.0484\n1.4509\n1.6367\n1.4200\n1.9104\n0.8253\n1.1014\n0.9313\n1.6568\n1.2782\n\n\nx_5\n0.4102\n0.5854\n0.5301\n1.0645\n1.2777\n2.0910\n2.0046\n1.1160\n1.0262\n1.7268\n2.2526\n2.0053\n2.1983\n1.2724\n1.6174\n1.5978\n2.3012\n1.8972\n\n\nx_6\n0.6403\n0.6734\n0.4902\n1.6541\n1.3102\n2.0046\n3.2961\n1.3658\n2.1582\n2.6066\n2.7869\n2.4754\n2.3180\n1.8782\n1.9604\n2.2224\n3.0209\n2.9851\n\n\nx_7\n0.2264\n0.6813\n0.9565\n0.9605\n0.7676\n1.1160\n1.3658\n2.2370\n1.1397\n2.0217\n1.7425\n1.4582\n1.7065\n1.2660\n1.8500\n1.3857\n2.3256\n1.4360\n\n\nx_8\n0.0279\n0.5162\n0.7575\n1.5344\n1.0484\n1.0262\n2.1582\n1.1397\n3.1976\n2.4686\n2.6294\n2.0770\n2.1077\n1.4005\n1.6343\n2.0925\n2.2970\n2.9207\n\n\nx_9\n0.6776\n1.2483\n1.1096\n2.4493\n1.4509\n1.7268\n2.6066\n2.0217\n2.4686\n3.8815\n3.3607\n2.4745\n3.0405\n2.4247\n2.8433\n3.0134\n3.4315\n3.3814\n\n\nx_10\n0.8178\n1.1523\n0.6516\n1.9649\n1.6367\n2.2526\n2.7869\n1.7425\n2.6294\n3.3607\n4.7920\n3.0654\n3.5766\n2.7664\n3.4915\n3.4792\n4.0401\n3.2582\n\n\nx_11\n0.4180\n0.6240\n0.6540\n1.5684\n1.4200\n2.0053\n2.4754\n1.4582\n2.0770\n2.4745\n3.0654\n3.1407\n3.3492\n2.5045\n2.9033\n2.5216\n3.3064\n2.7858\n\n\nx_12\n0.7019\n0.8392\n1.3722\n1.7126\n1.9104\n2.1983\n2.3180\n1.7065\n2.1077\n3.0405\n3.5766\n3.3492\n6.7801\n4.6939\n3.9369\n4.9600\n4.8216\n3.7575\n\n\nx_13\n0.4730\n0.6469\n0.9620\n1.3214\n0.8253\n1.2724\n1.8782\n1.2660\n1.4005\n2.4247\n2.7664\n2.5045\n4.6939\n5.2116\n3.8491\n4.6837\n3.8005\n3.5731\n\n\nx_14\n0.7924\n1.1093\n0.6709\n1.6471\n1.1014\n1.6174\n1.9604\n1.8500\n1.6343\n2.8433\n3.4915\n2.9033\n3.9369\n3.8491\n5.2003\n4.0112\n3.8169\n3.0359\n\n\nx_15\n0.5532\n0.4286\n1.0418\n1.7876\n0.9313\n1.5978\n2.2224\n1.3857\n2.0925\n3.0134\n3.4792\n2.5216\n4.9600\n4.6837\n4.0112\n6.7694\n5.0850\n4.5844\n\n\nx_16\n0.6499\n0.6817\n0.7424\n1.5110\n1.6568\n2.3012\n3.0209\n2.3256\n2.2970\n3.4315\n4.0401\n3.3064\n4.8216\n3.8005\n3.8169\n5.0850\n6.7135\n5.3989\n\n\nx_17\n0.0084\n0.5511\n0.5799\n1.4273\n1.2782\n1.8972\n2.9851\n1.4360\n2.9207\n3.3814\n3.2582\n2.7858\n3.7575\n3.5731\n3.0359\n4.5844\n5.3989\n7.9999\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nfine tune the model to only one variable\n\nfrom fastcore.basics import *\n\n\nfrom IPython.display import HTML\n\n\nvar_learning = {\n    'TA': [{'lr': 1e-3, 'n': 3}],  \n    'SW_IN': [{'lr': 1e-3, 'n': 4}],  \n    'SW_IN': [{'lr': 1e-3, 'n': 4}],  \n    'LW_IN': [{'lr': 1e-3, 'n': 3}],  \n    'VPD': [{'lr': 1e-3, 'n': 3}],  \n    'WS': [{'lr': 1e-3, 'n': 3}],  \n    'PA': [{'lr': 1e-3, 'n': 3}],  \n    # 'P': [{'lr': 1e-3, 'n': 3}], missing on purpose  \n    'SWC' : [{'lr': 1e-3, 'n': 5}, {'lr': 1e-5, 'n': 1}],\n    'TS' : [{'lr': 1e-3, 'n': 5}],\n\n\n}\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\ndef fine_tune(var_learning, learn):\n    spec_models = {}\n    spec_dls = {}\n    spec_learn = {}\n    spec_items = {}\n    for var in tqdm(var_learning.keys()):\n        display(HTML(f\"&lt;h4&gt; {var} | Gap len 6-336  finetune&lt;/h4&gt;\"))\n        spec_dls[var] = imp_dataloader(hai, hai_era, var_sel = var, block_len=100+336, gap_len=gen_gap_len(6, 336), bs=20, control_lags=[1], n_rep=3, shifts=gen_shifts(50)).cpu()\n        if show_metrics:\n            display(HTML(\"Metrics generic model\"))\n            display(metric_valid(learn, dls=spec_dls[var].valid))\n        for i, train in enumerate(var_learning[var]):\n            lr, n = train\n            display(HTML(f\"train {i}\"))\n            spec_models[var] = learn.model.copy()\n            spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], lr, n, base / f\"{var}_specialized_gap_6-336_v1_{i}\")\n            plt.show()\n    return spec_models, spec_dls, spec_learn, spec_items\n\n\nspec_models, spec_dls, spec_learn, spec_items = fine_tune(var_learning, learn_A1v)\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-73.308072\n-59.729135\n0.155103\n23:04\n\n\n1\n1\n-87.049737\n-73.262853\n0.139768\n20:37\n\n\n2\n2\n-92.947376\n-82.557740\n0.131374\n21:16\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n53.445060\n43.464660\n0.286705\n23:25\n\n\n1\n1\n49.458273\n42.814378\n0.285042\n23:47\n\n\n2\n2\n48.186476\n43.087234\n0.283170\n22:43\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n104.665918\n106.041969\n0.414556\n22:34\n\n\n1\n1\n101.284686\n107.526856\n0.419593\n23:28\n\n\n2\n2\n99.767878\n108.885340\n0.420613\n23:43\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n52.165032\n50.852617\n0.295505\n23:31\n\n\n1\n1\n45.131744\n36.635116\n0.272213\n22:41\n\n\n2\n2\n41.990330\n32.914802\n0.264974\n23:06\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n175.891940\n246.176982\n0.761467\n22:58\n\n\n1\n1\n165.224262\n235.823659\n0.719208\n23:03\n\n\n2\n2\n159.586716\n246.720599\n0.727202\n21:00\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-99.014311\n-97.738537\n0.127062\n21:53\n\n\n1\n1\n-123.031773\n-104.182228\n0.120468\n22:45\n\n\n2\n2\n-130.901483\n-133.160406\n0.104076\n24:11\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n172.303569\n200.127139\n0.599423\n23:57\n\n\n1\n1\n132.754943\n76.512459\n0.305065\n22:01\n\n\n2\n2\n93.005439\n57.391193\n0.270830\n20:58\n\n\n\n\n\n\n\n\n\n\ntrain 1\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n172.303569\n200.127139\n0.599423\n23:57\n\n\n1\n1\n132.754943\n76.512459\n0.305065\n22:01\n\n\n2\n2\n93.005439\n57.391193\n0.270830\n20:58\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n68.246805\n54.157492\n0.268725\n23:05\n\n\n1\n1\n57.107431\n39.662085\n0.247396\n21:34\n\n\n2\n2\n57.394308\n53.605346\n0.260903\n20:52\n\n\n\n\n\n\n\n\n\n\n\nvar_learning2 = {\n    'TA': [{'lr': 1e-3, 'n': 3}],  \n    'VPD': [{'lr': 1e-3, 'n': 2}],  \n    'PA': [{'lr': 1e-3, 'n': 2}],  \n    'SWC' : [{'lr': 1e-3, 'n': 3}, {'lr': 1e-5, 'n': 1}],\n    'TS' : [{'lr': 1e-3, 'n': 2}],\n}\n\n\ndef fine_tune2(var_learning, spec_dls, spec_learn, spec_items):\n    spec_learn = spec_learn.copy()\n    for var in tqdm(var_learning.keys()):\n        display(HTML(f\"&lt;h4&gt; {var} | Gap len 6-336  finetune 2 &lt;/h4&gt;\"))\n        for i, train in enumerate(var_learning[var]):\n            lr, n = train['lr'], train['n']\n            v = train.get('v', 2)\n            display(HTML(f\"train {i}\"))\n            spec_learn[var], _ = train_or_load(spec_learn[var].model, spec_dls[var], n, lr, path=base / f\"{var}_specialized_gap_6-336_v{v}_{i}\")\n            plt.show()\n    return spec_dls, spec_learn, spec_items\n\n\nspec_dls2, spec_learn2, spec_items2 = fine_tune2(var_learning2, spec_dls, spec_learn, spec_items)\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-97.718002\n-90.171141\n0.125761\n21:44\n\n\n1\n1\n-100.493585\n-88.338616\n0.126856\n20:23\n\n\n2\n2\n-103.769092\n-73.432097\n0.135440\n20:41\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n45.373127\n42.291928\n0.282366\n20:34\n\n\n1\n1\n46.487908\n41.780965\n0.283791\n20:26\n\n\n2\n2\n46.903155\n45.729009\n0.290985\n20:43\n\n\n3\n3\n46.917554\n39.832071\n0.278899\n20:29\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n98.188426\n107.646729\n0.414824\n20:22\n\n\n1\n1\n95.761911\n108.694974\n0.416340\n20:30\n\n\n2\n2\n97.285924\n106.717304\n0.415862\n20:30\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n42.370522\n31.185345\n0.254092\n20:27\n\n\n1\n1\n37.933048\n29.823322\n0.255506\n20:25\n\n\n2\n2\n35.904875\n25.101849\n0.245977\n20:28\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n158.971839\n247.131486\n0.730278\n20:37\n\n\n1\n1\n158.584698\n253.701959\n0.736434\n20:34\n\n\n2\n2\n156.970299\n263.529629\n0.749744\n20:22\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-131.680055\n-112.508634\n0.115626\n23:56\n\n\n1\n1\n-145.849450\n-95.348580\n0.114896\n24:38\n\n\n2\n2\n-145.281960\n-107.879192\n0.116886\n24:47\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n35.588624\n-20.680066\n0.174256\n24:56\n\n\n1\n1\n10.683595\n-23.630941\n0.167582\n24:32\n\n\n2\n2\n-34.885516\n-76.834793\n0.124991\n25:11\n\n\n3\n3\n-47.704883\n0.936394\n0.166095\n24:47\n\n\n4\n4\n-61.871826\n-75.499625\n0.120001\n24:45\n\n\n\n\n\n\n\n\n\n\ntrain 1\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-82.762576\n-101.858651\n0.103941\n24:41\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n40.478151\n29.605951\n0.230160\n25:47\n\n\n1\n1\n36.143091\n21.822619\n0.211396\n28:18\n\n\n2\n2\n30.493916\n-0.701642\n0.181059\n24:59\n\n\n3\n3\n26.646244\n17.138843\n0.201734\n27:48\n\n\n4\n4\n19.231903\n10.231487\n0.202251\n29:09\n\n\n\n\n\n\n\n\n\n\n\nvar_learning3 = {\n    'TA': [{'lr': 1e-5, 'n': 1, 'v': 3}],  \n    'PA': [{'lr': 1e-5, 'n': 1, 'v': 3}],  \n}\n\n\nspec_dls3, spec_learn3, spec_items3 = fine_tune2(var_learning3, spec_dls, spec_learn, spec_items)\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-98.172894\n-86.769272\n0.131181\n28:21\n\n\n\n\n\n\n\n\n\n\n\ntrain 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-155.452174\n-135.116148\n0.105047\n27:51\n\n\n\n\n\n\n\n\n\n\n\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(hai.columns)):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 &lt;/h4&gt;\"))\n    spec_dls[var] = imp_dataloader(hai, hai_era, var_sel = var, block_len=100+336, gap_len=gen_gap_len(12, 336), bs=20, control_lags=[1], n_rep=3, shifts=gen_shifts(50)).cpu()\n    spec_models[var] = learn_A1v.model.copy()\n    if show_metrics:\n        display(HTML(\"Metrics generic model\"))\n        display(metric_valid(learn_A1v, dls=spec_dls[var].valid))\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 3, 1e-3, base / f\"{var}_specialized_gap_12-336_v1\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-31.277383\n-59.727349\n0.155292\n23:16\n\n\n1\n1\n-57.410593\n-83.423924\n0.135217\n24:38\n\n\n2\n2\n-69.265146\n-81.197435\n0.137000\n21:50\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n72.838760\n63.622077\n0.317182\n22:33\n\n\n1\n1\n60.514370\n53.416147\n0.296670\n22:32\n\n\n2\n2\n55.572469\n48.728888\n0.287208\n25:43\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n110.011858\n112.541507\n0.435194\n24:28\n\n\n1\n1\n105.027452\n106.923631\n0.417517\n24:23\n\n\n2\n2\n102.755247\n106.489712\n0.415183\n24:53\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n67.771759\n71.121287\n0.306581\n25:12\n\n\n1\n1\n58.432390\n49.351550\n0.275882\n23:40\n\n\n2\n2\n49.884968\n41.432351\n0.264706\n25:51\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n169.196013\n241.890813\n0.734134\n24:18\n\n\n1\n1\n165.496950\n235.745148\n0.711773\n23:20\n\n\n2\n2\n161.570979\n223.982837\n0.696192\n22:10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-73.724512\n-103.872036\n0.124549\n27:03\n\n\n1\n1\n-111.420458\n-130.842109\n0.104550\n30:01\n\n\n2\n2\n-133.129972\n-140.989809\n0.100966\n24:07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n259.814830\n216.069186\n0.602446\n24:34\n\n\n1\n1\n248.654373\n209.606269\n0.607980\n23:06\n\n\n2\n2\n242.001831\n206.787110\n0.605378\n25:11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n207.952983\n194.924715\n0.646890\n24:35\n\n\n1\n1\n192.451676\n184.576861\n0.595377\n24:16\n\n\n2\n2\n169.086573\n117.395571\n0.388920\n22:44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n63.610211\n40.488381\n0.255747\n21:39\n\n\n1\n1\n57.377678\n39.280813\n0.247606\n21:41\n\n\n2\n2\n50.239735\n23.977059\n0.231418\n21:34\n\n\n\n\n\n\n\n\n\n\n\n\n\nspec_learn2 = {}\nfor var in tqdm(['TA', 'SW_IN', 'WS', 'PA', 'VPD', 'TS', 'SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 &lt;/h4&gt; | Training 2\"))\n    spec_learn2[var], _ = train_or_load(spec_learn[var].model.copy(), spec_dls[var], 2, 1e-3, base / f\"{var}_specialized_gap_12-336_v2.pickle\")\n    plt.show()\n    \n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-82.582890\n-78.921598\n0.137891\n21:39\n\n\n1\n1\n-86.119136\n-89.580120\n0.128288\n21:18\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n52.668345\n46.674095\n0.288313\n21:53\n\n\n1\n1\n51.089022\n44.378815\n0.283374\n22:28\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n164.019476\n232.919238\n0.710805\n22:37\n\n\n1\n1\n164.185435\n225.276840\n0.707446\n22:50\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-147.345027\n-136.658365\n0.103830\n22:42\n\n\n1\n1\n-157.427610\n-119.642745\n0.108054\n22:36\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n46.489590\n30.196986\n0.249948\n22:27\n\n\n1\n1\n45.226931\n46.153450\n0.273411\n21:01\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n39.863970\n17.692497\n0.221164\n20:40\n\n\n1\n1\n31.964869\n3.826767\n0.209781\n20:34\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n114.122129\n96.942318\n0.352660\n20:44\n\n\n1\n1\n67.486398\n34.946166\n0.239454\n20:39\n\n\n\n\n\n\n\n\n\n\nPA and VPD are overfitting so repeat training 2 with only one batch\n\nspec_learn3 = {}\nfor var in tqdm(['PA', 'VPD']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn[var].model.copy(), spec_dls[var], 1, 1e-3, base / f\"{var}_specialized_gap_12-336_v3.pickle\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-135.310833\n-106.027287\n0.120461\n24:33\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n41.435748\n32.315761\n0.254877\n24:13\n\n\n\n\n\n\n\n\n\n\n\nspec_learn3 = {}\nfor var in tqdm(['TS']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n16.858002\n-7.042792\n0.189336\n23:31\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n3.990423\n-0.629562\n0.185285\n24:57\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 2, 1e-3, base / f\"{var}_specialized_gap_12-336_v3_2\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-1.621724\n-38.955631\n0.146205\n23:45\n\n\n1\n1\n-35.639923\n-57.006918\n0.130909\n22:09\n\n\n\n\n\n\n\n\n\n\nthe training loss is getting worse … so tring with smaller learning rate\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3_3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-75.585425\n-79.693433\n0.113264\n25:09\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3+4 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3_4\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-79.65696\n-81.566511\n0.113204\n25:25\n\n\n\n\n\n\n\n\n\n\n\nspec_learn4 = {}\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3+4 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v4\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-0.957575\n-7.084297\n0.173571\n24:35\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['WS']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n158.930217\n224.42966\n0.701698\n24:04\n\n\n\n\n\n\n\n\n\n\nthis is overfitting PA\n\nfor var in tqdm(['PA']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1++3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn[var].model.copy(), spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-168.729458\n-122.183789\n0.10763\n23:40\n\n\n\n\n\n\n\n\n\n\n\nspec_learn4 = {}\nfor var in tqdm(['PA']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+3+4 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_12-336_v4\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-169.911168\n-117.59576\n0.109397\n21:33\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe goal of this section is to figure why the the generic model for all gaps works better then the generic model with gap in only one variable\n\nshow_results(learn_A1v, items = spec_items['TA'], hide_no_gap=True)\n\nNameError: name 'spec_items' is not defined\n\n\n\nshow_results(spec_learn2['TA'], items = spec_items['TA'], hide_no_gap=True)\n\nNameError: name 'spec_learn2' is not defined\n\n\n\nshow_results(learn_Aa, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls)\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\nno control to correlation is bad\n\nwith with_settings(learn_Aa.model, use_control=False):\n    display(show_results(learn_Aa, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\nthere is correlation here but no control,\nthe error is huggher and the uncertainty is higher\n\ndisplay(show_results(learn_A1v_nc, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_A1v_nc, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_A1v, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\nthis has the control\n\nitems_Aa_TA = [MeteoImpItem(i.i, i.shift, 'TA', i.gap_len) for i in items_Aa]\n\n\ndisplay(show_results(learn_Aa, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_Aa, items = items_Aa_TA, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA'], gap_len=30)]\n\n\n\n\n\n\n\nso the problem is that this is worse than the one above, even though it should not be the case\n\ndisplay(show_results(learn_A1v, items = items_Aa_TA, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA'], gap_len=30)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_Aa, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\neven for *** longer gaps is the same issue where the generic model trained with gaps in all variables is worse than the generic model with gaps in none\n\ndisplay(show_results(learn_A1v, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\nthis is just very bad!\n\ndisplay(show_results(learn_A1v, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\n\n\n\ndef plot_model_params(model):\n    sns.set(rc={\"figure.figsize\":(15, 10)})\n\n    sns.heatmap(array2df(model.H.squeeze(0)), annot=True,     vmin=-1, vmax=1.5, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"H\")\n    plt.show()\n    \n    sns.set(rc={\"figure.figsize\":(15, 15)})\n\n    sns.heatmap(array2df(model.A.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"A\")\n    plt.show()\n    \n    \n    sns.set(rc={\"figure.figsize\":(15, 15)})\n\n    sns.heatmap(array2df(model.B.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"B\")\n    plt.show()\n    \n    sns.heatmap(array2df(model.P0.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"P0\")\n    plt.show()\n    \n    sns.heatmap(array2df(model.m0.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"m0\")\n    plt.show()\n    \n#     sns.set(rc={\"figure.figsize\":(15, 15)})\n\n#     sns.heatmap(array2df(model.Q.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n#     cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n#     plt.title(\"Q\")\n#     plt.show()\n    \n\n\nplot_model_params(learn_A1v.model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(learn_Aa.model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(learn_A1v30.model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(spec_learn2['TA'].model)\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(learn_Aa.model)\n\n\n\n\n\n\n\n\n\n\n\nsns.heatmap(array2df((learn_Aa.model.H - learn_A1v30.model.H).squeeze(0)), annot=True,  center=0,\ncmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \nplt.title(\"diff H\")\n\nText(0.5, 1.0, 'diff H')\n\n\n\n\n\n\nsns.heatmap(array2df((learn_Aa.model.A - learn_A1v30.model.A).squeeze(0)), annot=True,  center=0,\ncmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \nplt.title(\"diff A\")\n\nText(0.5, 1.0, 'diff A')\n\n\n\n\n\n\nsns.heatmap(array2df((learn_Aa.model.B - learn_A1v30.model.B).squeeze(0)), annot=True,  center=0,\ncmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \nplt.title(\"diff B\")\n\nText(0.5, 1.0, 'diff B')\n\n\n\n\n\n\nsns.set(rc={\"figure.figsize\":(15, 10)})\n\nsns.heatmap(array2df(learn_A1v.model.H.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\nsns.set(rc={\"figure.figsize\":(15, 15)})\n\nsns.heatmap(array2df(learn_A1v.model.A.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nthis is okay but not much better\n\ndisplay(show_results(spec_learn2['TA'], items = items_Aa_TA, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA'], gap_len=30)]\n\n\n\n\n\n\n\n\n\n\n\n\ngap_models = {}\ngap_dls = {}\ngap_learn = {}\ngap_items = {}\nfor gap_len in tqdm([6,24,48,7*48]):\n    print(f\"Gap len: {gap_len}\")\n    gap_dls[gap_len] = imp_dataloader(hai, hai_era, var_sel = gen_var_sel(list(hai.columns), n_var=1), block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=1, shifts=gen_shifts(50)).cpu()\n    gap_models[gap_len] = learn_A1v.model.copy()\n    if display_metric: display(metric_valid(learn_A1v, dls=gap_dls[gap_len].valid))\n    gap_learn[gap_len], gap_items[gap_len] = train_or_load(gap_models[gap_len], gap_dls[gap_len], 3, 2e-5, base / f\"gap_1_any_var_gap_len_{gap_len}_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nGap len: 6\n\n\nNameError: name 'display_metric' is not defined\n\n\n\n\n\nas an experiment TA for a gap of 24 fine tuned\n\ndls_TA24 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'TA',\n    block_len=100+24,\n    gap_len=24,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=1).cpu()\n\n\nmodel_TA24 = learn_A1v.model.copy()\n\n\nmetric_valid(learn_A1v, dls= dls_TA24.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.338763\n0.184055\n\n\nstd\n2.551372\n0.034550\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 1e-4, base / \"TA_gap_24_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-2.51402\n-3.892453\n0.174431\n02:41\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 1e-4, base / \"TA_gap_24_v1_2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-4.713286\n-5.854372\n0.161322\n02:37\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 1e-4, base / \"TA_gap_24_v1_3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-6.744339\n-7.335044\n0.152876\n02:39\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 5e-5, base / \"TA_gap_24_v1_4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.932901\n-8.451432\n0.144813\n02:42\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 3, 3e-5, base / \"TA_gap_24_v1_4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.932901\n-8.451432\n0.144813\n02:42\n\n\n\n\n\n\n\n\n\n\n\nmetric_valid(learn_TA24, dls= dls_TA24.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-9.049459\n0.140859\n\n\nstd\n2.104846\n0.023429\n\n\n\n\n\n\n\n\ndls_TA48 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'TA',\n    block_len=100+48,\n    gap_len=48,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=1).cpu()\n\n\nmodel_TA48 = learn_TA24.model.copy()\n\n\nmetric_valid(learn_TA24, dls= dls_TA48.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-11.247340\n0.170793\n\n\nstd\n8.504741\n0.035950\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 2e-4, base / \"TA_gap_48_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-11.938200\n-13.136611\n0.119232\n02:38\n\n\n1\n1\n-13.676607\n-14.978353\n0.111058\n02:38\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 1e-4, base / \"TA_gap_48_v1_2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-15.563658\n-15.840925\n0.107003\n02:44\n\n\n1\n1\n-15.958545\n-16.626610\n0.103149\n02:54\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 1e-4, base / \"TA_gap_48_v1_3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-16.665217\n-17.394887\n0.099992\n02:40\n\n\n1\n1\n-17.165897\n-17.974066\n0.097933\n02:43\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 1e-4, base / \"TA_gap_48_v1_4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-18.111798\n-18.427618\n0.096457\n02:49\n\n\n1\n1\n-18.591682\n-19.413278\n0.092474\n02:52\n\n\n\n\n\n\n\n\n\n\n\nmodel_TA24_v2 = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca = hai, \n    pred_only_gap=True)\n\n\nlearn_TA24_v2, items_TA24_v2 = train_or_load(model_TA24, dls_TA24, 3, 1e-3, base / \"TA_gap_24_v2_1\")\n\n\nTA_models = {}\nTA_dls = {}\nTA_learn = {}\nTA_items = {}\nfor gap_len in tqdm([6,24,48,7*48]):\n    display(HTML(f\"&lt;h4&gt; TA | Gap len: {gap_len} &lt;/h4&gt;\"))\n    TA_dls[gap_len] = imp_dataloader(hai, hai_era, var_sel = 'TA', block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=2, shifts=gen_shifts(50)).cpu()\n    TA_models[gap_len] = learn_A1v.model.copy()\n    display(metric_valid(learn_A1v, dls=TA_dls[gap_len].valid))\n    TA_learn[gap_len], TA_items[gap_len] = train_or_load(TA_models[gap_len], TA_dls[gap_len], 4, 1e-4, base / f\"TA_gap_len_{gap_len}_v1.pickle\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.093156\n0.102643\n\n\nstd\n0.403933\n0.025537\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-3.623101\n-4.139968\n0.074969\n03:57\n\n\n1\n-5.193550\n-5.537112\n0.060948\n04:14\n\n\n2\n-6.309246\n-6.533323\n0.052087\n04:33\n\n\n3\n-7.058736\n-7.211129\n0.046813\n04:22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-0.660455\n0.200111\n\n\nstd\n2.088128\n0.031036\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-4.815664\n-5.862635\n0.162549\n05:23\n\n\n1\n-8.257145\n-9.265403\n0.141516\n05:27\n\n\n2\n-10.721531\n-11.764655\n0.126987\n05:17\n\n\n3\n-13.121913\n-13.537241\n0.119559\n05:31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n2.231489\n0.227216\n\n\nstd\n4.624743\n0.032391\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-5.077721\n-6.730453\n0.189337\n06:57\n\n\n1\n-10.696784\n-12.647214\n0.166578\n06:37\n\n\n2\n-15.724886\n-16.361526\n0.153221\n06:56\n\n\n3\n-18.714646\n-19.920346\n0.142614\n06:41\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n30.665105\n0.248255\n\n\nstd\n18.911530\n0.021534\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n18.701340\n-1.243045\n0.226205\n17:41\n\n\n1\n-0.477330\n-23.318493\n0.212453\n17:30\n\n\n2\n-19.797870\n-42.075069\n0.200830\n47:23\n\n\n3\n-36.470670\n-55.291288\n0.192844\n18:24\n\n\n\n\n\n\n\n\n\n\n\n\ndls_TA = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'TA',\n    block_len=70+336,\n    gap_len=gen_gap_len(12,336),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=3).cpu()\n\n\nmodel_TA = learn_A1v.model.copy()\n\n\nmetric_valid(learn_A1v, dls= dls_TA.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n18.120834\n0.240742\n\n\nstd\n19.321141\n0.032218\n\n\n\n\n\n\n\n\nlearn_TA, items_TA = train_or_load(model_TA, dls_TA, 2, 5e-4, base / \"TA_gap_12-336_v1,\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-26.097331\n-37.440739\n0.175528\n25:36\n\n\n1\n1\n-50.443927\n-66.472712\n0.149401\n26:02\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA, items_TA = train_or_load(learn_TA.model, dls_TA, 2, 5e-4, base / \"TA_gap_12-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-70.902275\n-72.341130\n0.137253\n24:20\n\n\n1\n-78.379055\n-76.962878\n0.132967\n23:55\n\n\n\n\n\n\n\n\n\nlearn_TA, items_TA = train_or_load(learn_TA.model, dls_TA, 2, 5e-4, base / \"TA_gap_12-336_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-85.106441\n-82.588917\n0.128094\n24:21\n\n\n1\n-86.267741\n-83.305525\n0.129777\n23:29\n\n\n\n\n\n\n\n\n\n\n\n\ndef metrics_valid_gap_lens(learn, var, gaps = [6,12,24,48,7*48]):\n    for gap_len in tqdm(gaps):\n        dls = imp_dataloader(hai, hai_era, var_sel = var, block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=1, shifts=gen_shifts(50)).cpu()\n        display(HTML(f\"&lt;strong&gt; Metrics | gap len: {gap_len} | Var: {var} &lt;/strong&gt;\"))\n        display(metric_valid(learn, dls=dls.valid))\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\nmetrics_valid_gap_lens(learn_A1v, 'TA')\n\n\n\n\n Metrics | gap len: 6 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.649933\n0.095968\n\n\nstd\n0.338268\n0.018444\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.243049\n0.134065\n\n\nstd\n0.710149\n0.021311\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.125709\n0.188222\n\n\nstd\n2.047966\n0.027886\n\n\n\n\n\n\n\n Metrics | gap len: 48 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n0.638268\n0.217725\n\n\nstd\n6.192707\n0.031585\n\n\n\n\n\n\n\n Metrics | gap len: 336 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n21.469734\n0.242877\n\n\nstd\n9.829365\n0.010609\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(learn_A1v_nc, 'TA')\n\n\n\n\n Metrics | gap len: 6 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.430541\n0.055689\n\n\nstd\n0.097171\n0.009796\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.790838\n0.113533\n\n\nstd\n0.833381\n0.030206\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-0.992221\n0.187918\n\n\nstd\n4.834171\n0.068321\n\n\n\n\n\n\n\n Metrics | gap len: 48 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n5.597832\n0.235436\n\n\nstd\n12.132120\n0.075510\n\n\n\n\n\n\n\n Metrics | gap len: 336 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n96.691128\n0.293088\n\n\nstd\n98.837141\n0.085536\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(spec_learn['TA'], 'TA')\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n Metrics | gap len: 6 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-6.332682\n0.049233\n\n\nstd\n0.226535\n0.007010\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-10.478126\n0.072882\n\n\nstd\n0.762210\n0.010581\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-15.977039\n0.103628\n\n\nstd\n4.186457\n0.024085\n\n\n\n\n\n\n\n Metrics | gap len: 48 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-25.748188\n0.125292\n\n\nstd\n11.229092\n0.031731\n\n\n\n\n\n\n\n Metrics | gap len: 336 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-136.930894\n0.152361\n\n\nstd\n61.110281\n0.026692\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(spec_learn[\"SWC\"], 'SWC', gaps=[6,12,24,30])\n\n\n\n\n Metrics | gap len: 6 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-4.637960\n0.051318\n\n\nstd\n0.299619\n0.012608\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-6.503471\n0.078820\n\n\nstd\n0.751687\n0.020815\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-6.256985\n0.114699\n\n\nstd\n1.533914\n0.030592\n\n\n\n\n\n\n\n Metrics | gap len: 30 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.957636\n0.144434\n\n\nstd\n2.228843\n0.032844\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(spec_learn3[\"SWC\"], 'SWC', gaps=[6,12,24,30])\n\n\n\n\n Metrics | gap len: 6 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-7.409364\n0.040791\n\n\nstd\n0.602997\n0.008078\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-14.166100\n0.049997\n\n\nstd\n1.298232\n0.009736\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-26.025482\n0.058914\n\n\nstd\n2.035570\n0.009232\n\n\n\n\n\n\n\n Metrics | gap len: 30 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-31.526816\n0.064446\n\n\nstd\n2.601954\n0.008842\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(learn_Aa, 'SWC', gaps=[6,12,24,30])\n\n\n\n\n Metrics | gap len: 6 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-10.621700\n0.020779\n\n\nstd\n1.637564\n0.009605\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-21.012826\n0.024119\n\n\nstd\n1.456359\n0.007166\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-36.451721\n0.034461\n\n\nstd\n4.163576\n0.011231\n\n\n\n\n\n\n\n Metrics | gap len: 30 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-43.786023\n0.038519\n\n\nstd\n5.267340\n0.010467\n\n\n\n\n\n\n\nthis is pretty weird the model with gaps in all variables is performing better that the one with only partial gaps ….\nlet’s so some finetuning\n\n\n\n\ndls_SWC_30 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'SWC',\n    block_len=120,\n    gap_len=30,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=1\n).cpu()\n\n\nmodel_SWC_30 = spec_learn['SWC'].copy()\n\n\nif show_metrics: metric_valid(model_SWC_30, dls=model_SWC_30.valid)\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 3e-4, base / \"SWC_gap_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-10.998339\n-15.672505\n0.099315\n02:51\n\n\n1\n-17.301334\n-22.166437\n0.085715\n02:51\n\n\n\n\n\n\n\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 1e-3, base / \"SWC_gap_30_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-27.642977\n-34.401863\n0.056773\n02:56\n\n\n1\n-33.920003\n-38.631508\n0.050980\n02:57\n\n\n\n\n\n\n\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 1e-3, base / \"SWC_gap_30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-38.766184\n-42.612718\n0.041158\n02:55\n\n\n1\n-39.593180\n-42.984206\n0.041554\n02:52\n\n\n\n\n\n\n\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 1e-3, base / \"SWC_gap_30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-38.766184\n-42.612718\n0.041158\n02:55\n\n\n1\n-39.593180\n-42.984206\n0.041554\n02:52\n\n\n\n\n\n\n\n\n\nshow_results(learn_SWC_30, hide_no_gap=True, items=items_SWC_30)\n\n[MeteoImpItem(i=1674, shift=30, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1693, shift=46, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1570, shift=-83, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1682, shift=54, var_sel=['SWC'], gap_len=30)]\n\n\n\n\n\n\n\n\nshow_results(learn_Aa, hide_no_gap=True, items=items_SWC_30)\n\n[MeteoImpItem(i=1674, shift=30, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1693, shift=46, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1570, shift=-83, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1682, shift=54, var_sel=['SWC'], gap_len=30)]\n\n\n\n\n\n\n\n\nshow_results(learn_A1v, hide_no_gap=True, items=items_SWC_30, dls=dls_SWC_30)\n\n[MeteoImpItem(i=1674, shift=30, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1693, shift=46, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1570, shift=-83, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1682, shift=54, var_sel=['SWC'], gap_len=30)]\n\n\n\n\n\n\n\n\nshow_results(spec_learn['SWC'], hide_no_gap=True, items=spec_items['SWC'], dls=spec_dls['SWC'])\n\n[MeteoImpItem(i=510, shift=10, var_sel=['SWC'], gap_len=243), MeteoImpItem(i=504, shift=0, var_sel=['SWC'], gap_len=109), MeteoImpItem(i=494, shift=-94, var_sel=['SWC'], gap_len=116), MeteoImpItem(i=481, shift=-17, var_sel=['SWC'], gap_len=148)]\n\n\n\n\n\n\n\n\nshow_results(learn_SWC_30)\n\n[MeteoImpItem(i=1892, shift=-50, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1667, shift=68, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1761, shift=41, var_sel=['SWC'], gap_len=30)]"
  },
  {
    "objectID": "results/train_8_feb_results.html",
    "href": "results/train_8_feb_results.html",
    "title": "Training Kalman Filter for Results - 8 Feb",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\nfrom meteo_imp.gaussian import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\nfrom fastcore.foundation import patch\nfrom pathlib import Path, PosixPath\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image\n\nfrom tqdm.auto import tqdm\n\n\nreset_seed()\n\n\nhaiB = pd.read_parquet(hai_big_path)\nhai_eraB = pd.read_parquet(hai_era_big_path)\n\n\nlist(haiB.columns)\n\n['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN']\n\n\n\nbase = here(\"analysis/results/trained_8feb\")\n\n\nbase.mkdir(exist_ok=True)\n\n\n@patch\ndef add_end(self: PosixPath, end): return self.parent / (self.name + end)\n\n\ndef train_or_load(model, dls, lr, n, path, keep=True):\n    save_models = SaveModelsBatch(times_epoch=5)\n    csv_logger = CSVLogger(path.add_end(\"log.csv\"), append=True)\n    learn = Learner(dls, model, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models, csv_logger], metrics=rmse_gap) \n    items = random.choices(dls.valid.items, k=4) \n    if path.add_end(\".pickle\").exists() and keep:\n        learn.model = torch.load(path.add_end(\".pickle\"))\n        display(csv_logger.read_log())\n        plot = Image(filename=path.add_end(\"_loss_plot.png\"))\n        display(plot)\n    else:\n        learn.fit(lr, n)\n        \n        torch.save(learn.model, path.add_end(\".pickle\"))\n        learn.recorder.plot_loss()\n        plt.savefig(path.add_end(\"_loss_plot.png\"))\n    return learn, items                           \n\n\n\nModel Av (All varying) all variables with a varing numbers of variables\n\ndls_Av = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Av = KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = haiB, pred_only_gap=True, use_conditional=False)\n\n\nlearn_Av, items_Av = train_or_load(model_Av, dls_Av, 2, 1e-3, base / \"All_gap_varying_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n26.214190\n30.346192\n0.386696\n36:35\n\n\n1\n1\n17.806891\n21.144149\n0.366064\n38:40\n\n\n\n\n\n\n\n\n\n\n\ndef metric_valid(learn):\n    nrmse = []\n    for input, target in tqdm(learn.dls.valid):\n        nrmse.append(learn.metrics[0](learn.model(input), target))\n    return torch.tensor(nrmse).mean()\n\n\nmetric_valid(learn_Av)\n\n\nmodel_Av.copy??\n\n\nlearn_Av, items_Av = train_or_load(model_Av, dls_Av, 1, 5e-4, base / \"All_gap_varying_30_v2\")\n\n\nshow_results(learn_Av, items=items_Av, control_map=control_map)\n\n\n\n\ndls_A1v = imp_dataloader(haiB[:30_000], hai_eraB, var_sel = gen_var_sel(list(haiB.columns), n_var=1), block_len=800, gap_len=672, bs=5, control_lags=[1], shifts=gen_shifts(300), n_rep=1).cpu()\n\n\nmodel_A1v = KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = haiB, pred_only_gap=True, use_conditional=False)\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 2, 1e-3, base / \"1_gap_varying_672_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n1566.155526\n1446.904870\n0.980710\n01:21\n\n\n1\n1421.453610\n1254.100269\n0.943964\n01:20\n\n\n\n\n\n\n\n\n\n\n\n\ndls_A1v_nc = imp_dataloader(haiB[:30_000], hai_eraB, var_sel = gen_var_sel(list(haiB.columns), n_var=1), block_len=800, gap_len=672, bs=5, control_lags=[1], shifts=gen_shifts(300), n_rep=1).cpu()\n\n\nmodel_A1v_nc = KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = haiB, pred_only_gap=True, use_control=False)\n\n\nlearn_A1v_nc, items_A1v_nc = train_or_load(model_A1v_nc, dls_A1v_nc, 1, 1e-3, base / \"1_gap_varying_672_no_control_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n2452.263967\n1800.175459\n1.147391\n01:21\n\n\n\n\n\n\n\n\n\n\n\n\n\ndls_Aa = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=1).cpu()\n\n\nmodel_Aa = model_Av.copy()\n\n\nlearn_Aa, items_Aa = train_or_load(model_Aa, dls_Aa, 2, 3e-4, base / \"All_gap_all_30_v1\", keep=False)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n33.282077\n39.916592\n0.410030\n02:31\n\n\n1\n31.570166\n40.208255\n0.410228\n02:20\n\n\n\n\n\n\n\n\n\nshow_results(learn_Aa, items=items_Aa, control_map=control_map)\n\n[MeteoImpItem(i=1647, shift=-60, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=30), MeteoImpItem(i=1765, shift=-60, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=30), MeteoImpItem(i=1785, shift=-60, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=30), MeteoImpItem(i=1717, shift=-60, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=30)]\n\n\n\n\n\n\n\n\n\n\nModel Av (All varying) all variables with a varing numbers of variables\n\ndls_Av12 = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=80, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_Av12 = model_Av.copy()\n\n\nlearn_Av12, items_Av12 = train_or_load(model_Av12, dls_Av12, 1, 1e-4, base / \"All_gap_varying_12_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-5.242486\n-5.854822\n0.276692\n13:14\n\n\n\n\n\n\n\n\n\nshow_results(learn_Av12, items=items_Av12, control_map=control_map)\n\n[MeteoImpItem(i=2681, shift=-8, var_sel=['WS', 'VPD', 'SW_IN', 'P', 'TA', 'LW_IN'], gap_len=12), MeteoImpItem(i=2429, shift=-8, var_sel=['WS', 'PA', 'TA', 'LW_IN', 'VPD', 'P'], gap_len=12), MeteoImpItem(i=2846, shift=-8, var_sel=['WS', 'P', 'VPD'], gap_len=12), MeteoImpItem(i=2377, shift=-40, var_sel=['LW_IN', 'PA'], gap_len=12)]\n\n\n\n\n\n\n\n\n\n\n\ndls_TA = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_TA = model_Av.copy()\n\n\nlearn_TA, save_models_TA, items_TA = train_or_load(model_TA, dls_TA, 2, 2e-4, base / \"TA_30_v1.pickle\")\n\nFileNotFoundError: [Errno 2] No such file or directory: '/home/simone/Documents/uni/Thesis/GPFA_imputation/analysis/results/trained_8feb/TA_30_v1.picklelog.csv'\n\n\n\nlearn_TA, items_TA, loggerTA = train_or_load(model_TA, dls_TA, 1, 2e-4, base / \"TA_30_v2.pickle\")\n\n\nloggerTA.read_log()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-22.20629\n-22.448926\n0.097302\n11:15\n\n\n\n\n\n\n\n\nshow_results(learn_TA, items=items_TA, control_map=control_map, hide_no_gap=True)\n\n\n\nModel TA\n\ndls_TA96 = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=1).cpu()\n\n\nmodel_TA96 = learn_Av.model.copy()\n\n\nlearn_TA96, save_models_TA96, items_TA96 = train_or_load(model_TA96, dls_TA96, 2, 2e-4, base / \"TA_96_v1.pickle\")\n\n\nshow_results(learn_TA96, items=items_TA96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in list(haiB.columns):\n    print(var)\n    spec_dls[var] = imp_dataloader(haiB[:40000], hai_eraB, var_sel = var, block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=1).cpu()\n    spec_models[var] = learn_Av.model.copy()\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 2, 2e-4, base / f\"{var}_specialized_v1.pickle\")\n   \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-39.692397\n-42.531067\n0.131699\n00:42\n\n\n1\n-45.283476\n-33.323406\n0.144391\n00:37\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n49.694907\n6.908584\n0.222646\n00:38\n\n\n1\n45.584341\n10.284428\n0.239156\n00:39\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n27.420796\n-6.238979\n0.190993\n00:40\n\n\n1\n24.375462\n-4.962333\n0.200030\n00:38\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-111.954325\n-106.969053\n0.075821\n00:42\n\n\n1\n-118.061853\n-119.208259\n0.065790\n00:40\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n122.597622\n113.888477\n0.592852\n00:41\n\n\n1\n124.965769\n113.211150\n0.584031\n00:40\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n85.440672\n82.872583\n0.552278\n00:39\n\n\n1\n85.644038\n82.978043\n0.553163\n00:38\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n31.828594\n27.359172\n0.290250\n00:43\n\n\n1\n29.653514\n26.873250\n0.288271\n00:38\n\n\n\n\n\n\n\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(haiB.columns)):\n    print(var)\n    spec_dls[var] = imp_dataloader(haiB[:30000], hai_eraB, var_sel = var, block_len=500, gap_len=192, bs=20, control_lags=[1], n_rep=1).cpu()\n    spec_models[var] = learn_Av.model.copy()\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_192_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n90.643715\n-16.383068\n0.177646\n00:36\n\n\n\n\n\n\n\n\nSW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n62.269008\n103.003446\n0.385411\n00:40\n\n\n\n\n\n\n\n\nVPD\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n95.957126\n285.851886\n0.495180\n00:41\n\n\n\n\n\n\n\n\nPA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-51.859173\n-69.926766\n0.125275\n00:43\n\n\n\n\n\n\n\n\nP\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n289.581748\n212.260866\n0.520403\n00:40\n\n\n\n\n\n\n\n\nWS\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n208.126112\n177.603145\n0.593094\n00:39\n\n\n\n\n\n\n\n\nLW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n63.759073\n62.438796\n0.308980\n00:41\n\n\n\n\n\n\n\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(haiB.columns)):\n    print(var) # 1 week\n    spec_dls[var] = imp_dataloader(haiB[:30000], hai_eraB, var_sel = var, block_len=500, gap_len=336, bs=20, control_lags=[1], n_rep=1).cpu()\n    spec_models[var] = learn_Av.model.copy()\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_336_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n244.441678\n-39.264206\n0.179186\n00:42\n\n\n\n\n\n\n\n\nSW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n92.375716\n216.062288\n0.416458\n00:43\n\n\n\n\n\n\n\n\nVPD\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n182.519898\n378.167553\n0.452476\n00:45\n\n\n\n\n\n\n\n\nPA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n22.902731\n-134.399585\n0.125682\n00:45\n\n\n\n\n\n\n\n\nP\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n449.964537\n412.074092\n0.671240\n00:44\n\n\n\n\n\n\n\n\nWS\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n373.671535\n304.890972\n0.592034\n00:45\n\n\n\n\n\n\n\n\nLW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n108.351227\n109.968726\n0.309430\n00:44\n\n\n\n\n\n\n\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(haiB.columns)):\n    print(var) # 2 weeks\n    spec_dls[var] = imp_dataloader(haiB[:30000], hai_eraB, var_sel = var, block_len=1000, gap_len=672, bs=20, control_lags=[1], n_rep=1).cpu()\n    spec_models[var] = learn_Av.model.copy()\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_672_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n657.307163\n118.648190\n0.204232\n01:09\n\n\n\n\n\n\n\n\nSW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n262.173894\n525.063575\n0.456621\n01:11\n\n\n\n\n\n\n\n\nVPD\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n493.652948\n311.636245\n0.362475\n01:12\n\n\n\n\n\n\n\n\nPA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n82.584014\n-149.931723\n0.136190\n01:14\n\n\n\n\n\n\n\n\nP\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n970.131864\n821.798107\n0.729170\n01:18\n\n\n\n\n\n\n\n\nWS\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n806.418451\n579.607119\n0.573610\n01:16\n\n\n\n\n\n\n\n\nLW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n228.687149\n231.744756\n0.317265\n01:15\n\n\n\n\n\n\n\n\n\ndef train_specialized(gap_len=30, block_len=120, bs=20, df=haiB, control=hai_eraB):\n    \n\n\n\n\n\n\ndls_SW_IN = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN', block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN = model_Av.copy()\n\n\nlearn_SW_IN, items_SW_IN = train_or_load(model_SW_IN, dls_SW_IN,  2, 2e-4, base / \"SW_IN_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n8.902548\n7.420177\n0.267329\n14:57\n\n\n1\n5.782914\n5.721588\n0.249692\n10:34\n\n\n\n\n\n\n\n\n\nshow_results(learn_SW_IN, items=items_SW_IN, control_map=control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=1588, shift=-60, var_sel=['SW_IN'], gap_len=30), MeteoImpItem(i=1863, shift=12, var_sel=['SW_IN'], gap_len=30), MeteoImpItem(i=1704, shift=36, var_sel=['SW_IN'], gap_len=30), MeteoImpItem(i=1545, shift=12, var_sel=['SW_IN'], gap_len=30)]\n\n\n\n\n\n\n\n\n\nModel SW_IN (All varying) all variables with a varing numbers of variables\n\ndls_SW_IN96 = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN96 = model_SW_IN.copy()\n\n\nlearn_SW_IN96, save_models_SW_IN96, items_SW_IN96 = train_or_load(model_SW_IN96, dls_SW_IN96, 2, 2e-4, base / \"SW_IN_96_v1.pickle\")\n\n\nshow_results(learn_SW_IN96, items=items_SW_IN96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\n\ndls_LW_IN = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN', block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN = model_Av.copy()\n\n\nlearn_LW_IN, save_models_LW_IN, items_LW_IN = train_or_load(model_LW_IN, dls_LW_IN, 2, 2e-4, base / \"LW_IN_12_v1.pickle\")\n\n\nshow_results(learn_LW_IN, items=items_LW_IN, control_map=control_map, hide_no_gap=True)\n\n\n\nModel LW_IN (All varying) all variables with a varing numbers of variables\n\ndls_LW_IN96 = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN96 = model_LW_IN.copy()\n\n\nlearn_LW_IN96, save_models_LW_IN96, items_LW_IN96 = train_or_load(model_LW_IN96, dls_LW_IN96, 2, 2e-4, base / \"LW_IN_96_v1.pickle\")\n\n\nshow_results(learn_LW_IN96, items=items_LW_IN96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\n\n\nModel Av_nc (All varying) all variables with a varing numbers of variables\n\ndls_Av_nc = imp_dataloader(haiB, \n                           control = hai_eraB.sample(frac=1).reset_index(drop=True).set_index(hai_eraB.index), #reshuffle so cannot use control\n                           var_sel = gen_var_sel(list(haiB.columns)), block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Av_nc =  KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = haiB, pred_only_gap=True, use_conditional=False, use_control=False)\n\n\nlearn_Av_nc, items_Av_nc = train_or_load(model_Av_nc, dls_Av_nc, 2, 1e-3, base / \"All_gap_varying_30_no_control_v1\")\n\n\n\n\n\n\n    \n      \n      50.00% [1/2 38:45&lt;38:45]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n48.175866\n46.578383\n0.431465\n38:45\n\n\n\n\n\n    \n      \n      1.19% [9/758 00:35&lt;49:33 51.5082]\n    \n    \n\n\nKeyboardInterrupt: \n\n\n\nshow_results(learn_Av_nc, items=items_Av_nc, control_map=control_map)"
  },
  {
    "objectID": "results/train_12_feb_results.html",
    "href": "results/train_12_feb_results.html",
    "title": "Training Kalman Filter for Results - 12 Feb",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\nfrom meteo_imp.gaussian import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\nfrom fastcore.foundation import patch\nfrom pathlib import Path, PosixPath\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image\n\nfrom tqdm.auto import tqdm\n\n\nfrom fastcore.basics import *\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_big_path)\nhai_era = pd.read_parquet(hai_era_big_path)\n\n\nbase = here(\"analysis/results/trained_13feb\")\n\n\nbase.mkdir(exist_ok=True)\n\n\n@patch\ndef add_end(self: PosixPath, end): return self.parent / (self.name + end)\n\n\ndef train_or_load(model, dls, lr, n, path, keep=True):\n    save_models = SaveModelsBatch(times_epoch=5)\n    csv_logger = CSVLogger(path.add_end(\"log.csv\"))\n    learn = Learner(dls, model, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models, csv_logger], metrics=rmse_gap) \n    items = random.choices(dls.valid.items, k=4) \n    if path.add_end(\".pickle\").exists() and keep:\n        learn.model = torch.load(path.add_end(\".pickle\"))\n        display(csv_logger.read_log())\n        plot = Image(filename=path.add_end(\"_loss_plot.png\"))\n        display(plot)\n    else:\n        learn.fit(lr, n)\n        \n        torch.save(learn.model, path.add_end(\".pickle\"))\n        learn.recorder.plot_loss()\n        plt.savefig(path.add_end(\"_loss_plot.png\"))\n    return learn, items                           \n\n\ndef metric_valid(learn, dls=None):\n    nrmse = []\n    losses = []\n    dls = ifnone(dls, learn.dls.valid)\n    for input, target in tqdm(dls):\n        pred = learn.model(input)\n        nrmse.append(learn.metrics[0](pred, target))\n        losses.append(learn.loss_func(pred, target).item())\n    metric = pd.DataFrame({'loss': losses, 'rmse': nrmse})\n    return metric.agg(['mean', 'std'])\n\n\nhai.columns\n\nIndex(['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], dtype='object')\n\n\n\n\n\ndls_A1v = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns), n_var=1),\n    block_len=100+336,\n    gap_len=gen_gap_len(12, 336),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5).cpu()\n\n\nmodel_A1v = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    pred_only_gap=True)\n\n\nmodel_A1v.B.shape\n\ntorch.Size([1, 18, 14])\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 3, 1e-3, base / \"1_gap_varying_12-336_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n196.765350\n163.493486\n0.579074\n44:01\n\n\n1\n1\n138.298704\n123.299909\n0.490741\n44:31\n\n\n2\n2\n113.640141\n116.746793\n0.488059\n42:31\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 3, 1e-5, base / \"1_gap_varying_12-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n196.765350\n163.493486\n0.579074\n44:01\n\n\n1\n138.298704\n123.299909\n0.490741\n44:31\n\n\n2\n113.640141\n116.746793\n0.488059\n42:31\n\n\n\n\n\n\n\n\n\nshow_results(learn_A1v)\n\n[MeteoImpItem(i=451, shift=11, var_sel=['WS'], gap_len=328), MeteoImpItem(i=455, shift=-70, var_sel=['TA'], gap_len=326), MeteoImpItem(i=501, shift=23, var_sel=['SW_IN'], gap_len=322)]\n\n\n\n\n\n\n\n\nmetric_valid(learn_A1v)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n96.871684\n0.428096\n\n\nstd\n38.539188\n0.113949\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(learn_A1v.model, dls_A1v, 1, 5e-6, base / \"1_gap_varying_3-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n84.676688\n91.879223\n0.42373\n30:58\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_A1v_nc = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    pred_only_gap=True,\n    use_control=False\n)\n\n\nlearn_A1v_nc, items_A1v_nc = train_or_load(model_A1v_nc, dls_A1v, 3, 1e-3, base / \"1_gap_varying_336_no_control_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n225.831814\n196.530018\n0.679575\n42:16\n\n\n1\n181.730136\n179.931006\n0.622102\n42:47\n\n\n2\n174.721447\n175.969022\n0.592775\n41:48\n\n\n\n\n\n\n\n\n\n\n\n\ndls_Aa = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = list(hai.columns),\n    block_len=120,\n    gap_len=30,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5\n).cpu()\n\n\ndls_Aa = imp_dataloader(hai, hai_era, var_sel = list(hai.columns), block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa = learn_A1v.model.copy()\n\n\nmetric_valid(learn_A1v, dls=dls_Aa.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n310.588176\n0.908583\n\n\nstd\n56.248472\n0.204352\n\n\n\n\n\n\n\n\nlearn_Aa, items_Aa = train_or_load(model_Aa, dls_Aa, 3, 3e-4, base / \"All_gap_all_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n68.115461\n62.590038\n0.419928\n31:16\n\n\n1\n24.302401\n30.411800\n0.399196\n31:34\n\n\n2\n11.095821\n15.858787\n0.388143\n31:03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngap_models = {}\ngap_dls = {}\ngap_learn = {}\ngap_items = {}\nfor gap_len in tqdm([6,24,48,7*48]):\n    print(f\"Gap len: {gap_len}\")\n    gap_dls[gap_len] = imp_dataloader(hai, hai_era, var_sel = gen_var_sel(list(hai.columns), n_var=1), block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=1, shifts=gen_shifts(50)).cpu()\n    gap_models[gap_len] = learn_A1v.model.copy()\n    display(metric_valid(learn_A1v, dls=gap_dls[gap_len].valid))\n    gap_learn[gap_len], gap_items[gap_len] = train_or_load(gap_models[gap_len], gap_dls[gap_len], 3, 2e-5, base / f\"gap_1_any_var_gap_len_{gap_len}_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nGap len: 6\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n1.247630\n0.234464\n\n\nstd\n3.109537\n0.123313\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n2.278421\n1.701985\n0.248359\n01:33\n\n\n1\n1\n1.197995\n1.562936\n0.247491\n01:30\n\n\n2\n2\n0.676090\n1.439395\n0.245816\n01:28\n\n\n\n\n\n\n\n\n\n\nGap len: 24\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n13.170145\n0.378981\n\n\nstd\n9.782837\n0.108107\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n8.605524\n9.680723\n0.346301\n02:09\n\n\n1\n1\n8.196774\n9.658409\n0.345941\n02:03\n\n\n2\n2\n7.940351\n9.333798\n0.343399\n02:04\n\n\n\n\n\n\n\n\n\n\nGap len: 48\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n23.275539\n0.386880\n\n\nstd\n8.201757\n0.079038\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n20.522270\n28.226681\n0.428597\n02:44\n\n\n1\n1\n20.123038\n27.948954\n0.425583\n02:38\n\n\n2\n2\n21.275250\n27.780100\n0.422899\n02:39\n\n\n\n\n\n\n\n\n\n\nGap len: 336\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n162.622499\n0.430899\n\n\nstd\n97.689637\n0.138170\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n158.012086\n189.406813\n0.460826\n06:31\n\n\n1\n1\n158.148799\n192.724921\n0.462467\n06:20\n\n\n2\n2\n157.097036\n192.511927\n0.461974\n06:43\n\n\n\n\n\n\n\n\n\n\n\n\n\nfine tune the model to only one variable\n\nfrom fastcore.basics import *\n\n\nfrom IPython.display import HTML\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(hai.columns)):\n    display(HTML(f\"&lt;h3&gt; {var} &lt;/h3&gt;\"))\n    spec_dls[var] = imp_dataloader(hai, hai_era, var_sel = var, block_len=100+336, gap_len=gen_gap_len(3, 336), bs=20, control_lags=[1], n_rep=1, shifts=gen_shifts(50)).cpu()\n    spec_models[var] = learn_A1v.model.copy()\n    print(\"Metrics of generic model for only one variable\")\n    display(metric_valid(learn_A1v, dls=spec_dls[var].valid))\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 2, 1e-5, base / f\"{var}_specialized_gap_3-336_v1.pickle\")\n    plt.show()\n\ndo some additional training for LW_IN and SW_IN\n\nspec_learn2 = {}\nfor var in tqdm(['SW_IN', 'LW_IN']):\n    print(var)\n    print(metric_valid(spec_learn[var].model, dls=spec_dls[var].valid))\n    spec_learn2[var], _ = train_or_load(spec_learn[var].model, spec_dls[var], 3, 2e-5, base / f\"{var}_specialized_gap_3-336_v3.pickle\")\n    plt.show()"
  },
  {
    "objectID": "results/train_6_feb_results.html",
    "href": "results/train_6_feb_results.html",
    "title": "Training Kalman Filter for Results - 6 Feb",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\nfrom meteo_imp.gaussian import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\nfrom fastcore.foundation import patch\nfrom pathlib import Path, PosixPath\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\n\nreset_seed()\n\n\nhaiB = pd.read_parquet(hai_big_path)\nhai_eraB = pd.read_parquet(hai_era_big_path)\n\n\nlist(haiB.columns)\n\n['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN']\n\n\n\nbase = here(\"analysis/results/trained_8feb\")\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/pyprojroot/pyprojroot.py:51: UserWarning: Path doesn't exist: /home/simone/Documents/uni/Thesis/GPFA_imputation/analysis/results/trained_8feb\n  warnings.warn(\"Path doesn't exist: {}\".format(path))\n\n\n\nbase.mkdir(exist_ok=True)\n\n\n@patch\ndef add_end(self: PosixPath, end): return self.parent / (self.name + end)\n\n\ndef train_or_load(model, dls, lr, n, path, keep=True):\n    save_models = SaveModelsBatch(times_epoch=5)\n    learn = Learner(dls, model, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models], metrics=rmse_gap) \n    items = random.choices(dls.valid.items, k=4) \n    if path.add_end(\".pickle\").exists() and keep:\n        learn.model = torch.load(path.add_end(\".pickle\"))\n        save_models = torch.load(path.add_end(\"_save_models.pickle\"))\n        # learn = torch.load(path.add_end(\"_learn.pickle\"))\n    else:\n        learn.fit(lr, n)\n        \n        torch.save(learn.model, path.add_end(\".pickle\"))\n        torch.save(save_models, path.add_end(\"_save_models.pickle\"))\n        # torch.save(learn, path.add_end(\"_learn.pickle\"))\n        learn.recorder.plot_loss()\n    return learn, save_models, items                           \n\n\n\nModel Av (All varying) all variables with a varing numbers of variables\n\ndls_Av = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Av = KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = haiB, pred_only_gap=True, use_conditional=False)\n\n\nlearn_Av, save_models_Av, items_Av = train_or_load(model_Av, dls_Av, 2, 1e-3, base / \"All_gap_varying_12_v1.pickle\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-5.528869\n-4.755432\n0.315470\n25:56\n\n\n1\n-8.360162\n-9.218511\n0.282865\n25:32\n\n\n\n\n\n\n\n\n\nshow_results(learn_Av, items=items_Av, control_map=control_map)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n[MeteoImpItem(i=2258, shift=-10, var_sel=['TA', 'PA', 'VPD', 'LW_IN'], gap_len=12), MeteoImpItem(i=1912, shift=-50, var_sel=['VPD', 'SW_IN', 'TA', 'PA', 'LW_IN', 'P'], gap_len=12), MeteoImpItem(i=1852, shift=-20, var_sel=['TA', 'SW_IN', 'P', 'WS', 'PA', 'VPD'], gap_len=12), MeteoImpItem(i=2197, shift=40, var_sel=['LW_IN', 'P', 'WS', 'PA', 'TA', 'SW_IN', 'VPD'], gap_len=12)]\n\n\nSchemaValidationError: Invalid specification\n\n        altair.vegalite.v5.api.Chart, validating 'required'\n\n        'data' is a required property\n        \n\n\nalt.HConcatChart(...)\n\n\n\n\nModel Av (All varying) all variables with a varing numbers of variables\n\nreset_seed(100)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\ndls_Av96 = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=150, gap_len=30, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Av96 = model_Av.copy()\n\n\nlearn_Av96, sAv96e_models_Av96, items_Av96 = train_or_load(model_Av96, dls_Av96, 1, 1e-3, base / \"All_gap_varying_30_v1.pickle\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n7.840665\n3.947803\n0.345720\n39:47\n\n\n\n\n\nPicklingError: Can't pickle &lt;class 'meteo_imp.kalman.fastai.SaveModelsBatch'&gt;: it's not the same object as meteo_imp.kalman.fastai.SaveModelsBatch\n\n\n\nshow_results(learn_Av96, items=items_Av96, control_map=control_map)\n\n\n\n\n\n\ndls_Aa = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa = model_Av.copy()\n\n\nlearn_Aa, save_models_Aa, items_Aa = train_or_load(model_Aa, dls_Aa, 2, 2e-4, base / \"All_gap_all_v1.pickle\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-17.658522\n-19.488553\n0.299405\n18:14\n\n\n1\n-20.148824\n-19.363829\n0.298983\n19:30\n\n\n\n\n\n\n\n\n\nshow_results(learn_Aa, items=items_Aa, control_map=control_map)\n\n[MeteoImpItem(i=1844, shift=-10, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=2247, shift=-10, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=2253, shift=30, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12), MeteoImpItem(i=1975, shift=-20, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=12)]\n\n\n\n\n\n\n\n\n\nModel Aa (All varying) all variables with a varing numbers of variables\n\ndls_Aa96 = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=200, gap_len=36, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa96 = model_Aa.copy()\n\n\nlearn_Aa96, save_models_Aa96, items_Aa96 = train_or_load(model_Aa96, dls_Aa96, 2, 2e-4, base / \"All_gap_all_96_v1.pickle\")\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/454 00:00&lt;?]\n    \n    \n\n\nValueError: Expected parameter covariance_matrix (Tensor of shape (7, 7)) of distribution MultivariateNormal(loc: torch.Size([7]), covariance_matrix: torch.Size([7, 7])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[ 0.0464,  0.0853,  0.0814, -0.0031, -0.0969, -0.0054, -0.0313],\n        [ 0.0853,  0.2309,  0.1449, -0.0055, -0.1849, -0.0164, -0.0815],\n        [ 0.0814,  0.1449,  0.1417, -0.0038, -0.1716, -0.0370, -0.0575],\n        [-0.0031, -0.0055, -0.0038,  0.0013,  0.0101,  0.0027,  0.0033],\n        [-0.0969, -0.1849, -0.1716,  0.0101,  1.0555,  0.0276,  0.0835],\n        [-0.0054, -0.0164, -0.0370,  0.0027,  0.0276,  0.0684, -0.0029],\n        [-0.0313, -0.0815, -0.0575,  0.0033,  0.0835, -0.0029,  0.1097]],\n       dtype=torch.float64, grad_fn=&lt;ExpandBackward0&gt;)\n\n\n\nshow_results(learn_Aa96, items=items_Aa96, control_map=control_map)\n\n\n\n\n\n\ndls_TA = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_TA = model_Av.copy()\n\n\nlearn_TA, save_models_TA, items_TA = train_or_load(model_TA, dls_TA, 2, 2e-4, base / \"TA_12_v1.pickle\")\n\n\nshow_results(learn_TA, items=items_TA, control_map=control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=1895, shift=30, var_sel=['TA'], gap_len=12), MeteoImpItem(i=2186, shift=-30, var_sel=['TA'], gap_len=12), MeteoImpItem(i=2220, shift=-50, var_sel=['TA'], gap_len=12), MeteoImpItem(i=1844, shift=-50, var_sel=['TA'], gap_len=12)]\n\n\n\n\n\n\n\n\n\nModel TA\n\ndls_TA96 = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_TA96 = model_TA.copy()\n\n\nlearn_TA96, save_models_TA96, items_TA96 = train_or_load(model_TA96, dls_TA96, 2, 2e-4, base / \"TA_96_v1.pickle\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-70.275536\n-74.080324\n0.107727\n21:53\n\n\n1\n-71.829675\n-72.279019\n0.109978\n22:05\n\n\n\n\n\n\n\n\n\nshow_results(learn_TA96, items=items_TA96, control_map=control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=603, shift=-35, var_sel=['TA'], gap_len=96), MeteoImpItem(i=536, shift=-175, var_sel=['TA'], gap_len=96), MeteoImpItem(i=528, shift=105, var_sel=['TA'], gap_len=96), MeteoImpItem(i=642, shift=105, var_sel=['TA'], gap_len=96)]\n\n\n\n\n\n\n\n\n\n\n\n\ndls_SW_IN = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN', block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN = model_Av.copy()\n\n\nlearn_SW_IN, save_models_SW_IN, items_SW_IN = train_or_load(model_SW_IN, dls_SW_IN, 2, 2e-4, base / \"SW_IN_12_v1.pickle\")\n\n\nshow_results(learn_SW_IN, items=items_SW_IN, control_map=control_map, hide_no_gap=True)\n\n\n\nModel SW_IN (All varying) all variables with a varing numbers of variables\n\ndls_SW_IN96 = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN96 = model_SW_IN.copy()\n\n\nlearn_SW_IN96, save_models_SW_IN96, items_SW_IN96 = train_or_load(model_SW_IN96, dls_SW_IN96, 2, 2e-4, base / \"SW_IN_96_v1.pickle\")\n\n\nshow_results(learn_SW_IN96, items=items_SW_IN96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\n\ndls_LW_IN = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN', block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN = model_Av.copy()\n\n\nlearn_LW_IN, save_models_LW_IN, items_LW_IN = train_or_load(model_LW_IN, dls_LW_IN, 2, 2e-4, base / \"LW_IN_12_v1.pickle\")\n\n\nshow_results(learn_LW_IN, items=items_LW_IN, control_map=control_map, hide_no_gap=True)\n\n\n\nModel LW_IN (All varying) all variables with a varing numbers of variables\n\ndls_LW_IN96 = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN96 = model_LW_IN.copy()\n\n\nlearn_LW_IN96, save_models_LW_IN96, items_LW_IN96 = train_or_load(model_LW_IN96, dls_LW_IN96, 2, 2e-4, base / \"LW_IN_96_v1.pickle\")\n\n\nshow_results(learn_LW_IN96, items=items_LW_IN96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\n\n\nModel Av_nc (All varying) all variables with a varing numbers of variables\n\ndls_Av_nc = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Av_nc = model_Av.copy()\nmodel_Av_nc.use_control = False\n\n\nlearn_Av_nc, save_models_Av_nc, items_Av_nc = train_or_load(model_Av_nc, dls_Av_nc, 10, 5e-4, base / \"All_gap_varying_12_no_control_v1.pickle\")\n\n\nshow_results(learn_Av_nc, items=items_Av_nc, control_map=control_map)\n\n\nInteractiveSequence(save_models_Av_nc.show_results(learn_Av_nc))()\n\n\n\nModel Av_nc (All varying) all variables with a varing numbers of variables\n\ndls_Av_nc96 = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Av_nc96 = model_Av_nc.copy()\n\n\nlearn_Av_nc96, save_models_Av_nc96, items_Av_nc96 = train_or_load(model_Av_nc96, dls_Av_nc96, 2, 2e-4, base / \"All_gap_varying_96_no_control_v1.pickle\")\n\n\nshow_results(learn_Av_nc96, items=items_Av_nc96, control_map=control_map)\n\n\n\n\n\n\ndls_Aa_nc = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa_nc = model_Av_nc.copy()\n\n\nlearn_Aa_nc, save_models_Aa_nc, items_Aa_nc = train_or_load(model_Aa_nc, dls_Aa_nc, 2, 2e-4, base / \"All_gap_all_no_control_v1.pickle\")\n\n\nshow_results(learn_Aa_nc, items=items_Aa_nc, control_map=control_map)\n\n\n\nModel Aa_nc (All varying) all variables with a varing numbers of variables\n\ndls_Aa_nc96 = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa_nc96 = model_Aa_nc.copy()\n\n\nlearn_Aa_nc96, save_models_Aa_nc96, items_Aa_nc96 = train_or_load(model_Aa_nc96, dls_Aa_nc96, 2, 2e-4, base / \"All_gap_all_96_no_control_v1.pickle\")\n\n\nshow_results(learn_Aa_nc96, items=items_Aa_nc96, control_map=control_map)\n\n\n\n\n\n\ndls_TA_nc = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_TA_nc = model_Av_nc.copy()\n\n\nlearn_TA_nc, save_models_TA_nc, items_TA_nc = train_or_load(model_TA_nc, dls_TA_nc, 2, 2e-4, base / \"TA_nc_12_no_control_v1.pickle\")\n\n\nshow_results(learn_TA_nc, items=items_TA_nc, control_map=control_map, hide_no_gap=True)\n\n\n\nModel TA_nc (All varying) all variables with a varing numbers of variables\n\ndls_TA_nc96 = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_TA_nc96 = model_TA_nc.copy()\n\n\nlearn_TA_nc96, save_models_TA_nc96, items_TA_nc96 = train_or_load(model_TA_nc96, dls_TA_nc96, 2, 2e-4, base / \"TA_nc_96_no_control_v1.pickle\")\n\n\nshow_results(learn_TA_nc96, items=items_TA_nc96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\n\ndls_SW_IN_nc = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN_nc', block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN_nc = model_Av_nc.copy()\n\n\nlearn_SW_IN_nc, save_models_SW_IN_nc, items_SW_IN_nc = train_or_load(model_SW_IN_nc, dls_SW_IN_nc, 2, 2e-4, base / \"SW_IN_nc_12_no_control_v1.pickle\")\n\n\nshow_results(learn_SW_IN_nc, items=items_SW_IN_nc, control_map=control_map, hide_no_gap=True)\n\n\n\nModel SW_IN_nc (All varying) all variables with a varing numbers of variables\n\ndls_SW_IN_nc96 = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN_nc', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN_nc96 = model_SW_IN_nc.copy()\n\n\nlearn_SW_IN_nc96, save_models_SW_IN_nc96, items_SW_IN_nc96 = train_or_load(model_SW_IN_nc96, dls_SW_IN_nc96, 2, 2e-4, base / \"SW_IN_nc_96_no_control_v1.pickle\")\n\n\nshow_results(learn_SW_IN_nc96, items=items_SW_IN_nc96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\n\ndls_LW_IN_nc = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN', block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN_nc = model_Av_nc.copy()\n\n\nlearn_LW_IN_nc, save_models_LW_IN_nc, items_LW_IN_nc = train_or_load(model_LW_IN_nc, dls_LW_IN_nc, 2, 2e-4, base / \"LW_IN_12_no_control_v1.pickle\")\n\n\nshow_results(learn_LW_IN_nc, items=items_LW_IN_nc, control_map=control_map, hide_no_gap=True)\n\n\n\nModel LW_IN_nc (All varying) all variables with a varing numbers of variables\n\ndls_LW_IN_nc96 = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN_nc', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN_nc96 = model_LW_IN_nc.copy()\n\n\nlearn_LW_IN_nc96, save_models_LW_IN_nc96, items_LW_IN_nc96 = train_or_load(model_LW_IN_nc96, dls_LW_IN_nc96, 2, 2e-4, base / \"LW_IN_96_no_control_v1.pickle\")\n\n\nshow_results(learn_LW_IN_nc96, items=items_LW_IN_nc96, control_map=control_map, hide_no_gap=True)"
  },
  {
    "objectID": "results/result_plotting-22-feb.html",
    "href": "results/result_plotting-22-feb.html",
    "title": "Plotting for results",
    "section": "",
    "text": "This notebook produces all results plots. It generates some gap in the data, fill with a method (filter, MDS …), compute metrics and then makes all relevant plots\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport altair as alt\n\n\nfrom meteo_imp.kalman.results import *\nfrom meteo_imp.data import *\nfrom meteo_imp.utils import *\nimport pandas as pd\nimport numpy as np\nfrom pyprojroot import here\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG, Image\n\n\nimport vl_convert as vlc\nfrom pyprojroot import here\nbase_path_img = here(\"manuscript/Master Thesis - Meteorological time series imputation using Kalman filters - Simone Massaro/images2/\")\nbase_path_tbl = here(\"manuscript/Master Thesis - Meteorological time series imputation using Kalman filters - Simone Massaro/tables2/\")\n\nbase_path_img.mkdir(exist_ok=True), base_path_tbl.mkdir(exist_ok=True)\n\ndef save_plot(plot, path):\n    png_data = vlc.vegalite_to_png(vl_spec=plot.to_json(), scale=3)\n    with open(base_path_img / (path + \".png\"), \"wb\") as f:\n        f.write(png_data)\n\ndef show_plot(path): return Image(filename=base_path_img / (path + \".png\"))\n\n\nreset_seed()\nn_rep = 500\n\n\nhai = pd.read_parquet(hai_big_path).reindex(columns=var_type.categories)\nhai_era = pd.read_parquet(hai_era_big_path)\n\n\nalt.data_transformers.disable_max_rows() # it is safe to do so as the plots are rendered using vl-convert and then showed as images\n\nDataTransformerRegistry.enable('default')\n\n\n\n\n\nbase_path_old = here(\"analysis/results/trained_14feb\")\nbase_path = here(\"analysis/results/trained_21feb\")\n\n\ndef l_model(x, base_path=base_path): return torch.load(base_path / x)\n\n\nmodels_var_old = [\n    {'var': 'TA',    'model': l_model(\"TA_specialized_gap_12-336_v2.pickle.pickle\",base_path_old )},\n    {'var': 'SW_IN', 'model': l_model(\"SW_IN_specialized_gap_12-336_v2.pickle.pickle\",base_path_old )},\n    {'var': 'LW_IN', 'model': l_model(\"LW_IN_specialized_gap_12-336_v1.pickle\",base_path_old )},\n    {'var': 'VPD',   'model': l_model(\"VPD_specialized_gap_12-336_v1.pickle\",base_path_old )},\n    {'var': 'WS',    'model': l_model(\"WS_specialized_gap_12-336_v3.pickle\",base_path_old )},\n    {'var': 'PA',    'model': l_model(\"PA_specialized_gap_12-336_v3.pickle\",base_path_old )},\n    {'var': 'P',     'model': l_model(\"P_specialized_gap_12-336_v1.pickle\",base_path_old )},\n    {'var': 'TS',    'model': l_model(\"TS_specialized_gap_12-336_v3.pickle\",base_path_old )},\n    {'var': 'SWC',   'model': l_model(\"SWC_specialized_gap_12-336_v3_4.pickle\",base_path_old )},\n]\nmodels_var_old = pd.DataFrame.from_records(models_var_old)\n\n\nmodels_var = [\n    {'var': 'TA',    'model': l_model(\"TA_specialized_gap_6-336_v3_0.pickle\",base_path)},\n    {'var': 'SW_IN', 'model': l_model(\"SW_IN_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'LW_IN', 'model': l_model(\"LW_IN_specialized_gap_6-336_v1.pickle\",base_path)},\n    {'var': 'VPD',   'model': l_model(\"VPD_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'WS',    'model': l_model(\"WS_specialized_gap_6-336_v1.pickle\",base_path)},\n    {'var': 'PA',    'model': l_model(\"PA_specialized_gap_6-336_v3_0.pickle\",base_path)},\n    {'var': 'P',     'model': l_model(\"1_gap_varying_6-336_v3.pickle\",base_path)},\n    {'var': 'TS',    'model': l_model(\"TS_specialized_gap_6-336_v2_0.pickle\",base_path)},\n    {'var': 'SWC',   'model': l_model(\"SWC_specialized_gap_6-336_v2_1.pickle\",base_path)},\n]\nmodels_var = pd.DataFrame.from_records(models_var)\n\n\n\n\nmodels_old_new = pd.concat([models_var.assign(version='new'), models_var_old.assign(version='old')])\n\n\n@cache_disk(\"new_old\")\ndef get_new_old(n_rep=n_rep):\n    \n    kcomp_control = KalmanImpComparison(models_old_new, hai, hai_era, block_len=100+48*7)\n\n    res = kcomp_control.compare(n_rep =n_rep, gap_len = [12, 24, 48, 48*7], var = list(hai.columns))\n    \n    return res\n\n\nres_old_new = get_new_old(30)\n\n\np = plot_compare(res_old_new, 'version', scale_domain=[\"new\", \"old\"])\nsave_plot(p, \"old_new\")\np\n\n\n\n\n\n\n\nfrom functools import partial\n\n\ntable_compare(res_old_new, 'version')\n\n\n\n\n\n\n\n\nversion\nold\nnew\n\n\n\n\nRMSE Standardized\nmean\nstd\nmean\nstd\ndiff.\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\nTA\n6.0\n0.065378\n0.034118\n0.053102\n0.030440\n-0.012275\n\n\n12.0\n0.089724\n0.049703\n0.081287\n0.048136\n-0.008436\n\n\n24.0\n0.127353\n0.083480\n0.116559\n0.071883\n-0.010795\n\n\n168.0\n0.138959\n0.060207\n0.135035\n0.058048\n-0.003924\n\n\nSW_IN\n6.0\n0.219956\n0.203877\n0.203373\n0.173751\n-0.016584\n\n\n12.0\n0.275117\n0.177155\n0.263598\n0.155561\n-0.011519\n\n\n24.0\n0.268877\n0.138038\n0.268416\n0.137251\n-0.000461\n\n\n168.0\n0.264838\n0.088349\n0.264030\n0.084793\n-0.000808\n\n\nLW_IN\n6.0\n0.280027\n0.193981\n0.280712\n0.203033\n0.000684\n\n\n12.0\n0.321675\n0.185618\n0.334790\n0.195499\n0.013116\n\n\n24.0\n0.365977\n0.274446\n0.356476\n0.234537\n-0.009501\n\n\n168.0\n0.440799\n0.172347\n0.432702\n0.166955\n-0.008097\n\n\nVPD\n6.0\n0.163775\n0.140889\n0.119206\n0.132791\n-0.044569\n\n\n12.0\n0.218189\n0.166097\n0.182499\n0.127480\n-0.035690\n\n\n24.0\n0.226380\n0.126265\n0.209695\n0.106824\n-0.016685\n\n\n168.0\n0.254821\n0.132731\n0.242704\n0.132320\n-0.012116\n\n\nWS\n6.0\n0.386589\n0.199048\n0.407558\n0.202452\n0.020969\n\n\n12.0\n0.417816\n0.125848\n0.418406\n0.129669\n0.000591\n\n\n24.0\n0.542897\n0.315976\n0.564967\n0.341026\n0.022070\n\n\n168.0\n0.545839\n0.248422\n0.554766\n0.262952\n0.008927\n\n\nPA\n6.0\n0.061107\n0.062039\n0.057638\n0.044863\n-0.003469\n\n\n12.0\n0.072143\n0.048526\n0.073675\n0.043362\n0.001532\n\n\n24.0\n0.077781\n0.064367\n0.086437\n0.069553\n0.008656\n\n\n168.0\n0.073142\n0.020995\n0.075264\n0.025069\n0.002122\n\n\nP\n6.0\n0.394970\n0.500444\n0.471042\n0.503129\n0.076072\n\n\n12.0\n0.551082\n1.226294\n0.802319\n2.101092\n0.251237\n\n\n24.0\n0.449901\n0.450820\n0.613983\n0.438106\n0.164082\n\n\n168.0\n0.815211\n0.774186\n1.007419\n0.723227\n0.192207\n\n\nSWC\n6.0\n0.064453\n0.042231\n0.082461\n0.054295\n0.018008\n\n\n12.0\n0.050170\n0.021756\n0.086962\n0.042916\n0.036791\n\n\n24.0\n0.097666\n0.074399\n0.141850\n0.087580\n0.044185\n\n\n168.0\n0.157057\n0.088799\n0.505197\n0.238485\n0.348140\n\n\nTS\n6.0\n0.075355\n0.071843\n0.088789\n0.083617\n0.013433\n\n\n12.0\n0.093737\n0.064550\n0.132530\n0.097411\n0.038794\n\n\n24.0\n0.161774\n0.131750\n0.197921\n0.104006\n0.036147\n\n\n168.0\n0.230206\n0.086792\n0.286576\n0.116493\n0.056370\n\n\n\n\n\n\n\nThe gap is a only in the variable that is gap-filled\n\nhai.columns\n\nIndex(['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], dtype='object')\n\n\n\n@cache_disk(cache_dir / \"the_results\")\ndef get_the_results(n_rep=20):\n    comp_Av = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 446, time_series=False)\n    results_Av = comp_Av.compare(gap_len = [12,24, 48, 336], var=list(hai.columns), n_rep=n_rep) \n    return results_Av\n\nresults_Av = get_the_results(n_rep)\n\n\n\n\n\nall_res = results_Av.query('var != \"P\"').groupby(['method']).agg({'rmse_stand': 'mean'}).T\n\n\nall_res\n\n\n\n\n\n\n\nmethod\nKalmanFilter\nERA\nMDS\n\n\n\n\nrmse_stand\n0.204628\n0.307361\n0.482837\n\n\n\n\n\n\n\npercentage of improvement across all variables\n\n(all_res[\"ERA\"] - all_res[\"KalmanFilter\"]) / all_res[\"ERA\"] * 100 \n\nrmse_stand    33.42398\ndtype: float64\n\n\n\n(all_res[\"MDS\"] - all_res[\"KalmanFilter\"]) / all_res[\"MDS\"] * 100 \n\nrmse_stand    57.619542\ndtype: float64\n\n\n\nres_var = results_Av.groupby(['method', 'var']).agg({'rmse_stand': 'mean'}) \n\n\nres_var = res_var.reset_index().pivot(columns='method', values='rmse_stand', index='var')\n\n\npd.DataFrame({'ERA': (res_var[\"ERA\"] - res_var[\"KalmanFilter\"]) / res_var[\"ERA\"] * 100, 'MDS': (res_var[\"MDS\"] - res_var[\"KalmanFilter\"]) / res_var[\"MDS\"] * 100 })\n\n\n\n\n\n\n\n\nERA\nMDS\n\n\nvar\n\n\n\n\n\n\nTA\n54.540802\n77.713711\n\n\nSW_IN\n12.004508\n35.516142\n\n\nLW_IN\n5.166063\n52.289627\n\n\nVPD\n44.402821\n65.407769\n\n\nWS\n21.064305\n40.321732\n\n\nPA\n28.784191\n90.751559\n\n\nP\n-18.544370\n-22.084360\n\n\nSWC\nNaN\n41.543006\n\n\nTS\nNaN\n25.772326\n\n\n\n\n\n\n\n\nres_var2 = results_Av.groupby(['method', 'var', 'gap_len']).agg({'rmse_stand': 'mean'}) \n\n\nres_var2 = res_var2.reset_index().pivot(columns='method', values='rmse_stand', index=['var', 'gap_len'])\n\n\npd.DataFrame({'ERA': (res_var2[\"ERA\"] - res_var2[\"KalmanFilter\"]) / res_var2[\"ERA\"] * 100, 'MDS': (res_var2[\"MDS\"] - res_var2[\"KalmanFilter\"]) / res_var2[\"MDS\"] * 100 })\n\n\n\n\n\n\n\n\n\nERA\nMDS\n\n\nvar\ngap_len\n\n\n\n\n\n\nTA\n6.0\n69.897582\n85.052698\n\n\n12.0\n58.766166\n79.376385\n\n\n24.0\n51.538443\n75.395970\n\n\n168.0\n41.823614\n73.000401\n\n\nSW_IN\n6.0\n9.519984\n29.746651\n\n\n12.0\n11.165399\n30.639223\n\n\n24.0\n14.232051\n34.811941\n\n\n168.0\n12.305658\n42.651906\n\n\nLW_IN\n6.0\n21.023524\n59.136518\n\n\n12.0\n9.110040\n52.211404\n\n\n24.0\n-3.553292\n50.720632\n\n\n168.0\n-4.260023\n48.223005\n\n\nVPD\n6.0\n66.980942\n79.449579\n\n\n12.0\n47.785633\n69.081018\n\n\n24.0\n33.663749\n56.728120\n\n\n168.0\n32.272332\n57.702579\n\n\nWS\n6.0\n32.402977\n45.724043\n\n\n12.0\n25.209162\n43.275430\n\n\n24.0\n15.543672\n37.142502\n\n\n168.0\n12.735569\n36.436106\n\n\nPA\n6.0\n39.823585\n91.511486\n\n\n12.0\n30.995845\n90.532461\n\n\n24.0\n24.727301\n89.319180\n\n\n168.0\n20.691181\n91.421434\n\n\nP\n6.0\n-18.485009\n-13.917879\n\n\n12.0\n-28.935358\n-37.127331\n\n\n24.0\n-24.423076\n-29.998707\n\n\n168.0\n-7.725322\n-11.587796\n\n\nSWC\n6.0\nNaN\n61.302664\n\n\n12.0\nNaN\n47.976950\n\n\n24.0\nNaN\n42.535719\n\n\n168.0\nNaN\n23.301469\n\n\nTS\n6.0\nNaN\n64.264901\n\n\n12.0\nNaN\n46.699870\n\n\n24.0\nNaN\n27.050291\n\n\n168.0\nNaN\n-15.268479\n\n\n\n\n\n\n\n\n\n\nvar\nTA       77.713711\nSW_IN    35.516142\nLW_IN    52.289627\nVPD      65.407769\nWS       40.321732\nPA       90.751559\nP       -22.084360\nSWC      41.543006\nTS       25.772326\ndtype: float64\n\n\n\n\n\n\nfrom itertools import product\nimport altair as alt\n\n\np = the_plot(results_Av)\nsave_plot(p, \"the_plot\")\nshow_plot(\"the_plot\")\n\n\n\n\n\np = the_plot_stand(results_Av)\nsave_plot(p, \"the_plot_stand\")\nshow_plot(\"the_plot_stand\")\n\n\n\n\n\np = the_plot_stand2(results_Av)\nsave_plot(p, \"the_plot_stand2\")\nshow_plot(\"the_plot_stand2\")\n\n\n\n\n\np = the_plot_stand3(results_Av)\nsave_plot(p, \"the_plot_stand3\")\nshow_plot(\"the_plot_stand3\")\n\n\n\n\n\n\n\n\nt = the_table(results_Av)\nthe_table_latex(t, base_path_tbl / \"the_table.tex\", label=\"the_table\",\n                caption=\"RMSE Comparison imputation methods. The best method for each gap length is highligthed in bold\")\nt\n\n\n\n\n\n\n\n\n\nKalmanFilter\nERA\nMDS\n\n\n\nRMSE\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\n\nTA\n6.0\n0.405453\n0.258301\n1.346910\n0.997843\n2.712546\n1.896914\n\n\n12.0\n0.606836\n0.400849\n1.471695\n0.900611\n2.942435\n1.748131\n\n\n24.0\n0.741275\n0.368468\n1.529614\n0.800256\n3.012819\n1.611311\n\n\n168.0\n1.020608\n0.444591\n1.754334\n0.643160\n3.780087\n1.315472\n\n\nSW_IN\n6.0\n44.636609\n40.464629\n49.333113\n66.241975\n63.536627\n85.401585\n\n\n12.0\n48.155186\n33.868178\n54.207691\n49.769296\n69.427115\n68.936352\n\n\n24.0\n56.564277\n30.042752\n65.950367\n40.930505\n86.770917\n59.603564\n\n\n168.0\n61.582820\n25.740161\n70.224393\n34.883199\n107.384249\n53.606111\n\n\nLW_IN\n6.0\n10.902409\n7.736087\n13.804628\n12.987987\n26.680077\n15.022366\n\n\n12.0\n13.421656\n7.734502\n14.766929\n12.584725\n28.085478\n13.457335\n\n\n24.0\n14.593819\n7.840046\n14.093052\n12.227900\n29.614461\n12.416763\n\n\n168.0\n17.062880\n6.425136\n16.365697\n11.129569\n32.954558\n8.833972\n\n\nVPD\n6.0\n0.428187\n0.363168\n1.296787\n1.547397\n2.083592\n2.149288\n\n\n12.0\n0.660623\n0.504761\n1.265213\n1.288794\n2.136626\n2.095549\n\n\n24.0\n0.827563\n0.501975\n1.247527\n1.032319\n1.912472\n1.605013\n\n\n168.0\n1.125680\n0.633392\n1.662069\n1.127314\n2.661345\n1.965431\n\n\nWS\n6.0\n0.616774\n0.316972\n0.912428\n0.508295\n1.136367\n0.783146\n\n\n12.0\n0.715412\n0.350974\n0.956550\n0.524247\n1.261203\n0.796744\n\n\n24.0\n0.801851\n0.343378\n0.949427\n0.446912\n1.275665\n0.608630\n\n\n168.0\n0.950211\n0.363124\n1.088887\n0.348541\n1.494891\n0.615371\n\n\nPA\n6.0\n0.045046\n0.034294\n0.074856\n0.061726\n0.530665\n0.441476\n\n\n12.0\n0.053359\n0.041613\n0.077328\n0.058476\n0.563603\n0.427426\n\n\n24.0\n0.059481\n0.038666\n0.079021\n0.051491\n0.556899\n0.404451\n\n\n168.0\n0.066325\n0.047544\n0.083628\n0.053654\n0.773143\n0.384029\n\n\nP\n6.0\n0.134093\n0.274033\n0.113173\n0.315504\n0.117710\n0.305539\n\n\n12.0\n0.178871\n0.295419\n0.138729\n0.297227\n0.130442\n0.281377\n\n\n24.0\n0.206231\n0.253588\n0.165750\n0.288432\n0.158641\n0.265257\n\n\n168.0\n0.239885\n0.173820\n0.222682\n0.201782\n0.214975\n0.197499\n\n\nSWC\n6.0\n0.508379\n0.487342\nNaN\nNaN\n1.313730\n1.556829\n\n\n12.0\n0.664855\n0.471849\nNaN\nNaN\n1.278001\n1.323011\n\n\n24.0\n0.779066\n0.640996\nNaN\nNaN\n1.355740\n1.472185\n\n\n168.0\n1.493784\n0.947799\nNaN\nNaN\n1.947605\n1.488284\n\n\nTS\n6.0\n0.341080\n0.431992\nNaN\nNaN\n0.954469\n0.889126\n\n\n12.0\n0.534363\n0.783787\nNaN\nNaN\n1.002555\n0.876784\n\n\n24.0\n0.786670\n0.851931\nNaN\nNaN\n1.078373\n0.856964\n\n\n168.0\n1.659875\n1.077782\nNaN\nNaN\n1.440008\n0.764040\n\n\n\n\n\n\n\n\nt = the_table(results_Av, 'rmse_stand')\nthe_table_latex(t, base_path_tbl / \"the_table_stand.tex\", stand = True, label=\"the_table_stand\", \n                caption = \"Comparison of imputation methods using Standardized RMSE. The best method for each gap length is highligthed in bold\")\nt\n\n\n\n\n\n\n\n\n\nKalmanFilter\nERA\nMDS\n\n\n\nRMSE\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\n\nTA\n6.0\n0.051164\n0.032595\n0.169965\n0.125917\n0.342294\n0.239370\n\n\n12.0\n0.076576\n0.050583\n0.185712\n0.113647\n0.371303\n0.220595\n\n\n24.0\n0.093541\n0.046497\n0.193021\n0.100984\n0.380185\n0.203330\n\n\n168.0\n0.128790\n0.056103\n0.221378\n0.081160\n0.477006\n0.165998\n\n\nSW_IN\n6.0\n0.218804\n0.198354\n0.241826\n0.324711\n0.311450\n0.418630\n\n\n12.0\n0.236052\n0.166018\n0.265721\n0.243964\n0.340325\n0.337919\n\n\n24.0\n0.277272\n0.147267\n0.323282\n0.200637\n0.425342\n0.292171\n\n\n168.0\n0.301873\n0.126176\n0.344233\n0.170994\n0.526387\n0.262772\n\n\nLW_IN\n6.0\n0.259855\n0.184387\n0.329028\n0.309564\n0.635910\n0.358053\n\n\n12.0\n0.319900\n0.184349\n0.351964\n0.299952\n0.669407\n0.320751\n\n\n24.0\n0.347838\n0.186865\n0.335903\n0.291448\n0.705850\n0.295949\n\n\n168.0\n0.406688\n0.153141\n0.390071\n0.265269\n0.785460\n0.210555\n\n\nVPD\n6.0\n0.098019\n0.083135\n0.296855\n0.354224\n0.476967\n0.492006\n\n\n12.0\n0.151227\n0.115548\n0.289627\n0.295025\n0.489108\n0.479704\n\n\n24.0\n0.189442\n0.114910\n0.285579\n0.236314\n0.437795\n0.367413\n\n\n168.0\n0.257686\n0.144994\n0.380474\n0.258060\n0.609224\n0.449918\n\n\nWS\n6.0\n0.379454\n0.195008\n0.561347\n0.312715\n0.699120\n0.481810\n\n\n12.0\n0.440138\n0.215927\n0.588492\n0.322529\n0.775922\n0.490176\n\n\n24.0\n0.493318\n0.211254\n0.584110\n0.274951\n0.784819\n0.374443\n\n\n168.0\n0.584592\n0.223403\n0.669909\n0.214431\n0.919692\n0.378591\n\n\nPA\n6.0\n0.052675\n0.040103\n0.087534\n0.072180\n0.620545\n0.516250\n\n\n12.0\n0.062397\n0.048661\n0.090425\n0.068381\n0.659061\n0.499820\n\n\n24.0\n0.069556\n0.045215\n0.092405\n0.060212\n0.651223\n0.472953\n\n\n168.0\n0.077558\n0.055597\n0.097793\n0.062741\n0.904092\n0.449073\n\n\nP\n6.0\n0.478431\n0.977725\n0.403790\n1.125691\n0.419979\n1.090136\n\n\n12.0\n0.638197\n1.054031\n0.494974\n1.060481\n0.465404\n1.003928\n\n\n24.0\n0.735816\n0.904779\n0.591382\n1.029100\n0.566018\n0.946414\n\n\n168.0\n0.855891\n0.620173\n0.794512\n0.719941\n0.767011\n0.704660\n\n\nSWC\n6.0\n0.057037\n0.054677\nNaN\nNaN\n0.147393\n0.174667\n\n\n12.0\n0.074593\n0.052939\nNaN\nNaN\n0.143384\n0.148434\n\n\n24.0\n0.087407\n0.071916\nNaN\nNaN\n0.152106\n0.165171\n\n\n168.0\n0.167594\n0.106338\nNaN\nNaN\n0.218510\n0.166977\n\n\nTS\n6.0\n0.060276\n0.076342\nNaN\nNaN\n0.168674\n0.157127\n\n\n12.0\n0.094433\n0.138512\nNaN\nNaN\n0.177172\n0.154946\n\n\n24.0\n0.139021\n0.150554\nNaN\nNaN\n0.190571\n0.151443\n\n\n168.0\n0.293335\n0.190466\nNaN\nNaN\n0.254479\n0.135022\n\n\n\n\n\n\n\n\n\n\n\n@cache_disk(cache_dir / \"the_results_ts\")\ndef get_the_results_ts():\n    comp_Av = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 446, time_series=True, rmse=False)\n    results_Av = comp_Av.compare(gap_len = [12,24, 336], var=list(hai.columns), n_rep=4) \n    return results_Av\n\nresults_ts = get_the_results_ts()\n\n\nts = plot_timeseries(results_ts.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=0)\nsave_plot(ts, \"timeseries_1\")\nshow_plot(\"timeseries_1\")\n\n\n\n\n\n%time ts = plot_timeseries(results_ts.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=0)\n%time save_plot(ts, \"timeseries_2\")\nshow_plot(\"timeseries_2\")\n\nCPU times: user 4.29 s, sys: 0 ns, total: 4.29 s\nWall time: 4.32 s\nCPU times: user 38.1 s, sys: 12.9 s, total: 51 s\nWall time: 50.4 s\n\n\n\n\n\n\nfrom tqdm.auto import tqdm\n\n\n@cache_disk(cache_dir / \"ts_plot\")\ndef plot_additional_ts():\n    for idx in tqdm(results_ts.idx_rep.unique()):\n        if idx == 0: continue # skip first plot as is done above\n        ts1 = plot_timeseries(results_ts.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=idx)\n        save_plot(ts1, f\"timeseries_1_{idx}\")\n        ts2 = plot_timeseries(results_ts.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=idx)\n        save_plot(ts2, f\"timeseries_2_{idx}\")        \n\n\nplot_additional_ts()\n\n\n\n\n\n\n\n\n@cache_disk(\"gap_len\")\ndef get_g_len(n_rep=n_rep):\n    return KalmanImpComparison(models_var, hai, hai_era, block_len=48*7+100).compare(gap_len = [2,6,12,24,48,48*2, 48*3, 48*7], var=list(hai.columns), n_rep=n_rep)\n\n\ngap_len = get_g_len(n_rep)\n\n\np = plot_gap_len(gap_len, hai, hai_era)\nsave_plot(p, \"gap_len\")\np\n\n\n\n\n\n\n\nt = table_gap_len(gap_len)\ntable_gap_len_latex(t, base_path_tbl / \"gap_len.tex\", label=\"gap_len\",\n                caption=\"RMSE Comparison Kalman filter for different gap lengths\")\nt\n\n\n\n\n\n\n\n\n\n1\n3\n6\n12\n24\n48\n72\n168\n\n\nVariable\nRMSE\n\n\n\n\n\n\n\n\n\n\n\n\nTA\nmean\n0.194354\n0.286686\n0.439898\n0.627774\n0.825657\n0.903880\n0.938702\n0.986020\n\n\nstd\n0.170168\n0.191621\n0.319131\n0.394308\n0.441307\n0.449117\n0.408243\n0.347736\n\n\nSW_IN\nmean\n27.869381\n37.736508\n44.485780\n50.089934\n57.305556\n58.099263\n58.937990\n59.044984\n\n\nstd\n40.674507\n38.361149\n38.011124\n34.547960\n29.069868\n25.844578\n26.383210\n25.259337\n\n\nLW_IN\nmean\n5.627963\n7.948118\n11.455198\n13.292480\n15.916159\n17.051384\n16.284372\n17.491457\n\n\nstd\n6.612598\n7.564646\n8.072892\n8.415339\n8.774831\n8.078440\n7.010162\n7.286120\n\n\nVPD\nmean\n0.180114\n0.319469\n0.501255\n0.688441\n0.946403\n0.957061\n1.061895\n1.104943\n\n\nstd\n0.204093\n0.358347\n0.484957\n0.608219\n0.686398\n0.624435\n0.609132\n0.602005\n\n\nWS\nmean\n0.361465\n0.494700\n0.616030\n0.770239\n0.830321\n0.905966\n0.916504\n0.914477\n\n\nstd\n0.323714\n0.301577\n0.373032\n0.435992\n0.459242\n0.397321\n0.339037\n0.300834\n\n\nPA\nmean\n0.022211\n0.033831\n0.048117\n0.052795\n0.059260\n0.067833\n0.065873\n0.069754\n\n\nstd\n0.018450\n0.022981\n0.047558\n0.030419\n0.039748\n0.063229\n0.050773\n0.050060\n\n\nP\nmean\n0.091639\n0.165085\n0.116147\n0.200376\n0.185713\n0.208122\n0.215432\n0.238309\n\n\nstd\n0.273413\n0.538698\n0.157560\n0.343884\n0.203738\n0.190703\n0.186401\n0.150946\n\n\nSWC\nmean\n0.186740\n0.332085\n0.452526\n0.617151\n0.751877\n0.909684\n0.959049\n1.460350\n\n\nstd\n0.208227\n0.293192\n0.294969\n0.432676\n0.579932\n0.666566\n0.676641\n0.900856\n\n\nTS\nmean\n0.151836\n0.231586\n0.381999\n0.590929\n0.676975\n0.994570\n1.357970\n1.663379\n\n\nstd\n0.154047\n0.206538\n0.427692\n0.828540\n0.589255\n0.815527\n1.027602\n1.059882\n\n\n\n\n\n\n\n\ng_len_agg = gap_len.groupby('gap_len').agg({'rmse_stand': 'mean'})\n(g_len_agg.iloc[0])/g_len_agg.iloc[-1]\n\nrmse_stand    0.316083\ndtype: float64\n\n\n\ng_len_agg = gap_len.groupby(['gap_len', 'var']).agg({'rmse_stand': 'mean'})\n(g_len_agg.loc[1.])/g_len_agg.loc[168.]\n\n\n\n\n\n\n\n\nrmse_stand\n\n\nvar\n\n\n\n\n\nTA\n0.197110\n\n\nSW_IN\n0.472003\n\n\nLW_IN\n0.321755\n\n\nVPD\n0.163008\n\n\nWS\n0.395269\n\n\nPA\n0.318417\n\n\nP\n0.384538\n\n\nSWC\n0.127873\n\n\nTS\n0.091282\n\n\n\n\n\n\n\n\ng_len_agg\n\n\n\n\n\n\n\n\n\nrmse_stand\n\n\ngap_len\nvar\n\n\n\n\n\n1.0\nTA\n0.024525\n\n\nSW_IN\n0.136613\n\n\nLW_IN\n0.134140\n\n\nVPD\n0.041231\n\n\nWS\n0.222382\n\n\n...\n...\n...\n\n\n168.0\nWS\n0.562608\n\n\nPA\n0.081568\n\n\nP\n0.850266\n\n\nSWC\n0.163843\n\n\nTS\n0.293954\n\n\n\n\n72 rows × 1 columns\n\n\n\n\ng_len_agg_std = gap_len.groupby('gap_len').agg({'rmse_stand': 'std'})\n(g_len_agg_std.iloc[0])/g_len_agg_std.iloc[-1]\n\nrmse_stand    1.125078\ndtype: float64\n\n\n\ngap_len.groupby(['gap_len', 'var']).agg({'rmse_stand': 'std'}).unstack(\"var\").drop(columns=('rmse_stand', \"P\")).plot()\n\n&lt;AxesSubplot: xlabel='gap_len'&gt;\n\n\n\n\n\n\n# with open(base_path_tbl / \"gap_len.tex\") as f:\n    # print(f.readlines())\n\n\n\n\n\nmodels_nc = pd.DataFrame({'model': [ l_model(\"1_gap_varying_336_no_control_v1.pickle\"), l_model(\"1_gap_varying_6-336_v3.pickle\")],\n                          'type':   [ 'No Control',                                       'Use Control'                         ]})                                        \n\n\n@cache_disk(\"use_control\")\ndef get_control(n_rep=n_rep):\n    \n    kcomp_control = KalmanImpComparison(models_nc, hai, hai_era, block_len=100+48*7)\n\n    k_results_control = kcomp_control.compare(n_rep =n_rep, gap_len = [12, 24, 48, 48*7], var = list(hai.columns))\n    \n    return k_results_control\n\n\nfrom time import sleep\n\n\nk_results_control = get_control(n_rep)\n\n\nk_results_control\n\n\n\n\n\n\n\n\nvar\nloss\nlikelihood\ngap_len\ngap_idx\ntype\nrmse\nrmse_stand\n\n\n\n\n0\nTA\n-5.742477\n2.142377\n6.0\n0\nNo Control\n0.227680\n0.028731\n\n\n1\nTA\n-5.755538\n2.148783\n6.0\n1\nNo Control\n0.228037\n0.028776\n\n\n2\nTA\n-1.232816\n2.132641\n6.0\n2\nNo Control\n1.813357\n0.228826\n\n\n3\nTA\n-4.693027\n2.118098\n6.0\n3\nNo Control\n0.906512\n0.114392\n\n\n4\nTA\n-5.179578\n2.151546\n6.0\n4\nNo Control\n0.726325\n0.091654\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\nTS\n115.193441\n0.966260\n168.0\n495\nUse Control\n1.770984\n0.312970\n\n\n996\nTS\n82.913345\n1.474181\n168.0\n496\nUse Control\n1.403469\n0.248022\n\n\n997\nTS\n93.694508\n1.154413\n168.0\n497\nUse Control\n1.526679\n0.269796\n\n\n998\nTS\n118.712986\n1.494637\n168.0\n498\nUse Control\n1.798757\n0.317878\n\n\n999\nTS\n93.694508\n1.154413\n168.0\n499\nUse Control\n1.526679\n0.269796\n\n\n\n\n36000 rows × 8 columns\n\n\n\n\np = plot_compare(k_results_control, 'type', scale_domain=[\"Use Control\", \"No Control\"])\nsave_plot(p, \"use_control\")\np\n\n\n\n\n\n\n\nfrom functools import partial\n\n\nt = table_compare(k_results_control, 'type')\ntable_compare_latex(t, base_path_tbl / \"control.tex\", label=\"control\",\n                caption=\"Comparison between generic model with control variable (Use Control) and generic model without control variable (No Control). 50 samples for each variable and each gap length. The best result for each for each gap length is highligthed in bold\")\nt\n\n\n\n\n\n\n\n\ntype\nUse Control\nNo Control\n\n\n\n\nRMSE Standardized\nmean\nstd\nmean\nstd\ndiff.\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\nTA\n6.0\n0.092151\n0.055814\n0.094594\n0.059489\n0.002443\n\n\n12.0\n0.118358\n0.074072\n0.160712\n0.106870\n0.042354\n\n\n24.0\n0.146578\n0.076799\n0.232744\n0.159137\n0.086166\n\n\n168.0\n0.174324\n0.064495\n0.294072\n0.147849\n0.119748\n\n\nSW_IN\n6.0\n0.255795\n0.164694\n0.331166\n0.270628\n0.075371\n\n\n12.0\n0.314755\n0.164839\n0.518890\n0.319771\n0.204135\n\n\n24.0\n0.339310\n0.128516\n0.608981\n0.277150\n0.269671\n\n\n168.0\n0.357922\n0.103167\n0.693166\n0.248412\n0.335244\n\n\nLW_IN\n6.0\n0.299442\n0.201432\n0.330486\n0.185322\n0.031044\n\n\n12.0\n0.367537\n0.197000\n0.485509\n0.193452\n0.117972\n\n\n24.0\n0.400412\n0.175611\n0.561446\n0.186706\n0.161035\n\n\n168.0\n0.466036\n0.141424\n0.666359\n0.161485\n0.200323\n\n\nVPD\n6.0\n0.137863\n0.104531\n0.178626\n0.139476\n0.040762\n\n\n12.0\n0.226616\n0.183575\n0.320797\n0.262730\n0.094181\n\n\n24.0\n0.272840\n0.206783\n0.407586\n0.296989\n0.134746\n\n\n168.0\n0.325113\n0.154287\n0.499408\n0.252086\n0.174294\n\n\nWS\n6.0\n0.571539\n0.523304\n0.429625\n0.262009\n-0.141914\n\n\n12.0\n0.660302\n0.408751\n0.680223\n0.391765\n0.019921\n\n\n24.0\n0.676472\n0.374177\n0.767313\n0.430367\n0.090841\n\n\n168.0\n0.781636\n0.264823\n0.888977\n0.318094\n0.107340\n\n\nPA\n6.0\n0.099325\n0.066109\n0.144519\n0.105120\n0.045194\n\n\n12.0\n0.116360\n0.061026\n0.338753\n0.209793\n0.222393\n\n\n24.0\n0.136620\n0.076087\n0.562996\n0.378688\n0.426376\n\n\n168.0\n0.158988\n0.068315\n0.879748\n0.439831\n0.720760\n\n\nP\n6.0\n0.542198\n0.990843\n0.507613\n0.973715\n-0.034584\n\n\n12.0\n0.627167\n1.034936\n0.605429\n1.025064\n-0.021738\n\n\n24.0\n0.628179\n0.575428\n0.614481\n0.583966\n-0.013698\n\n\n168.0\n0.880507\n0.572737\n0.860985\n0.597007\n-0.019522\n\n\nSWC\n6.0\n0.152144\n0.107350\n0.151053\n0.141370\n-0.001092\n\n\n12.0\n0.244768\n0.154341\n0.305445\n0.233699\n0.060678\n\n\n24.0\n0.392553\n0.222757\n0.538003\n0.337738\n0.145450\n\n\n168.0\n0.719138\n0.314318\n0.842404\n0.431371\n0.123267\n\n\nTS\n6.0\n0.168111\n0.114636\n0.118928\n0.076226\n-0.049183\n\n\n12.0\n0.226302\n0.134730\n0.203284\n0.132946\n-0.023019\n\n\n24.0\n0.286258\n0.152198\n0.285367\n0.174358\n-0.000891\n\n\n168.0\n0.362279\n0.176402\n0.359044\n0.191313\n-0.003235\n\n\n\n\n\n\n\n\n\n\n\nmodels_gap_single = pd.DataFrame.from_records([\n    {'Gap':  'All variables', 'gap_single_var': False, 'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")},\n    {'Gap':  'Only one var',  'gap_single_var': True,  'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")},\n])\n\n\n@cache_disk(\"gap_single\")\ndef get_gap_single(n_rep):\n    kcomp_single = KalmanImpComparison(models_gap_single, hai, hai_era, block_len=130)\n\n    return kcomp_single.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nres_single = get_gap_single(n_rep)\n\n\np = plot_compare(res_single, \"Gap\", scale_domain=[\"Only one var\", \"All variables\"])\nsave_plot(p, \"gap_single_var\")\nshow_plot(\"gap_single_var\")\n\n\n\n\n\nt = table_compare(res_single, 'Gap')\ntable_compare_latex(t, base_path_tbl / \"gap_single_var.tex\")\nt\n\n\n\n\n\n\n\n\nGap\nOnly one var\nAll variables\n\n\n\n\nRMSE Standardized\nmean\nstd\nmean\nstd\ndiff.\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\nTA\n3.0\n0.028752\n0.023775\n0.048794\n0.047658\n0.020042\n\n\n6.0\n0.039721\n0.028151\n0.078262\n0.052441\n0.038541\n\n\n12.0\n0.073367\n0.049696\n0.128982\n0.082850\n0.055615\n\n\n15.0\n0.079064\n0.056618\n0.137702\n0.091974\n0.058638\n\n\nSW_IN\n3.0\n0.199067\n0.204647\n0.202038\n0.243918\n0.002970\n\n\n6.0\n0.221603\n0.171424\n0.227588\n0.215318\n0.005985\n\n\n12.0\n0.259870\n0.167001\n0.284290\n0.221368\n0.024420\n\n\n15.0\n0.276890\n0.165700\n0.306884\n0.216898\n0.029994\n\n\nLW_IN\n3.0\n0.193706\n0.169206\n0.187165\n0.174342\n-0.006541\n\n\n6.0\n0.276217\n0.215933\n0.287791\n0.233930\n0.011574\n\n\n12.0\n0.325436\n0.197345\n0.327702\n0.223464\n0.002266\n\n\n15.0\n0.336995\n0.187926\n0.344642\n0.211682\n0.007647\n\n\nVPD\n3.0\n0.069788\n0.070253\n0.104548\n0.105131\n0.034761\n\n\n6.0\n0.102529\n0.096734\n0.161396\n0.140558\n0.058866\n\n\n12.0\n0.155312\n0.115690\n0.233406\n0.192941\n0.078093\n\n\n15.0\n0.182637\n0.133815\n0.285944\n0.227135\n0.103307\n\n\nWS\n3.0\n0.296947\n0.179404\n0.300916\n0.181068\n0.003969\n\n\n6.0\n0.365649\n0.202113\n0.371086\n0.211384\n0.005437\n\n\n12.0\n0.459507\n0.242230\n0.468449\n0.248098\n0.008941\n\n\n15.0\n0.481226\n0.229505\n0.493956\n0.249458\n0.012730\n\n\nPA\n3.0\n0.024047\n0.016437\n0.028570\n0.020405\n0.004523\n\n\n6.0\n0.035908\n0.027415\n0.040551\n0.032281\n0.004643\n\n\n12.0\n0.056931\n0.073786\n0.063504\n0.090716\n0.006573\n\n\n15.0\n0.060922\n0.090392\n0.063848\n0.099443\n0.002926\n\n\nP\n3.0\n0.289715\n0.449747\n0.282935\n0.460265\n-0.006780\n\n\n6.0\n0.386047\n0.524038\n0.395858\n0.592313\n0.009811\n\n\n12.0\n0.389821\n0.550310\n0.393073\n0.640380\n0.003252\n\n\n15.0\n0.441714\n0.675003\n0.449441\n0.767405\n0.007728\n\n\nSWC\n3.0\n0.012841\n0.032253\n0.013246\n0.032911\n0.000405\n\n\n6.0\n0.017308\n0.025438\n0.021633\n0.029543\n0.004325\n\n\n12.0\n0.025562\n0.034110\n0.031826\n0.035939\n0.006264\n\n\n15.0\n0.033616\n0.040491\n0.040751\n0.052965\n0.007135\n\n\nTS\n3.0\n0.023794\n0.022665\n0.030596\n0.036472\n0.006803\n\n\n6.0\n0.046397\n0.044287\n0.065120\n0.054703\n0.018723\n\n\n12.0\n0.082995\n0.087660\n0.105948\n0.111548\n0.022952\n\n\n15.0\n0.113905\n0.113941\n0.137897\n0.137050\n0.023993\n\n\n\n\n\n\n\n\nres_singl_perc = res_single.groupby(['Gap', 'var', 'gap_len']).agg({'rmse_stand': 'mean'}).reset_index().pivot(columns = 'Gap', values='rmse_stand', index=['var', 'gap_len'])\n\n\npd.DataFrame({'Only one var': (res_singl_perc[\"All variables\"] - res_singl_perc[\"Only one var\"]) / res_singl_perc[\"All variables\"] * 100})\n\n\n\n\n\n\n\n\n\nOnly one var\n\n\nvar\ngap_len\n\n\n\n\n\nTA\n3.0\n41.074754\n\n\n6.0\n49.246103\n\n\n12.0\n43.118382\n\n\n15.0\n42.583240\n\n\nSW_IN\n3.0\n1.470098\n\n\n6.0\n2.629962\n\n\n12.0\n8.589888\n\n\n15.0\n9.773784\n\n\nLW_IN\n3.0\n-3.494816\n\n\n6.0\n4.021548\n\n\n12.0\n0.691594\n\n\n15.0\n2.218951\n\n\nVPD\n3.0\n33.248307\n\n\n6.0\n36.473299\n\n\n12.0\n33.458173\n\n\n15.0\n36.128386\n\n\nWS\n3.0\n1.318961\n\n\n6.0\n1.465055\n\n\n12.0\n1.908674\n\n\n15.0\n2.577221\n\n\nPA\n3.0\n15.830854\n\n\n6.0\n11.450242\n\n\n12.0\n10.350316\n\n\n15.0\n4.583045\n\n\nP\n3.0\n-2.396308\n\n\n6.0\n2.478452\n\n\n12.0\n0.827264\n\n\n15.0\n1.719407\n\n\nSWC\n3.0\n3.057701\n\n\n6.0\n19.993345\n\n\n12.0\n19.682335\n\n\n15.0\n17.509377\n\n\nTS\n3.0\n22.233453\n\n\n6.0\n28.751278\n\n\n12.0\n21.663945\n\n\n15.0\n17.398938\n\n\n\n\n\n\n\n\nres_singl_perc = res_single.groupby(['Gap', 'var']).agg({'rmse_stand': 'mean'}).reset_index().pivot(columns = 'Gap', values='rmse_stand', index=['var'])\n\n\npd.DataFrame({'Only one var': (res_singl_perc[\"All variables\"] - res_singl_perc[\"Only one var\"]) / res_singl_perc[\"All variables\"] * 100})\n\n\n\n\n\n\n\n\nOnly one var\n\n\nvar\n\n\n\n\n\nTA\n43.895946\n\n\nSW_IN\n6.207878\n\n\nLW_IN\n1.302744\n\n\nVPD\n35.022198\n\n\nWS\n1.901429\n\n\nPA\n9.500090\n\n\nP\n0.920959\n\n\nSWC\n16.871540\n\n\nTS\n21.342404\n\n\n\n\n\n\n\n\n\n\nmodels_gap_single = pd.DataFrame.from_records([\n    {'Gap':  'All variables',            'gap_single_var': False,  'model': l_model(\"All_gap_all_30_v1.pickle\")  },\n    {'Gap':  'Only one var | generic',   'gap_single_var': True,   'model': l_model(\"1_gap_varying_6-336_v3.pickle\")  },\n    {'Gap':  'Only one var | fine tuned','gap_single_var': True,   'model': l_model(\"1_gap_varying_tuned_6-30_v1.pickle\")  }\n])\n\n\nmodels_gap_single\n\n\n\n\n\n\n\n\nGap\ngap_single_var\nmodel\n\n\n\n\n0\nAll variables\nFalse\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n1\nOnly one var | generic\nTrue\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n2\nOnly one var | fine tuned\nTrue\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n\n\n\n\n\n\n@cache_disk(\"multi_gap\")\ndef get_multi_gap(n_rep):\n    kcomp_single = KalmanImpComparison(models_gap_single, hai, hai_era, block_len=130)\n\n    return kcomp_single.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nk_results_single = get_multi_gap(n_rep)\n\n\np = plot_compare(k_results_single, \"Gap\", scale_domain=[\"Only one var | fine tuned\", \"All variables\", \"Only one var | generic\"])\nsave_plot(p, \"gap_single_var\")\np\n\n\n\n\n\n\n\nt = table_gap_single(k_results_single, 'Gap')\n\n\n\n\n\n\n\n\nGap\nAll variables\nOnly one var | fine tuned\nOnly one var | generic\n\n\n\nRMSE Standardized\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\n\nTA\n3.0\n0.049142\n0.039257\n0.045595\n0.035802\n0.061059\n0.043398\n\n\n6.0\n0.085800\n0.056598\n0.067081\n0.052359\n0.090001\n0.060024\n\n\n12.0\n0.129696\n0.090364\n0.098341\n0.056265\n0.121936\n0.068288\n\n\n15.0\n0.153630\n0.095850\n0.118438\n0.082591\n0.134282\n0.075872\n\n\nSW_IN\n3.0\n0.206717\n0.245049\n0.221908\n0.201797\n0.219821\n0.205589\n\n\n6.0\n0.257691\n0.251566\n0.264143\n0.171401\n0.268079\n0.180135\n\n\n12.0\n0.288366\n0.223825\n0.297092\n0.158044\n0.308926\n0.170064\n\n\n15.0\n0.309123\n0.209180\n0.303225\n0.146023\n0.311964\n0.156975\n\n\nLW_IN\n3.0\n0.212700\n0.194965\n0.220202\n0.173989\n0.216754\n0.174078\n\n\n6.0\n0.295903\n0.207946\n0.302787\n0.199732\n0.303669\n0.199189\n\n\n12.0\n0.329140\n0.229067\n0.372643\n0.209362\n0.366244\n0.219349\n\n\n15.0\n0.368030\n0.252221\n0.378528\n0.193469\n0.377619\n0.208391\n\n\nVPD\n3.0\n0.090785\n0.085969\n0.094815\n0.070971\n0.091829\n0.065940\n\n\n6.0\n0.164443\n0.178822\n0.165489\n0.127027\n0.158179\n0.134617\n\n\n12.0\n0.238165\n0.210442\n0.227116\n0.147755\n0.206106\n0.145080\n\n\n15.0\n0.265751\n0.194125\n0.254020\n0.154401\n0.225379\n0.139372\n\n\nWS\n3.0\n0.311401\n0.180608\n0.368460\n0.233485\n0.453697\n0.439167\n\n\n6.0\n0.402319\n0.222570\n0.470035\n0.280695\n0.580315\n0.482407\n\n\n12.0\n0.477512\n0.248510\n0.563454\n0.274841\n0.653231\n0.406969\n\n\n15.0\n0.492510\n0.248111\n0.557210\n0.284555\n0.633615\n0.417017\n\n\nPA\n3.0\n0.035727\n0.025997\n0.049086\n0.042177\n0.067908\n0.048931\n\n\n6.0\n0.050235\n0.033742\n0.069733\n0.049907\n0.095048\n0.060527\n\n\n12.0\n0.069240\n0.089894\n0.099269\n0.073622\n0.120894\n0.091509\n\n\n15.0\n0.069967\n0.049172\n0.103711\n0.064688\n0.124702\n0.070197\n\n\nP\n3.0\n0.356274\n0.560915\n0.356234\n0.579094\n0.330790\n0.604417\n\n\n6.0\n0.487482\n0.699952\n0.577830\n0.717142\n0.529142\n0.776792\n\n\n12.0\n0.476344\n0.871444\n0.623294\n0.828331\n0.558888\n0.865192\n\n\n15.0\n0.574291\n1.002602\n0.727980\n0.983861\n0.685553\n1.031111\n\n\nSWC\n3.0\n0.016651\n0.032101\n0.049046\n0.042418\n0.090200\n0.066280\n\n\n6.0\n0.027773\n0.042202\n0.071308\n0.054779\n0.141239\n0.100214\n\n\n12.0\n0.043361\n0.042561\n0.115449\n0.077651\n0.233595\n0.147693\n\n\n15.0\n0.049897\n0.060443\n0.136793\n0.104069\n0.279767\n0.191376\n\n\nTS\n3.0\n0.047393\n0.044903\n0.075194\n0.062338\n0.108125\n0.089884\n\n\n6.0\n0.084594\n0.067924\n0.111484\n0.085914\n0.159213\n0.116278\n\n\n12.0\n0.163922\n0.120530\n0.181564\n0.118314\n0.235643\n0.158650\n\n\n15.0\n0.187739\n0.118778\n0.214730\n0.132328\n0.263299\n0.165076\n\n\n\n\n\n\n\n\n\n\n\n# @cache_disk(cache_dir / \"gap_single_ts\")\ndef get_gap_single_ts():\n    \n    kcomp_single_ts = KalmanImpComparison(models_gap_single, hai, hai_era, block_len=130, rmse=False, time_series=True)\n\n    return kcomp_single_ts.compare(n_rep =4, gap_len = [12, 24, 30], var = list(hai.columns))\n    \ng_single_ts = get_gap_single_ts()\n\n\n\n\n\ng_single_ts.columns\n\nIndex(['var', 'loss', 'likelihood', 'gap_len', 'gap_idx', 'Gap',\n       'gap_single_var', 'pred', 'targ'],\n      dtype='object')\n\n\n\nplot_timeseries(g_single_ts.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=0, \n                scale_color=alt.Scale(scheme='accent', domain =[\"Only one var | fine tuned\", \"All variables\", \"Only one var | generic\"]), ctx_len={6.: 50, 12.: 50, 15.: 50}, compare='Gap')\n\n\n\n\n\n\n\nplot_timeseries(g_single_ts.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=0, \n                scale_color=alt.Scale(scheme='accent', domain =[\"Only one var | fine tuned\", \"All variables\", \"Only one var | generic\"]), ctx_len={6.: 50, 12.: 50, 15.: 50}, compare='Gap')\n\n\n\n\n\n\n\n\n\nhere I am testing by using the model tarined with gaps in only one variables on gaps of all variables\n\nmodels_gap_single2 = pd.DataFrame.from_records([\n    {'Gap':  'all var test | all var train',  'gap_single_var': False,  'model': l_model(\"All_gap_all_30_v1.pickle\")  },\n    {'Gap':  'all var test | one var train',  'gap_single_var': False,  'model': l_model(\"1_gap_varying_tuned_6-30_v1.pickle\")  },\n    {'Gap':  'one var test | one var train',  'gap_single_var': True,   'model': l_model(\"1_gap_varying_tuned_6-30_v1.pickle\")  }\n])\n\n\nmodels_gap_single2\n\n\n\n\n\n\n\n\nGap\ngap_single_var\nmodel\n\n\n\n\n0\nall var test | all var train\nFalse\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n1\nall var test | one var train\nFalse\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n2\none var test | one var train\nTrue\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n\n\n\n\n\n\n@cache_disk(\"multi_gap2\")\ndef get_multi_gap2(n_rep):\n    kcomp_single = KalmanImpComparison(models_gap_single2, hai, hai_era, block_len=130)\n\n    return kcomp_single.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nk_results_single2 = get_multi_gap2(25)\n\n\n\n\n\nscale2 = [\"all var test | one var train\", \"one var test | one var train\", \"all var test | all var train\"]\n\n\nplot_compare(k_results_single2, \"Gap\", scale_domain=scale2)\n\n\n\n\n\n\n\ndef get_gap_single_ts2():\n    \n    kcomp_single_ts = KalmanImpComparison(models_gap_single2, hai, hai_era, block_len=130, rmse=False, time_series=True)\n\n    return kcomp_single_ts.compare(n_rep =1, gap_len = [12, 24, 30], var = list(hai.columns))\n    \ng_single_ts2 = get_gap_single_ts2()\n\n\n\n\n\nplot_timeseries(g_single_ts2.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=0, \n                scale_color=alt.Scale(scheme='accent', domain = scale2), ctx_len={6.: 50, 12.: 50, 15.: 50}, compare='Gap')\n\n\n\n\n\n\n\nplot_timeseries(g_single_ts2.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=0, \n                scale_color=alt.Scale(scheme='accent', domain =scale2), ctx_len={6.: 50, 12.: 50, 15.: 50}, compare='Gap')\n\n\n\n\n\n\n\n\n\ncomparing model trained on gaps with a variable number of variables missing\n\nmodels_gap_single3 = pd.DataFrame.from_records([\n    {'Gap':  'all var test | all var train',  'gap_single_var': False,  'model': l_model(\"All_gap_all_30_v1.pickle\")  },\n    {'Gap':  'all var test | multi var train',  'gap_single_var': False,  'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")},\n    {'Gap':  'one var test | multi var train',  'gap_single_var': True,  'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")},\n    {'Gap':  'one var test | one var train',  'gap_single_var': True,   'model': l_model(\"1_gap_varying_tuned_6-30_v1.pickle\")  }\n])\n\n\nmodels_gap_single3\n\n\n\n\n\n\n\n\nGap\ngap_single_var\nmodel\n\n\n\n\n0\nall var test | all var train\nFalse\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n1\nall var test | multi var train\nFalse\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n2\none var test | multi var train\nTrue\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n3\none var test | one var train\nTrue\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n\n\n\n\n\n\n@cache_disk(\"multi_gap3\")\ndef get_multi_gap3(n_rep):\n    kcomp_single = KalmanImpComparison(models_gap_single3, hai, hai_era, block_len=130)\n\n    return kcomp_single.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nk_results_single3 = get_multi_gap3(50)\n\n\n\n\n\nscale3 = [\"one var test | multi var train\", \"all var test | multi var train\", \"all var test | all var train\"]\n\n\nplot_compare(k_results_single3, \"Gap\", scale_domain=scale3)\n\n\n\n\n\n\n\ndef get_gap_single_ts3():\n    \n    kcomp_single_ts = KalmanImpComparison(models_gap_single3, hai, hai_era, block_len=130, rmse=False, time_series=True)\n\n    return kcomp_single_ts.compare(n_rep =1, gap_len = [12, 24, 30], var = list(hai.columns))\n    \ng_single_ts3 = get_gap_single_ts3()\n\n\n\n\n\nplot_timeseries(g_single_ts3.query(\"var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS']\"), idx_rep=0, \n                scale_color=alt.Scale(scheme='accent', domain = scale3), ctx_len={6.: 50, 12.: 50, 15.: 50}, compare='Gap')\n\n\n\n\n\n\n\nplot_timeseries(g_single_ts3.query(\"var in ['PA', 'P', 'TS', 'SWC']\"), idx_rep=0, \n                scale_color=alt.Scale(scheme='accent', domain =scale3), ctx_len={6.: 50, 12.: 50, 15.: 50}, compare='Gap')\n\n\n\n\n\n\n\n\n\n\n\nmodels_generic = models_var.copy()\n\n\nmodels_generic.model = l_model(\"1_gap_varying_6-336_v3.pickle\") \nmodels_generic['type'] = 'Generic'\n\n\nmodels_generic\n\n\n\n\n\n\n\n\nvar\nmodel\ntype\n\n\n\n\n0\nTA\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n1\nSW_IN\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n2\nLW_IN\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n3\nVPD\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n4\nWS\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n5\nPA\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n6\nP\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n7\nTS\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n8\nSWC\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n\n\n\n\n\n\nmodels_var['type'] = 'Finetuned one var'\n\n\nmodels_gen_vs_spec = pd.concat([models_generic, models_var])\n\n\nmodels_gen_vs_spec\n\n\n\n\n\n\n\n\nvar\nmodel\ntype\n\n\n\n\n0\nTA\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n1\nSW_IN\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n2\nLW_IN\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n3\nVPD\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n4\nWS\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n5\nPA\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n6\nP\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n7\nTS\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n8\nSWC\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nGeneric\n\n\n0\nTA\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nFinetuned one var\n\n\n1\nSW_IN\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nFinetuned one var\n\n\n2\nLW_IN\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nFinetuned one var\n\n\n3\nVPD\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nFinetuned one var\n\n\n4\nWS\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nFinetuned one var\n\n\n5\nPA\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nFinetuned one var\n\n\n6\nP\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nFinetuned one var\n\n\n7\nTS\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nFinetuned one var\n\n\n8\nSWC\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\nFinetuned one var\n\n\n\n\n\n\n\n\n@cache_disk(\"generic\")\ndef get_generic(n_rep=n_rep):\n\n    comp_generic = KalmanImpComparison(models_gen_vs_spec, hai, hai_era, block_len=100+48*7)\n\n    return comp_generic.compare(n_rep =n_rep, gap_len = [12, 24, 48, 48*7], var = list(hai.columns))\nk_results_generic = get_generic(n_rep)\n\n\np = plot_compare(k_results_generic, 'type', scale_domain=[\"Finetuned one var\", \"Generic\"])\nsave_plot(p, \"generic\")\np\n\n\n\n\n\n\n\nt = table_compare(k_results_generic, 'type')\ntable_compare_latex(t, base_path_tbl / \"generic.tex\", label='generic')\nt\n\n\n\n\n\n\n\n\ntype\nGeneric\nFinetuned one var\n\n\n\n\nRMSE Standardized\nmean\nstd\nmean\nstd\ndiff.\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\nTA\n6.0\n0.092151\n0.055814\n0.054999\n0.032663\n-0.037153\n\n\n12.0\n0.118358\n0.074072\n0.077345\n0.052042\n-0.041013\n\n\n24.0\n0.146578\n0.076799\n0.100970\n0.058986\n-0.045608\n\n\n168.0\n0.174324\n0.064495\n0.128855\n0.048376\n-0.045469\n\n\nSW_IN\n6.0\n0.255795\n0.164694\n0.208728\n0.167255\n-0.047067\n\n\n12.0\n0.314755\n0.164839\n0.257761\n0.163914\n-0.056994\n\n\n24.0\n0.339310\n0.128516\n0.280578\n0.131607\n-0.058733\n\n\n168.0\n0.357922\n0.103167\n0.297530\n0.118822\n-0.060391\n\n\nLW_IN\n6.0\n0.299442\n0.201432\n0.256844\n0.199753\n-0.042598\n\n\n12.0\n0.367537\n0.197000\n0.326104\n0.209362\n-0.041433\n\n\n24.0\n0.400412\n0.175611\n0.358755\n0.185939\n-0.041657\n\n\n168.0\n0.466036\n0.141424\n0.405833\n0.165308\n-0.060203\n\n\nVPD\n6.0\n0.137863\n0.104531\n0.096682\n0.089465\n-0.041182\n\n\n12.0\n0.226616\n0.183575\n0.169084\n0.164870\n-0.057532\n\n\n24.0\n0.272840\n0.206783\n0.202490\n0.159921\n-0.070350\n\n\n168.0\n0.325113\n0.154287\n0.263913\n0.153488\n-0.061201\n\n\nWS\n6.0\n0.571539\n0.523304\n0.365262\n0.200693\n-0.206276\n\n\n12.0\n0.660302\n0.408751\n0.481919\n0.265006\n-0.178383\n\n\n24.0\n0.676472\n0.374177\n0.510818\n0.270480\n-0.165653\n\n\n168.0\n0.781636\n0.264823\n0.569137\n0.189984\n-0.212499\n\n\nPA\n6.0\n0.099325\n0.066109\n0.054017\n0.032223\n-0.045308\n\n\n12.0\n0.116360\n0.061026\n0.059966\n0.030824\n-0.056394\n\n\n24.0\n0.136620\n0.076087\n0.072074\n0.068788\n-0.064546\n\n\n168.0\n0.158988\n0.068315\n0.081116\n0.053864\n-0.077872\n\n\nP\n6.0\n0.542198\n0.990843\n0.542198\n0.990843\n0.000000\n\n\n12.0\n0.627167\n1.034936\n0.627167\n1.034936\n0.000000\n\n\n24.0\n0.628179\n0.575428\n0.628179\n0.575428\n0.000000\n\n\n168.0\n0.880507\n0.572737\n0.880507\n0.572737\n0.000000\n\n\nSWC\n6.0\n0.152144\n0.107350\n0.050597\n0.033592\n-0.101547\n\n\n12.0\n0.244768\n0.154341\n0.067366\n0.043308\n-0.177402\n\n\n24.0\n0.392553\n0.222757\n0.078784\n0.053435\n-0.313769\n\n\n168.0\n0.719138\n0.314318\n0.169313\n0.103684\n-0.549825\n\n\nTS\n6.0\n0.168111\n0.114636\n0.065352\n0.068001\n-0.102759\n\n\n12.0\n0.226302\n0.134730\n0.088370\n0.087491\n-0.137932\n\n\n24.0\n0.286258\n0.152198\n0.129457\n0.127674\n-0.156801\n\n\n168.0\n0.362279\n0.176402\n0.316745\n0.217719\n-0.045534\n\n\n\n\n\n\n\n\nres_singl_perc = k_results_generic.groupby(['type', 'var']).agg({'rmse_stand': 'mean'}).reset_index().pivot(columns = 'type', values='rmse_stand', index=['var'])\n\n\n(res_singl_perc[\"Generic\"] - res_singl_perc[\"Finetuned one var\"]) / res_singl_perc[\"Generic\"] * 100\n\nvar\nTA       31.847749\nSW_IN    17.604408\nLW_IN    12.122588\nVPD      23.925198\nWS       28.357855\nPA       47.745394\nP         0.000000\nSWC      75.735168\nTS       42.478176\ndtype: float64\n\n\n\n\n\n\nmodels_train = pd.DataFrame.from_records([\n    # {'Train':  'All variables',  'model': l_model(\"All_gap_all_30_v1.pickle\")  },\n    {'Train':  'Only one var',   'model': l_model(\"1_gap_varying_6-336_v3.pickle\")  },\n    {'Train':  'Multi vars',     'model': l_model(\"all_varying_gap_varying_len_6-30_v3.pickle\")  },\n    {'Train':  'Random params',  'model': l_model(\"rand_all_varying_gap_varying_len_6-30_v4.pickle\")  }\n])\n\n\nmodels_train\n\n\n\n\n\n\n\n\nTrain\nmodel\n\n\n\n\n0\nOnly one var\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n1\nMulti vars\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n2\nRandom params\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n\n\n\n\n\n\n@cache_disk(\"train\")\ndef get_train(n_rep):\n    kcomp = KalmanImpComparison(models_train, hai, hai_era, block_len=130)\n\n    return kcomp.compare(n_rep =n_rep, gap_len = [6, 12, 24, 30], var = list(hai.columns))\n\n\nres_train = get_train(n_rep)\n\n\nres_train_agg = res_train.groupby(['Train', 'gap_len']).agg({'rmse_stand': 'mean'}).reset_index()\n\n\nres_train_agg\n\n\n\n\n\n\n\n\nTrain\ngap_len\nrmse_stand\n\n\n\n\n0\nMulti vars\n3.0\n0.129015\n\n\n1\nMulti vars\n6.0\n0.163224\n\n\n2\nMulti vars\n12.0\n0.212245\n\n\n3\nMulti vars\n15.0\n0.217804\n\n\n4\nOnly one var\n3.0\n0.195662\n\n\n5\nOnly one var\n6.0\n0.256032\n\n\n6\nOnly one var\n12.0\n0.317933\n\n\n7\nOnly one var\n15.0\n0.337527\n\n\n8\nRandom params\n3.0\n0.198214\n\n\n9\nRandom params\n6.0\n0.236883\n\n\n10\nRandom params\n12.0\n0.283699\n\n\n11\nRandom params\n15.0\n0.290204\n\n\n\n\n\n\n\n\nalt.Chart(res_train_agg).mark_bar().encode(\n        x = alt.X('gap_len', title='gap length [h]', axis=alt.Axis(labelAngle=0)),\n        y = alt.Y('rmse_stand', title=\"Standardized RMSE\"),\n        color=alt.Color('Train', scale=alt.Scale(domain=[\"Multi vars\", \"Only one var\", \"Random params\"], scheme='accent')),\n        xOffset=alt.XOffset('Train:N', scale= alt.Scale(domain=[\"Multi vars\", \"Only one var\", \"Random params\"])),\n    ).properties(width=500, height=300).configure_legend(orient=\"bottom\")\n\n\n\n\n\n\n\np = plot_compare(res_train, \"Train\", scale_domain=[\"Multi vars\", \"Only one var\", \"Random params\"])\nsave_plot(p, \"train_compare\")\nshow_plot(\"train_compare\")\n\n\n\n\n\nt = table_compare3(res_train, 'Train')\ntable_compare3_latex(t, base_path_tbl / \"train.tex\", label=\"train_compare\")\nt\n\n\n\n\n\n\n\n\nTrain\nMulti vars\nOnly one var\nRandom params\n\n\n\nRMSE Standardized\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\n\nTA\n3.0\n0.029316\n0.024026\n0.064266\n0.045855\n0.062565\n0.050788\n\n\n6.0\n0.047570\n0.040525\n0.095005\n0.072717\n0.086882\n0.067355\n\n\n12.0\n0.071804\n0.058126\n0.125767\n0.083388\n0.115255\n0.089975\n\n\n15.0\n0.074747\n0.048259\n0.126504\n0.069100\n0.115649\n0.074126\n\n\nSW_IN\n3.0\n0.189022\n0.179283\n0.210766\n0.182537\n0.255083\n0.210510\n\n\n6.0\n0.231952\n0.183018\n0.272504\n0.184780\n0.303011\n0.230394\n\n\n12.0\n0.273972\n0.178823\n0.313473\n0.171075\n0.345330\n0.216703\n\n\n15.0\n0.261540\n0.147736\n0.303385\n0.147863\n0.326338\n0.168216\n\n\nLW_IN\n3.0\n0.209100\n0.174233\n0.226141\n0.186266\n0.288581\n0.198206\n\n\n6.0\n0.271782\n0.192016\n0.304418\n0.203732\n0.355630\n0.227813\n\n\n12.0\n0.334969\n0.205066\n0.364683\n0.217252\n0.402762\n0.234177\n\n\n15.0\n0.349063\n0.193528\n0.402901\n0.217125\n0.399911\n0.214815\n\n\nVPD\n3.0\n0.066147\n0.067398\n0.103740\n0.085860\n0.151077\n0.127722\n\n\n6.0\n0.098182\n0.074594\n0.144168\n0.093344\n0.186584\n0.134454\n\n\n12.0\n0.158447\n0.136362\n0.211297\n0.157147\n0.228595\n0.176609\n\n\n15.0\n0.176550\n0.133618\n0.226348\n0.142954\n0.243083\n0.161260\n\n\nWS\n3.0\n0.304563\n0.160289\n0.499445\n0.557471\n0.460426\n0.302247\n\n\n6.0\n0.375432\n0.197191\n0.622664\n0.840854\n0.491942\n0.269835\n\n\n12.0\n0.475421\n0.246422\n0.682434\n0.522670\n0.543404\n0.285233\n\n\n15.0\n0.477836\n0.250926\n0.652838\n0.402816\n0.546776\n0.292906\n\n\nPA\n3.0\n0.023626\n0.019933\n0.070958\n0.050700\n0.062816\n0.050095\n\n\n6.0\n0.035030\n0.020677\n0.095294\n0.067078\n0.087910\n0.071787\n\n\n12.0\n0.050801\n0.027830\n0.125376\n0.083567\n0.116205\n0.078518\n\n\n15.0\n0.056982\n0.043883\n0.129951\n0.086356\n0.131319\n0.102346\n\n\nP\n3.0\n0.299489\n0.556391\n0.379744\n1.156559\n0.330911\n0.554337\n\n\n6.0\n0.349589\n0.516015\n0.457703\n0.642463\n0.385590\n0.605383\n\n\n12.0\n0.425566\n0.605321\n0.559601\n0.644050\n0.431093\n0.679138\n\n\n15.0\n0.423285\n0.530192\n0.625427\n0.785776\n0.441201\n0.632718\n\n\nSWC\n3.0\n0.010608\n0.013968\n0.094149\n0.077060\n0.117970\n0.095640\n\n\n6.0\n0.016923\n0.028378\n0.152065\n0.116823\n0.154318\n0.120571\n\n\n12.0\n0.029902\n0.041090\n0.243060\n0.178547\n0.243638\n0.174359\n\n\n15.0\n0.034230\n0.043006\n0.301561\n0.194696\n0.272256\n0.220770\n\n\nTS\n3.0\n0.029260\n0.038676\n0.111752\n0.083746\n0.054495\n0.059122\n\n\n6.0\n0.042557\n0.045828\n0.160466\n0.147993\n0.080075\n0.088891\n\n\n12.0\n0.089320\n0.098119\n0.235704\n0.157743\n0.127013\n0.124471\n\n\n15.0\n0.106004\n0.070791\n0.268831\n0.154897\n0.135304\n0.099415\n\n\n\n\n\n\n\n\n\n\n\n\nimport polars as pl\nfrom fastai.vision.data import get_grid\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nimport statsmodels.api as sm\n\n\ndef auto_corr_df(data, nlags=96):\n    autocorr = {}\n    for col in data.columns:\n        autocorr[col] = sm.tsa.acf(data[col], nlags=nlags)\n    return pd.DataFrame(autocorr)\n\n\nauto_corr = auto_corr_df(hai).T[[1,3,6,12,24,48, 72, 96]]\nauto_corr.columns = auto_corr.columns /2\n\n\nhai.corr().abs().mean()\n\nTA       0.473646\nSW_IN    0.302420\nLW_IN    0.354683\nVPD      0.416145\nWS       0.195750\nPA       0.222202\nP        0.169337\nSWC      0.313373\nTS       0.465442\ndtype: float64\n\n\n\naxes = get_grid(2,1,2, figsize=(16.9,8))\nsns.heatmap(hai.corr(), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), ax=axes[1], square=True, cbar=False)\nsns.heatmap(auto_corr, annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, ax=axes[0], cbar=True)\naxes[0].set(ylabel=\"Variable\", xlabel=\"Lag [h]\", title=\"Temporal Autocorrelation (a)\")\naxes[1].set(xlabel=\"Variable\", ylabel=\"Variable\", title=\"Inter-variable Correlation (b)\");\nplt.tight_layout()\nplt.savefig(base_path_img / \"correlation.png\")\n\n\n\n\n\n\n\n\nout_dir = here(\"../fluxnet/gap_stat\")\n\n\nsite_info = pl.read_parquet(out_dir / \"../site_info.parquet\").select([\n    pl.col(\"start\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"end\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"site\").cast(pl.Categorical).sort()\n])\n\n\ndef duration_n_obs(duration):\n    \"converts a duration into a n of fluxnet observations\"\n    return abs(int(duration.total_seconds() / (30 * 60)))\n\n\nfiles = out_dir.ls()\nfiles.sort() # need to sort to match the site_info\nsites = []\nfor i, path in enumerate(files):\n    sites.append(pl.scan_parquet(path).with_columns([\n        pl.lit(site_info[i, \"site\"]).alias(\"site\"),\n        pl.lit(duration_n_obs(site_info[i, \"start\"] -  site_info[i, \"end\"])).alias(\"total_obs\"),\n        pl.col(\"TIMESTAMP_END\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\").alias(\"end\"),\n    ]).drop(\"TIMESTAMP_END\"))\n\ngap_stat = pl.concat(sites)\n\n\npl.read_parquet(files[0])\n\n\n\n\nshape: (31574, 3)\nTIMESTAMP_END\ngap_len\nvariable\ni64\nu32\nstr\n200901010030\n16992\n\"TA_F_MDS_QC\"\n200912211100\n5\n\"TA_F_MDS_QC\"\n200912211700\n1\n\"TA_F_MDS_QC\"\n201001061300\n1\n\"TA_F_MDS_QC\"\n201001071300\n3\n\"TA_F_MDS_QC\"\n201001081130\n2\n\"TA_F_MDS_QC\"\n201001081830\n1\n\"TA_F_MDS_QC\"\n201001091000\n2\n\"TA_F_MDS_QC\"\n201001191000\n1\n\"TA_F_MDS_QC\"\n201001191130\n2\n\"TA_F_MDS_QC\"\n201001291900\n1\n\"TA_F_MDS_QC\"\n201002131030\n2\n\"TA_F_MDS_QC\"\n...\n...\n...\n201103241630\n3\n\"NEE_VUT_95_QC\"\n201103241930\n29\n\"NEE_VUT_95_QC\"\n201103251900\n8\n\"NEE_VUT_95_QC\"\n201103260030\n1\n\"NEE_VUT_95_QC\"\n201103260200\n1\n\"NEE_VUT_95_QC\"\n201103260300\n1\n\"NEE_VUT_95_QC\"\n201103261230\n1\n\"NEE_VUT_95_QC\"\n201103261330\n2\n\"NEE_VUT_95_QC\"\n201103261630\n1\n\"NEE_VUT_95_QC\"\n201103261930\n2\n\"NEE_VUT_95_QC\"\n201103262100\n1\n\"NEE_VUT_95_QC\"\n201103270000\n13441\n\"NEE_VUT_95_QC\"\n\n\n\n\n\ngap_stat.head().collect()\n\n\n\n\nshape: (5, 5)\ngap_len\nvariable\nsite\ntotal_obs\nend\nu32\nstr\nstr\ni32\ndatetime[μs]\n16992\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-01-01 00:30:00\n5\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-12-21 11:00:00\n1\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-12-21 17:00:00\n1\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2010-01-06 13:00:00\n3\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2010-01-07 13:00:00\n\n\n\n\n\ndef plot_var_dist(var, small=False, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") &lt; 200 if small else True\n    ).with_column(pl.col(\"gap_len\") / (24 *2 * 7)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - { 'gaps &lt; 200' if small else 'all gaps'}\")\n    if not small: ax.set_yscale('log')\n    ax.set_xlabel(\"gap length (weeks)\")\n    ax.set_ylabel(f\"{'Log' if not small else ''} n gaps\")\n    # plt.xscale('log') \n\n\nplot_var_dist('TA_F_QC')\n\n\n\n\n\ncolor_map = dict(zip(scale_meteo.domain, list(sns.color_palette('Set2', n_colors=len(hai.columns)).as_hex())))\n\n\nqc_map = {\n    'TA': 'TA_F_QC',\n    'SW_IN': 'SW_IN_F_QC',\n    'LW_IN': 'LW_IN_F_QC',\n    'VPD': 'VPD_F_QC',\n    'WS': 'WS_F_QC',\n    'PA': 'PA_F_QC',\n    'P': 'P_F_QC',\n    'TS': 'TS_F_MDS_1_QC',\n    'SWC': 'SWC_F_MDS_1_QC',\n}\n\n\ndef pl_in(col, values):\n    expr = False\n    for val in values:\n        expr |= pl.col(col) == val\n    return expr\n\n\ngap_stat.filter(pl_in('variable', qc_map.values())\n               ).with_columns([\n    pl.when(pl.col(\"gap_len\") &lt; 48*7).then(True).otherwise(False).alias(\"short\"),\n    pl.count().alias(\"total\"),\n    pl.count().alias(\"total len\"),\n]).groupby(\"short\").agg([\n    (pl.col(\"gap_len\").count() / pl.col(\"total\")).alias(\"frac_num\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total len\")).alias(\"frac_len\")\n]).collect()\n\n\n\n\nshape: (2, 3)\nshort\nfrac_num\nfrac_len\nbool\nlist[f64]\nlist[f64]\nfalse\n[0.010775, 0.010775, ... 0.010775]\n[63.844386, 63.844386, ... 63.844386]\ntrue\n[0.989225, 0.989225, ... 0.989225]\n[7.261655, 7.261655, ... 7.261655]\n\n\n\n\n\ngap_stat.filter(pl_in('variable', qc_map.values())\n               ).with_columns([\n    pl.when(pl.col(\"gap_len\") &lt; 48*7).then(True).otherwise(False).alias(\"short\"),\n    pl.count().alias(\"total\"),\n    pl.count().alias(\"total len\"),\n]).groupby(\"short\").agg([\n    (pl.col(\"gap_len\").count() / pl.col(\"total\")).alias(\"frac_num\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total len\")).alias(\"frac_len\")\n]).collect()\n\n\n\n\nshape: (2, 3)\nshort\nfrac_num\nfrac_len\nbool\nlist[f64]\nlist[f64]\nfalse\n[0.010775, 0.010775, ... 0.010775]\n[63.844386, 63.844386, ... 63.844386]\ntrue\n[0.989225, 0.989225, ... 0.989225]\n[7.261655, 7.261655, ... 7.261655]\n\n\n\n\n\nfrac_miss = gap_stat.filter(\n    pl_in('variable', qc_map.values())\n).groupby([\"site\", \"variable\"]).agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()).alias(\"frac_gap\")\n])\n\n\nfrac_miss.groupby('variable').agg([\n    pl.col(\"frac_gap\").max().alias(\"max\"),\n    pl.col(\"frac_gap\").min().alias(\"min\"),\n    pl.col(\"frac_gap\").std().alias(\"std\"),\n    pl.col(\"frac_gap\").mean().alias(\"mean\"),\n]).collect()\n\n\n\n\nshape: (9, 5)\nvariable\nmax\nmin\nstd\nmean\nstr\nf64\nf64\nf64\nf64\n\"P_F_QC\"\n1.503296\n0.000011\n0.330422\n0.311598\n\"SWC_F_MDS_1_QC...\n1.487143\n0.000011\n0.29985\n0.314847\n\"SW_IN_F_QC\"\n2.193984\n0.000038\n0.21952\n0.16772\n\"TS_F_MDS_1_QC\"\n3.090642\n0.000011\n0.38542\n0.282106\n\"VPD_F_QC\"\n2.298799\n0.000011\n0.297668\n0.245457\n\"TA_F_QC\"\n2.299027\n0.000011\n0.256043\n0.198846\n\"WS_F_QC\"\n2.675324\n0.000798\n0.323944\n0.257303\n\"LW_IN_F_QC\"\n7.074603\n0.000019\n0.704469\n0.578246\n\"PA_F_QC\"\n2.675381\n0.000011\n0.420542\n0.387658\n\n\n\n\n\nfrac_miss.sort(\"frac_gap\", reverse=True).collect()\n\n\n\n\nshape: (1798, 4)\nsite\nvariable\nmean\nfrac_gap\nstr\nstr\nf64\nf64\n\"US-LWW\"\n\"LW_IN_F_QC\"\n11267.590909\n7.074603\n\"US-Wi5\"\n\"LW_IN_F_QC\"\n70128.0\n3.992031\n\"ES-LgS\"\n\"LW_IN_F_QC\"\n175344.0\n3.333093\n\"US-LWW\"\n\"TS_F_MDS_1_QC\"\n1320.646341\n3.090642\n\"US-Wi5\"\n\"TS_F_MDS_1_QC\"\n62.142857\n2.897194\n\"US-Wi0\"\n\"LW_IN_F_QC\"\n1369.694444\n2.814601\n\"US-ORv\"\n\"PA_F_QC\"\n3.899334\n2.675381\n\"US-ORv\"\n\"WS_F_QC\"\n3.899251\n2.675324\n\"US-LWW\"\n\"PA_F_QC\"\n409.701754\n2.665944\n\"US-Wi5\"\n\"WS_F_QC\"\n39.57814\n2.349861\n\"US-Wi5\"\n\"PA_F_QC\"\n51.453972\n2.322707\n\"US-Wi5\"\n\"TA_F_QC\"\n45.790249\n2.299027\n...\n...\n...\n...\n\"CN-Qia\"\n\"SWC_F_MDS_1_QC...\n1.0\n0.000019\n\"CN-Din\"\n\"PA_F_QC\"\n1.0\n0.000019\n\"FI-Lom\"\n\"P_F_QC\"\n1.0\n0.000019\n\"CN-Qia\"\n\"TS_F_MDS_1_QC\"\n1.0\n0.000019\n\"CN-Cha\"\n\"TA_F_QC\"\n1.0\n0.000019\n\"CN-Din\"\n\"TS_F_MDS_1_QC\"\n1.0\n0.000019\n\"US-Me6\"\n\"P_F_QC\"\n1.0\n0.000011\n\"US-Me6\"\n\"PA_F_QC\"\n1.0\n0.000011\n\"US-Me6\"\n\"TS_F_MDS_1_QC\"\n1.0\n0.000011\n\"US-Me6\"\n\"TA_F_QC\"\n1.0\n0.000011\n\"US-Me6\"\n\"VPD_F_QC\"\n1.0\n0.000011\n\"US-Me6\"\n\"SWC_F_MDS_1_QC...\n1.0\n0.000011\n\n\n\n\n\nsite_info.filter((pl.col(\"site\") == \"US-LWW\"))\n\n\n\n\nshape: (1, 3)\nstart\nend\nsite\ndatetime[μs]\ndatetime[μs]\ncat\n1997-01-01 00:30:00\n1999-01-01 00:00:00\n\"US-LWW\"\n\n\n\n\n\n246556\n\n\ngap_stat.filter((pl.col(\"site\") == \"US-LWW\") & (pl.col(\"variable\") == \"LW_IN_F_QC\" )).collect()\n\n\n\n\nshape: (22, 5)\ngap_len\nvariable\nsite\ntotal_obs\nend\nu32\nstr\nstr\ni32\ndatetime[μs]\n246556\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2000-01-01 00:30:00\n19\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-02-11 08:30:00\n6\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-03-08 08:00:00\n1\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-03-08 11:30:00\n1\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-03-08 13:00:00\n50\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-11-18 13:00:00\n21\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-11-19 22:30:00\n22\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-11-24 23:30:00\n55\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-11-26 06:30:00\n172\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-11-27 19:30:00\n70\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-01 23:00:00\n3\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-03 11:30:00\n35\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-03 16:30:00\n92\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-04 11:00:00\n14\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-11 05:00:00\n36\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-11 18:00:00\n36\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-12 17:30:00\n87\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-13 18:00:00\n70\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-16 01:30:00\n37\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-17 16:00:00\n10\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-21 07:30:00\n494\n\"LW_IN_F_QC\"\n\"US-LWW\"\n35039\n2014-12-21 17:30:00\n\n\n\n\n\ndef plot_var_dist_small(var, ax=None, small=True):\n    if ax is None: ax = get_grid(1)[0]\n    \n    color = color_map[var]\n    var_qc = qc_map[var]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var_qc)\n    ).filter(\n        pl.col(\"gap_len\") &lt; (24 * 2 *7) if small else True \n    ).with_column(pl.col(\"gap_len\") / (2 if small else 48 * 7)\n                 ).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax, edgecolor=\"white\", color=color)\n    ax.set_title(f\"{var} - { 'gap length &lt;  1 week' if small else 'all gaps'}\")\n    ax.set_xlabel(f\"gap length ({ 'hour' if small else 'week'})\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n\n\nvars = gap_stat.select(pl.col(\"variable\").unique()).collect()\n\n\nvars.filter(pl.col(\"variable\").str.contains(\"TA\"))\n\n\nfor ax, var in zip(get_grid(9,3,3, figsize=(15,12), sharey=False), list(var_type.categories)):\n    plot_var_dist_small(var, ax=ax)\nplt.savefig(base_path_img / \"gap_len_dist_small.png\")\n\n\nfor ax, var in zip(get_grid(9,3,3, figsize=(15,12), sharey=False), list(var_type.categories)):\n    plot_var_dist_small(var, ax=ax, small=False)\nplt.savefig(base_path_img / \"gap_len_dist.png\")\n\nmethods colors\n\nprint(sns.color_palette('Dark2').as_hex())"
  },
  {
    "objectID": "results/train_14_feb_results.html",
    "href": "results/train_14_feb_results.html",
    "title": "Training Kalman Filter for Results - 14 Feb",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\nfrom meteo_imp.gaussian import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\nfrom fastcore.foundation import patch\nfrom pathlib import Path, PosixPath\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image, HTML\n\nfrom tqdm.auto import tqdm\n\n\nfrom fastcore.basics import *\n\n\nshow_metrics = False\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_big_path)\nhai_era = pd.read_parquet(hai_era_big_path)\n\n\nbase = here(\"analysis/results/trained_14feb\")\n\n\nbase.mkdir(exist_ok=True)\n\n\n@patch\ndef add_end(self: PosixPath, end): return self.parent / (self.name + end)\n\n\ndef train_or_load(model, dls, lr, n, path, keep=True):\n    save_models = SaveModelsBatch(times_epoch=5)\n    csv_logger = CSVLogger(path.add_end(\"log.csv\"))\n    learn = Learner(dls, model, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models, csv_logger], metrics=rmse_gap) \n    items = random.choices(dls.valid.items, k=4) \n    if path.add_end(\".pickle\").exists() and keep:\n        learn.model = torch.load(path.add_end(\".pickle\"))\n        display(csv_logger.read_log())\n        plot = Image(filename=path.add_end(\"_loss_plot.png\"))\n        display(plot)\n    else:\n        learn.fit(lr, n)\n        \n        torch.save(learn.model, path.add_end(\".pickle\"))\n        learn.recorder.plot_loss()\n        plt.savefig(path.add_end(\"_loss_plot.png\"))\n    return learn, items                           \n\n\ndef metric_valid(learn, dls=None):\n    nrmse = []\n    losses = []\n    dls = ifnone(dls, learn.dls.valid)\n    for input, target in tqdm(dls, leave=False):\n        pred = learn.model(input)\n        nrmse.append(learn.metrics[0](pred, target))\n        losses.append(learn.loss_func(pred, target).item())\n    metric = pd.DataFrame({'loss': losses, 'rmse': nrmse})\n    return metric.agg(['mean', 'std'])\n\n\nhai.columns\n\nIndex(['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], dtype='object')\n\n\n\n\n\ndls_A1v = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns), n_var=1),\n    block_len=100+336,\n    gap_len=gen_gap_len(12, 336),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5).cpu()\n\n\nlen(hai)\n\n227952\n\n\n\nlen(dls_A1v.train)*20, len(dls_A1v.valid)*20\n\n(2080, 520)\n\n\n\nmodel_A1v = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca = hai, \n    pred_only_gap=True)\n\n\nmodel_A1v.B.shape\n\ntorch.Size([1, 18, 14])\n\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 3, 1e-3, base / \"1_gap_varying_12-336_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n218.223338\n180.709305\n0.644005\n44:15\n\n\n1\n1\n151.410605\n141.214027\n0.522214\n44:12\n\n\n2\n2\n131.718513\n125.997503\n0.487121\n39:18\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(learn_A1v.model, dls_A1v, 3, 1e-5, base / \"1_gap_varying_12-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n118.741593\n123.699397\n0.478724\n41:16\n\n\n1\n1\n113.616615\n123.144674\n0.477401\n45:19\n\n\n2\n2\n116.975438\n122.501000\n0.476022\n43:10\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(learn_A1v.model, dls_A1v, 1, 1e-3, base / \"1_gap_varying_12-336_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n117.073355\n113.886802\n0.448564\n46:56\n\n\n\n\n\n\n\n\n\n\n\nshow_results(learn_A1v)\n\n[MeteoImpItem(i=476, shift=56, var_sel=['WS'], gap_len=67), MeteoImpItem(i=425, shift=-2, var_sel=['VPD'], gap_len=251), MeteoImpItem(i=504, shift=27, var_sel=['TA'], gap_len=81)]\n\n\n\n\n\n\n\n\nmetric_valid(learn_A1v)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n113.886802\n0.448564\n\n\nstd\n34.077131\n0.089174\n\n\n\n\n\n\n\n\n\n\n\nmodel_A1v_nc = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca=hai,\n    pred_only_gap=True,\n    use_control=False\n)\n\n\nlearn_A1v_nc, items_A1v_nc = train_or_load(model_A1v_nc, dls_A1v, 3, 1e-3, base / \"1_gap_varying_336_no_control_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n231.587168\n190.550897\n0.671689\n39:28\n\n\n1\n1\n190.622703\n177.561463\n0.611599\n39:41\n\n\n2\n2\n174.056532\n164.573917\n0.574693\n39:39\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndls_Aa = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = list(hai.columns),\n    block_len=120,\n    gap_len=30,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5\n).cpu()\n\n\ndls_Aa = imp_dataloader(hai, hai_era, var_sel = list(hai.columns), block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa = learn_A1v.model.copy()\n\n\nif show_metrics: metric_valid(learn_A1v, dls=dls_Aa.valid)\n\n\nlearn_Aa, items_Aa = train_or_load(model_Aa, dls_Aa, 3, 3e-4, base / \"All_gap_all_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n70.709546\n74.130236\n0.411005\n25:47\n\n\n1\n1\n-2.735788\n7.333283\n0.381263\n28:48\n\n\n2\n2\n-16.577652\n-9.151317\n0.374610\n32:13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfine tune the model to only one variable\n\nfrom fastcore.basics import *\n\n\nfrom IPython.display import HTML\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(hai.columns)):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 &lt;/h4&gt;\"))\n    spec_dls[var] = imp_dataloader(hai, hai_era, var_sel = var, block_len=100+336, gap_len=gen_gap_len(12, 336), bs=20, control_lags=[1], n_rep=3, shifts=gen_shifts(50)).cpu()\n    spec_models[var] = learn_A1v.model.copy()\n    if show_metrics:\n        display(HTML(\"Metrics generic model\"))\n        display(metric_valid(learn_A1v, dls=spec_dls[var].valid))\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 3, 1e-3, base / f\"{var}_specialized_gap_12-336_v1\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-31.277383\n-59.727349\n0.155292\n23:16\n\n\n1\n1\n-57.410593\n-83.423924\n0.135217\n24:38\n\n\n2\n2\n-69.265146\n-81.197435\n0.137000\n21:50\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n72.838760\n63.622077\n0.317182\n22:33\n\n\n1\n1\n60.514370\n53.416147\n0.296670\n22:32\n\n\n2\n2\n55.572469\n48.728888\n0.287208\n25:43\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n110.011858\n112.541507\n0.435194\n24:28\n\n\n1\n1\n105.027452\n106.923631\n0.417517\n24:23\n\n\n2\n2\n102.755247\n106.489712\n0.415183\n24:53\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n67.771759\n71.121287\n0.306581\n25:12\n\n\n1\n1\n58.432390\n49.351550\n0.275882\n23:40\n\n\n2\n2\n49.884968\n41.432351\n0.264706\n25:51\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n169.196013\n241.890813\n0.734134\n24:18\n\n\n1\n1\n165.496950\n235.745148\n0.711773\n23:20\n\n\n2\n2\n161.570979\n223.982837\n0.696192\n22:10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-73.724512\n-103.872036\n0.124549\n27:03\n\n\n1\n1\n-111.420458\n-130.842109\n0.104550\n30:01\n\n\n2\n2\n-133.129972\n-140.989809\n0.100966\n24:07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n259.814830\n216.069186\n0.602446\n24:34\n\n\n1\n1\n248.654373\n209.606269\n0.607980\n23:06\n\n\n2\n2\n242.001831\n206.787110\n0.605378\n25:11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n207.952983\n194.924715\n0.646890\n24:35\n\n\n1\n1\n192.451676\n184.576861\n0.595377\n24:16\n\n\n2\n2\n169.086573\n117.395571\n0.388920\n22:44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n63.610211\n40.488381\n0.255747\n21:39\n\n\n1\n1\n57.377678\n39.280813\n0.247606\n21:41\n\n\n2\n2\n50.239735\n23.977059\n0.231418\n21:34\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspec_learn2 = {}\nfor var in tqdm(['TA', 'SW_IN', 'WS', 'PA', 'VPD', 'TS', 'SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 &lt;/h4&gt; | Training 2\"))\n    spec_learn2[var], _ = train_or_load(spec_learn[var].model.copy(), spec_dls[var], 2, 1e-3, base / f\"{var}_specialized_gap_12-336_v2.pickle\")\n    plt.show()\n    \n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-82.582890\n-78.921598\n0.137891\n21:39\n\n\n1\n1\n-86.119136\n-89.580120\n0.128288\n21:18\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n52.668345\n46.674095\n0.288313\n21:53\n\n\n1\n1\n51.089022\n44.378815\n0.283374\n22:28\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n164.019476\n232.919238\n0.710805\n22:37\n\n\n1\n1\n164.185435\n225.276840\n0.707446\n22:50\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-147.345027\n-136.658365\n0.103830\n22:42\n\n\n1\n1\n-157.427610\n-119.642745\n0.108054\n22:36\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n46.489590\n30.196986\n0.249948\n22:27\n\n\n1\n1\n45.226931\n46.153450\n0.273411\n21:01\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n39.863970\n17.692497\n0.221164\n20:40\n\n\n1\n1\n31.964869\n3.826767\n0.209781\n20:34\n\n\n\n\n\n\n\n\n\n\n | Training 2\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n114.122129\n96.942318\n0.352660\n20:44\n\n\n1\n1\n67.486398\n34.946166\n0.239454\n20:39\n\n\n\n\n\n\n\n\n\n\nPA and VPD are overfitting so repeat training 2 with only one batch\n\nspec_learn3 = {}\nfor var in tqdm(['PA', 'VPD']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn[var].model.copy(), spec_dls[var], 1, 1e-3, base / f\"{var}_specialized_gap_12-336_v3.pickle\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-135.310833\n-106.027287\n0.120461\n24:33\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n41.435748\n32.315761\n0.254877\n24:13\n\n\n\n\n\n\n\n\n\n\n\nspec_learn3 = {}\nfor var in tqdm(['TS']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n16.858002\n-7.042792\n0.189336\n23:31\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n3.990423\n-0.629562\n0.185285\n24:57\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 2, 1e-3, base / f\"{var}_specialized_gap_12-336_v3_2\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-1.621724\n-38.955631\n0.146205\n23:45\n\n\n1\n1\n-35.639923\n-57.006918\n0.130909\n22:09\n\n\n\n\n\n\n\n\n\n\nthe training loss is getting worse … so tring with smaller learning rate\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3_3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-75.585425\n-79.693433\n0.113264\n25:09\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3+4 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3_4\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-79.65696\n-81.566511\n0.113204\n25:25\n\n\n\n\n\n\n\n\n\n\n\nspec_learn4 = {}\nfor var in tqdm(['SWC']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3+4 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v4\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-0.957575\n-7.084297\n0.173571\n24:35\n\n\n\n\n\n\n\n\n\n\n\nfor var in tqdm(['WS']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+2+3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn2[var].model.copy(), spec_dls[var], 1, 1e-5, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n158.930217\n224.42966\n0.701698\n24:04\n\n\n\n\n\n\n\n\n\n\nthis is overfitting PA\n\nfor var in tqdm(['PA']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1++3 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn[var].model.copy(), spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_12-336_v3\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-168.729458\n-122.183789\n0.10763\n23:40\n\n\n\n\n\n\n\n\n\n\n\nspec_learn4 = {}\nfor var in tqdm(['PA']):\n    display(HTML(f\"&lt;h4&gt; {var} | Gap len 12-336 | Training 1+3+4 &lt;/h4&gt;\"))\n    spec_learn3[var], _ = train_or_load(spec_learn3[var].model.copy(), spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_12-336_v4\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-169.911168\n-117.59576\n0.109397\n21:33\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe goal of this section is to figure why the the generic model for all gaps works better then the generic model with gap in only one variable\n\nshow_results(learn_A1v, items = spec_items['TA'], hide_no_gap=True)\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\n\nshow_results(spec_learn2['TA'], items = spec_items['TA'], hide_no_gap=True)\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\n\nshow_results(learn_Aa, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls)\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\nno control to correlation is bad\n\nwith with_settings(learn_Aa.model, use_control=False):\n    display(show_results(learn_Aa, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\nthere is correlation here but no control,\nthe error is huggher and the uncertainty is higher\n\ndisplay(show_results(learn_A1v_nc, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_A1v_nc, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_A1v, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\nthis has the control\n\nitems_Aa_TA = [MeteoImpItem(i.i, i.shift, 'TA', i.gap_len) for i in items_Aa]\n\n\ndisplay(show_results(learn_Aa, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_Aa, items = items_Aa_TA, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA'], gap_len=30)]\n\n\n\n\n\n\n\nso the problem is that this is worse than the one above, even though it should not be the case\n\ndisplay(show_results(learn_A1v, items = items_Aa_TA, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA'], gap_len=30)]\n\n\n\n\n\n\n\n\ndisplay(show_results(learn_Aa, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\neven for *** longer gaps is the same issue where the generic model trained with gaps in all variables is worse than the generic model with gaps in none\n\ndisplay(show_results(learn_A1v, items = spec_items['TA'], hide_no_gap=True, dls=learn_A1v.dls))\n\n[MeteoImpItem(i=493, shift=-3, var_sel=['TA'], gap_len=263), MeteoImpItem(i=485, shift=-30, var_sel=['TA'], gap_len=318), MeteoImpItem(i=520, shift=29, var_sel=['TA'], gap_len=28), MeteoImpItem(i=418, shift=-10, var_sel=['TA'], gap_len=234)]\n\n\n\n\n\n\n\nthis is just very bad!\n\ndisplay(show_results(learn_A1v, items = items_Aa, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], gap_len=30)]\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\n\narray2df(learn_A1v.model.H.squeeze(0))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n0\n-0.003530\n-0.087569\n0.325754\n0.616742\n-0.042507\n-0.385458\n-0.297762\n-0.108117\n0.011227\n-0.051951\n-0.000688\n0.112157\n-0.027581\n0.053041\n-0.105348\n-0.055541\n0.106392\n0.014787\n\n\n1\n0.946844\n0.041798\n-0.017661\n-0.020581\n0.047038\n0.055534\n-0.010425\n-0.076334\n-0.016647\n-0.031737\n-0.031730\n0.113744\n0.050940\n0.025652\n-0.070724\n0.036109\n0.149151\n-0.012291\n\n\n2\n-0.010224\n-0.884454\n-0.102889\n-0.103207\n0.093886\n0.069696\n0.018569\n-0.068739\n0.009643\n0.000172\n-0.038645\n-0.003050\n-0.001318\n0.008244\n-0.102659\n-0.030466\n0.116563\n0.047369\n\n\n3\n0.001584\n0.009621\n0.151377\n0.336784\n0.730931\n0.214649\n0.141528\n-0.046515\n0.029864\n-0.039576\n-0.043306\n-0.026176\n0.045314\n-0.064704\n0.072526\n0.037909\n-0.165163\n-0.004306\n\n\n4\n0.023567\n0.036631\n-0.017066\n0.005542\n-0.070376\n-0.575263\n0.666550\n-0.041005\n0.012976\n0.120496\n-0.075441\n-0.080716\n0.104171\n-0.051610\n-0.013336\n0.079982\n-0.155137\n-0.028661\n\n\n5\n0.026580\n-0.014091\n0.044044\n0.038691\n-0.032200\n0.041902\n-0.080478\n-0.868049\n0.007711\n0.056614\n0.010028\n-0.099202\n0.030342\n-0.049511\n-0.016448\n-0.037645\n-0.089816\n-0.012628\n\n\n6\n-0.030374\n0.009719\n-0.016296\n-0.016841\n-0.037159\n0.063819\n0.063641\n-0.003837\n0.954560\n0.015126\n0.037151\n-0.016768\n-0.057569\n0.003242\n0.008403\n0.018665\n-0.022354\n-0.038982\n\n\n7\n0.017852\n0.030895\n-0.776759\n0.460665\n-0.000548\n0.044894\n-0.008753\n0.087636\n-0.050842\n-0.063789\n0.120345\n-0.112963\n0.078077\n-0.072172\n-0.186599\n-0.100295\n0.198485\n-0.014681\n\n\n8\n-0.006296\n-0.087112\n0.277213\n0.343271\n-0.516143\n0.380091\n0.342484\n-0.075277\n0.007326\n-0.105407\n0.004668\n0.099787\n0.218749\n0.131804\n0.106198\n0.014668\n0.151682\n0.000401\n\n\n\n\n\n\n\n\n\n\ndef plot_model_params(model):\n    sns.set(rc={\"figure.figsize\":(15, 10)})\n\n    sns.heatmap(array2df(model.H.squeeze(0)), annot=True,     vmin=-1, vmax=1.5, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"H\")\n    plt.show()\n    \n    sns.set(rc={\"figure.figsize\":(15, 15)})\n\n    sns.heatmap(array2df(model.A.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"A\")\n    plt.show()\n    \n    \n    sns.set(rc={\"figure.figsize\":(15, 15)})\n\n    sns.heatmap(array2df(model.B.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n    plt.title(\"B\")\n    plt.show()\n#     sns.set(rc={\"figure.figsize\":(15, 15)})\n\n#     sns.heatmap(array2df(model.Q.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n#     cmap=sns.diverging_palette(20, 220, n=200), square=True, cbar=False) \n#     plt.title(\"Q\")\n#     plt.show()\n    \n\n\nplot_model_params(learn_A1v.model)\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(learn_Aa.model)\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(spec_learn2['TA'].model)\n\n\n\n\n\n\n\n\n\n\n\nplot_model_params(learn_Aa.model)\n\n\n\n\n\n\n\n\n\n\n\nsns.set(rc={\"figure.figsize\":(15, 10)})\n\nsns.heatmap(array2df(learn_A1v.model.H.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\nsns.set(rc={\"figure.figsize\":(15, 15)})\n\nsns.heatmap(array2df(learn_A1v.model.A.squeeze(0)), annot=True,     vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200), square=True)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nthis is okay but not much better\n\ndisplay(show_results(spec_learn2['TA'], items = items_Aa_TA, hide_no_gap=True, dls=learn_Aa.dls))\n\n[MeteoImpItem(i=1591, shift=-24, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1836, shift=-12, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1643, shift=0, var_sel=['TA'], gap_len=30), MeteoImpItem(i=1664, shift=-48, var_sel=['TA'], gap_len=30)]\n\n\n\n\n\n\n\n\n\n\n\n\ngap_models = {}\ngap_dls = {}\ngap_learn = {}\ngap_items = {}\nfor gap_len in tqdm([6,24,48,7*48]):\n    print(f\"Gap len: {gap_len}\")\n    gap_dls[gap_len] = imp_dataloader(hai, hai_era, var_sel = gen_var_sel(list(hai.columns), n_var=1), block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=1, shifts=gen_shifts(50)).cpu()\n    gap_models[gap_len] = learn_A1v.model.copy()\n    if display_metric: display(metric_valid(learn_A1v, dls=gap_dls[gap_len].valid))\n    gap_learn[gap_len], gap_items[gap_len] = train_or_load(gap_models[gap_len], gap_dls[gap_len], 3, 2e-5, base / f\"gap_1_any_var_gap_len_{gap_len}_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nGap len: 6\n\n\nNameError: name 'display_metric' is not defined\n\n\n\n\n\nas an experiment TA for a gap of 24 fine tuned\n\ndls_TA24 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'TA',\n    block_len=100+24,\n    gap_len=24,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=1).cpu()\n\n\nmodel_TA24 = learn_A1v.model.copy()\n\n\nmetric_valid(learn_A1v, dls= dls_TA24.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.338763\n0.184055\n\n\nstd\n2.551372\n0.034550\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 1e-4, base / \"TA_gap_24_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-2.51402\n-3.892453\n0.174431\n02:41\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 1e-4, base / \"TA_gap_24_v1_2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-4.713286\n-5.854372\n0.161322\n02:37\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 1e-4, base / \"TA_gap_24_v1_3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-6.744339\n-7.335044\n0.152876\n02:39\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 1, 5e-5, base / \"TA_gap_24_v1_4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.932901\n-8.451432\n0.144813\n02:42\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA24, items_TA24 = train_or_load(model_TA24, dls_TA24, 3, 3e-5, base / \"TA_gap_24_v1_4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-7.932901\n-8.451432\n0.144813\n02:42\n\n\n\n\n\n\n\n\n\n\n\nmetric_valid(learn_TA24, dls= dls_TA24.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-9.049459\n0.140859\n\n\nstd\n2.104846\n0.023429\n\n\n\n\n\n\n\n\ndls_TA48 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'TA',\n    block_len=100+48,\n    gap_len=48,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=1).cpu()\n\n\nmodel_TA48 = learn_TA24.model.copy()\n\n\nmetric_valid(learn_TA24, dls= dls_TA48.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-11.247340\n0.170793\n\n\nstd\n8.504741\n0.035950\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 2e-4, base / \"TA_gap_48_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-11.938200\n-13.136611\n0.119232\n02:38\n\n\n1\n1\n-13.676607\n-14.978353\n0.111058\n02:38\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 1e-4, base / \"TA_gap_48_v1_2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-15.563658\n-15.840925\n0.107003\n02:44\n\n\n1\n1\n-15.958545\n-16.626610\n0.103149\n02:54\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 1e-4, base / \"TA_gap_48_v1_3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-16.665217\n-17.394887\n0.099992\n02:40\n\n\n1\n1\n-17.165897\n-17.974066\n0.097933\n02:43\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA48, items_TA48 = train_or_load(model_TA24, dls_TA24, 2, 1e-4, base / \"TA_gap_48_v1_4\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-18.111798\n-18.427618\n0.096457\n02:49\n\n\n1\n1\n-18.591682\n-19.413278\n0.092474\n02:52\n\n\n\n\n\n\n\n\n\n\n\nmodel_TA24_v2 = KalmanFilterSR.init_local_slope_pca(\n    n_dim_obs= len(hai.columns),\n    n_dim_state=len(hai.columns),\n    n_dim_contr = len(hai_era.columns),\n    df_pca = hai, \n    pred_only_gap=True)\n\n\nlearn_TA24_v2, items_TA24_v2 = train_or_load(model_TA24, dls_TA24, 3, 1e-3, base / \"TA_gap_24_v2_1\")\n\n\nTA_models = {}\nTA_dls = {}\nTA_learn = {}\nTA_items = {}\nfor gap_len in tqdm([6,24,48,7*48]):\n    display(HTML(f\"&lt;h4&gt; TA | Gap len: {gap_len} &lt;/h4&gt;\"))\n    TA_dls[gap_len] = imp_dataloader(hai, hai_era, var_sel = 'TA', block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=2, shifts=gen_shifts(50)).cpu()\n    TA_models[gap_len] = learn_A1v.model.copy()\n    display(metric_valid(learn_A1v, dls=TA_dls[gap_len].valid))\n    TA_learn[gap_len], TA_items[gap_len] = train_or_load(TA_models[gap_len], TA_dls[gap_len], 4, 1e-4, base / f\"TA_gap_len_{gap_len}_v1.pickle\")\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.093156\n0.102643\n\n\nstd\n0.403933\n0.025537\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-3.623101\n-4.139968\n0.074969\n03:57\n\n\n1\n-5.193550\n-5.537112\n0.060948\n04:14\n\n\n2\n-6.309246\n-6.533323\n0.052087\n04:33\n\n\n3\n-7.058736\n-7.211129\n0.046813\n04:22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-0.660455\n0.200111\n\n\nstd\n2.088128\n0.031036\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-4.815664\n-5.862635\n0.162549\n05:23\n\n\n1\n-8.257145\n-9.265403\n0.141516\n05:27\n\n\n2\n-10.721531\n-11.764655\n0.126987\n05:17\n\n\n3\n-13.121913\n-13.537241\n0.119559\n05:31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n2.231489\n0.227216\n\n\nstd\n4.624743\n0.032391\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-5.077721\n-6.730453\n0.189337\n06:57\n\n\n1\n-10.696784\n-12.647214\n0.166578\n06:37\n\n\n2\n-15.724886\n-16.361526\n0.153221\n06:56\n\n\n3\n-18.714646\n-19.920346\n0.142614\n06:41\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n30.665105\n0.248255\n\n\nstd\n18.911530\n0.021534\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n18.701340\n-1.243045\n0.226205\n17:41\n\n\n1\n-0.477330\n-23.318493\n0.212453\n17:30\n\n\n2\n-19.797870\n-42.075069\n0.200830\n47:23\n\n\n3\n-36.470670\n-55.291288\n0.192844\n18:24\n\n\n\n\n\n\n\n\n\n\n\n\ndls_TA = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'TA',\n    block_len=70+336,\n    gap_len=gen_gap_len(12,336),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=3).cpu()\n\n\nmodel_TA = learn_A1v.model.copy()\n\n\nmetric_valid(learn_A1v, dls= dls_TA.valid)\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n18.120834\n0.240742\n\n\nstd\n19.321141\n0.032218\n\n\n\n\n\n\n\n\nlearn_TA, items_TA = train_or_load(model_TA, dls_TA, 2, 5e-4, base / \"TA_gap_12-336_v1,\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-26.097331\n-37.440739\n0.175528\n25:36\n\n\n1\n1\n-50.443927\n-66.472712\n0.149401\n26:02\n\n\n\n\n\n\n\n\n\n\n\nlearn_TA, items_TA = train_or_load(learn_TA.model, dls_TA, 2, 5e-4, base / \"TA_gap_12-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-70.902275\n-72.341130\n0.137253\n24:20\n\n\n1\n-78.379055\n-76.962878\n0.132967\n23:55\n\n\n\n\n\n\n\n\n\nlearn_TA, items_TA = train_or_load(learn_TA.model, dls_TA, 2, 5e-4, base / \"TA_gap_12-336_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-85.106441\n-82.588917\n0.128094\n24:21\n\n\n1\n-86.267741\n-83.305525\n0.129777\n23:29\n\n\n\n\n\n\n\n\n\n\n\n\ndef metrics_valid_gap_lens(learn, var, gaps = [6,12,24,48,7*48]):\n    for gap_len in tqdm(gaps):\n        dls = imp_dataloader(hai, hai_era, var_sel = var, block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=1, shifts=gen_shifts(50)).cpu()\n        display(HTML(f\"&lt;strong&gt; Metrics | gap len: {gap_len} | Var: {var} &lt;/strong&gt;\"))\n        display(metric_valid(learn, dls=dls.valid))\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\nmetrics_valid_gap_lens(learn_A1v, 'TA')\n\n\n\n\n Metrics | gap len: 6 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.649933\n0.095968\n\n\nstd\n0.338268\n0.018444\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.243049\n0.134065\n\n\nstd\n0.710149\n0.021311\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-2.125709\n0.188222\n\n\nstd\n2.047966\n0.027886\n\n\n\n\n\n\n\n Metrics | gap len: 48 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n0.638268\n0.217725\n\n\nstd\n6.192707\n0.031585\n\n\n\n\n\n\n\n Metrics | gap len: 336 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n21.469734\n0.242877\n\n\nstd\n9.829365\n0.010609\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(learn_A1v_nc, 'TA')\n\n\n\n\n Metrics | gap len: 6 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.430541\n0.055689\n\n\nstd\n0.097171\n0.009796\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.790838\n0.113533\n\n\nstd\n0.833381\n0.030206\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-0.992221\n0.187918\n\n\nstd\n4.834171\n0.068321\n\n\n\n\n\n\n\n Metrics | gap len: 48 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n5.597832\n0.235436\n\n\nstd\n12.132120\n0.075510\n\n\n\n\n\n\n\n Metrics | gap len: 336 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n96.691128\n0.293088\n\n\nstd\n98.837141\n0.085536\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(spec_learn['TA'], 'TA')\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n Metrics | gap len: 6 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-6.332682\n0.049233\n\n\nstd\n0.226535\n0.007010\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-10.478126\n0.072882\n\n\nstd\n0.762210\n0.010581\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-15.977039\n0.103628\n\n\nstd\n4.186457\n0.024085\n\n\n\n\n\n\n\n Metrics | gap len: 48 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-25.748188\n0.125292\n\n\nstd\n11.229092\n0.031731\n\n\n\n\n\n\n\n Metrics | gap len: 336 | Var: TA \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-136.930894\n0.152361\n\n\nstd\n61.110281\n0.026692\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(spec_learn[\"SWC\"], 'SWC', gaps=[6,12,24,30])\n\n\n\n\n Metrics | gap len: 6 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-4.637960\n0.051318\n\n\nstd\n0.299619\n0.012608\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-6.503471\n0.078820\n\n\nstd\n0.751687\n0.020815\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-6.256985\n0.114699\n\n\nstd\n1.533914\n0.030592\n\n\n\n\n\n\n\n Metrics | gap len: 30 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-3.957636\n0.144434\n\n\nstd\n2.228843\n0.032844\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(spec_learn3[\"SWC\"], 'SWC', gaps=[6,12,24,30])\n\n\n\n\n Metrics | gap len: 6 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-7.409364\n0.040791\n\n\nstd\n0.602997\n0.008078\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-14.166100\n0.049997\n\n\nstd\n1.298232\n0.009736\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-26.025482\n0.058914\n\n\nstd\n2.035570\n0.009232\n\n\n\n\n\n\n\n Metrics | gap len: 30 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-31.526816\n0.064446\n\n\nstd\n2.601954\n0.008842\n\n\n\n\n\n\n\n\nmetrics_valid_gap_lens(learn_Aa, 'SWC', gaps=[6,12,24,30])\n\n\n\n\n Metrics | gap len: 6 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-10.621700\n0.020779\n\n\nstd\n1.637564\n0.009605\n\n\n\n\n\n\n\n Metrics | gap len: 12 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-21.012826\n0.024119\n\n\nstd\n1.456359\n0.007166\n\n\n\n\n\n\n\n Metrics | gap len: 24 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-36.451721\n0.034461\n\n\nstd\n4.163576\n0.011231\n\n\n\n\n\n\n\n Metrics | gap len: 30 | Var: SWC \n\n\n\n\n\n\n\n\n\n\n\n\nloss\nrmse\n\n\n\n\nmean\n-43.786023\n0.038519\n\n\nstd\n5.267340\n0.010467\n\n\n\n\n\n\n\nthis is pretty weird the model with gaps in all variables is performing better that the one with only partial gaps ….\nlet’s so some finetuning\n\n\n\n\ndls_SWC_30 = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = 'SWC',\n    block_len=120,\n    gap_len=30,\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=1\n).cpu()\n\n\nmodel_SWC_30 = spec_learn['SWC'].copy()\n\n\nif show_metrics: metric_valid(model_SWC_30, dls=model_SWC_30.valid)\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 3e-4, base / \"SWC_gap_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-10.998339\n-15.672505\n0.099315\n02:51\n\n\n1\n-17.301334\n-22.166437\n0.085715\n02:51\n\n\n\n\n\n\n\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 1e-3, base / \"SWC_gap_30_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-27.642977\n-34.401863\n0.056773\n02:56\n\n\n1\n-33.920003\n-38.631508\n0.050980\n02:57\n\n\n\n\n\n\n\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 1e-3, base / \"SWC_gap_30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-38.766184\n-42.612718\n0.041158\n02:55\n\n\n1\n-39.593180\n-42.984206\n0.041554\n02:52\n\n\n\n\n\n\n\n\n\nlearn_SWC_30, items_SWC_30 = train_or_load(model_SWC_30, dls_SWC_30, 2, 1e-3, base / \"SWC_gap_30_v3\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-38.766184\n-42.612718\n0.041158\n02:55\n\n\n1\n-39.593180\n-42.984206\n0.041554\n02:52\n\n\n\n\n\n\n\n\n\nshow_results(learn_SWC_30, hide_no_gap=True, items=items_SWC_30)\n\n[MeteoImpItem(i=1674, shift=30, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1693, shift=46, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1570, shift=-83, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1682, shift=54, var_sel=['SWC'], gap_len=30)]\n\n\n\n\n\n\n\n\nshow_results(learn_Aa, hide_no_gap=True, items=items_SWC_30)\n\n[MeteoImpItem(i=1674, shift=30, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1693, shift=46, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1570, shift=-83, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1682, shift=54, var_sel=['SWC'], gap_len=30)]\n\n\n\n\n\n\n\n\nshow_results(learn_A1v, hide_no_gap=True, items=items_SWC_30, dls=dls_SWC_30)\n\n[MeteoImpItem(i=1674, shift=30, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1693, shift=46, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1570, shift=-83, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1682, shift=54, var_sel=['SWC'], gap_len=30)]\n\n\n\n\n\n\n\n\nshow_results(spec_learn['SWC'], hide_no_gap=True, items=spec_items['SWC'], dls=spec_dls['SWC'])\n\n[MeteoImpItem(i=510, shift=10, var_sel=['SWC'], gap_len=243), MeteoImpItem(i=504, shift=0, var_sel=['SWC'], gap_len=109), MeteoImpItem(i=494, shift=-94, var_sel=['SWC'], gap_len=116), MeteoImpItem(i=481, shift=-17, var_sel=['SWC'], gap_len=148)]\n\n\n\n\n\n\n\n\nshow_results(learn_SWC_30)\n\n[MeteoImpItem(i=1892, shift=-50, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1667, shift=68, var_sel=['SWC'], gap_len=30), MeteoImpItem(i=1761, shift=41, var_sel=['SWC'], gap_len=30)]"
  },
  {
    "objectID": "results/train_10_feb_results.html",
    "href": "results/train_10_feb_results.html",
    "title": "Training Kalman Filter for Results - 10 Feb",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\nfrom meteo_imp.gaussian import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\nfrom fastcore.foundation import patch\nfrom pathlib import Path, PosixPath\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport random\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image\n\nfrom tqdm.auto import tqdm\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_big_path)\nhai_era = pd.read_parquet(hai_era_big_path)\n\n\nbase = here(\"analysis/results/trained_10feb\")\n\n\nbase.mkdir(exist_ok=True)\n\n\n@patch\ndef add_end(self: PosixPath, end): return self.parent / (self.name + end)\n\n\ndef train_or_load(model, dls, lr, n, path, keep=True):\n    save_models = SaveModelsBatch(times_epoch=5)\n    csv_logger = CSVLogger(path.add_end(\"log.csv\"))\n    learn = Learner(dls, model, KalmanLoss(only_gap=True), cbs = [Float64Callback, save_models, csv_logger], metrics=rmse_gap) \n    items = random.choices(dls.valid.items, k=4) \n    if path.add_end(\".pickle\").exists() and keep:\n        learn.model = torch.load(path.add_end(\".pickle\"))\n        display(csv_logger.read_log())\n        plot = Image(filename=path.add_end(\"_loss_plot.png\"))\n        display(plot)\n    else:\n        learn.fit(lr, n)\n        \n        torch.save(learn.model, path.add_end(\".pickle\"))\n        learn.recorder.plot_loss()\n        plt.savefig(path.add_end(\"_loss_plot.png\"))\n    return learn, items                           \n\n\ndef metric_valid(learn, dls=None):\n    nrmse = []\n    losses = []\n    dls = ifnone(dls, learn.dls.valid)\n    for input, target in tqdm(dls):\n        pred = learn.model(input)\n        nrmse.append(learn.metrics[0](pred, target))\n        losses.append(learn.loss_func(pred, target))\n    metric = pd.DataFrame({'loss': losses, 'rmse': nrmse})\n    return metric.agg(['mean', 'std'])\n\n\n\n\ndls_A1v = imp_dataloader(\n    df = hai,\n    control = hai_era,\n    var_sel = gen_var_sel(list(hai.columns), n_var=1),\n    block_len=100+336,\n    gap_len=gen_gap_len(3, 336),\n    bs=20,\n    control_lags=[1],\n    shifts=gen_shifts(50),\n    n_rep=5).cpu()\n\n\nmodel_A1v = KalmanFilterSR.init_local_slope_pca(len(hai.columns),len(hai.columns), df_pca = hai, pred_only_gap=True)\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 3, 1e-3, base / \"1_gap_varying_3-336_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n193.353548\n146.601647\n0.573572\n56:22\n\n\n1\n119.679036\n104.690430\n0.456970\n28:54\n\n\n2\n95.036298\n96.871684\n0.428096\n28:10\n\n\n\n\n\n\n\n\n\nlearn_A1v_3_336, items_A1v_3_336 = train_or_load(learn_A1v_3_336.model, dls_A1v_3_336, 3, 3e-4, base / \"1_gap_varying_3-336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n84.311968\n90.426712\n0.423037\n28:23\n\n\n1\n82.107866\n91.031837\n0.421232\n28:10\n\n\n2\n77.419564\n87.781942\n0.414014\n28:10\n\n\n\n\n\n\n\n\n\nmetric_valid(learn_A1v)\n\n\n\n\ntensor(0.6385)\n\n\n\n\n\n\ndls_A1v = imp_dataloader(hai, hai_era, var_sel = gen_var_sel(list(hai.columns), n_var=1), block_len=100+336, gap_len=336, bs=20, control_lags=[1], shifts=gen_shifts(100), n_rep=5).cpu()\n\n\nmodel_A1v = KalmanFilterSR.init_local_slope_pca(len(hai.columns),len(hai.columns), df_pca = hai, pred_only_gap=True)\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 3, 1e-3, base / \"1_gap_varying_336_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n380.364158\n326.784700\n0.654934\n37:33\n\n\n1\n1\n262.864349\n251.467301\n0.544807\n36:21\n\n\n2\n2\n226.285225\n224.678909\n0.510671\n37:07\n\n\n\n\n\n\n\n\n\n\n\nlearn_A1v, items_A1v = train_or_load(learn_A1v.model, dls_A1v, 3, 5e-4, base / \"1_gap_varying_336_v2\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n210.306824\n226.292560\n0.515331\n34:10\n\n\n1\n194.038559\n215.131606\n0.501236\n34:28\n\n\n2\n179.726920\n219.458045\n0.494683\n35:07\n\n\n\n\n\n\n\n\n\nmetric_valid(learn_A1v)\n\n\n\n\ntensor(0.6385)\n\n\n\n\n\n\nfrom fastcore.basics import *\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(hai.columns)):\n    print(var) # 1 week\n    spec_dls[var] = imp_dataloader(hai, hai_era, var_sel = var, block_len=500, gap_len=336, bs=20, control_lags=[1], n_rep=1, shifts=gen_shifts(50)).cpu()\n    spec_models[var] = learn_A1v.model.copy()\n    print(metric_valid(learn_A1v, dls=spec_dls[var].valid))\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 3, 2e-5, base / f\"{var}_specialized_gap_336_v2.pickle\")\n    plt.show()\n    \n\n\n\n\nTA\n\n\n\n\n\ntensor(0.1922)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-81.093778\n-104.159000\n0.166248\n06:16\n\n\n1\n-84.834773\n-106.050784\n0.165263\n06:00\n\n\n2\n-87.109282\n-102.665867\n0.166997\n06:06\n\n\n\n\n\n\n\n\nSW_IN\n\n\n\n\n\ntensor(0.4037)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n160.621421\n174.782384\n0.388784\n06:06\n\n\n1\n157.405052\n166.831662\n0.377933\n05:59\n\n\n2\n154.271775\n159.928757\n0.369818\n05:54\n\n\n\n\n\n\n\n\nVPD\n\n\n\n\n\ntensor(0.3689)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n140.376146\n117.570909\n0.328519\n06:00\n\n\n1\n138.040741\n117.735460\n0.328837\n06:00\n\n\n2\n134.452653\n114.205389\n0.325095\n06:05\n\n\n\n\n\n\n\n\nPA\n\n\n\n\n\ntensor(0.1740)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-128.848067\n-137.885327\n0.148628\n06:01\n\n\n1\n-132.304557\n-137.966789\n0.148846\n06:06\n\n\n2\n-136.150283\n-143.414496\n0.146501\n06:09\n\n\n\n\n\n\n\n\nP\n\n\n\n\n\ntensor(0.7784)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n519.043227\n472.439331\n0.783632\n06:06\n\n\n1\n517.332604\n471.824805\n0.781104\n06:01\n\n\n2\n518.413386\n471.067908\n0.777673\n05:53\n\n\n\n\n\n\n\n\nWS\n\n\n\n\n\ntensor(0.8421)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n333.000027\n458.373029\n0.827292\n05:54\n\n\n1\n331.586878\n456.518645\n0.820882\n05:55\n\n\n2\n330.133510\n455.193754\n0.816275\n05:50\n\n\n\n\n\n\n\n\nLW_IN\n\n\n\n\n\ntensor(0.5680)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n269.813482\n284.994682\n0.554712\n05:50\n\n\n1\n266.140870\n280.236376\n0.546786\n05:50\n\n\n2\n263.271720\n275.480894\n0.538800\n06:05\n\n\n\n\n\n\n\n\n\nspec_learn2 = {}\nfor var in tqdm(['SW_IN', 'LW_IN']):\n    print(var) # 1 week\n    print(metric_valid(spec_learn[var].model, dls=spec_dls[var].valid))\n    spec_learn2[var], _ = train_or_load(spec_learn[var].model, spec_dls[var], 3, 5e-5, base / f\"{var}_specialized_gap_336_v3.pickle\")\n    plt.show()\n    \n\n\n\n\nSW_IN\n\n\n\n\n\ntensor(0.4037)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n147.081102\n153.592512\n0.361692\n05:47\n\n\n1\n144.984204\n147.233722\n0.353274\n05:55\n\n\n2\n141.840183\n142.120207\n0.347047\n05:50\n\n\n\n\n\n\n\n\nLW_IN\n\n\n\n\n\ntensor(0.5680)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n257.296672\n271.268047\n0.532025\n05:54\n\n\n1\n254.709431\n267.527692\n0.526062\n05:57\n\n\n2\n251.984433\n264.003732\n0.520479\n05:49\n\n\n\n\n\n\n\n\n\n\n\n\ndls_A1v_nc = imp_dataloader(hai, hai_era, var_sel = gen_var_sel(list(hai.columns), n_var=1), block_len=100+336, gap_len=336, bs=5, control_lags=[1], shifts=gen_shifts(100), n_rep=5).cpu()\n\n\nmodel_A1v_nc = KalmanFilterSR.init_local_slope_pca(len(hai.columns),len(hai.columns), df_pca = hai, pred_only_gap=True, use_control=False)\n\n\nlearn_A1v_nc, items_A1v_nc = train_or_load(model_A1v_nc, dls_A1v_nc, 3, 1e-3, base / \"1_gap_varying_336_no_control_v1\", keep=False)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n378.058011\n371.085483\n0.681375\n43:37\n\n\n1\n367.492877\n360.823709\n0.665902\n44:50\n\n\n2\n339.345100\n364.711690\n0.674149\n45:11\n\n\n\n\n\n\n\n\n\n\n\n\ndls_Aa = imp_dataloader(hai, hai_era, var_sel = list(hai.columns), block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Aa = learn_A1v_3_336.model.copy()\n\n\nlearn_Aa, items_Aa = train_or_load(model_Aa, dls_Aa, 2, 3e-4, base / \"All_gap_all_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n68.920740\n73.037300\n0.457492\n24:24\n\n\n1\n50.691507\n59.431252\n0.440490\n24:14\n\n\n\n\n\n\n\n\n\n\n\n\ndls_A1v30 = imp_dataloader(hai, hai_era, var_sel = gen_var_sel(list(hai.columns), n_var=1), block_len=100+30, gap_len=30, bs=20, control_lags=[1], shifts=gen_shifts(50), n_rep=5).cpu()\n\n\nmodel_A1v30 = learn_A1v_3_336.model.copy()\n\nloss generic model\n\nmetric_valid(learn_A1v, dls=dls_A1v30.valid)\n\n\n\n\n\nlearn_A1v30, items_A1v30 = train_or_load(model_A1v30, dls_A1v30, 2, 1e-3, base / \"1_gap_varying_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n10.150969\n10.634211\n0.350033\n12:53\n\n\n1\n6.587447\n8.556643\n0.334815\n12:49\n\n\n\n\n\n\n\n\n\n\n\ngap_models = {}\ngap_dls = {}\ngap_learn = {}\ngap_items = {}\nfor gap_len in tqdm(list():\n    print(f\"Gap len: {gep_len}\")\n    gap_dls[gap_len] = imp_dataloader(hai, hai_era, var_sel = gen_var_sel(list(hai.columns), n_var=1), block_len=gap_len+100, gap_len=gap_len, bs=20, control_lags=[1], n_rep=4, shifts=gen_shifts(50)).cpu()\n    gap_models[gap_len] = learn_A1v.model.copy()\n    print(metric_valid(learn_A1v, dls=spec_dls[var].valid))\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 3, 2e-5, base / f\"{var}_specialized_gap_336_v2.pickle\")\n    plt.show()\n    \n\n\n\n\nTA\n\n\n\n\n\ntensor(0.1922)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-81.093778\n-104.159000\n0.166248\n06:16\n\n\n1\n-84.834773\n-106.050784\n0.165263\n06:00\n\n\n2\n-87.109282\n-102.665867\n0.166997\n06:06\n\n\n\n\n\n\n\n\nSW_IN\n\n\n\n\n\ntensor(0.4037)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n160.621421\n174.782384\n0.388784\n06:06\n\n\n1\n157.405052\n166.831662\n0.377933\n05:59\n\n\n2\n154.271775\n159.928757\n0.369818\n05:54\n\n\n\n\n\n\n\n\nVPD\n\n\n\n\n\ntensor(0.3689)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n140.376146\n117.570909\n0.328519\n06:00\n\n\n1\n138.040741\n117.735460\n0.328837\n06:00\n\n\n2\n134.452653\n114.205389\n0.325095\n06:05\n\n\n\n\n\n\n\n\nPA\n\n\n\n\n\ntensor(0.1740)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-128.848067\n-137.885327\n0.148628\n06:01\n\n\n1\n-132.304557\n-137.966789\n0.148846\n06:06\n\n\n2\n-136.150283\n-143.414496\n0.146501\n06:09\n\n\n\n\n\n\n\n\nP\n\n\n\n\n\ntensor(0.7784)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n519.043227\n472.439331\n0.783632\n06:06\n\n\n1\n517.332604\n471.824805\n0.781104\n06:01\n\n\n2\n518.413386\n471.067908\n0.777673\n05:53\n\n\n\n\n\n\n\n\nWS\n\n\n\n\n\ntensor(0.8421)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n333.000027\n458.373029\n0.827292\n05:54\n\n\n1\n331.586878\n456.518645\n0.820882\n05:55\n\n\n2\n330.133510\n455.193754\n0.816275\n05:50\n\n\n\n\n\n\n\n\nLW_IN\n\n\n\n\n\ntensor(0.5680)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n269.813482\n284.994682\n0.554712\n05:50\n\n\n1\n266.140870\n280.236376\n0.546786\n05:50\n\n\n2\n263.271720\n275.480894\n0.538800\n06:05\n\n\n\n\n\n\n\n\n# === OLD ===\n\n\n\n\nModel Av (All varying) all variables with a varing numbers of variables\n\ndls_Av = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Av = KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = haiB, pred_only_gap=True, use_conditional=False)\n\n\nlearn_Av, items_Av = train_or_load(model_Av, dls_Av, 2, 1e-3, base / \"All_gap_varying_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n26.214190\n30.346192\n0.386696\n36:35\n\n\n1\n1\n17.806891\n21.144149\n0.366064\n38:40\n\n\n\n\n\n\n\n\n\n\n\nmetric_valid(learn_Av)\n\n\nmodel_Av.copy??\n\n\nlearn_Av, items_Av = train_or_load(model_Av, dls_Av, 1, 5e-4, base / \"All_gap_varying_30_v2\")\n\n\nshow_results(learn_Av, items=items_Av, control_map=control_map)\n\n\n\n\ndls_A1v = imp_dataloader(haiB[:30_000], hai_eraB, var_sel = gen_var_sel(list(haiB.columns), n_var=1), block_len=800, gap_len=672, bs=5, control_lags=[1], shifts=gen_shifts(300), n_rep=1).cpu()\n\n\nmodel_A1v = KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = haiB, pred_only_gap=True, use_conditional=False)\n\n\nlearn_A1v, items_A1v = train_or_load(model_A1v, dls_A1v, 2, 1e-3, base / \"1_gap_varying_672_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n1566.155526\n1446.904870\n0.980710\n01:21\n\n\n1\n1421.453610\n1254.100269\n0.943964\n01:20\n\n\n\n\n\n\n\n\n\n\n\n\n\ndls_Aa = imp_dataloader(haiB, hai_eraB, var_sel = list(haiB.columns), block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=1).cpu()\n\n\nmodel_Aa = model_Av.copy()\n\n\nlearn_Aa, items_Aa = train_or_load(model_Aa, dls_Aa, 2, 3e-4, base / \"All_gap_all_30_v1\", keep=False)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n33.282077\n39.916592\n0.410030\n02:31\n\n\n1\n31.570166\n40.208255\n0.410228\n02:20\n\n\n\n\n\n\n\n\n\nshow_results(learn_Aa, items=items_Aa, control_map=control_map)\n\n[MeteoImpItem(i=1647, shift=-60, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=30), MeteoImpItem(i=1765, shift=-60, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=30), MeteoImpItem(i=1785, shift=-60, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=30), MeteoImpItem(i=1717, shift=-60, var_sel=['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN'], gap_len=30)]\n\n\n\n\n\n\n\n\n\n\nModel Av (All varying) all variables with a varing numbers of variables\n\ndls_Av12 = imp_dataloader(haiB, hai_eraB, var_sel = gen_var_sel(list(haiB.columns)), block_len=80, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_Av12 = model_Av.copy()\n\n\nlearn_Av12, items_Av12 = train_or_load(model_Av12, dls_Av12, 1, 1e-4, base / \"All_gap_varying_12_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-5.242486\n-5.854822\n0.276692\n13:14\n\n\n\n\n\n\n\n\n\nshow_results(learn_Av12, items=items_Av12, control_map=control_map)\n\n[MeteoImpItem(i=2681, shift=-8, var_sel=['WS', 'VPD', 'SW_IN', 'P', 'TA', 'LW_IN'], gap_len=12), MeteoImpItem(i=2429, shift=-8, var_sel=['WS', 'PA', 'TA', 'LW_IN', 'VPD', 'P'], gap_len=12), MeteoImpItem(i=2846, shift=-8, var_sel=['WS', 'P', 'VPD'], gap_len=12), MeteoImpItem(i=2377, shift=-40, var_sel=['LW_IN', 'PA'], gap_len=12)]\n\n\n\n\n\n\n\n\n\n\n\ndls_TA = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_TA = model_Av.copy()\n\n\nlearn_TA, save_models_TA, items_TA = train_or_load(model_TA, dls_TA, 2, 2e-4, base / \"TA_30_v1.pickle\")\n\nFileNotFoundError: [Errno 2] No such file or directory: '/home/simone/Documents/uni/Thesis/GPFA_imputation/analysis/results/trained_8feb/TA_30_v1.picklelog.csv'\n\n\n\nlearn_TA, items_TA, loggerTA = train_or_load(model_TA, dls_TA, 1, 2e-4, base / \"TA_30_v2.pickle\")\n\n\nloggerTA.read_log()\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n0\n-22.20629\n-22.448926\n0.097302\n11:15\n\n\n\n\n\n\n\n\nshow_results(learn_TA, items=items_TA, control_map=control_map, hide_no_gap=True)\n\n\n\nModel TA\n\ndls_TA96 = imp_dataloader(haiB, hai_eraB, var_sel = 'TA', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=1).cpu()\n\n\nmodel_TA96 = learn_Av.model.copy()\n\n\nlearn_TA96, save_models_TA96, items_TA96 = train_or_load(model_TA96, dls_TA96, 2, 2e-4, base / \"TA_96_v1.pickle\")\n\n\nshow_results(learn_TA96, items=items_TA96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in list(haiB.columns):\n    print(var)\n    spec_dls[var] = imp_dataloader(haiB[:40000], hai_eraB, var_sel = var, block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=1).cpu()\n    spec_models[var] = learn_Av.model.copy()\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 2, 2e-4, base / f\"{var}_specialized_v1.pickle\")\n   \n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-39.692397\n-42.531067\n0.131699\n00:42\n\n\n1\n-45.283476\n-33.323406\n0.144391\n00:37\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n49.694907\n6.908584\n0.222646\n00:38\n\n\n1\n45.584341\n10.284428\n0.239156\n00:39\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n27.420796\n-6.238979\n0.190993\n00:40\n\n\n1\n24.375462\n-4.962333\n0.200030\n00:38\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-111.954325\n-106.969053\n0.075821\n00:42\n\n\n1\n-118.061853\n-119.208259\n0.065790\n00:40\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n122.597622\n113.888477\n0.592852\n00:41\n\n\n1\n124.965769\n113.211150\n0.584031\n00:40\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n85.440672\n82.872583\n0.552278\n00:39\n\n\n1\n85.644038\n82.978043\n0.553163\n00:38\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n31.828594\n27.359172\n0.290250\n00:43\n\n\n1\n29.653514\n26.873250\n0.288271\n00:38\n\n\n\n\n\n\n\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(haiB.columns)):\n    print(var)\n    spec_dls[var] = imp_dataloader(haiB[:30000], hai_eraB, var_sel = var, block_len=500, gap_len=192, bs=20, control_lags=[1], n_rep=1).cpu()\n    spec_models[var] = learn_Av.model.copy()\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_192_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n90.643715\n-16.383068\n0.177646\n00:36\n\n\n\n\n\n\n\n\nSW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n62.269008\n103.003446\n0.385411\n00:40\n\n\n\n\n\n\n\n\nVPD\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n95.957126\n285.851886\n0.495180\n00:41\n\n\n\n\n\n\n\n\nPA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n-51.859173\n-69.926766\n0.125275\n00:43\n\n\n\n\n\n\n\n\nP\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n289.581748\n212.260866\n0.520403\n00:40\n\n\n\n\n\n\n\n\nWS\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n208.126112\n177.603145\n0.593094\n00:39\n\n\n\n\n\n\n\n\nLW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n63.759073\n62.438796\n0.308980\n00:41\n\n\n\n\n\n\n\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(haiB.columns)):\n    print(var) # 1 week\n    spec_dls[var] = imp_dataloader(haiB[:30000], hai_eraB, var_sel = var, block_len=500, gap_len=336, bs=20, control_lags=[1], n_rep=1).cpu()\n    spec_models[var] = learn_Av.model.copy()\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_336_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n244.441678\n-39.264206\n0.179186\n00:42\n\n\n\n\n\n\n\n\nSW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n92.375716\n216.062288\n0.416458\n00:43\n\n\n\n\n\n\n\n\nVPD\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n182.519898\n378.167553\n0.452476\n00:45\n\n\n\n\n\n\n\n\nPA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n22.902731\n-134.399585\n0.125682\n00:45\n\n\n\n\n\n\n\n\nP\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n449.964537\n412.074092\n0.671240\n00:44\n\n\n\n\n\n\n\n\nWS\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n373.671535\n304.890972\n0.592034\n00:45\n\n\n\n\n\n\n\n\nLW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n108.351227\n109.968726\n0.309430\n00:44\n\n\n\n\n\n\n\n\n\nspec_models = {}\nspec_dls = {}\nspec_learn = {}\nspec_items = {}\nfor var in tqdm(list(haiB.columns)):\n    print(var) # 2 weeks\n    spec_dls[var] = imp_dataloader(haiB[:30000], hai_eraB, var_sel = var, block_len=1000, gap_len=672, bs=20, control_lags=[1], n_rep=1).cpu()\n    spec_models[var] = learn_Av.model.copy()\n    spec_learn[var], spec_items[var] = train_or_load(spec_models[var], spec_dls[var], 1, 1e-4, base / f\"{var}_specialized_gap_672_v1.pickle\")\n    plt.show()\n    \n\n\n\n\nTA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n657.307163\n118.648190\n0.204232\n01:09\n\n\n\n\n\n\n\n\nSW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n262.173894\n525.063575\n0.456621\n01:11\n\n\n\n\n\n\n\n\nVPD\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n493.652948\n311.636245\n0.362475\n01:12\n\n\n\n\n\n\n\n\nPA\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n82.584014\n-149.931723\n0.136190\n01:14\n\n\n\n\n\n\n\n\nP\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n970.131864\n821.798107\n0.729170\n01:18\n\n\n\n\n\n\n\n\nWS\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n806.418451\n579.607119\n0.573610\n01:16\n\n\n\n\n\n\n\n\nLW_IN\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n228.687149\n231.744756\n0.317265\n01:15\n\n\n\n\n\n\n\n\n\ndef train_specialized(gap_len=30, block_len=120, bs=20, df=haiB, control=hai_eraB):\n    \n\n\n\n\n\n\ndls_SW_IN = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN', block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN = model_Av.copy()\n\n\nlearn_SW_IN, items_SW_IN = train_or_load(model_SW_IN, dls_SW_IN,  2, 2e-4, base / \"SW_IN_30_v1\")\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n8.902548\n7.420177\n0.267329\n14:57\n\n\n1\n5.782914\n5.721588\n0.249692\n10:34\n\n\n\n\n\n\n\n\n\nshow_results(learn_SW_IN, items=items_SW_IN, control_map=control_map, hide_no_gap=True)\n\n[MeteoImpItem(i=1588, shift=-60, var_sel=['SW_IN'], gap_len=30), MeteoImpItem(i=1863, shift=12, var_sel=['SW_IN'], gap_len=30), MeteoImpItem(i=1704, shift=36, var_sel=['SW_IN'], gap_len=30), MeteoImpItem(i=1545, shift=12, var_sel=['SW_IN'], gap_len=30)]\n\n\n\n\n\n\n\n\n\nModel SW_IN (All varying) all variables with a varing numbers of variables\n\ndls_SW_IN96 = imp_dataloader(haiB, hai_eraB, var_sel = 'SW_IN', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_SW_IN96 = model_SW_IN.copy()\n\n\nlearn_SW_IN96, save_models_SW_IN96, items_SW_IN96 = train_or_load(model_SW_IN96, dls_SW_IN96, 2, 2e-4, base / \"SW_IN_96_v1.pickle\")\n\n\nshow_results(learn_SW_IN96, items=items_SW_IN96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\n\ndls_LW_IN = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN', block_len=100, gap_len=12, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN = model_Av.copy()\n\n\nlearn_LW_IN, save_models_LW_IN, items_LW_IN = train_or_load(model_LW_IN, dls_LW_IN, 2, 2e-4, base / \"LW_IN_12_v1.pickle\")\n\n\nshow_results(learn_LW_IN, items=items_LW_IN, control_map=control_map, hide_no_gap=True)\n\n\n\nModel LW_IN (All varying) all variables with a varing numbers of variables\n\ndls_LW_IN96 = imp_dataloader(haiB, hai_eraB, var_sel = 'LW_IN', block_len=350, gap_len=96, bs=20, control_lags=[1], n_rep=5).cpu()\n\n\nmodel_LW_IN96 = model_LW_IN.copy()\n\n\nlearn_LW_IN96, save_models_LW_IN96, items_LW_IN96 = train_or_load(model_LW_IN96, dls_LW_IN96, 2, 2e-4, base / \"LW_IN_96_v1.pickle\")\n\n\nshow_results(learn_LW_IN96, items=items_LW_IN96, control_map=control_map, hide_no_gap=True)\n\n\n\n\n\n\n\nModel Av_nc (All varying) all variables with a varing numbers of variables\n\ndls_Av_nc = imp_dataloader(haiB, \n                           control = hai_eraB.sample(frac=1).reset_index(drop=True).set_index(hai_eraB.index), #reshuffle so cannot use control\n                           var_sel = gen_var_sel(list(haiB.columns)), block_len=120, gap_len=30, bs=20, control_lags=[1], n_rep=10).cpu()\n\n\nmodel_Av_nc =  KalmanFilterSR.init_local_slope_pca(len(haiB.columns),len(haiB.columns), df_pca = haiB, pred_only_gap=True, use_conditional=False, use_control=False)\n\n\nlearn_Av_nc, items_Av_nc = train_or_load(model_Av_nc, dls_Av_nc, 2, 1e-3, base / \"All_gap_varying_30_no_control_v1\")\n\n\n\n\n\n\n    \n      \n      50.00% [1/2 38:45&lt;38:45]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n48.175866\n46.578383\n0.431465\n38:45\n\n\n\n\n\n    \n      \n      1.19% [9/758 00:35&lt;49:33 51.5082]\n    \n    \n\n\nKeyboardInterrupt: \n\n\n\nshow_results(learn_Av_nc, items=items_Av_nc, control_map=control_map)"
  },
  {
    "objectID": "Additional latent kernel.html",
    "href": "Additional latent kernel.html",
    "title": "Additional latent kernel",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.gpfa.data_preparation import *\nfrom meteo_imp.gpfa.results import *\nfrom meteo_imp.gpfa.gpfa import *\nfrom meteo_imp.gpfa.results import _display_as_row\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import *\nfrom gpytorch.kernels import *\nimport gpytorch\n\ncp = here() / \".cache\" / \"add_kernel.pickle\"\n\n\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=1000)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-21 18:00:00\n-0.81\n0.0\n1.892\n\n\n2000-01-21 18:30:00\n-0.81\n0.0\n1.744\n\n\n2000-01-21 19:00:00\n-0.90\n0.0\n2.118\n\n\n2000-01-21 19:30:00\n-1.09\n0.0\n2.528\n\n\n2000-01-21 20:00:00\n-1.23\n0.0\n2.677\n\n\n\n\n1000 rows × 3 columns\n\n\n\n\n\n\n\ndef _get_lengthscale_info(kernel: RBFKernel, suffix=''):\n        ls = kernel.lengthscale.detach().item()\n        return pd.DataFrame({\n            'lengthscale'+suffix: [ls]\n        }) \n\n\ndef _get_outscale_info(kernel: ScaleKernel, suffix=''):\n        ls = kernel.outputscale.detach().item()\n        return pd.DataFrame({\n            'outscale'+suffix: [ls]\n        }) \n\n\nclass GPFAMultiRbf(GPFA):\n    latent_kernel = lambda x: AdditiveKernel(RBFKernel(), ScaleKernel(RBFKernel()))\n    \n    \n    def get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -&gt; dict[str, pd.DataFrame]:\n        \"Model info for a GPFA with a RBFKernel\"\n        out = {}\n\n        latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n\n        out[\"Lambda\"] = pd.concat([\n            None if var_names is None else pd.Series(var_names),\n            pd.DataFrame(\n                self.covar_module.Lambda.detach().cpu().numpy(),\n                columns=latent_names)],\n            axis=1)\n\n        ls_all = []\n        l_kernels = self.covar_module.latent_kernels\n        for kernel in list(l_kernels):\n            ls_s = [_get_lengthscale_info(kernel.kernels[0], \"_k0\")]\n            ls_s.append(_get_lengthscale_info(kernel.kernels[1].base_kernel, \"_k1\")) # this is a scale kernel\n            ls_all.append(pd.concat(ls_s, axis=1)) # attach multiple columns\n\n        ls_all = pd.concat(ls_all)\n        ls_all.insert(0, 'latent', latent_names)\n        out[\"Lengthscale\"] = ls_all\n        \n        os_all = []\n        l_kernels = self.covar_module.latent_kernels\n        for kernel in list(l_kernels):\n            os_s = [pd.DataFrame({'outscale_k0': [1] })] # there is no scaling here\n            os_s.append(_get_outscale_info(kernel.kernels[1], \"_k1\"))\n            os_all.append(pd.concat(os_s, axis=1)) # attach multiple columns\n\n        os_all = pd.concat(os_all)\n        os_all.insert(0, 'latent', latent_names)\n        out[\"Outscale\"] = os_all\n\n        psi = self.covar_module.psi.detach().cpu().numpy()\n        out[\"Psi\"] = pd.DataFrame({\n            'variable': var_names,\n            'psi': psi \n        })\n\n        out[\"Likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.item()]})\n\n        return out\n\n\nk = GPFAMultiRbf(torch.tensor([1,2,3]), torch.tensor([1,2,3]), gpytorch.likelihoods.GaussianLikelihood(), 2)\n\n\nk\n\nGPFAMultiRbf(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): AdditiveKernel(\n        (kernels): ModuleList(\n          (0): RBFKernel(\n            (raw_lengthscale_constraint): Positive()\n          )\n          (1): ScaleKernel(\n            (base_kernel): RBFKernel(\n              (raw_lengthscale_constraint): Positive()\n            )\n            (raw_outputscale_constraint): Positive()\n          )\n        )\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\n\n_display_as_row(k.get_info())\n\n\n  Lambda \n\n\n\n\nz0\n\n\n\n\n0.5875\n\n\n0.4568\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale_k0\nlengthscale_k1\n\n\n\n\nz0\n0.6931\n0.6931\n\n\n\n\n\nOutscale\n\n\n\nlatent\noutscale_k0\noutscale_k1\n\n\n\n\nz0\n1\n0.6931\n\n\n\n\n\nPsi\n\n\n\nvariable\npsi\n\n\n\n\nNone\n0.6931\n\n\nNone\n0.6931\n\n\n\n\n\nLikelihood\n\n\n\nnoise\n\n\n\n\n0.6932\n\n\n\n\n\n \n\n\n\nreset_seed()\ndata = GPFADataTest(hai[:150]).add_random_missing()\n\n\n\n\n\n@cache_disk(cp)\ndef compute_small():\n    reset_seed()\n    data = GPFADataTest(hai[:150]).add_random_missing()\n    imp = GPFAImputationExplorer(data.data, latent_dims=3, model=GPFAMultiRbf)\n    return imp.fit().to_result(data.data_compl_tidy)\n\n\nres_small = compute_small()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n@cache_disk(cp)\ndef compute_large():\n    reset_seed()\n    data = GPFADataTest(hai[:1500]).add_random_missing()\n    imp = GPFAImputationExplorer(data.data, latent_dims=2, model=GPFAMultiRbf)\n    return imp.fit(), data\n\n\n# imp_large = compute_large()\n\n\n\n\n\n\nres_small.display_results(plot_args={'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9917\n\n\nSW_IN\n0.9725\n\n\nVPD\n0.9708\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\n\n\n\n\nTA\n0.0842\n\n\nSW_IN\n6.1635\n\n\nVPD\n0.0437\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9833\n\n\nSW_IN\n0.9712\n\n\nVPD\n0.9618\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\n\n\n\n\nTA\n0.1155\n\n\nSW_IN\n8.3501\n\n\nVPD\n0.0538\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\n0\nz0\nz1\nz2\n\n\n\n\nTA\n-0.1227\n0.8634\n0.1707\n\n\nSW_IN\n0.8996\n0.5550\n-0.6289\n\n\nVPD\n0.5291\n0.7391\n0.5174\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale_k0\nlengthscale_k1\n\n\n\n\nz0\n5.7209\n5.1237\n\n\nz1\n6.9906\n6.2228\n\n\nz2\n4.5014\n4.3961\n\n\n\n\n\nOutscale\n\n\n\nlatent\noutscale_k0\noutscale_k1\n\n\n\n\nz0\n1\n0.1827\n\n\nz1\n1\n0.1025\n\n\nz2\n1\n0.2387\n\n\n\n\n\nPsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0014\n\n\nSW_IN\n0.0344\n\n\nVPD\n0.0238\n\n\n\n\n\nLikelihood\n\n\n\nnoise\n\n\n\n\n0.0050"
  },
  {
    "objectID": "var_distribution.html",
    "href": "var_distribution.html",
    "title": "Variable distribution",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport polars as pl\n\nfrom meteo_imp.fluxnet.gap_finder import scan_fluxnet_csv\n\nfrom meteo_imp.utils import cache_disk\n\n\n\nload Hainich dataset\n\nhai_path = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\"\n# hai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=20_000)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = scan_fluxnet_csv(hai_path, convert_dates=True).rename(meteo_vars).select([pl.col(\"end\").alias(\"time\"), *meteo_vars.values()])\n\nhai.fetch(10)\n\n\n\n\nshape: (10, 5)\ntime\nTA\nSW_IN\nLW_IN\nVPD\ndatetime[μs]\nf64\nf64\nf64\nf64\n2000-01-01 00:30:00\n-0.6\n0.0\n302.475\n0.222\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.09\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.11\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n2000-01-01 03:00:00\n-0.4\n0.0\n301.677\n0.111\n2000-01-01 03:30:00\n-0.36\n0.0\n301.677\n0.109\n2000-01-01 04:00:00\n-0.35\n0.0\n301.677\n0.107\n2000-01-01 04:30:00\n-0.28\n0.0\n308.046\n0.122\n2000-01-01 05:00:00\n-0.27\n0.0\n308.046\n0.138\n\n\n\n\n\nhai_td = hai.melt('time')\n\n\nhai_td.fetch(3)\n\n\n\n\nshape: (12, 3)\ntime\nvariable\nvalue\ndatetime[μs]\nstr\nf64\n2000-01-01 00:30:00\n\"TA\"\n-0.6\n2000-01-01 01:00:00\n\"TA\"\n-0.65\n2000-01-01 01:30:00\n\"TA\"\n-0.58\n2000-01-01 00:30:00\n\"SW_IN\"\n0.0\n2000-01-01 01:00:00\n\"SW_IN\"\n0.0\n2000-01-01 01:30:00\n\"SW_IN\"\n0.0\n2000-01-01 00:30:00\n\"LW_IN\"\n302.475\n2000-01-01 01:00:00\n\"LW_IN\"\n302.475\n2000-01-01 01:30:00\n\"LW_IN\"\n301.677\n2000-01-01 00:30:00\n\"VPD\"\n0.222\n2000-01-01 01:00:00\n\"VPD\"\n0.122\n2000-01-01 01:30:00\n\"VPD\"\n0.09\n\n\n\n\n\n\n\n\nhai.drop('time').collect().to_pandas().hist(figsize=(15,10));\n\n\n\n\n\n# should to the binning before the plot\n# alt.Chart(hai_td.collect().to_pandas()).mark_line().encode(\n#     x = 'value',\n#     y = 'density()',\n#     facet = alt.Facet('variable', columns=2)\n# )\n\n\n\n\nCode inspired from source: https://towardsdatascience.com/altair-plot-deconstruction-visualizing-the-correlation-structure-of-weather-data-38fb5668c5b1\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_p.corr()\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\n\n\nTA\n1.000000\n0.432321\n0.639556\n0.735412\n\n\nSW_IN\n0.432321\n1.000000\n0.126278\n0.533506\n\n\nLW_IN\n0.639556\n0.126278\n1.000000\n0.270424\n\n\nVPD\n0.735412\n0.533506\n0.270424\n1.000000\n\n\n\n\n\n\n\n\ndef corr_mask(size):\n    corr_mask = np.zeros((size,size), dtype=bool)\n    for i in range(size):\n        for j in range(size):\n            corr_mask[i,j] = True if i &gt;= j else False\n    return corr_mask\n\n\ncorr_mask(len(hai_p.columns))\n\narray([[ True, False, False, False],\n       [ True,  True, False, False],\n       [ True,  True,  True, False],\n       [ True,  True,  True,  True]])\n\n\n\nhai_p = hai_p[sorted(hai_p.columns)] # need to properly plot half a corr matrix\n\n\ncor_hai = (hai_p\n              .corr().mask(~corr_mask(len(hai_p.columns))).stack()\n              .reset_index()     # The stacking results in an index on the correlation values, we need the index as normal columns for Altair\n              .rename(columns={0: 'correlation', 'level_0': 'variable', 'level_1': 'variable2'}))\ncor_hai['correlation_label'] = cor_hai['correlation'].map('{:.2f}'.format)  # Round to 2 decimal\ncor_hai\n\n\n\n\n\n\n\n\nvariable\nvariable2\ncorrelation\ncorrelation_label\n\n\n\n\n0\nLW_IN\nLW_IN\n1.000000\n1.00\n\n\n1\nSW_IN\nLW_IN\n0.126278\n0.13\n\n\n2\nSW_IN\nSW_IN\n1.000000\n1.00\n\n\n3\nTA\nLW_IN\n0.639556\n0.64\n\n\n4\nTA\nSW_IN\n0.432321\n0.43\n\n\n5\nTA\nTA\n1.000000\n1.00\n\n\n6\nVPD\nLW_IN\n0.270424\n0.27\n\n\n7\nVPD\nSW_IN\n0.533506\n0.53\n\n\n8\nVPD\nTA\n0.735412\n0.74\n\n\n9\nVPD\nVPD\n1.000000\n1.00\n\n\n\n\n\n\n\n\nimport numpy as np\n\ndef compute_2d_histogram(var1, var2, df, density=True, bins=20):\n    H, xedges, yedges = np.histogram2d(df[var1], df[var2], bins=bins, density=density)\n    H[H == 0] = np.nan\n\n    # Create a nice variable that shows the bin boundaries\n    \n    x_width = xedges[1] - xedges[0] # all bins have same width\n    xedges = pd.Series(xedges[:-1] + x_width /2)\n    \n    y_width = yedges[1] - yedges[0] # all bins have same width\n    yedges = pd.Series(yedges[:-1] + y_width /2)\n    \n    # Cast to long format using melt\n    res = pd.DataFrame(H, \n                       index=yedges, \n                       columns=xedges).reset_index().melt(\n                            id_vars='index'\n                       ).rename(columns={'index': 'value2', \n                                         'value': 'count',\n                                         'variable': 'value'})\n    \n\n    res['variable'] = var1\n    res['variable2'] = var2 \n    return res.dropna() # Drop all combinations for which no values where found\n\n\nh, xe, ye = np.histogram2d(hai_p['VPD'], hai_p['TA'])\n\n\nx_width = xe[1] - xe[0] # all bins have same width\nxe = pd.Series(xe + x_width /2)\n\n\nh.shape\n\n(10, 10)\n\n\n\nxe\n\n0      2.38335\n1      7.15005\n2     11.91675\n3     16.68345\n4     21.45015\n5     26.21685\n6     30.98355\n7     35.75025\n8     40.51695\n9     45.28365\n10    50.05035\ndtype: float64\n\n\n\nhai_binned = pd.concat([compute_2d_histogram(var1, var2, hai_p) for var1 in meteo_vars.values() for var2 in meteo_vars.values()])\nhai_binned.head()\n\n\n\n\n\n\n\n\nvalue2\nvalue\ncount\nvariable\nvariable2\n\n\n\n\n0\n-17.19025\n-17.19025\n0.000083\nTA\nTA\n\n\n21\n-14.47075\n-14.47075\n0.000224\nTA\nTA\n\n\n42\n-11.75125\n-11.75125\n0.000574\nTA\nTA\n\n\n63\n-9.03175\n-9.03175\n0.001188\nTA\nTA\n\n\n84\n-6.31225\n-6.31225\n0.003781\nTA\nTA\n\n\n\n\n\n\n\n\n# Define selector\nvar_sel_cor = alt.selection_single(fields=['variable', 'variable2'], clear=False, \n                                  init={'variable': 'TA', 'variable2': 'SW_IN'})\n\n# Define correlation heatmap\nbase = alt.Chart(cor_hai).encode(\n    x='variable2:O',\n    y='variable:O'    \n)\n\ntext = base.mark_text().encode(\n    text='correlation_label',\n    color=alt.condition(\n        alt.datum.correlation &gt; 0.5, \n        alt.value('white'),\n        alt.value('black')\n    )\n)\n\ncor_plot = base.mark_rect().encode(\n    color=alt.condition(var_sel_cor, alt.value('pink'), 'correlation:Q')\n).add_selection(var_sel_cor)\n\n# Define 2d binned histogram plot\nscat_plot = alt.Chart(hai_binned).transform_filter(\n    var_sel_cor\n).mark_rect().encode(\n    alt.X('value:N', axis=alt.Axis(format=\".4\")), \n    alt.Y('value2:N', axis=alt.Axis(format=\".4\"), sort='descending'),\n    alt.Color('count:Q', scale=alt.Scale(scheme='blues'))\n)\n\n# Combine all plots. hconcat plots both side-by-side \nalt.hconcat((cor_plot + text).properties(width=350, height=350), scat_plot.properties(width=350, height=350)).resolve_scale(color='independent')\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_td_p = hai_td.collect().to_pandas().set_index('time')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPFA Imputation",
    "section": "",
    "text": "GPFA Imputation\nThis is the homepage of the website that contains all the analysis. Use the sidebar for nagivation\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nAdditional latent kernel\n\n\n\n\nAnalyze gaps fluxnet\n\n\n\n\nCode for presentation of 18th January\n\n\n\n\nCompare Loss functions\n\n\n\n\nExploration of Loglikelihood computations\n\n\n\n\nFill one day gap in SW_IN\n\n\n\n\nFilter loss - (float32)\n\n\n\n\nGap Length Variation\n\n\n\n\nHainich with ERA-Interim\n\n\n\n\nHainich with ERA-Interim\n\n\n\n\nHainich with ERA-Interim controls\n\n\n\n\nInit Lambda\n\n\n\n\nKalman Filter Hainich\n\n\n\n\nKernel visualization\n\n\n\n\nLocal Level Hainich\n\n\n\n\nLog Transform\n\n\n\n\nMeterological Time series Imputation using Kalman Filters\n\n\n\n\nMultiple Latent\n\n\n\n\nMultiple latent …\n\n\n\n\nNotes\n\n\n\n\nPlotting for results\n\n\n\n\nPlotting for results\n\n\n\n\nPositive semi-definite\n\n\n\n\nProcess Noise estimation\n\n\n\n\nResults\n\n\n\n\nSimple GP Hainich\n\n\n\n\nSingle latent\n\n\n\n\nTrain Kalman filter using Fastai using float64\n\n\n\n\nTrain multiple latents\n\n\n\n\nTraining Kalman Filter for Results\n\n\n\n\nTraining Kalman Filter for Results - 10 Feb\n\n\n\n\nTraining Kalman Filter for Results - 12 Feb\n\n\n\n\nTraining Kalman Filter for Results - 14 Feb\n\n\n\n\nTraining Kalman Filter for Results - 21 Feb\n\n\n\n\nTraining Kalman Filter for Results - 6 Feb\n\n\n\n\nTraining Kalman Filter for Results - 8 Feb\n\n\n\n\nVariable distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Train multiple latent.html",
    "href": "Train multiple latent.html",
    "title": "Train multiple latents",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\nfrom meteo_imp.results import _display_as_row \n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\n\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=300)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    #\"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-07 04:00:00\n3.48\n0.0\n0.065\n\n\n2000-01-07 04:30:00\n3.48\n0.0\n0.063\n\n\n2000-01-07 05:00:00\n3.47\n0.0\n0.063\n\n\n2000-01-07 05:30:00\n3.43\n0.0\n0.057\n\n\n2000-01-07 06:00:00\n3.41\n0.0\n0.055\n\n\n\n\n300 rows × 3 columns\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nn_obs = 200\nn_latents = range(1,4) \n\n\ndata = GPFADataTest(hai[:n_obs])\n\n\ncache_file_gaps = cache_path / \"hai_train_multi_latents.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\nmodel_save_dir = here() / \"analysis/trained_models\"\n\n\nimps = [GPFAImputation(data.data, latent_dims=i) for i in n_latents]\n\n\ntotal_iter = 0\n\n\ndef train_save(n_iter=10):\n    global total_iter\n    total_iter += n_iter\n    for imp, n_lat in zip(imps, n_latents):\n        imp.fit(n_iter)\n        imp.learner.save(model_save_dir / f\"GPFA_l_{n_lat}_train_{total_iter}_1ker_{n_obs}_obs.pickle\")\n\n\n\n\n\ntrain_save(100)\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_idx=0\nprint(\"n_latent: \" + str(imps[p_idx].learner.latent_dims))\ndisplay(imps[p_idx].learner.plot_progress())\n_display_as_row(imps[p_idx].learner.model.get_info(imps[p_idx].learner.var_names))\n\nn_latent: 1\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  Lambda \n\n\n\n\nvariable\nz0\n\n\n\n\nTA\n0.7965\n\n\nSW_IN\n-0.0502\n\n\nVPD\n0.4993\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.5707\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0013\n\n\nSW_IN\n0.9871\n\n\nVPD\n0.6120\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0027\n\n\n\n\n\n \n\n\n\n\n\n\np_idx=1\nprint(\"n_latent: \" + str(imps[p_idx].learner.latent_dims))\ndisplay(imps[p_idx].learner.plot_progress())\n_display_as_row(imps[p_idx].learner.model.get_info(imps[p_idx].learner.var_names))\n\nn_latent: 2\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  Lambda \n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n-0.1784\n0.7830\n\n\nSW_IN\n0.9565\n0.3294\n\n\nVPD\n0.0957\n0.5738\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n4.7033\n\n\nz1\n6.8953\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0007\n\n\nSW_IN\n0.0261\n\n\nVPD\n0.5750\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0031\n\n\n\n\n\n \n\n\n\n\n\n\np_idx=2\nprint(\"n_latent: \" + str(imps[p_idx].learner.latent_dims))\ndisplay(imps[p_idx].learner.plot_progress())\n_display_as_row(imps[p_idx].learner.model.get_info(imps[p_idx].learner.var_names))\n\nn_latent: 3\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n  Lambda \n\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\nTA\n0.5749\n0.1098\n0.6824\n\n\nSW_IN\n-0.7796\n-0.0461\n0.6270\n\n\nVPD\n0.1813\n0.7577\n0.5405\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.2282\n\n\nz1\n1.8765\n\n\nz2\n4.0832\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0006\n\n\nSW_IN\n0.0285\n\n\nVPD\n0.0035\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0017"
  },
  {
    "objectID": "Kernel exploration.html",
    "href": "Kernel exploration.html",
    "title": "Kernel visualization",
    "section": "",
    "text": "Kernel visualization\n\nfrom meteo_imp.gpfa.gpfa import GPFAKernel\nimport gpytorch\nimport torch\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport pandas as pd\n\n\nk = gpytorch.kernels.RBFKernel()\n\n\nk.lengthscale = 3\n\n\nk(torch.tensor([1]), torch.tensor([2, 3])).evaluate()\n\ntensor([[0.9460, 0.8007]], grad_fn=&lt;RBFCovarianceBackward&gt;)\n\n\n\nalt.renderers.enable('mimetype')\n\nRendererRegistry.enable('mimetype')\n\n\n\ndef plot_kernel(kernel, t_range = torch.arange(-10, 10)):\n    y = kernel(torch.tensor([0]), t_range).evaluate().detach().numpy().squeeze()\n    return alt.Chart(pd.DataFrame({'x': t_range, 'y': y})).mark_line().encode(x='x', y ='y')\n    #plt.plot(t_range, y\n    # return y\n\n\nplot_kernel(k)"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Notes\nNotes: - Kernel Lengthscale: too short to handle big gaps - how to encode time? - normalization - parameters init\nnext steps:\n\n\n\nthings to try: - more variables - more kernels - basic variable dist - log - deploy to website\nThings to consider: - different latent kernel for different latent variable?\n\nTA\nSW_IN\nLW_IN\nVPD\nWS\nWD\nP\nPA\nSWC\nPPFD\nCO2 flux\nH02 flux\nLE\n\nCode improvements: - move normalization out of the learner class - use something like fastai pipelines for data transformation …"
  },
  {
    "objectID": "kalman/kalman_float64.html",
    "href": "kalman/kalman_float64.html",
    "title": "Train Kalman filter using Fastai using float64",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nimport pandas as pd\nimport torch\nimport numpy as np\n\nNameError: name 'ListNormal' is not defined\n\n\n\n@cache_disk(\"full_hai\")\ndef load_data():\n    return read_fluxnet_csv(hai_path, None, num_dtype=np.float64)\n\nhai = load_data()\n\n\nmodel = KalmanFilter(**KalmanFilterTester(dtype=torch.float64).params)\n\n\ndls = make_dataloader(hai, 200, 10, bs=10) \n\n\nlen(hai) / 200 / 10 * .8\n\n91.1808\n\n\n\nlen(dls.train)\n\n91\n\n\n\nlearn = Learner(dls, model, loss_func=imp_ll_loss, cbs=[ShowGraphCallback, Float64Callback])\n\n\nlen(dls.train)\n\n91\n\n\n\n\n\nlearn.fit(10, 1e-3)\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/91 00:00&lt;?]\n    \n    \n\n\nNotImplementedError: Module [KalmanFilter] is missing the required \"forward\" function\n\n\n\nlearn.recorder.plot_loss()\n\n\n# learn.fit(10, 1e-2)\n\n\n# learn.recorder.plot_loss()\n\n\n# torch.save(learn.model, \"trained_model_20_dec_f64.pickle\")\n\n\ntrained_state = learn.model.state_dict()\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\n\n\ndef gap2res(var_sel, gap_len, gap_start, block_start=1000, block_end=1200):\n    data = MeteoDataTest(hai.iloc[block_start:block_end, :]).add_gap(gap_len, var_sel, gap_start)\n    imp = KalmanImputation(data.data)\n    imp.model.filter = learn.model # ensure float64 support\n    imp.model.filter.load_state_dict(trained_state)\n    return imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\ndisplay_as_row(learn.model.get_info())\n\n\ncheck_posdef(learn.model.obs_cov.to(torch.float32))\n\n\ngap2res(var_sel, gap_start=30, gap_len=15, block_start=500, block_end=700).display_results()\n\n\n\n\n\ncompute the loss for all predictions not only the gap\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False), cbs=[ShowGraphCallback, Float64Callback])\n\n\nlearn.fit(10, 2e-3)\n\n\nlearn.recorder.plot_loss()\n\nHorrible idea … probably by smoothing is too easy to predict the points that the parameters of the model have basically no influence.\nNeed to try with filtering\n\n\n\ncompute the loss for all predictions not only the gap\n\nmodel = KalmanFilter(**KalmanFilterTester(dtype=torch.float64).params)\n\n\nmodel.use_smooth = False\n\n\nmodel.check_args = None\n\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False), cbs=[ShowGraphCallback, Float64Callback])\n\n\nlearn.fit(12, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n7412.622489\n6652.202241\n03:45\n\n\n1\n5049.731810\n2825.582725\n03:44\n\n\n2\n1962.345019\n903.253100\n03:47\n\n\n3\n318.255686\n-117.658645\n03:44\n\n\n4\n-323.398913\n-448.278246\n03:47\n\n\n5\n-654.908696\n-679.999570\n03:43\n\n\n6\n-803.473786\n-873.061342\n03:46\n\n\n7\n-1016.195842\n-1018.383004\n03:45\n\n\n8\n-1142.367950\n-1148.339634\n03:46\n\n\n9\n-1260.992943\n-1299.262096\n03:41\n\n\n10\n-1431.255912\n-1435.102765\n03:46\n\n\n11\n-1686.184436\n-1593.321410\n03:40\n\n\n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nlearn.fit(10, 1e-3)\n\n\nlearn.recorder.plot_loss()\n\n\nlearn.fit(10, 5e-4)\n\n\nlearn.recorder.plot_loss()\n\n\ntrained_state = learn.model.state_dict()\n\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\n\n\ndef gap2res(var_sel, gap_len, gap_start, block_start=1000, block_end=1200):\n    data = MeteoDataTest(hai.iloc[block_start:block_end, :]).add_gap(gap_len, var_sel, gap_start)\n    imp = KalmanImputation(data.data)\n    imp.model.filter = learn.model # ensure float64 support\n    imp.model.filter.load_state_dict(trained_state)\n    return imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\nfrom ipywidgets import interact_manual, IntSlider"
  },
  {
    "objectID": "kalman/KalmanFilter_PyTorch.html",
    "href": "kalman/KalmanFilter_PyTorch.html",
    "title": "Kalman Filter Hainich",
    "section": "",
    "text": "Kalman filter model on Hainich data using PyTorch\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.data import hai, units\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\nfrom meteo_imp.kalman.model import *\nfrom ipywidgets import interact, interact_manual, IntSlider\n\n\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n200001011712.0\n-0.60\n0.0\n0.222\n\n\n200001011712.0\n-0.65\n0.0\n0.122\n\n\n200001011712.0\n-0.58\n0.0\n0.090\n\n\n200001011712.0\n-0.51\n0.0\n0.110\n\n\n200001011712.0\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n200001044480.0\n4.74\n0.0\n1.191\n\n\n200001044480.0\n4.75\n0.0\n1.057\n\n\n200001044480.0\n4.76\n0.0\n0.935\n\n\n200001044480.0\n4.62\n0.0\n1.162\n\n\n200001044480.0\n4.51\n0.0\n1.636\n\n\n\n\n200 rows × 3 columns\n\n\n\n\ndata = MeteoDataTest(hai).add_gap(10, ['TA', 'SW_IN', 'VPD'], 30)\n\n\ndata.data\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n200001011712.0\n-0.60\n0.0\n0.222\n\n\n200001011712.0\n-0.65\n0.0\n0.122\n\n\n200001011712.0\n-0.58\n0.0\n0.090\n\n\n200001011712.0\n-0.51\n0.0\n0.110\n\n\n200001011712.0\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n200001044480.0\n4.74\n0.0\n1.191\n\n\n200001044480.0\n4.75\n0.0\n1.057\n\n\n200001044480.0\n4.76\n0.0\n0.935\n\n\n200001044480.0\n4.62\n0.0\n1.162\n\n\n200001044480.0\n4.51\n0.0\n1.636\n\n\n\n\n200 rows × 3 columns\n\n\n\n\nimport numpy as np\n\n\nimp = KalmanImputation(data.data)\n\n\nimp.fit(n_iter = 50)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat iteration 9 expected positive definite matrix. \ntensor([[ 1.8868, -0.4285, -1.4917],\n        [-0.4203,  1.1536, -0.5259],\n        [-1.4795, -0.5314,  1.7019]], grad_fn=&lt;SubBackward0&gt;)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat iteration 9 expected positive definite matrix. \ntensor([[ 0.2549, -0.1689, -0.1888],\n        [-0.1536,  0.4337, -0.2591],\n        [-0.1773, -0.2612,  0.2829]], grad_fn=&lt;AddBackward0&gt;)\nat iteration 9 expected positive definite matrix. \ntensor([[ 0.6063, -0.3421, -0.3056],\n        [-0.3361,  0.6454, -0.3535],\n        [-0.3007, -0.3570,  0.5838]], grad_fn=&lt;AddBackward0&gt;)\n\n\nValueError: Expected parameter covariance_matrix (Tensor of shape (3, 3)) of distribution MultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[ 0.6063, -0.3421, -0.3056],\n        [-0.3361,  0.6454, -0.3535],\n        [-0.3007, -0.3570,  0.5838]], grad_fn=&lt;ExpandBackward0&gt;)\n\n\n\nimp.model.plot_loss()\n\n\nimp.impute()\n\n\ndata.data.columns\n\n\nres = imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nres.display_results()\n\n\n\n\ndef gap2res(var_sel, gap_len, gap_start, n_iter):\n    data = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\n    return KalmanImputation(data.data).fit(n_iter=n_iter).to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10, 5)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]"
  },
  {
    "objectID": "kalman/kalman_fastai.html",
    "href": "kalman/kalman_fastai.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch"
  },
  {
    "objectID": "kalman/kalman_fastai.html#filter-loss---float32",
    "href": "kalman/kalman_fastai.html#filter-loss---float32",
    "title": "Meteo Imputation",
    "section": "Filter loss - (float32)",
    "text": "Filter loss - (float32)\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai = load_data()\n\n\nmodel = KalmanFilter.init_random(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1]).cuda()\n\n\nmodel.use_smooth = False\n\n\ndls = make_dataloader(hai[:5*20_000], block_len=200, gap_len=1, bs=30) # about 5 year of data \n\n\ndls.one_batch()[0][0].dtype, dls.one_batch()[0][0].device\n\n(torch.float32, device(type='cuda', index=0))\n\n\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False, reduction='sum'), cbs=[ShowGraphCallback], metrics=[msk_rmse, msk_r2])\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n28334.117188\n24884.890625\n1.044047\n-0.270700\n00:10\n\n\n1\n27505.285156\n23820.916016\n0.936844\n-0.022787\n00:09\n\n\n2\n26766.705078\n22994.548828\n0.868220\n0.122282\n00:09\n\n\n3\n26070.080078\n22316.949219\n0.818773\n0.219742\n00:09\n\n\n4\n25482.609375\n21709.919922\n0.778877\n0.294147\n00:09\n\n\n5\n24909.001953\n21147.484375\n0.743662\n0.356575\n00:09\n\n\n6\n24372.490234\n20604.521484\n0.716608\n0.402541\n00:09\n\n\n7\n23862.095703\n20079.589844\n0.694325\n0.439092\n00:09\n\n\n8\n23340.068359\n19563.205078\n0.676239\n0.467748\n00:09\n\n\n9\n22824.031250\n19030.230469\n0.661495\n0.490591\n00:09\n\n\n\n\n\n\n\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n20513.478516\n18492.871094\n0.646592\n0.512978\n00:09\n\n\n1\n20331.521484\n17940.644531\n0.636259\n0.528079\n00:09\n\n\n2\n20015.253906\n17363.677734\n0.626718\n0.541712\n00:09\n\n\n3\n19642.833984\n16716.089844\n0.616619\n0.556175\n00:10\n\n\n4\n19214.849609\n15884.863281\n0.600617\n0.578533\n00:09\n\n\n5\n18648.689453\n14677.888672\n0.547030\n0.651080\n00:10\n\n\n6\n17847.457031\n13356.031250\n0.496326\n0.713167\n00:09\n\n\n7\n17063.013672\n12896.752930\n0.455412\n0.758161\n00:09\n\n\n8\n16343.417969\n12419.463867\n0.433779\n0.779173\n00:10\n\n\n9\n15676.717773\n11876.413086\n0.415929\n0.795271\n00:13\n\n\n\n\n\n\n\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n13020.457031\n11358.107422\n0.402912\n0.806103\n00:11\n\n\n1\n12650.278320\n10791.625000\n0.392657\n0.815727\n00:10\n\n\n2\n12294.549805\n10184.606445\n0.383261\n0.823566\n00:10\n\n\n3\n11870.844727\n9569.536133\n0.376152\n0.828884\n00:10\n\n\n4\n11425.293945\n8862.309570\n0.366659\n0.839007\n00:11\n\n\n5\n10938.154297\n8113.016602\n0.363477\n0.842085\n00:11\n\n\n6\n10377.956055\n7253.085938\n0.363612\n0.841904\n00:10\n\n\n7\n9712.108398\n5912.989746\n0.365726\n0.841298\n00:10\n\n\n8\n8721.796875\n1818.676758\n0.379970\n0.830193\n00:11\n\n\n9\n6890.971191\n-1213.163086\n0.375065\n0.835149\n00:14\n\n\n\n\n\n\n\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n991.025146\n-375.008484\n0.374033\n0.836182\n00:10\n\n\n1\n382.140930\n-1426.713867\n0.376774\n0.833246\n00:09\n\n\n2\n-10.901829\n-1699.835083\n0.374207\n0.835683\n00:09\n\n\n3\n-249.258392\n-1820.056519\n0.373657\n0.836395\n00:09\n\n\n4\n-377.162689\n-1793.390137\n0.375005\n0.834909\n00:10\n\n\n5\n-475.895691\n-1815.924683\n0.375007\n0.834904\n00:09\n\n\n6\n-518.640808\n-1835.481812\n0.374665\n0.835544\n00:09\n\n\n7\n-586.902527\n-1827.023193\n0.374206\n0.835841\n00:10\n\n\n8\n-655.092407\n-1729.346924\n0.375481\n0.834565\n00:10\n\n\n9\n-669.299683\n-1798.226685\n0.374862\n0.835274\n00:09\n\n\n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\ndisplay_as_row(learn.model.get_info(var_names=hai.columns))\n\n\n  trans_matrix (A) \n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.1146\n0.5802\n0.1363\n\n\nz_1\n0.1496\n0.6129\n0.3537\n\n\nz_2\n0.3043\n0.6128\n0.0236\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.7217\n0.1717\n0.0886\n\n\nz_1\n0.1717\n0.1250\n-0.0129\n\n\nz_2\n0.0886\n-0.0129\n0.0281\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n0.2728\n\n\nz_1\n-0.0485\n\n\nz_2\n0.4645\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.4154\n0.6550\n0.3770\n\n\nSW_IN\n0.0119\n0.5495\n0.5917\n\n\nVPD\n0.1603\n0.6821\n-0.0631\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.8515\n0.1525\n0.9209\n\n\nSW_IN\n0.1525\n0.2047\n0.2631\n\n\nVPD\n0.9209\n0.2631\n1.8438\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.6057\n\n\nSW_IN\n0.4861\n\n\nVPD\n0.1418\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n-0.0218\n\n\nz_1\n0.3442\n\n\nz_2\n0.0868\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.1216\n1.0998\n0.7476\n\n\nz_1\n1.0998\n1.9714\n0.9649\n\n\nz_2\n0.7476\n0.9649\n0.6006\n\n\n\n\n\n \n\n\nThis is filtering! this is not smoothing\n\n# torch.save(learn.model, \"model_trained_30dec.pickle\")\n\n\nshow_results(learn, bind_interaction=False)\n\n\n\n\n\n\n\nlearn.model.use_smooth = True\nshow_results(learn, items=[10, 110, 130])\nlearn.model.use_smooth = False\n\n\n\n\n\n\n\nshow_results(learn, items=[10, 110, 130])\n\n\n\n\n\n\n\n# learn.fit(10, 1e-2)\n\n\n# learn.recorder.plot_loss()\n\n\nlearn.model.use_smooth = False\n\n\nshow_results(learn, items=[1,2,3])\n\n\n\n\n\n\n\n# torch.save(learn.model, \"trained_filter_29_dec_1.pickle\")"
  },
  {
    "objectID": "kalman/kalman_fastai.html#filter-loss---float64",
    "href": "kalman/kalman_fastai.html#filter-loss---float64",
    "title": "Meteo Imputation",
    "section": "Filter loss - (float64)",
    "text": "Filter loss - (float64)\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai64 = load_data(np.float64)\n\n\nmodel64 = KalmanFilter.init_random(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1], dtype=torch.float64).cuda()\n\n\nmodel64.use_smooth = False\n\n\ndls64 = make_dataloader(hai64[:5*20_000], block_len=200, gap_len=1, bs=10) # about 5 year of data \n\n\ndls64.one_batch()[0][0].dtype, dls64.one_batch()[0][0].device\n\n(torch.float64, device(type='cuda', index=0))\n\n\n\nlearn64 = Learner(dls, model, loss_func=KalmanLoss(only_gap=False, reduction='sum'), cbs=[ShowGraphCallback, Float16Callback], metrics=[msk_rmse, msk_r2])\n\nNameError: name 'Float16Callback' is not defined\n\n\n\nlearn64.fit(10, 2e-3)\n\n\nlearn64.recorder.plot_loss()\n\n\ndisplay_as_row(learn64.model.get_info(var_names=hai64.columns))\n\n\nshow_results(learn64, bind_interaction=False)\n\n\n# learn.fit(10, 1e-2)\n\n\n# learn.recorder.plot_loss()\n\n\nlearn.model.use_smooth = False\n\n\nshow_results(learn, items=[1,2,3])"
  },
  {
    "objectID": "kalman/filter_fill_one_day_gap_SW_IN.html",
    "href": "kalman/filter_fill_one_day_gap_SW_IN.html",
    "title": "Fill one day gap in SW_IN",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.fastai import _add_lags_df\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastai.vision.data import get_grid\nfrom fastcore.foundation import L\nfrom fastcore.transform import *\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\n\n\nfrom ipywidgets import widgets, HBox, VBox\nfrom typing import Sequence\n\n\nclass InteractiveSequence():\n    def __init__(self,s: Sequence, start=0):\n        self.s =s\n        self.i = start\n        self.output = widgets.Output()\n        self.button_next = widgets.Button(description=\"Next\", icon=\"arrow-right\")\n        self.button_prev = widgets.Button(description=\"Previous\", icon=\"arrow-left\", disabled=True)\n        self.button_next.on_click(self.on_next)\n        self.button_prev.on_click(self.on_prev)\n        self.label = widgets.Label(f\"of {len(self.s)-1}\")\n        self.slider = widgets.IntSlider(start, 0, len(s)-1, 1)\n        self.slider.observe(self.on_slide, names=\"value\")\n    def update_view(self):\n        self.button_enable()\n        self.slider.value = self.i\n        with self.output:\n            display(self.s[self.i])\n        self.output.clear_output(wait=True)\n    def button_enable(self):\n        if self.i &lt; len(self.s) - 1:  self.button_next.disabled = False\n        else: self.button_next.disabled = True\n        \n        if self.i == 0: self.button_prev.disabled = True\n        else: self.button_prev.disabled = False\n        \n    def on_next(self, b):\n        self.i +=1\n        self.update_view()\n    def on_prev(self, b):\n        self.i -=1\n        self.update_view()\n    def on_slide(self, change):\n        self.i = change['new']\n        self.update_view()\n        \n    def __call__(self):\n        self.update_view()\n        display(VBox([HBox([self.slider, self.label]), HBox([self.button_prev, self.button_next])]), self.output)\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path64)\n\n\nhai[8_110:8_160].SW_IN.plot()\n\n&lt;AxesSubplot: xlabel='time'&gt;\n\n\n\n\n\n\ndf = hai[8_110-80:8_160+80]\ncontrol = _add_lags_df(hai_era.loc[df.index], 1)\n\n\ndf.plot()\n\n&lt;AxesSubplot: xlabel='time'&gt;\n\n\n\n\n\n\ncontrol.plot()\n\n&lt;AxesSubplot: xlabel='time'&gt;\n\n\n\n\n\n\ndf.columns\n\nIndex(['TA', 'SW_IN', 'VPD'], dtype='object')\n\n\n\nmask = df.astype(dtype=bool)\nmask.iloc[:,:] = True\nmask.iloc[100:115, 1] = False\n\n\nmask.sum(0)\n\nTA       210\nSW_IN    195\nVPD      210\ndtype: int64\n\n\n\npipe = Pipeline([MeteoImpDf2Tensor, MeteoImpNormalize(*get_stats(df), *get_stats(control)), ToTuple])\n\n\npipe[1]\n\n\n  \n    MeteoImpNormalize -- {'mean_data': tensor([ 18.2214, 355.5345,  13.8018], dtype=torch.float64), 'std_data': tensor([  7.3713, 336.5250,   8.4760], dtype=torch.float64), 'mean_control': tensor([ 16.4072, 325.8798,  10.0621,  16.3431, 324.6188,   9.9632],\n       dtype=torch.float64), 'std_control': tensor([  6.5264, 307.3572,   7.4125,   6.4755, 307.5501,   7.2901],\n       dtype=torch.float64)}\n  \n  (MeteoImpTensor,object) -&gt; encodes\n\n  (MeteoImpTensor,object) -&gt; decodes\n(NormalsParams,object) -&gt; decodes\n\n\n\n\n\ndata = MeteoImpDf(df, mask, control)\n\n\ninput, targ = pipe(data)\n\ninput = input[0].unsqueeze(0), input[1].unsqueeze(0), input[2].unsqueeze(0), \n\ntarg = targ[0].unsqueeze(0), targ[1].unsqueeze(0), targ[2].unsqueeze(0), \n\n\n# input[0][~input[1]] = torch.nan \n\n\nk = KalmanFilter.init_local_slope_pca(3,3, df)\n\n\nk\n\n\nKalman Filter (3 obs, 6 state, 6 contr)  $A$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.0000\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n1.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\n$Q$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n\n$b$\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\nx_4\n0.0000\n\n\nx_5\n0.0000\n\n\n\n\n\n$H$\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.0050\n0.6587\n-0.7524\n0.0000\n0.0000\n0.0000\n\n\ny_1\n1.0000\n-0.0083\n-0.0006\n0.0000\n0.0000\n0.0000\n\n\ny_2\n0.0066\n0.7524\n0.6587\n0.0000\n0.0000\n0.0000\n\n\n\n\n\n$R$\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n\n$d$\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n\n$B$\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0050\n-1.0000\n-0.0066\n0.0050\n1.0000\n0.0066\n\n\nx_1\n-0.6587\n0.0083\n-0.7524\n0.6587\n-0.0083\n0.7524\n\n\nx_2\n0.7524\n0.0006\n-0.6587\n-0.7524\n-0.0006\n0.6587\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\n$m_0$\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\nx_4\n0.0000\n\n\nx_5\n0.0000\n\n\n\n\n\n$P_0$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n3.0000\n\n\n\n\n\n \n\n\n\npred = k(input)\n\n_LinAlgError: linalg.cholesky: (Batch element 0): The factorization could not be completed because the input is not positive-definite (the leading minor of order 2 is not positive-definite).\n\n\n\nloss_f = KalmanLoss(only_gap=True)\nloss_f_all = KalmanLoss(only_gap=False)\n\n\nloss_f(pred, targ) \n\nNameError: name 'pred' is not defined\n\n\n\npipe1 = Pipeline(pipe.fs[-2,-1])\n\n\npipe1\n\nPipeline: MeteoImpNormalize -- {'mean_data': tensor([ 18.2214, 355.5345,  13.8018], dtype=torch.float64), 'std_data': tensor([  7.3713, 336.5250,   8.4760], dtype=torch.float64), 'mean_control': tensor([ 16.4072, 325.8798,  10.0621,  16.3431, 324.6188,   9.9632],\n       dtype=torch.float64), 'std_control': tensor([  6.5264, 307.3572,   7.4125,   6.4755, 307.5501,   7.2901],\n       dtype=torch.float64)} -&gt; ToTuple\n\n\n\ndef plot_pred(pred, data, control=True):\n    pred_dec = preds2df([pipe.decode(pred)], [data])[0]\n\n    data_pred = pd.merge(data.tidy(hai_control), pred_dec.tidy(), on=[\"time\", \"variable\"])\n\n    # title = [f\"loss: {loss.item():.6f}\"] + [format_metric(name, val) for name, val in metrics.items()]\n    return plot_variable(data_pred, variable=\"SW_IN\", ys=[\"value\", \"mean\", \"control\"], error=True, control=control)\n\n\nplot_pred(pred, data)\n\nNameError: name 'pred' is not defined\n\n\n\nfrom tqdm.auto import tqdm\n\n\ndef train(k, data_gen, loss_f, n_iter, lr, control=True):\n    k.train()\n    data, input, targ = data_gen\n    optimizer = torch.optim.Adam(k.parameters(), lr=lr) \n\n    t_info = pd.DataFrame(columns=['loss', 'rmse_gap', 'rmse', 'plot'])\n    pd.DataFrame({k: pd.Series(dtype=t) for k, t in [('loss', float), ('rmse_gap', float), ('rmse', float), ('plot', object)]})\n    for i in tqdm(range(n_iter)):\n        # Zero gradients from previous iteration\n        optimizer.zero_grad()\n        # Output from model\n        pred= k(input)\n        loss = loss_f(pred, targ)\n        t_info.loc[i, 'loss'] = loss_f(pred, targ).item()\n        t_info.loc[i, 'loss_all'] = loss_f_all(pred, targ).item()\n        t_info.loc[i, 'rmse_gap'] = rmse_gap(pred, targ)\n        t_info.loc[i, 'rmse'] = rmse_mask(pred, targ)\n        t_info.loc[i, 'plot'] = plot_pred(pred, data, control)\n        # backpropagate gradients\n        loss.backward()\n        optimizer.step()\n    return t_info\n\n\ndef data_gen_one_gap(df, g_inter, df_control, device='cpu', all_gap=False):\n    control = _add_lags_df(df_control.loc[df.index], 1)\n    mask = df.astype(dtype=bool)\n    mask.iloc[:,:] = True\n    col_sel = [0,1,2] if all_gap else 1\n    mask.iloc[g_inter[0]:g_inter[1], col_sel] = False \n    pipe = Pipeline([MeteoImpDf2Tensor, MeteoImpNormalize(*get_stats(df), *get_stats(control)), ToTuple])\n    data = MeteoImpDf(df, mask, control)\n    input, targ = pipe(data)\n    input = input[0].unsqueeze(0).to(device), input[1].unsqueeze(0).to(device), input[2].unsqueeze(0).to(device), \n    targ = targ[0].unsqueeze(0).to(device), targ[1].unsqueeze(0).to(device), targ[2].unsqueeze(0).to(device), \n    return data, input, targ\n\n\ndef plot_train_info(t_info):\n    ax = get_grid(4,2,2, figsize=(8,6))\n\n    t_info.loss.plot(ax=ax[0], title=\"loss gap\")\n    t_info.loss_all.plot(ax=ax[1], title=\"loss all\")\n    t_info.rmse.plot(ax=ax[2], title=\"rmse\")\n    t_info.rmse_gap.plot(ax=ax[3], title=\"rmse gap\")\n    \n    InteractiveSequence(t_info['plot'], start=len(t_info)-1)()\n\n\n\n\ndata_gen0 = data_gen_one_gap(hai[8_110-80:8_160+80], (95, 120), hai_era )\n\n\nk0 = KalmanFilter.init_local_slope_pca(3,3, None)\nt_info0 = train(k0, data_gen0, loss_f, 40, 1e-2)\nplot_train_info(t_info0)\n\n\n\n\nNameError: name '_smooth_gain' is not defined\n\n\n\nk = KalmanFilter.init_local_slope_pca(3,3, None)\nt_info = train(k, (data, input, targ), loss_f, 40, 1e-2)\n\n\n\n\nNameError: name '_smooth_gain' is not defined\n\n\n\nax = get_grid(4,2,2, figsize=(8,6))\n\nt_info.loss.plot(ax=ax[0], title=\"loss gap\")\nt_info.loss_all.plot(ax=ax[1], title=\"loss all\")\nt_info.rmse.plot(ax=ax[2], title=\"rmse\")\nt_info.rmse_gap.plot(ax=ax[3], title=\"rmse gap\")\n\n\nInteractiveSequence(t_info['plot'])()\n\n\n\n\n\nk = KalmanFilter.init_local_slope_pca(3,3, df)\nt_info_all = train(k, loss_f_all, 00)\n\n\nax = get_grid(4,2,2, figsize=(8,6))\n\nt_info_all.loss.plot(ax=ax[0], title=\"loss gap\")\nt_info_all.loss_all.plot(ax=ax[1], title=\"loss all\")\nt_info_all.rmse.plot(ax=ax[2], title=\"rmse\")\nt_info_all.rmse_gap.plot(ax=ax[3], title=\"rmse gap\")\n\n\nint_s = InteractiveSequence(t_info_all['plot'])()\n\n\nk\n\n\n\n\n\ndata_gen = data_gen_one_gap(hai[8_110-80:8_160+80], (95, 120), hai_era )\n\n\nk2 = KalmanFilter.init_local_slope_pca(3,3, None)\nt_info2 = train(k2, data_gen, loss_f_all, 20, 1e-2)\nplot_train_info(t_info2)\n\n\nt_data_gen = data_gen_one_gap(hai[8_110-80:8_160+80], (120, 130), hai_era )\nplot_pred(k2(t_data_gen[1]), t_data_gen[0])\n\n\nk2\n\n\nk2 = KalmanFilter.init_local_slope_pca(3,3, None)\nt_info2 = train(k2, data_gen, loss_f, 20, 1e-2)\nplot_train_info(t_info2)\n\n\n\n\n\ndata_gen3 = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era )\n\n\nk3 = KalmanFilter.init_local_slope_pca(3,3, None)\ntrain(k3, data_gen3, loss_f_all, 5, 1e-2) # get to decent parameters\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info3 = train(k3, data_gen3, loss_f_all, 40, 1e-2)\n\n\nplot_train_info(t_info3)\n\n\nt_info3.iloc[-2:-1]\n\n\nk3\n\n\nk0.trans_cov, k3.trans_cov\n\n\nk0.trans_matrix, k3.trans_matrix\n\n\nk0 = KalmanFilter.init_local_slope_pca(3,3, None)\ndef par_diff(k0, k1):\n    out = {}\n    for (n,p0), (_,p1) in zip(k0.named_parameters(), k1.named_parameters()):\n        out[n] = (p0-p1).abs().mean().detach().item()\n    return pd.DataFrame(out,index=[0])\n\n\npar_diff(k0, k3)\n\n\npar_diff(k2, k3)\n\n\npar_diff(k0, k2)\n\n\n\n\n\ndata_gen_gall = data_gen_one_gap(hai[8_110-80:8_160+80], (90, 130), hai_era, all_gap=True)\n\n\nk_gall = KalmanFilter.init_local_slope_pca(3,3)\n\n\nt_info_gall = train(k_gall, data_gen_gall, loss_f_all, 40, 1e-2)\n\n\nplot_train_info(t_info_gall)\n\n\n\n\n\ndata_gen4 = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era)\n\n\nk4 = KalmanFilter.init_random(3,3,6, dtype=torch.float64)\n\n\nt_info4 = train(k4, data_gen4, loss_f_all, 100, 1e-2)\n\n\nplot_train_info(t_info4)\n\n\nt_info4 = train(k4, data_gen4, loss_f_all, 100, 3e-2)\n\n\nplot_train_info(t_info4)\n\n\nk4\n\n\n\n\n\n\n\ndata_gen_sls = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era)\n\n\nk_sls = KalmanFilter.init_local_slope_pca(3,1, hai)\n\n\nt_info_sls = train(k_sls, data_gen_sls, loss_f_all, 70, 2e-2)\n\n\nplot_train_info(t_info_sls)\n\n\n\n\n\ndata_gen5 = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era)\n\n\nk5 = KalmanFilter.init_random(3,1,6, dtype=torch.float64)\n\n\nt_info5 = train(k5, data_gen5, loss_f_all, 200, 2e-2)\n\n\nplot_train_info(t_info5)\n\n\nt_info5 = train(k5, data_gen5, loss_f_all, 200, 2e-2)\n\n\nplot_train_info(t_info5)\n\n\nt_info5 = train(k5, data_gen5, loss_f_all, 500, 2e-2)\n\n\nplot_train_info(t_info5)\n\n\nt_info5 = train(k5, data_gen4, loss_f_all, 100, 3e-2)\n\n\nplot_train_info(t_info5)\n\n\nk4\n\n\n\n\n\n\ndata_gen6 = data_gen_one_gap(hai[8_110-80:8_160+80], (95, 120), hai_era )\n\n\nk6 = KalmanFilter.init_local_slope_pca(3,3, None)\nk6.use_smooth = False\n# train(k6, data_gen6, loss_f_all, 5, 1e-2) # get to decent parameters\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info6 = train(k6, data_gen5, loss_f_all, 25, 1e-2)\n\n\nplot_train_info(t_info6)\n\n\n\n\n\n\n\ndata_gen_nc = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era.sample(frac=1).reset_index(drop=True).set_index(hai_era.index) )\n\n\nk_nc = KalmanFilter.init_local_slope_pca(3,3, None, use_control=False)\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info_nc = train(k_nc, data_gen_nc, loss_f_all, 20, 1e-2)\n\n\nplot_train_info(t_info_nc)\n\n\ndata, input, targ = data_gen_nc\n\n\nt_data_g = data_gen_one_gap(hai[8_110-80:8_160+80], (120, 200), hai_era) \n\n\nplot_pred(k_nc(t_data_g[1]), t_data_g[0])\n\n\n\n\n\ndata_gen_nc2 = data_gen_one_gap(hai[8_110-80:8_160+80], (95, 105),\n                               hai_era,#.sample(frac=1).reset_index(drop=True).set_index(hai_era.index),\n                              all_gap=True)\n\n\nk_nc2 = KalmanFilter.init_local_slope_pca(3,3, None, use_control=False)\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info_nc2 = train(k_nc2, data_gen_nc2, loss_f_all, 20, 1e-2)\n\n\nplot_train_info(t_info_nc2)\n\n\ndata_gen_nc3 = data_gen_one_gap(hai[8_110-80:8_160+80], (90, 130),\n                               hai_era.sample(frac=1).reset_index(drop=True).set_index(hai_era.index),\n                              all_gap=True)\n\n\nk_nc3 = KalmanFilter.init_local_slope_pca(3,3,None, use_control=False)\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info_nc3 = train(k_nc3, data_gen_nc3, loss_f_all, 20, 1e-2, control=False)\n\n\nplot_train_info(t_info_nc3)"
  },
  {
    "objectID": "kalman/Process_noise.html",
    "href": "kalman/Process_noise.html",
    "title": "Process Noise estimation",
    "section": "",
    "text": "Process Noise estimation\n\nfor a local level model using data from Hainich\n\n\nfrom meteo_imp.data import hai\n\n\nta_diff = (hai.TA - hai.TA.shift(-1))\nta_diff.hist(bins=20)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nthis is the difference between TA and the previous time step which is not too far from a normal distribution\n\nta_diff.std()\n\n0.12497036855024059\n\n\nso This should be the value of the Q in the kalman filter"
  },
  {
    "objectID": "kalman/Loss_comparison.html",
    "href": "kalman/Loss_comparison.html",
    "title": "Compare Loss functions",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\n\nIn order to optimize the parameters of a KalmanFilter there are multiple ways, one is to use EM like pykalman another is to optimize using the likelihood like statsmodels is doing.\nStatsmodels doesn’t support gap in the dataset\nThere are 2 main approach for the loss:\n\nuse only the filter. This means that the models runs only the kalman filter and for every observations tries to predict the next one. This doesn’t consider the gaps and doesn’t use the smoother. The loss is calculated for the whole period\npredict the gap after smoothing. Run the Kalman smoother and then compute the loglikelihood only for the gap\n\nin addition between different batches the loss can be: - averaged - summed\nThe model predictions are mean and stadard deviation for each observation in the dataset, with shape [n_batches, n_obs, n_variables]\nFor each variable at each observation if computes the log-likelihood of the univariate distribution, which is then summed across all the variables and all the observation in a batch`\n\n\nModel training:\n\n~5 years of data, 100k observations or 500 blocks\n80% of blocks are training 20% validation\ngap of fixed size of 10 only for TA and SW_IN\n\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai = load_data()\nhai64 = load_data(np.float64)\n\n\ndef train_loss(reduction, use_smooth):\n    model = KalmanFilter.init_random(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1], dtype=torch.float64).cuda()\n    model.use_smooth = use_smooth\n    only_gap = use_smooth\n    dls = make_dataloader(hai64[:5*20_000], block_len=200, gap_len=10, bs=20) # about 5 year of data\n    loss_func = loss_func=KalmanLoss(only_gap=only_gap, reduction=reduction)\n    learn = Learner(dls, model, loss_func, cbs=[ShowGraphCallback, Float64Callback], metrics=[msk_rmse, msk_r2])\n    learn.fit(20, 1e-2)\n    return learn\n\nload from cache trained models\n\nimport dill\n\n\nwith open(\"models_loss_comparison_30dec.pickle\", 'rb') as f:\n    (l_sum_smooth, l_sum_n_smooth, l_mean_smooth, l_mean_n_smooth) = dill.load(f)\n\n\n\n\n\nl_sum_smooth = train_loss('sum', use_smooth=True)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n10799.557130\n7761.787664\n0.494636\n0.706476\n00:33\n\n\n1\n6721.645770\n1878.776931\n0.464390\n0.739616\n00:33\n\n\n2\n4177.969558\n509.344728\n0.412180\n0.793374\n00:33\n\n\n3\n2260.037030\n-1837.433033\n0.246094\n0.927043\n00:33\n\n\n4\n-719.709847\n-9555.917596\n0.199754\n0.952821\n00:33\n\n\n5\n-4476.204630\n-13089.933634\n0.149999\n0.973490\n00:34\n\n\n6\n-8455.999841\n-17691.481401\n0.131274\n0.979583\n00:33\n\n\n7\n-11678.127028\n-17865.444855\n0.147398\n0.972290\n00:34\n\n\n8\n-14031.962195\n-18773.763584\n0.137897\n0.976673\n00:34\n\n\n9\n-15678.972540\n-18799.284560\n0.141924\n0.975796\n00:35\n\n\n10\n-16803.071401\n-18882.290928\n0.112587\n0.985104\n00:36\n\n\n11\n-17558.598880\n-18924.760868\n0.105362\n0.986287\n00:33\n\n\n12\n-18067.875428\n-18897.525877\n0.093465\n0.989777\n00:33\n\n\n13\n-18430.414265\n-19040.231475\n0.090060\n0.990478\n00:35\n\n\n14\n-18694.256430\n-19050.077039\n0.092960\n0.989644\n00:33\n\n\n15\n-18885.238713\n-19185.962157\n0.080832\n0.992125\n00:33\n\n\n16\n-19050.434482\n-19113.437583\n0.114341\n0.984587\n00:33\n\n\n17\n-19185.244701\n-19220.514641\n0.082792\n0.992028\n00:33\n\n\n18\n-19271.169882\n-19315.973143\n0.066270\n0.994715\n00:33\n\n\n19\n-19338.463470\n-19355.567454\n0.068022\n0.994381\n00:33\n\n\n\n\n\n\n\n\n\nl_sum_smooth.recorder.plot_loss()\n\n\n\n\n\nshow_results(l_sum_smooth, [10, 50, 100])\n\n\n\n\n\n\n\ndisplay_as_row(l_sum_smooth.model.get_info(var_names=hai64.columns))\n\n\n  trans_matrix (A) \n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.8793\n0.1644\n-0.0996\n\n\nz_1\n0.0674\n0.9061\n0.0915\n\n\nz_2\n-0.1359\n0.1807\n0.8870\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.1691\n-0.1189\n-0.1084\n\n\nz_1\n-0.1189\n0.1337\n0.0929\n\n\nz_2\n-0.1084\n0.0929\n0.3573\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n0.0393\n\n\nz_1\n0.0015\n\n\nz_2\n0.1445\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.1359\n0.1995\n-0.0180\n\n\nSW_IN\n-0.0492\n0.3400\n0.0636\n\n\nVPD\n0.3182\n0.0204\n0.3036\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.0000\n0.0000\n-0.0000\n\n\nSW_IN\n0.0000\n0.0000\n-0.0000\n\n\nVPD\n-0.0000\n-0.0000\n0.0000\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.2448\n\n\nSW_IN\n0.3635\n\n\nVPD\n0.2137\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n0.2512\n\n\nz_1\n0.6348\n\n\nz_2\n0.9272\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.0244\n0.4137\n-0.1930\n\n\nz_1\n0.4137\n1.5316\n0.9529\n\n\nz_2\n-0.1930\n0.9529\n2.2897\n\n\n\n\n\n \n\n\n\n\n\n\nl_sum_n_smooth = train_loss('sum', use_smooth=False)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n93576.557506\n49094.936151\n40.175671\n-1609.196695\n00:22\n\n\n1\n55729.858957\n25465.931891\n13.037290\n-168.944314\n00:22\n\n\n2\n40901.222382\n23625.398854\n9.227300\n-81.796224\n00:23\n\n\n3\n32801.561799\n21803.994577\n6.553250\n-44.308130\n00:23\n\n\n4\n28134.386174\n20767.311986\n4.905621\n-25.004391\n00:23\n\n\n5\n25155.250779\n19650.187167\n3.522742\n-11.580414\n00:23\n\n\n6\n23134.458851\n18852.613585\n2.670879\n-6.114421\n00:24\n\n\n7\n21590.111416\n18724.717731\n2.463030\n-4.972687\n00:23\n\n\n8\n20488.952666\n18046.332419\n1.743591\n-2.261957\n00:24\n\n\n9\n19599.282756\n17686.631493\n1.554881\n-1.386024\n00:24\n\n\n10\n18864.805491\n17168.486468\n1.231224\n-0.576305\n00:23\n\n\n11\n18219.402631\n16717.755567\n0.993929\n0.016635\n00:24\n\n\n12\n17652.747846\n16358.387093\n0.948614\n0.086813\n00:23\n\n\n13\n17166.811259\n15819.057481\n0.743849\n0.461778\n00:22\n\n\n14\n16669.927076\n15380.745776\n0.729747\n0.474362\n00:23\n\n\n15\n16170.134417\n14886.565936\n0.694190\n0.532979\n00:24\n\n\n16\n15616.390677\n14280.504826\n0.653767\n0.585480\n00:25\n\n\n17\n15008.060510\n13473.336895\n0.579920\n0.672648\n00:23\n\n\n18\n14325.386795\n12561.507037\n0.566265\n0.686859\n00:23\n\n\n19\n13516.621992\n11399.619833\n0.543082\n0.710991\n00:23\n\n\n\n\n\n\n\n\n\nl_sum_n_smooth.recorder.plot_loss()\n\n\n\n\nfit a bit more\n\nl_sum_n_smooth.fit(5, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n10346.725174\n9452.293934\n0.518965\n0.736902\n00:23\n\n\n1\n9009.084750\n7346.894680\n0.498220\n0.756269\n00:23\n\n\n2\n7605.196719\n6058.022931\n0.528030\n0.725869\n00:23\n\n\n3\n6738.237530\n5312.245515\n0.501561\n0.751644\n00:23\n\n\n4\n5908.156025\n4639.401604\n0.500624\n0.751768\n00:23\n\n\n\n\n\n\n\n\n\nl_sum_n_smooth.fit(10, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n4075.124830\n4743.987561\n0.504186\n0.748768\n00:22\n\n\n1\n4534.079512\n5471.174814\n0.494402\n0.757260\n00:22\n\n\n2\n4162.734111\n4298.111266\n0.494588\n0.758349\n00:22\n\n\n3\n3856.497019\n4796.030030\n0.499238\n0.751841\n00:23\n\n\n4\n3702.818910\n3923.179320\n0.493548\n0.759028\n00:23\n\n\n5\n3538.964910\n4498.824727\n0.483678\n0.767376\n00:24\n\n\n6\n3410.967823\n3545.588596\n0.478077\n0.772404\n00:24\n\n\n7\n3237.693183\n3850.488723\n0.495080\n0.754614\n00:24\n\n\n8\n3111.747285\n4198.825180\n0.487525\n0.764516\n00:23\n\n\n9\n3083.048830\n3667.951800\n0.490212\n0.759747\n00:24\n\n\n\n\n\n\n\n\n\nshow_results(l_sum_n_smooth, [10, 50, 100])\n\n\n\n\n\n\nwith smoother enabled just for predictions\n\nl_sum_n_smooth.model.use_smooth = True\ndisplay(show_results(l_sum_n_smooth, [10, 50, 100]))\nl_sum_n_smooth.model.use_smooth = False\n\n\n\n\n\n\n\ndisplay_as_row(l_sum_n_smooth.model.get_info(var_names=hai64.columns))\n\n\n  trans_matrix (A) \n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.9738\n0.0646\n0.0733\n\n\nz_1\n0.3341\n0.5742\n0.3603\n\n\nz_2\n-0.3077\n0.4121\n0.6017\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.5887\n0.1503\n-0.3697\n\n\nz_1\n0.1503\n0.0416\n-0.0951\n\n\nz_2\n-0.3697\n-0.0951\n0.2325\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n0.0393\n\n\nz_1\n-0.0971\n\n\nz_2\n0.0904\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.4275\n0.1589\n0.7232\n\n\nSW_IN\n0.1958\n0.8560\n0.3983\n\n\nVPD\n0.3929\n0.3593\n0.7669\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.1843\n-0.0151\n-0.0008\n\n\nSW_IN\n-0.0151\n0.0013\n0.0001\n\n\nVPD\n-0.0008\n0.0001\n0.0000\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.5145\n\n\nSW_IN\n0.8793\n\n\nVPD\n0.6626\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n-0.0205\n\n\nz_1\n-0.6986\n\n\nz_2\n-0.6000\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.8298\n1.1307\n-0.6162\n\n\nz_1\n1.1307\n1.2855\n0.1820\n\n\nz_2\n-0.6162\n0.1820\n0.7764\n\n\n\n\n\n \n\n\n\n\n\n\nl_mean_smooth = train_loss('mean', use_smooth=True)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n697.162786\n630.356973\n0.503804\n0.739012\n00:36\n\n\n1\n635.893690\n554.948438\n0.397441\n0.837237\n00:33\n\n\n2\n573.158679\n438.329959\n0.397376\n0.839449\n00:33\n\n\n3\n443.483282\n104.015152\n0.415593\n0.825035\n00:34\n\n\n4\n279.839509\n-134.222046\n0.440517\n0.801995\n00:33\n\n\n5\n73.469916\n-312.444000\n0.405823\n0.831356\n00:33\n\n\n6\n-80.695937\n-384.546196\n0.409707\n0.827130\n00:33\n\n\n7\n-197.950462\n-445.921780\n0.409722\n0.827629\n00:33\n\n\n8\n-283.184402\n-451.063827\n0.409284\n0.828156\n00:34\n\n\n9\n-340.951170\n-454.172981\n0.406012\n0.830759\n00:34\n\n\n10\n-379.005242\n-456.886911\n0.399631\n0.835787\n00:33\n\n\n11\n-417.681836\n-581.048350\n0.137346\n0.980816\n00:33\n\n\n12\n-524.525933\n-843.416952\n0.122917\n0.984063\n00:33\n\n\n13\n-655.300585\n-940.494837\n0.112013\n0.987288\n00:33\n\n\n14\n-754.148642\n-954.246318\n0.106991\n0.988218\n00:33\n\n\n15\n-823.230645\n-964.190164\n0.093932\n0.990694\n00:33\n\n\n16\n-870.296853\n-964.511931\n0.097596\n0.990023\n00:33\n\n\n17\n-902.821560\n-966.711134\n0.083416\n0.992802\n00:33\n\n\n18\n-925.295414\n-974.465392\n0.070724\n0.994710\n00:36\n\n\n19\n-942.501159\n-981.922380\n0.061404\n0.996135\n00:34\n\n\n\n\n\n\n\n\n\nl_mean_smooth.recorder.plot_loss()\n\n\n\n\n\nshow_results(l_mean_smooth, [10, 50, 100])\n\n\n\n\n\n\n\ndisplay_as_row(l_mean_smooth.model.get_info(var_names=hai64.columns))\n\n\n  trans_matrix (A) \n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.0959\n-0.1656\n-0.0462\n\n\nz_1\n0.1858\n0.6684\n-0.1010\n\n\nz_2\n-0.0301\n0.0735\n0.9870\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.2404\n0.3451\n0.0008\n\n\nz_1\n0.3451\n0.5604\n-0.0763\n\n\nz_2\n0.0008\n-0.0763\n0.3327\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n0.5843\n\n\nz_1\n0.5746\n\n\nz_2\n0.3402\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.2666\n-0.1751\n-0.0240\n\n\nSW_IN\n-0.0178\n0.2368\n0.1899\n\n\nVPD\n0.3308\n-0.2863\n0.0118\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.0000\n0.0000\n0.0000\n\n\nSW_IN\n0.0000\n0.0000\n-0.0000\n\n\nVPD\n0.0000\n-0.0000\n0.0000\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.5414\n\n\nSW_IN\n0.4390\n\n\nVPD\n0.6652\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n0.7301\n\n\nz_1\n0.3346\n\n\nz_2\n0.2617\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.4340\n0.7932\n0.2754\n\n\nz_1\n0.7932\n1.1764\n0.2383\n\n\nz_2\n0.2754\n0.2383\n0.5386\n\n\n\n\n\n \n\n\n\n\n\n\nl_mean_n_smooth = train_loss('mean', use_smooth=False)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n1057.629457\n888.496646\n1.018840\n-0.055726\n00:22\n\n\n1\n923.925165\n796.762860\n0.674683\n0.544355\n00:22\n\n\n2\n851.088850\n747.840067\n0.596738\n0.645535\n00:23\n\n\n3\n798.429587\n705.430505\n0.606329\n0.634376\n00:23\n\n\n4\n754.539451\n668.123294\n0.616092\n0.622592\n00:23\n\n\n5\n715.262041\n636.254135\n0.611814\n0.628103\n00:23\n\n\n6\n681.105511\n615.883004\n0.617857\n0.620452\n00:23\n\n\n7\n651.706290\n591.641635\n0.601030\n0.640745\n00:23\n\n\n8\n625.154468\n570.554773\n0.592957\n0.650125\n00:23\n\n\n9\n593.910834\n477.784763\n0.467038\n0.783076\n00:23\n\n\n10\n542.761901\n414.206483\n0.429209\n0.817010\n00:23\n\n\n11\n488.455552\n339.528152\n0.422597\n0.821515\n00:24\n\n\n12\n404.451756\n144.976959\n0.433326\n0.812972\n00:23\n\n\n13\n308.818492\n107.266816\n0.447009\n0.800872\n00:23\n\n\n14\n228.120486\n59.730369\n0.401824\n0.838990\n00:23\n\n\n15\n164.371803\n39.778540\n0.411961\n0.830528\n00:23\n\n\n16\n121.075008\n31.160392\n0.421206\n0.823260\n00:23\n\n\n17\n84.520273\n4.141408\n0.420663\n0.823536\n00:24\n\n\n18\n57.648469\n4.640894\n0.418233\n0.825792\n00:24\n\n\n19\n36.316722\n3.784453\n0.421853\n0.822295\n00:23\n\n\n\n\n\n\n\n\n\nl_mean_n_smooth.recorder.plot_loss()\n\n\n\n\n\nshow_results(l_mean_n_smooth, [10, 50, 100])\n\n\n\n\n\n\n\nl_mean_n_smooth.model.use_smooth = True\ndisplay(show_results(l_mean_n_smooth, [10, 50, 100]))\nl_mean_n_smooth.model.use_smooth = False\n\n\n\n\n\n\n\ndisplay_as_row(l_mean_n_smooth.model.get_info(var_names=hai64.columns))\n\n\n  trans_matrix (A) \n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.8564\n0.3303\n0.2741\n\n\nz_1\n0.0411\n0.5239\n-0.3802\n\n\nz_2\n0.1810\n-0.3956\n0.6654\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.1339\n0.1567\n-0.1591\n\n\nz_1\n0.1567\n0.1839\n-0.1865\n\n\nz_2\n-0.1591\n-0.1865\n0.1893\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n-0.1275\n\n\nz_1\n0.0851\n\n\nz_2\n0.1455\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.3125\n0.3005\n0.5561\n\n\nSW_IN\n0.1724\n0.6300\n0.5023\n\n\nVPD\n0.1824\n0.6207\n0.7000\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.0000\n-0.0000\n0.0001\n\n\nSW_IN\n-0.0000\n0.0000\n-0.0016\n\n\nVPD\n0.0001\n-0.0016\n0.1801\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.4672\n\n\nSW_IN\n0.0952\n\n\nVPD\n0.2722\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n-1.0630\n\n\nz_1\n0.2947\n\n\nz_2\n-0.3536\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n3.4376\n0.9167\n0.0107\n\n\nz_1\n0.9167\n0.5092\n-0.3225\n\n\nz_2\n0.0107\n-0.3225\n0.4000\n\n\n\n\n\n \n\n\n\n\n\n\nimport dill\n\n\nwith open(\"models_loss_comparison_30dec.pickle\", 'wb') as f:\n    # dill.dump([l_sum_smooth, l_sum_n_smooth, l_mean_smooth, l_mean_n_smooth], f)\n\n\n\n\n\ninteract_results(l_sum_smooth, hai64)\n\n\n\n\n&lt;function meteo_imp.kalman.fastai.interact_results.&lt;locals&gt;._inner(gap_len, items_idx, block_len, **var_names)&gt;"
  },
  {
    "objectID": "kalman/Hainich_Control.html",
    "href": "kalman/Hainich_Control.html",
    "title": "Hainich with ERA-Interim controls",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\n\n\nhai = pd.read_parquet(hai_path)\nhai64 = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path)\nhai_era64 = pd.read_parquet(hai_era_path64)\n\n\ndls = imp_dataloader(hai64, hai_era64, var_sel = gen_var_sel(['TA', 'SW_IN', 'VPD']), block_len=200, gap_len=gen_gap_len(10, min_v=3), bs=20, control_lags=[1])\n\n\n# dls = imp_dataloader(hai64, hai_era64, var_sel = gen_var_sel(['TA', 'SW_IN', 'VPD']), block_len=200, gap_len=10, bs=20, control_lags=[1])"
  },
  {
    "objectID": "kalman/Hainich_Control.html#simple-training",
    "href": "kalman/Hainich_Control.html#simple-training",
    "title": "Hainich with ERA-Interim controls",
    "section": "Simple training",
    "text": "Simple training\n\nloss_func_ng = loss_func=KalmanLoss(only_gap=False)\nlearn = Learner(dls, model, loss_func_ng, cbs=[ShowGraphCallback, Float64Callback], metrics=[rmse_mask, r2_mask])\n\n\nlearn.fit(10, 2e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n607.058369\n515.446269\n0.366365\n-18.953901\n01:39\n\n\n1\n475.921415\n359.786886\n0.180804\n-1.877576\n01:49\n\n\n2\n348.836041\n215.060710\n0.133364\n-0.778128\n01:59\n\n\n3\n213.805165\n71.989816\n0.132907\n-1.213813\n02:04\n\n\n4\n84.765372\n-51.247118\n0.119912\n-0.064529\n02:07\n\n\n5\n-41.113329\n-182.953342\n0.075950\n0.661874\n02:08\n\n\n6\n-169.925174\n-311.168974\n0.064809\n-0.035979\n02:04\n\n\n7\n-295.206480\n-432.871735\n0.063802\n0.936280\n02:04\n\n\n8\n-424.575996\n-562.614567\n0.048965\n0.908722\n01:59\n\n\n9\n-545.755660\n-679.265734\n0.045273\n0.648892\n02:00\n\n\n\n\n\n\n\n\n\nshow_results(learn)\n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nlearn.fit(10, 7e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n-701.508009\n-719.499257\n0.042491\n0.364471\n01:46\n\n\n1\n-733.036040\n-751.966214\n0.039660\n0.615754\n01:51\n\n\n2\n-766.875790\n-802.134089\n0.037965\n0.984316\n01:46\n\n\n3\n-802.225402\n-842.992511\n0.034145\n0.977250\n01:44\n\n\n4\n-840.689974\n-884.280286\n0.040979\n0.983124\n01:53\n\n\n5\n-880.406406\n-921.259744\n0.036642\n0.871848\n01:45\n\n\n6\n-920.637857\n-965.865153\n0.034881\n0.975765\n01:47\n\n\n7\n-960.791818\n-1002.702275\n0.034850\n0.629993\n01:49\n\n\n8\n-1003.434672\n-1048.419952\n0.031607\n0.975755\n01:48\n\n\n9\n-1035.639274\n-1066.565967\n0.040136\n0.435163\n01:50\n\n\n\n\n\n\n\n\n\nshow_results(learn)\n\n\n\n\n\n\ntrain only on gap\n\nlearn.loss_func =  KalmanLoss(only_gap=True)\n\n\nshow_results(learn)\n\n\n\n\n\n\n\nlearn.fit(10, 7e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n-0.005931\n-2.003364\n0.030327\n0.985116\n01:14\n\n\n1\n-1.177973\n-3.447589\n0.029919\n0.704350\n01:16\n\n\n2\n-1.061323\n0.247962\n0.034471\n0.881624\n01:22\n\n\n3\n-1.619825\n-0.819893\n0.031246\n0.990277\n01:17\n\n\n4\n-2.642889\n-2.878098\n0.031077\n-0.946337\n01:18\n\n\n5\n-3.325557\n2.120081\n0.038847\n0.991815\n01:20\n\n\n6\n-4.295325\n-1.668097\n0.035035\n0.985702\n01:18\n\n\n7\n-2.293203\n-0.351283\n0.033916\n0.910861\n01:19\n\n\n8\n-2.816982\n-2.604867\n0.031628\n0.669590\n01:19\n\n\n9\n-2.989582\n-2.357790\n0.032688\n0.983338\n01:17\n\n\n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nlearn.metrics = learn.metrics + [mk_metric(m) for m in [rmse_gap, r2_gap]]\n\n\nshow_results(learn)\n\n\n\n\n\n\n\nlearn\n\n\nKalman Filter (3 obs, 3 state, 6 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.9932\n0.2895\n-0.2506\n\n\nx_1\n0.1582\n0.5455\n0.3679\n\n\nx_2\n-0.1023\n0.6036\n0.4589\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0317\n-0.0000\n0.0032\n\n\nx_1\n-0.0000\n0.0277\n0.0079\n\n\nx_2\n0.0032\n0.0079\n0.0268\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0705\n\n\nx_1\n0.0512\n\n\nx_2\n-0.1263\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.0415\n0.0734\n0.0369\n\n\ny_1\n0.1477\n-0.5466\n0.4599\n\n\ny_2\n0.1749\n0.0563\n-0.0193\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0000\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0000\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0000\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0593\n\n\ny_1\n0.1921\n\n\ny_2\n0.4388\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.4222\n0.6555\n-0.0202\n-0.9495\n0.4507\n0.1714\n\n\nx_1\n0.0009\n-0.0957\n0.1792\n0.3218\n-0.5409\n-0.2363\n\n\nx_2\n0.2752\n0.8957\n-0.1440\n0.1622\n0.1775\n-0.0837\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.3115\n\n\nx_1\n0.5003\n\n\nx_2\n0.5133\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n2.3021\n0.7211\n0.6520\n\n\nx_1\n0.7211\n3.7494\n3.8710\n\n\nx_2\n0.6520\n3.8710\n5.0761\n\n\n\n\n\n \n\n\n\n# learn.save(\"trained_hainich_control_9_jan_v1\")\n\nPath('models/trained_hainich_control_9_jan_v1.pth')"
  },
  {
    "objectID": "kalman/Hainich_Control.html#multiple-losses-training",
    "href": "kalman/Hainich_Control.html#multiple-losses-training",
    "title": "Hainich with ERA-Interim controls",
    "section": "Multiple losses training",
    "text": "Multiple losses training\n\ninp, targ = dls.one_batch()\ninp[0].eq(targ[0]).all()\ninp[1].all(1).all(1).any()\n\ntensor(False, device='cuda:0')\n\n\n\ninp[1].all(1).all(1)\n\ntensor([False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False],\n       device='cuda:0')\n\n\n\ninp[1][18].all()\n\ntensor(False, device='cuda:0')\n\n\n\nfor _ in range(100):\n    inp, targ = dls.one_batch()\n    assert not inp[1].all(1).all(1).any()\n\n\nloss_func_g = loss_func=KalmanLoss(only_gap=True)\nlearn = Learner(dls, model, loss_func_g, cbs=[ShowGraphCallback, Float64Callback], metrics=imp_metrics)\n\n\nlearn.fit(5, 2e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n35.681259\n25.316821\n0.569263\n0.675936\n-11.686040\n-4623607617801388104142699888640.000000\n01:11\n\n\n1\n28.467558\n23.233029\n0.524849\n0.610395\n-2.279983\n-2785450798303600881197612269568.000000\n01:12\n\n\n2\n24.512202\n21.556061\n0.508569\n0.522891\n-1.991859\n-1787686587625842508214908747776.000000\n01:17\n\n\n3\n22.205898\n18.522426\n0.504695\n0.497300\n-2.575984\n-2311579056314112802293087731712.000000\n01:14\n\n\n4\n19.906797\n16.373493\n0.532846\n0.453842\n-1.954186\n-2977305927889536455278742994944.000000\n01:13\n\n\n\n\n\n\n\n\n\nlearn\n\n\nKalman Filter (3 obs, 3 state, 6 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0550\n0.8305\n0.9071\n\n\nx_1\n0.8076\n0.7187\n0.3710\n\n\nx_2\n1.0252\n0.9262\n0.4660\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.1091\n0.0170\n0.0334\n\n\nx_1\n0.0170\n0.0027\n0.0052\n\n\nx_2\n0.0334\n0.0052\n0.0103\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n1.0085\n\n\nx_1\n0.2683\n\n\nx_2\n0.1844\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.0586\n-0.0735\n0.2603\n\n\ny_1\n0.3055\n0.4040\n0.4354\n\n\ny_2\n0.2157\n0.9007\n0.4757\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0795\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.6049\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.1729\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0631\n\n\ny_1\n0.6257\n\n\ny_2\n0.8232\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n0.1065\n-0.1080\n-0.8654\n-0.3202\n-0.0629\n-0.2701\n\n\nx_1\n-1.0667\n-0.2948\n-0.0322\n-0.4331\n0.0113\n-0.1139\n\n\nx_2\n0.9766\n0.2488\n-0.2589\n0.5920\n0.3605\n-0.1070\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0949\n\n\nx_1\n0.0299\n\n\nx_2\n0.0380\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.1153\n-0.1604\n-0.1218\n\n\nx_1\n-0.1604\n0.4181\n0.8881\n\n\nx_2\n-0.1218\n0.8881\n3.4152\n\n\n\n\n\n \n\n\n\nshow_results(learn)\n\n\n\n\n\n\n\nlearn.model.use_smooth = False\n\n\nlearn.fit(5, 2e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n98.515476\n76.464946\n3.587664\n9.146169\n-556.182098\n-1785660235166445426744542481088512.000000\n00:53\n\n\n1\n65.572638\n42.698891\n1.761536\n4.240729\n-961.723482\n-978013546570620530329106643943424.000000\n00:56\n\n\n2\n55.393772\n45.721459\n1.547412\n3.812272\n-358.409783\n-482895226247397956117409612955648.000000\n00:58\n\n\n3\n50.176197\n45.659879\n1.704954\n4.215966\n-1291.738173\n-256935436439301020515829859483648.000000\n01:01\n\n\n4\n48.109062\n50.373996\n1.703977\n4.298851\n-94.260413\n-526513865521285616344450662924288.000000\n00:56\n\n\n\n\n\n\n\n\n\nlearn\n\n\nKalman Filter (3 obs, 3 state, 6 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n-0.1561\n0.6142\n0.6362\n\n\nx_1\n0.5261\n0.4458\n0.1055\n\n\nx_2\n1.2212\n1.1215\n0.6900\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n3.4574\n3.0900\n2.8140\n\n\nx_1\n3.0900\n2.8125\n2.5484\n\n\nx_2\n2.8140\n2.5484\n2.3123\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.8462\n\n\nx_1\n0.1179\n\n\nx_2\n0.0779\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.0486\n-0.0819\n0.2397\n\n\ny_1\n0.2923\n0.3889\n0.4084\n\n\ny_2\n0.2078\n0.8587\n0.4657\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.1062\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.3648\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.3026\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.3044\n\n\ny_1\n0.6826\n\n\ny_2\n0.8804\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n0.1861\n-0.1349\n-0.7508\n-0.2323\n-0.0994\n-0.2119\n\n\nx_1\n-0.8815\n-0.2600\n0.1871\n-0.2865\n0.0373\n0.0849\n\n\nx_2\n1.0668\n0.1513\n-0.1757\n0.6918\n0.2565\n-0.0567\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.1463\n\n\nx_1\n-0.1932\n\n\nx_2\n-0.0464\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.4236\n0.0944\n0.3904\n\n\nx_1\n0.0944\n0.2095\n0.6389\n\n\nx_2\n0.3904\n0.6389\n6.4219\n\n\n\n\n\n \n\n\n\nlearn.use_smooth = True\n\n\nshow_results(learn)"
  },
  {
    "objectID": "kalman/KalmanFilter_simple.html",
    "href": "kalman/KalmanFilter_simple.html",
    "title": "Local Level Hainich",
    "section": "",
    "text": "simple local level model on Hainich data\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport altair as alt\nalt.renderers.enable('mimetype')\n\nRendererRegistry.enable('mimetype')\n\n\n\nfrom meteo_imp.data import hai, units\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\nfrom meteo_imp.kalman.model import *\nfrom ipywidgets import interact, interact_manual, IntSlider\n\n\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n1.636\n\n\n\n\n200 rows × 3 columns\n\n\n\n\ndata = MeteoDataTest(hai).add_gap(10, ['TA', 'SW_IN', 'VPD'], 30)\n\n\ndata.data\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n1.636\n\n\n\n\n200 rows × 3 columns\n\n\n\nWarning WIP. Data is not standardized (at it should)\n\nimp = KalmanImputation(data.data, LocalLevelModel)\n\n\nimp.fit()\n\n&lt;meteo_imp.kalman.imputation.KalmanImputation&gt;\n\n\n\nimp.impute()\n\nTypeError: KalmanModel.predict() takes 2 positional arguments but 3 were given\n\n\n\ndata.data.columns\n\n\nres = imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nres.display_results()\n\n\n\n\ndef gap2res(var_sel, gap_len, gap_start, model, n_iter):\n    data = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\n    return KalmanImputation(data.data, model).fit(n_iter=n_iter).to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10, LocalLevelModel, 5)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\n\n\n\ngap_len = 30\ngap_start = 40\nvar_sel = ('TA', 'SW_IN', 'VPD')\nn_iter = 10\n\n\ndata = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\nimp_ls = KalmanImputation(data.data, LocalSlopeModel).fit(n_iter=n_iter)\nres_ls = imp_ls.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nres_ls.display_results()\n\n\nimp\n\n\ndata = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\nimp_ls = KalmanImputation(data.data, LocalSlopeModel).fit(n_iter=n_iter, smooth=False).to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nimp_ls.display_results()"
  },
  {
    "objectID": "kalman/kalman_obs_cov.html",
    "href": "kalman/kalman_obs_cov.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport torch\n@cache_disk(\"full_hai\")\ndef load_data():\n    return read_fluxnet_csv(hai_path, None, num_dtype=np.float64)\n\nhai = load_data()\nshortcut for having randomly initialized and with correct type paramters\nmodel = KalmanFilterTester(dtype=torch.float64).filter\n# model = KalmanFilter(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1])\nmore sensible default to the obs_cov\n# model._set_constraint('obs_cov', model.obs_cov * 1e-1, train=True)\nmodel.obs_cov\n\ntensor([[0.8357, 0.4744, 0.7844],\n        [0.4744, 0.8175, 0.4352],\n        [0.7844, 0.4352, 1.0748]], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;)\nimport pandas as pd\nclass PosDefCallback(TrainEvalCallback):\n    def __init__(self):\n        self.info_posdef = pd.DataFrame()\n    def after_batch(self):\n        means, cov_batch = self.pred\n        info = []\n        for covs in cov_batch:\n            for t, cov in enumerate(covs):\n                info.append(check_posdef(cov.detach(), 'pred', error=False, t=t, n_iter = self.train_iter))\n        self.info_posdef = pd.concat([self.info_posdef] + info)\nposdef_info = PosDefCallback()\nobs_cov_history = SaveParams('obs_cov')\nCollectDataCallback??\n\n\nInit signature:\nCollectDataCallback(\n    *,\n    after_create=None,\n    before_fit=None,\n    before_epoch=None,\n    before_train=None,\n    before_batch=None,\n    after_pred=None,\n    after_loss=None,\n    before_backward=None,\n    after_cancel_backward=None,\n    after_backward=None,\n    before_step=None,\n    after_cancel_step=None,\n    after_step=None,\n    after_cancel_batch=None,\n    after_batch=None,\n    after_cancel_train=None,\n    after_train=None,\n    before_validate=None,\n    after_cancel_validate=None,\n    after_validate=None,\n    after_cancel_epoch=None,\n    after_epoch=None,\n    after_cancel_fit=None,\n    after_fit=None,\n)\nSource:        \nclass CollectDataCallback(Callback):\n    \"Collect all batches, along with `pred` and `loss`, into `self.data`. Mainly for testing\"\n    def before_fit(self): self.data = L()\n    def after_batch(self): \n        self.data.append(self.learn.to_detach((self.xb,self.yb,self.pred,self.loss)))\nFile:           ~/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/data.py\nType:           type\nSubclasses:\nsaved_data = CollectDataCallback()\ndls = make_dataloader(hai[:5_000], 200, 10, bs=1)\nlearn = Learner(dls, model, loss_func=imp_ll_loss, cbs=[obs_cov_history, posdef_info])\n# learn.fit(5, 5e-3)\nmodel.state_dict()\n\nOrderedDict([('transition_matrices',\n              tensor([[0.0582, 0.0629, 0.1236],\n                      [0.0526, 0.5262, 0.4768],\n                      [0.9552, 0.9288, 0.0835]], dtype=torch.float64)),\n             ('transition_offsets',\n              tensor([0.1326, 0.1571, 0.3754], dtype=torch.float64)),\n             ('transition_cov_raw',\n              tensor([[ 1.2689,  0.0000,  0.0000],\n                      [ 0.6757,  0.6465,  0.0000],\n                      [ 0.7361, -0.0382,  0.2051]], dtype=torch.float64)),\n             ('obs_matrices',\n              tensor([[0.6834, 0.3122, 0.3091],\n                      [0.0313, 0.0404, 0.9319],\n                      [0.1521, 0.2650, 0.1304]], dtype=torch.float64)),\n             ('obs_offsets',\n              tensor([0.2519, 0.2334, 0.2257], dtype=torch.float64)),\n             ('obs_cov_raw',\n              tensor([[ 0.9142,  0.0000,  0.0000],\n                      [ 0.5190,  0.7404,  0.0000],\n                      [ 0.8581, -0.0137,  0.5816]], dtype=torch.float64)),\n             ('initial_state_mean',\n              tensor([0.4277, 0.2882, 0.9814], dtype=torch.float64)),\n             ('initial_state_cov_raw',\n              tensor([[0.9341, 0.0000, 0.0000],\n                      [0.9147, 0.6666, 0.0000],\n                      [0.5304, 0.4687, 0.3136]], dtype=torch.float64))])\ninput, mask = dls[0]\n\nValueError: too many values to unpack (expected 2)\ndls.train\n\n&lt;fastai.data.core.TfmdDL&gt;\nlearn.fit(10, 5e-2)\n\n\n\n\n\n\n    \n      \n      0.00% [0/10 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/20 00:00&lt;?]\n    \n    \n\n\nRuntimeError: expected scalar type Double but found Float\nlearn.recorder.plot_loss()\nlen(obs_cov_history.params)\nobs_cov_history.params[-1]\nis_posdef2(obs_cov_history.params[-1])\nposdef_log\nposdef_info.info_posdef\nimport matplotlib.pyplot as plt\nplt.scatter(posdef_info.info_posdef.t, posdef_info.info_posdef.is_sym)\nimport altair as alt\nalt.data_transformers.enable('data_server')\nalt.Chart(posdef_info.info_posdef).mark_bar().encode(\n    x = 't',\n    y = 'count(sym_upto)',\n    color='sym_upto:N'\n)\nposdef_info.info_posdef.dtypes\ntrained_state = learn.model.state_dict()"
  },
  {
    "objectID": "kalman/kalman_obs_cov.html#results",
    "href": "kalman/kalman_obs_cov.html#results",
    "title": "Meteo Imputation",
    "section": "Results",
    "text": "Results\n\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\n\n\ndef gap2res(var_sel, gap_len, gap_start, block_start=1000, block_end=1200):\n    data = MeteoDataTest(hai.iloc[block_start:block_end, :]).add_gap(gap_len, var_sel, gap_start)\n    imp = KalmanImputation(data.data)\n    imp.model.filter.load_state_dict(trained_state)\n    return imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\ndata = MeteoDataTest(hai)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\nfrom ipywidgets import interact_manual, IntSlider"
  },
  {
    "objectID": "kalman/Hainich_results_pres18_jan.html",
    "href": "kalman/Hainich_results_pres18_jan.html",
    "title": "Hainich with ERA-Interim",
    "section": "",
    "text": "Manual fine tuning learning process for presentation 18 Jan 2023\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import  *\nfrom meteo_imp.data import  _def_meteo_vars, units\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\n\nbase_dir = here(\"analysis/presentations/plots_18_jan\")\nbase_dir.mkdir(exist_ok=True)\n\n\ndef save_plot(p, path):\n    f_name = base_dir / (path + \".vl.json\")\n    with open(f_name, 'w') as f:\n        f.write(p.to_json())\n    return f_name\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path64)\n\n\n\n\npc = PCA().fit(hai)\n\n\nd0 = hai.iloc[0:1]\n\n\ntr0 = pc.transform(d0)\n\n\ntr0\n\narray([[-121.11917652,   -7.06844313,    0.87975241]])\n\n\n\nt0 = np.random.randn(5,1) * 10\ntt = np.hstack([t0, 2 * t0 + np.random.randn(5,1), 3 * t0 + np.random.randn(5,1)])\n\n\ntt = np.vstack([tt, -tt])\n\n\ntt.mean(0)\n\narray([1.77635684e-16, 0.00000000e+00, 0.00000000e+00])\n\n\n\ndef pca(X):\n    # Data matrix X, assumes 0-centered\n    n, m = X.shape\n    assert np.allclose(X.mean(axis=0), np.zeros(m))\n    # Compute covariance matrix\n    C = np.dot(X.T, X) / (n-1)\n    # Eigen decomposition\n    eigen_vals, eigen_vecs = np.linalg.eig(C)\n    # Project X onto PC space\n    X_pca = np.dot(X, eigen_vecs)\n    return X_pca, eigen_vecs\n\n\npca(tt)\n\n(array([[-9.15947487e+00, -3.03773243e-01,  3.81626348e-01],\n        [ 2.00045215e+01, -1.99595583e-02,  1.53745860e+00],\n        [ 1.33744310e+01,  2.01395206e-02, -3.19819214e-02],\n        [-8.99057811e+00,  1.24208069e-01,  9.57988123e-01],\n        [ 3.14616875e+01, -4.88141095e-02, -5.79117681e-01],\n        [ 9.15947487e+00,  3.03773243e-01, -3.81626348e-01],\n        [-2.00045215e+01,  1.99595583e-02, -1.53745860e+00],\n        [-1.33744310e+01, -2.01395206e-02,  3.19819214e-02],\n        [ 8.99057811e+00, -1.24208069e-01, -9.57988123e-01],\n        [-3.14616875e+01,  4.88141095e-02,  5.79117681e-01]]),\n array([[-0.26545847, -0.95659856,  0.12021234],\n        [-0.53518386,  0.04249433, -0.84366608],\n        [-0.80194142,  0.28829401,  0.52323659]]))\n\n\n\nsk_pc = PCA(2).fit(tt)\n\n\ntt @ sk_pc.components_.T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\ntr = sk_pc.transform(tt)\ntr\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\nsk_pc.components_.T\n\narray([[-0.26545847,  0.12021234],\n       [-0.53518386, -0.84366608],\n       [-0.80194142,  0.52323659]])\n\n\n\n(sk_pc.components_ @ tt.T).T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\ntt[0, None]\n\narray([[2.76792539, 4.56712931, 7.45746711]])\n\n\n\nsk_pc.components_.shape\n\n(3, 3)\n\n\n\ntt[0].shape\n\n(3,)\n\n\n\n(sk_pc.components_ @ tt[0])\n\narray([-9.15947487,  0.38162635])\n\n\n\ntt[0]\n\narray([2.76792539, 4.56712931, 7.45746711])\n\n\n\nsk_pc.components_.T @ tr[0]\n\narray([2.47733634, 4.58003795, 7.54504311])\n\n\n\nsk_pc.inverse_transform(tr[0])\n\narray([2.47733634, 4.58003795, 7.54504311])\n\n\n\nd0.to_numpy()\n\narray([[-0.6  ,  0.   ,  0.222]])\n\n\n\nhai.iloc[0]\n\nTA      -0.600\nSW_IN    0.000\nVPD      0.222\nName: 2000-01-01 00:30:00, dtype: float64\n\n\n\\[ x = y\\Lambda \\]\n\npc.components_\n\narray([[ 0.01681572,  0.99979324,  0.01143269],\n       [ 0.93010891, -0.01983747,  0.36674772],\n       [-0.36689868, -0.00446652,  0.93025018]])\n\n\n\nnp.linalg.inv(pc.components_)\n\narray([[ 0.01681572,  0.93010891, -0.36689868],\n       [ 0.99979324, -0.01983747, -0.00446652],\n       [ 0.01143269,  0.36674772,  0.93025018]])\n\n\n\n\n\nassuming that the control has the same dimensions of the observations then if we are doing a local slope model we have \\(B \\in \\mathbb{R}^{state \\times contr}\\): \\[ B = \\begin{bmatrix} -I & I \\\\ 0 & 0 \\end{bmatrix}\\]\n\nfrom torch import hstack, eye, vstack, ones, zeros, tensor\nfrom functools import partial\n\n\ndef set_dtype(*args, dtype=torch.float64):\n    return [partial(arg, dtype=dtype) for arg in args] \n\n\neye, ones, zeros, tensor = set_dtype(eye, ones, zeros, tensor)\n\n\ndef init_smart(n_dim_obs, n_dim_state, df, pca=True):\n    # n_dim_obs == n_dim_contr\n    if pca:\n        comp = PCA(n_dim_state).fit(df).components_\n        obs_matrix = tensor(comp.T) # transform state -&gt; obs\n        contr_matrix = tensor(comp) # transform obs -&gt; state\n    else:\n        obs_matrix, contr_matrix = eye(n_dim_obs), eye(n_dim_obs)\n        \n    return KalmanFilter(\n        trans_matrix =     vstack([hstack([eye(n_dim_state),                eye(n_dim_state)]),\n                                   hstack([zeros(n_dim_state, n_dim_state), eye(n_dim_state)])]),\n        trans_off =        zeros(n_dim_state * 2),        \n        trans_cov =        eye(n_dim_state * 2)*.1,        \n        obs_matrix =       hstack([obs_matrix, zeros(n_dim_obs, n_dim_state)]),\n        obs_off =          zeros(n_dim_obs),          \n        obs_cov =          eye(n_dim_obs)*.01,            \n        contr_matrix =     vstack([hstack([-contr_matrix,                  contr_matrix]),\n                                   hstack([ zeros(n_dim_state,n_dim_obs), zeros(n_dim_state, n_dim_obs)])]),\n        init_state_mean =  zeros(n_dim_state * 2),        \n        init_state_cov =   eye(n_dim_state * 2) * 3,\n    ) \n\n\nnp.hstack([np.eye(2), np.eye(2)])\n\narray([[1., 0., 1., 0.],\n       [0., 1., 0., 1.]])\n\n\n\ninit_smart(3,2, hai)\n\n\nKalman Filter (3 obs, 4 state, 6 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n1.0000\n0.0000\n1.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n1.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\n\n\n\n\ny_0\n0.0168\n0.9301\n0.0000\n0.0000\n\n\ny_1\n0.9998\n-0.0198\n0.0000\n0.0000\n\n\ny_2\n0.0114\n0.3667\n0.0000\n0.0000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0168\n-0.9998\n-0.0114\n0.0168\n0.9998\n0.0114\n\n\nx_1\n-0.9301\n0.0198\n-0.3667\n0.9301\n-0.0198\n0.3667\n\n\nx_2\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\n\n\n\n\n\n \n\n\n\nclass PersistentRecorder(Callback):\n    order = 70\n    name = \"per_recorder\"\n    attrs = ['lrs', 'iters', 'losses', 'values']\n    def before_fit(self):\n        \"Prepare state for training\"\n        for attr in self.attrs:\n            if not hasattr(self.per_recorder, attr): setattr(self.per_recorder, attr, [])\n\n    def after_batch(self):\n        for attr in self.attrs:\n            setattr(self.per_recorder, attr, getattr(self.recorder, attr))\n\n\nmodels = []\n\n\ndls = imp_dataloader(hai, hai_era, var_sel = ['TA', 'SW_IN', 'VPD'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nlen(dls.valid.items)\nitems = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n\n\ndef train_show_save(learn, n_iter, lr):\n    learn.fit(n_iter, lr)\n    models.append(learn.model.state_dict().copy())\n    learn.recorder.plot_loss()\n    items = [learn.dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n    return show_results(learn, items = items, control=hai_control)\n\n\n\n\n\nmodel = init_smart(3,3, hai, pca=False).cuda()\nmodel.var_names = _def_meteo_vars.values()\n\n\nloss = loss_func=KalmanLoss(only_gap=False)\nlearn = Learner(dls, model, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\nlearn.model.use_smooth = True\n\n\nshow_results(learn, items=items, control=hai_control)\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-87.370314\n-96.212915\n0.069196\n0.210351\n0.946861\n-250571814513532732431918956544.000000\n02:37\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1489\n0.0042\n0.0120\n0.8404\n-0.0794\n-0.1127\n\n\nx_1\n-0.0954\n1.1464\n-0.0546\n0.1177\n0.8560\n0.0599\n\n\nx_2\n-0.0225\n0.0420\n1.1357\n-0.0717\n-0.1190\n0.8475\n\n\nx_3\n0.1201\n0.0661\n0.1170\n1.1840\n-0.1150\n-0.1073\n\n\nx_4\n0.0770\n0.1299\n0.0068\n-0.1847\n1.0987\n-0.1500\n\n\nx_5\n0.1084\n0.0358\n0.1133\n-0.1809\n-0.0281\n1.1533\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0341\n0.0493\n0.0361\n-0.0008\n-0.0312\n0.0026\n\n\nx_1\n0.0493\n0.1125\n0.0531\n0.0475\n0.0047\n0.0509\n\n\nx_2\n0.0361\n0.0531\n0.0399\n0.0001\n-0.0327\n0.0058\n\n\nx_3\n-0.0008\n0.0475\n0.0001\n0.0608\n0.0614\n0.0585\n\n\nx_4\n-0.0312\n0.0047\n-0.0327\n0.0614\n0.0901\n0.0554\n\n\nx_5\n0.0026\n0.0509\n0.0058\n0.0585\n0.0554\n0.0594\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0008\n\n\nx_1\n-0.0083\n\n\nx_2\n0.0067\n\n\nx_3\n0.0023\n\n\nx_4\n-0.0007\n\n\nx_5\n0.0005\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.8238\n-0.1897\n-0.1014\n-0.0841\n0.1914\n0.1286\n\n\ny_1\n0.0814\n0.8353\n0.1048\n0.1040\n0.0127\n0.0689\n\n\ny_2\n-0.0742\n-0.1502\n0.8296\n0.1282\n0.1811\n-0.0746\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0084\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0084\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0084\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0017\n\n\ny_1\n-0.0046\n\n\ny_2\n0.0026\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0290\n-0.0485\n-0.0022\n0.9297\n0.0792\n-0.0247\n\n\nx_1\n-0.0267\n-0.8778\n-0.0233\n0.0113\n0.9099\n0.0261\n\n\nx_2\n-0.0018\n-0.0592\n-1.0044\n-0.0310\n0.0674\n0.9204\n\n\nx_3\n-0.0503\n-0.0312\n-0.0556\n-0.0440\n-0.0529\n-0.0587\n\n\nx_4\n-0.0375\n-0.0537\n-0.0246\n-0.0412\n-0.0352\n-0.0338\n\n\nx_5\n-0.0653\n-0.0179\n-0.0575\n-0.0655\n-0.0307\n-0.0449\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0393\n\n\nx_1\n0.0044\n\n\nx_2\n-0.0092\n\n\nx_3\n0.0207\n\n\nx_4\n-0.0094\n\n\nx_5\n-0.0021\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.5500\n-0.1402\n-0.0082\n0.5233\n0.3011\n0.0882\n\n\nx_1\n-0.1402\n2.4578\n0.1502\n0.0355\n0.5714\n-0.0459\n\n\nx_2\n-0.0082\n0.1502\n2.5045\n0.2739\n0.0634\n0.5195\n\n\nx_3\n0.5233\n0.0355\n0.2739\n2.3814\n-0.2201\n-0.1842\n\n\nx_4\n0.3011\n0.5714\n0.0634\n-0.2201\n2.3880\n-0.1162\n\n\nx_5\n0.0882\n-0.0459\n0.5195\n-0.1842\n-0.1162\n2.3874\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-108.023250\n-113.113034\n0.073622\n0.198386\n0.956828\n-105578612000578019271909572608.000000\n02:37\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1440\n-0.0057\n0.0107\n0.8262\n-0.0845\n-0.1205\n\n\nx_1\n-0.0534\n1.2089\n-0.0223\n0.0948\n0.7937\n0.0494\n\n\nx_2\n-0.0289\n0.0176\n1.1417\n-0.0702\n-0.1154\n0.8351\n\n\nx_3\n0.1277\n0.0589\n0.1074\n1.1787\n-0.1203\n-0.1024\n\n\nx_4\n0.0654\n0.1560\n0.0043\n-0.1781\n1.1087\n-0.1415\n\n\nx_5\n0.0959\n0.0439\n0.1197\n-0.1725\n-0.0340\n1.1496\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0207\n0.0254\n0.0210\n-0.0052\n-0.0232\n-0.0033\n\n\nx_1\n0.0254\n0.0548\n0.0255\n0.0259\n0.0034\n0.0281\n\n\nx_2\n0.0210\n0.0255\n0.0222\n-0.0056\n-0.0241\n-0.0023\n\n\nx_3\n-0.0052\n0.0259\n-0.0056\n0.0473\n0.0504\n0.0465\n\n\nx_4\n-0.0232\n0.0034\n-0.0241\n0.0504\n0.0695\n0.0473\n\n\nx_5\n-0.0033\n0.0281\n-0.0023\n0.0465\n0.0473\n0.0477\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0004\n\n\nx_1\n-0.0055\n\n\nx_2\n0.0048\n\n\nx_3\n0.0027\n\n\nx_4\n-0.0033\n\n\nx_5\n0.0021\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7902\n-0.2032\n-0.1217\n-0.0986\n0.2164\n0.1090\n\n\ny_1\n0.0160\n0.7417\n0.0555\n0.1016\n0.1041\n0.0374\n\n\ny_2\n-0.1103\n-0.1594\n0.7916\n0.1185\n0.2096\n-0.0949\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0077\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0077\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0077\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0009\n\n\ny_1\n-0.0053\n\n\ny_2\n-0.0021\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0277\n-0.0595\n-0.0019\n0.9181\n0.0838\n-0.0194\n\n\nx_1\n-0.0414\n-0.8661\n-0.0323\n0.0110\n0.8741\n0.0262\n\n\nx_2\n-0.0029\n-0.0647\n-1.0045\n-0.0319\n0.0757\n0.8957\n\n\nx_3\n-0.0678\n-0.0368\n-0.0564\n-0.0559\n-0.0610\n-0.0639\n\n\nx_4\n-0.0374\n-0.0690\n-0.0209\n-0.0485\n-0.0406\n-0.0354\n\n\nx_5\n-0.0591\n-0.0267\n-0.0753\n-0.0633\n-0.0407\n-0.0555\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0396\n\n\nx_1\n-0.0009\n\n\nx_2\n-0.0049\n\n\nx_3\n0.0153\n\n\nx_4\n-0.0139\n\n\nx_5\n-0.0094\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.6364\n-0.1456\n-0.0020\n0.5695\n0.3986\n0.1628\n\n\nx_1\n-0.1456\n2.3292\n0.1064\n0.2074\n0.7591\n0.1475\n\n\nx_2\n-0.0020\n0.1064\n2.4205\n0.3489\n0.1488\n0.6143\n\n\nx_3\n0.5695\n0.2074\n0.3489\n2.2351\n-0.4355\n-0.3277\n\n\nx_4\n0.3986\n0.7591\n0.1488\n-0.4355\n2.1625\n-0.3445\n\n\nx_5\n0.1628\n0.1475\n0.6143\n-0.3277\n-0.3445\n2.2518\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps\")\n\nPath('models/17_jan_all_gaps.pth')\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-149.288297\n-152.804403\n0.072760\n0.179190\n0.956827\n-61340364986143763995601928192.000000\n02:35\n\n\n1\n-160.099637\n-165.002097\n0.071263\n0.181697\n0.963587\n-48387608947773502947627892736.000000\n02:37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps_final\")\n\nPath('models/17_jan_all_gaps_final.pth')\n\n\n\n\n\nlearn.load(\"17_jan_all_gaps_final\");\n\n\np = show_results(learn, control=hai_control, items = [items[i] for i in [0,5,4]], show_metric=False, units=list(units.values()))\np\n\n\n\n\n\n\n\np0 = show_results(learn, control=hai_control, items = [items[i] for i in [0]], n_cols=2, show_metric=False, props={'width': 350, 'height': 250},  units=list(units.values()))\np0\n\n\n\n\n\n\n\np1 = show_results(learn, control=hai_control, items = [items[i] for i in [5]], n_cols=2, show_metric=False, props={'width': 350, 'height': 250}, units=list(units.values()))\np1\n\n\n\n\n\n\n\np2 = show_results(learn, control=hai_control, items = [items[i] for i in [4]], n_cols=2, show_metric=False, props={'width': 350, 'height': 250}, units=list(units.values()))\np2\n\n\n\n\n\n\n\nsave_plot(p0, \"results_gap_all_vars_g0\")\nsave_plot(p1, \"results_gap_all_vars_g1\")\nsave_plot(p2, \"results_gap_all_vars_g2\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/analysis/presentations/plots_18_jan/results_gap_all_vars_g2.vl.json')\n\n\n\ninteract_results(learn, hai, hai_era)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n&lt;function meteo_imp.kalman.fastai.interact_results.&lt;locals&gt;._inner(gap_len, items_idx, control_lags, block_len, shift, **var_names)&gt;\n\n\n\n\n\n\n\ndls2 = imp_dataloader(hai, hai_era, var_sel = ['TA'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nmodel2 = init_smart(3,3, hai, pca=True).cuda()\n\n\nmodel2\n\n\nKalman Filter (3 obs, 6 state, 6 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.0000\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n1.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\nx_4\n0.0000\n\n\nx_5\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.0168\n0.9301\n-0.3669\n0.0000\n0.0000\n0.0000\n\n\ny_1\n0.9998\n-0.0198\n-0.0045\n0.0000\n0.0000\n0.0000\n\n\ny_2\n0.0114\n0.3667\n0.9303\n0.0000\n0.0000\n0.0000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0168\n-0.9998\n-0.0114\n0.0168\n0.9998\n0.0114\n\n\nx_1\n-0.9301\n0.0198\n-0.3667\n0.9301\n-0.0198\n0.3667\n\n\nx_2\n0.3669\n0.0045\n-0.9303\n-0.3669\n-0.0045\n0.9303\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\nx_4\n0.0000\n\n\nx_5\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n3.0000\n\n\n\n\n\n \n\n\n\nloss2 = loss_func=KalmanLoss(only_gap=False)\nlearn2 = Learner(dls2, model2, loss2, cbs=[Float64Callback], metrics=imp_metrics)\n\n\ntrain_show_save(learn2, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-60.562467\n-78.105121\n0.041354\n0.095727\n0.974146\n-10.220484\n03:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn2, 3, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-99.062288\n-107.225395\n0.043633\n0.077547\n0.955371\n-5.651017\n02:56\n\n\n1\n-115.215551\n-122.628456\n0.048706\n0.075058\n0.960905\n-4.600422\n02:52\n\n\n2\n-130.109601\n-137.303035\n0.054807\n0.069069\n0.966896\n-3.014253\n02:58\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn2.loss_func = KalmanLoss(only_gap=True)\n\n\ntrain_show_save(learn2, 1, .5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-2.358371\n-2.383616\n0.058387\n0.077012\n0.958465\n-4.192017\n01:52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn2, 1, 2e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-2.387042\n-2.445100\n0.057919\n0.073482\n0.956432\n-3.904405\n01:51"
  },
  {
    "objectID": "Multi latent - GPFA Hainich.html",
    "href": "Multi latent - GPFA Hainich.html",
    "title": "Multiple Latent",
    "section": "",
    "text": "Trying to use more than 1 latent variable\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\n\ndef reset_seed():\n    torch.manual_seed(27);\n    np.random.seed(27);\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n302.475\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n\n\n...\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n330.202\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n330.202\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n330.202\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n\n\n\n\n200 rows × 4 columns\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values())\n\n\ncache_file_gaps = cache_path / \"hai_diff_latents.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\n@cache_disk(cache_file_gaps)\ndef compute_multiple_latent():\n    hai_r_gaps = [GPFAImputation(\n        data_r_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_r_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    hai_c_gaps = [GPFAImputationExplorer(\n        data_c_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_c_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    return hai_r_gaps, hai_c_gaps\n\n\nhai_r_gaps, hai_c_gaps = compute_multiple_latent()\n\n\n\n\n\n\nhai_r_gaps\n\n[&lt;gpfa_imputation.results.ImputationResult&gt;,\n &lt;gpfa_imputation.results.ImputationResult&gt;,\n &lt;gpfa_imputation.results.ImputationResult&gt;]\n\n\n\nhai_r_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6601\n\n\nSW_IN\n0.0865\n\n\nLW_IN\n-0.0341\n\n\nVPD\n0.9569\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.5224\n°C\n\n\nSW_IN\n46.1695\nW m-2\n\n\nLW_IN\n21.2045\nW m-2\n\n\nVPD\n0.0578\nhPa\n\n\n\n\n\n \n\n\n\nModel Info  Lambda \n\n\n\n\n0\nz0\n\n\n\n\nTA\n0.8015\n\n\nSW_IN\n0.4251\n\n\nLW_IN\n-0.1879\n\n\nVPD\n1.0596\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.6057\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4174\n\n\nSW_IN\n0.8049\n\n\nLW_IN\n0.9361\n\n\nVPD\n0.0103\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0224\n\n\n\n\n\n \n\n\n\nai_r_gaps[0].plot_pred()\n\n\nhai_r_gaps[1].display_results()\n\n\nhai_r_gaps[2].display_results()\n\n\n\n\n\nhai_c_gaps[0].display_results()\n\n\nhai_c_gaps[1].display_results()\n\n\nhai_c_gaps[2].display_results()"
  },
  {
    "objectID": "extract_gap_all_fluxnet.html",
    "href": "extract_gap_all_fluxnet.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.fluxnet.gap_finder import *\n\nlinks are obtained from this https://fluxnet.org/data-and-manifest/ page(requires login) by running in the browser console this code\nvar x = document.querySelectorAll(\"a\");\nvar myarray = []\nfor (var i=0; i&lt;x.length; i++){\nvar nametext = x[i].textContent;\nvar cleantext = nametext.replace(/\\s+/g, ' ').trim();\nvar cleanlink = x[i].href;\nmyarray.push([cleantext,cleanlink]);\n};\nfunction make_table() {\n    var table = '&lt;table&gt;&lt;thead&gt;&lt;th&gt;Links&lt;/th&gt;&lt;/thead&gt;&lt;tbody&gt;';\n   for (var i=0; i&lt;myarray.length; i++) {\n            table += '&lt;tr&gt;&lt;td&gt;'+myarray[i][1]+'&lt;/td&gt;&lt;/tr&gt;';\n    };\n \n    var w = window.open(\"\");\nw.document.write(table); \n}\nmake_table()\ncode inspired from https://towardsdatascience.com/quickly-extract-all-links-from-a-web-page-using-javascript-and-the-browser-console-49bb6f48127b\nand then doing some smart copy pasting\nacually download in parallel all files with, so is faster than download with python\nparallel -a fluxnet_parallel_wget.txt --jobs 10 wget\n\nfrom fluxnet_links import all_fluxnet_link\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\n\ntest_file = Path(\"../../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip\")\ntmp_dir = Path(\"/tmp\")\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\n\n\nsite_info = download_and_find_gaps(all_fluxnet_link, download_dir, out_dir, tmp_dir)\n\n\n\n\nFileNotFoundError: [Errno 2] No such file or directory: '/run/media/simone/Simone DATI/fluxnet_all/FLX_AR-SLu_FLUXNET2015_FULLSET_2009-2011_1-4.zip'\n\n\n\nsite_info\n\n\n\n\nshape: (206, 3)\nstart\nend\nsite\ni64\ni64\nstr\n200901010030\n201201010000\n\"AR-SLu\"\n200901010030\n201301010000\n\"AR-Vir\"\n200201010030\n201301010000\n\"AT-Neu\"\n200701010030\n201001010000\n\"AU-Ade\"\n201001010030\n201501010000\n\"AU-ASM\"\n201001010030\n201501010000\n\"AU-Cpr\"\n201201010030\n201501010000\n\"AU-Cum\"\n200701010030\n201401010000\n\"AU-DaP\"\n200801010030\n201501010000\n\"AU-DaS\"\n200801010030\n201501010000\n\"AU-Dry\"\n201101010030\n201401010000\n\"AU-Emr\"\n200601010030\n200901010000\n\"AU-Fog\"\n...\n...\n...\n200301010030\n200401010000\n\"US-Wi1\"\n200301010030\n200401010000\n\"US-Wi2\"\n200201010030\n200501010000\n\"US-Wi3\"\n200201010030\n200601010000\n\"US-Wi4\"\n200401010030\n200501010000\n\"US-Wi5\"\n200201010030\n200401010000\n\"US-Wi6\"\n200501010030\n200601010000\n\"US-Wi7\"\n200201010030\n200301010000\n\"US-Wi8\"\n200401010030\n200601010000\n\"US-Wi9\"\n200401010030\n201501010000\n\"US-Wkg\"\n201101010030\n201401010000\n\"US-WPT\"\n200001010030\n201001010000\n\"ZM-Mon\"\n\n\n\n\n\nsite_info.write_parquet(out_dir / \"../site_info.parquet\")"
  },
  {
    "objectID": "GPFA Hainich - multi latent var.html",
    "href": "GPFA Hainich - multi latent var.html",
    "title": "Multiple latent …",
    "section": "",
    "text": "Trying to use more than 1 latent variable\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\n\ndef reset_seed():\n    torch.manual_seed(27);\n    np.random.seed(27);\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n302.475\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n\n\n...\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n330.202\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n330.202\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n330.202\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n\n\n\n\n200 rows × 4 columns\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values())\n\n\ncache_file_gaps = cache_path / \"hai_diff_latents.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\n@cache_disk(cache_file_gaps)\ndef compute_multiple_latent():\n    hai_r_gaps = [GPFAImputationExplorer(\n        data_r_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_r_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    hai_c_gaps = [GPFAImputationExplorer(\n        data_c_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_c_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    return hai_r_gaps, hai_c_gaps\n\n\nhai_r_gaps, hai_c_gaps = compute_multiple_latent()\n\n\n\n\n\n\nhai_r_gaps\n\n[&lt;gpfa_imputation.imputation.ImputationResult&gt;,\n &lt;gpfa_imputation.imputation.ImputationResult&gt;,\n &lt;gpfa_imputation.imputation.ImputationResult&gt;]\n\n\n\nhai_r_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6243\n\n\nSW_IN\n0.1244\n\n\nLW_IN\n0.0072\n\n\nVPD\n0.9597\n\n\n\n\nΛ\n\n\n\nvariable\nz0\n\n\n\n\ntime\n0.8015\n\n\nvariable\n0.4251\n\n\nmean\n-0.1879\n\n\nstd\n1.0596\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.6057\n\n\n\n\n\n \n\n\n\nhai_r_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1884\n\n\nLW_IN\n0.9673\n\n\nVPD\n0.5811\n\n\n\n\nΛ\n\n\n\nvariable\nz0\nz1\n\n\n\n\ntime\n0.8553\n0.5997\n\n\nvariable\n0.3110\n-0.1375\n\n\nmean\n-0.3717\n0.6710\n\n\nstd\n0.7057\n0.2981\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.0575\n\n\nz1\n7.4311\n\n\n\n\n\n \n\n\n\nhai_r_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.9635\n\n\nLW_IN\n0.9679\n\n\nVPD\n0.6259\n\n\n\n\nΛ\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\ntime\n0.8547\n0.4558\n-0.0728\n\n\nvariable\n0.4540\n-0.3101\n1.2743\n\n\nmean\n-0.2829\n0.7159\n0.0165\n\n\nstd\n0.7268\n0.1550\n0.2461\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.8582\n\n\nz1\n7.3777\n\n\nz2\n5.3624\n\n\n\n\n\n \n\n\n\n\n\n\nhai_c_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9793\n\n\nSW_IN\n0.0152\n\n\nLW_IN\n-0.0507\n\n\nVPD\n0.5711\n\n\n\n\nΛ\n\n\n\nvariable\nz0\n\n\n\n\ntime\n0.8542\n\n\nvariable\n0.1202\n\n\nmean\n0.1860\n\n\nstd\n0.6229\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.0641\n\n\n\n\n\n \n\n\n\nhai_c_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9789\n\n\nSW_IN\n0.1882\n\n\nLW_IN\n0.9552\n\n\nVPD\n0.6114\n\n\n\n\nΛ\n\n\n\nvariable\nz0\nz1\n\n\n\n\ntime\n0.5927\n0.6728\n\n\nvariable\n0.4136\n-0.0936\n\n\nmean\n-0.5082\n0.5232\n\n\nstd\n0.5586\n0.4200\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.8459\n\n\nz1\n6.5770\n\n\n\n\n\n \n\n\n\nhai_c_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n  r2 \n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9785\n\n\nSW_IN\n0.9640\n\n\nLW_IN\n0.9562\n\n\nVPD\n0.6513\n\n\n\n\nΛ\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\ntime\n-0.0527\n-0.4903\n0.7231\n\n\nvariable\n0.8765\n0.1260\n0.2360\n\n\nmean\n-0.0042\n-0.6728\n-0.3668\n\n\nstd\n0.2132\n-0.2787\n0.5878\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n4.5862\n\n\nz1\n6.9706\n\n\nz2\n5.3607"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#outline",
    "href": "presentations/presentation_bioclim_18_jan_23.html#outline",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\nKalman Filter\nPreliminary results\nNext steps"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#background",
    "href": "presentations/presentation_bioclim_18_jan_23.html#background",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Background",
    "text": "Background\n\nEC tower measures also meteorological variables (eg. air temperature, wind speed)\ntechnical issues (eg. broken sensor) result in meteo time series with gaps\nPresence of gaps is a problem in many EC data applications (eg. ecosystem modelling)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#dataset",
    "href": "presentations/presentation_bioclim_18_jan_23.html#dataset",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Dataset",
    "text": "Dataset\n\nFluxnet 2015 data from Hainich (more 20 years)\nGlobal meteo dataset (downscaled ERA-Interim from Fluxnet 2015)\nmeteorological measurements every 30 mins\nfocusing on 3 variables\n\nAir temperature: TA\nIncoming shortwave radiation: SW_IN\nVapour Pressure Deficit: VPD"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of gaps for all TA, SW_IN and VPD for all sites. 26 millions total obs"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of small gaps (&lt;1 week) for all TA, SW_IN and VPD for all sites. 26 millions total obs"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-fill-gaps",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-fill-gaps",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to fill gaps",
    "text": "How to fill gaps\n\nuse previous and following measurements for one variable and temporal auto-correlation (eg. diurnal cycles)\ncorrelation with other variables measures (eg. solar radiation and temperature)\nother measurements of meteo variables (eg. nearby station)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#state-of-the-art",
    "href": "presentations/presentation_bioclim_18_jan_23.html#state-of-the-art",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "State of the art",
    "text": "State of the art\nOneFlux pipeline (Fluxnet + ICOS + AmeriFlux)\n\nShort and medium gaps using Marginal Distribution Sampling (MDS)\nLong gaps filled with ERA data (global meteo dataset) using linear transformation to reduce site bias"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-mds-marginal-distribution-sampling-works",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-mds-marginal-distribution-sampling-works",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How MDS (Marginal Distribution Sampling) works",
    "text": "How MDS (Marginal Distribution Sampling) works\n\n\n\ntake a time window (7 days) around the gap\nuse 3 predictors variables (TA, SW_IN and VPD) and divide them in n discrete bins\nfor each bin (combination of conditions) find the average value of the missing variable\nfor each gap find the closest condition and fill with the average value\nif necessary increase the time window\nquality flag depends on the time window size"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-ta",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-ta",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling TA",
    "text": "MDS - gap filling TA"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-sw_in",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-sw_in",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling SW_IN",
    "text": "MDS - gap filling SW_IN"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-vpd",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-vpd",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling VPD",
    "text": "MDS - gap filling VPD"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#current-approaches-limitations",
    "href": "presentations/presentation_bioclim_18_jan_23.html#current-approaches-limitations",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Current approaches limitations",
    "text": "Current approaches limitations\n\ndon’t consider the observations before and after the gap\nEither MDS (variable correlation) or ERA data, don’t combine the information\nNo uncertainty for the predictions (only a quality flag)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#thesis-goal",
    "href": "presentations/presentation_bioclim_18_jan_23.html#thesis-goal",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Thesis goal",
    "text": "Thesis goal\n\ndevelop model to impute missing data in meteorological time series\ninclude all 3 imputation approaches\nprovide an uncertainty of the predictions"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-kalman-filter-works",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-kalman-filter-works",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How Kalman Filter works",
    "text": "How Kalman Filter works\n\n\nModels over time a latent variable (we are not observing it), the state of the system.\nThe current state \\(x_t\\) depends using:\n\nthe previous state \\(\\color{blue}{x_{t-1}}\\)\ncurrent observation \\(\\color{green}{y_t}\\)\ncontrol variable \\(\\color{purple}{c_t}\\) (ERA data)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#previous-state",
    "href": "presentations/presentation_bioclim_18_jan_23.html#previous-state",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "1. Previous state",
    "text": "1. Previous state\n\n\n\\[ x_t = A\\color{blue}{x_{t-1}} + \\varepsilon \\] where:\n\n\\(x_{t}\\) is the current state\n\\(\\color{blue}{x_{t-1}}\\) is the previous state\n\\(A\\) is a linear transformation of \\(\\color{blue}{x_{t-1}}\\)\n\\(\\varepsilon\\) is the “process” noise which is a random variable with a normal distribution with mean 0\n\n\n\n\n\n\nExample of Kalman Filter \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#current-observation",
    "href": "presentations/presentation_bioclim_18_jan_23.html#current-observation",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "2. Current Observation",
    "text": "2. Current Observation\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(\\color{green}{y_{t}}\\) is the current observation\n\\(H\\) is a linear transformation of \\(\\color{green}{y_{t}}\\)\n\\(\\nu\\) is the “observation” noise which is a random variable with a normal distribution with mean 0\n\nusing the rules of probabilistic inference if we observe \\(y_t\\) you can update the distribution of \\(x_t\\)\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gaps",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gaps",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gaps",
    "text": "Gaps\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#control-variable",
    "href": "presentations/presentation_bioclim_18_jan_23.html#control-variable",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "3. Control variable",
    "text": "3. Control variable\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + B\\color{purple}{c_t} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(B\\) is a linear transformation of \\(\\color{purple}{c_t}\\)\n\nUse the difference between current and previous value of control variable\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\) \\(B=[-1,1]\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#extra-variable-correlation",
    "href": "presentations/presentation_bioclim_18_jan_23.html#extra-variable-correlation",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Extra: Variable correlation",
    "text": "Extra: Variable correlation\n\n\nGap in two variables\n\n\n\n\n\n\n\nGap in only one variable"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-find-model-parameters",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-find-model-parameters",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to find model parameters",
    "text": "How to find model parameters\n\ncreate artificial gaps\npredicting gap in the model\ncompute the log likelihood of the predictions\nmaximise the log likelihood"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filter",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filter",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filter",
    "text": "Kalman Filter\npros:\n\nProbabilist model: the output of the model is a distribution of predictions, not a single value\nCombines all 3 approaches to gap filling in one model\ninterpretable paramters\ncomputationally efficient\n\ncons:\n\nkeeps tracks only of the local state"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #1",
    "text": "Kalman Filters gap #1"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-2",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-2",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #2",
    "text": "Kalman Filters gap #2"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-3",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-3",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #3",
    "text": "Kalman Filters gap #3"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#what-is-missing-in-the-model-development",
    "href": "presentations/presentation_bioclim_18_jan_23.html#what-is-missing-in-the-model-development",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "What is missing in the model development",
    "text": "What is missing in the model development\n\nimprove numerical stability of model (work in progress)\nfind optimal settings for training and inference\n\nn observations before after/gap\nhow to best generate artificial gaps\n\nHow to assess the model?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#use-of-gap-filling",
    "href": "presentations/presentation_bioclim_18_jan_23.html#use-of-gap-filling",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Use of gap filling",
    "text": "Use of gap filling\n\nwhat is the impact of better gap filling for data users?\n\nwhy better filling for short/medium gaps is useful\nhow can the uncertainty be used"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-settings",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-settings",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? settings",
    "text": "How to assess the model? settings\n\nhow to choose gap lengths?\nhow to choose number of variables missing?\nwhich variable to focus on?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-metrics",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-metrics",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Metrics",
    "text": "How to assess the model? Metrics\n\nRMSE - interpretation difficult as it’s relative to the variable\nr2 - gaps are often too short to interpret properly\n?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nTime series\n\n\n\n\n\n\nScatter plots"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nGap length / mean RMSE\n\n\n\n\n\n\nDistribution gaps vs filled"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#future-outlook",
    "href": "presentations/presentation_bioclim_18_jan_23.html#future-outlook",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Future outlook",
    "text": "Future outlook\n\noptimize performance model\nprovide pre-trained model on Fluxnet 2015 and then to fine-tune to local site\nprovide web-service for filling gaps\nreprocess Fluxnet 2015 dataset"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html",
    "href": "presentations/plots_for_presentation_18_jan_23.html",
    "title": "Code for presentation of 18th January",
    "section": "",
    "text": "::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n%load_ext autoreload\n%autoreload 2\n:::\n\nfrom meteo_imp.kalman.filter import *\nimport torch\nimport numpy as np\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.fastai import plot_variable\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\nfrom pyprojroot import here\n\n\nn_obs = 20\nbase_dir = here(\"presentations/plots_18_jan\")\nbase_dir.mkdir(exist_ok=True)\n\n\ndef save_plot(p, path):\n    f_name = base_dir / (path + \".vl.json\")\n    with open(f_name, 'w') as f:\n        f.write(p.to_json())\n    return f_name\n\n\nplt_props = {'width': 460, 'height': 400}\n\n\n\n\\[ x_t = A\\color{blue}{x_{-1}} + \\varepsilon\\]\n\ntorch.tensor([[0.]]).shape\n\ntorch.Size([1, 1])\n\n\n\nk0 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([0.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.3]]),\n    obs_cov = torch.tensor([[0.1]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.01]]),\n)\n\n\npred0 = k0.smooth(torch.ones(1,n_obs,1), torch.zeros(1,n_obs,1, dtype=bool), torch.zeros(1,n_obs,1))\n\n\ntime = pd.Index(range(n_obs), name=\"time\")\n\n\npred0.cov.shape\n\ntorch.Size([1, 20, 1, 1])\n\n\n\npred0_df = NormalsDf(pd.DataFrame(pred0.mean.squeeze(0), index=time, columns=[\"state\"]),\n                     pd.DataFrame(pred0.cov.squeeze(0).squeeze(-1), index=time, columns=[\"state\"]))\n\n\np0 = facet_variable(pred0_df.tidy(), ys=[\"mean\", \"mean\"], error=True, point=False, gap_area=False, props=plt_props)\np0\n\n\n\n\n\n\n\nsave_plot(p0, \"only_state\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/only_state.vl.json')\n\n\n\n\n\n\ntorch.tensor([[0.]]).shape\n\ntorch.Size([1, 1])\n\n\n\nk1 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nobs = 2 * torch.sin(torch.arange(n_obs) * 2 * torch.pi / (n_obs-2))\n\n\nplt.plot(obs)\n\n\n\n\n\nmask1 = torch.ones(1,n_obs,1, dtype=bool)\n\n\npred1 = k1.smooth(obs.unsqueeze(0).unsqueeze(-1), mask1, torch.zeros(1,n_obs,1))\n\n\ntime = pd.Index(range(n_obs), name=\"time\")\n\n\npred1.cov.shape\n\ntorch.Size([1, 20, 1, 1])\n\n\n\npred_df1 = NormalsDf(pd.DataFrame(pred1.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred1.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df1 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask1[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df1 = pd.merge(obs_df1, pred_df1, on=['time', 'variable'])\n\n\np1 = facet_variable(plot_df1, ys=[\"value\", \"mean\"], error=True, gap_area=False, props=plt_props)\np1\n\n\n\n\n\n\n\nsave_plot(p1, \"obs\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/obs.vl.json')\n\n\n\n\n\n\nk2 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nmask2 = torch.ones(1,n_obs,1, dtype=bool)\nmask2[0,11:16,0] = False\nmask2\n\ntensor([[[ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False],\n         [ True],\n         [ True],\n         [ True],\n         [ True]]])\n\n\n\npred2 = k2.smooth(obs.unsqueeze(0).unsqueeze(-1), mask2, torch.zeros(1,n_obs,1))\n\n\npred_df2 = NormalsDf(pd.DataFrame(pred2.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred2.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df2 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask2[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df2 = pd.merge(obs_df2, pred_df2, on=['time', 'variable'])\n\n\np2 = facet_variable(plot_df2, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 750, 'height': 500})\np2\n\n\n\n\n\n\n\nsave_plot(p2, \"gaps\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/gaps.vl.json')\n\n\n\n\n\n\nk3 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([-1., 1]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nmask3 = torch.ones(1,n_obs,1, dtype=bool)\nmask3[0,11:16,0] = False\nmask3\n\ntensor([[[ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False],\n         [ True],\n         [ True],\n         [ True],\n         [ True]]])\n\n\n\ncontr = torch.stack([(obs + 0.5)*1.2, ((obs + 0.5)*1.2).roll(-1)], dim=-1)\n\n\ncontr.shape\n\ntorch.Size([20, 2])\n\n\n\nplt.plot(contr[:,0])\nplt.plot(contr[:,1])\n\n\n\n\n\npred3 = k3.smooth(obs.unsqueeze(0).unsqueeze(-1), mask3, contr.unsqueeze(0))\n\n\npred_df3 = NormalsDf(pd.DataFrame(pred3.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred3.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df3 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask3[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df3 = pd.merge(obs_df3, pred_df3, on=['time', 'variable'])\n\n\np3 = (plot_variable(plot_df3, variable=\"var\", ys=[\"value\", \"mean\"], error=True, gap_area=False, props=plt_props) +\nalt.Chart(pd.DataFrame({'contr':contr[:,0], 'time': time, 'col': 'control'})).mark_line(strokeDash=[6,3], color='orange').encode(x='time', y='contr'))\np3\n\n\n\n\n\n\n\nsave_plot(p3, \"control\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/control.vl.json')\n\n\n\n\n\n\nk4 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([[.7], [.3]]),        \n    contr_matrix = torch.tensor([[0.]]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.diag(torch.tensor([0.1, 0.1])),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0., 0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.01]]),\n)\n\n\nk4\n\n\nKalman Filter (2 obs, 1 state, 1 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n0.5000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\n\n\n\n\ny_0\n0.7000\n\n\ny_1\n0.3000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\n\n\n\n\ny_0\n0.1000\n0.0000\n\n\ny_1\n0.0000\n0.1000\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\n\n\n\n\nx_0\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n0.0100\n\n\n\n\n\n \n\n\n\nmask4 = torch.ones(1,n_obs,2, dtype=bool)\nmask4[0,11:16,0] = False\nmask4\n\ntensor([[[ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True]]])\n\n\n\nobs4 = torch.stack([\n    obs *.7 + torch.randn_like(obs) * .1,\n    obs *.3 + torch.randn_like(obs) * .1\n], dim=-1)\n\n\nplt.plot(obs4)\nplt.plot(obs)\n\n\n\n\n\nobs4.shape\n\ntorch.Size([20, 2])\n\n\n\npred4 = k4.predict(obs4.unsqueeze(0), mask4, torch.zeros(1,n_obs,1))\n\n\npred_df4 = NormalsDf(pd.DataFrame(pred4.mean.squeeze(0), index=time, columns=[\"var1\", \"var2\"]),\n                     pd.DataFrame(pred4.std.squeeze(0).squeeze(-1), index=time, columns=[\"var1\", \"var2\"])).tidy()\n\n\nobs_df4_0 = pd.DataFrame({'time': time, 'var1': obs4[:,0].numpy(),\n                        'is_present': mask4[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \nobs_df4_1 = pd.DataFrame({'time': time, 'var2': obs4[:,1].numpy(),\n                        'is_present': mask4[0,:,1].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nobs_df4 = pd.concat([obs_df4_0, obs_df4_1])\n\n\nplot_df4 = pd.merge(obs_df4, pred_df4, on=['time', 'variable'])\n\n\np4 = facet_variable(plot_df4, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 350, 'height': 220}, n_cols=1)\np4\n\n\n\n\n\n\n\nsave_plot(p4, \"var_corr\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/var_corr.vl.json')\n\n\n\nmask5 = torch.ones(1,n_obs,2, dtype=bool)\nmask5[0,11:16,:] = False\nmask5\n\ntensor([[[ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [False, False],\n         [False, False],\n         [False, False],\n         [False, False],\n         [False, False],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True]]])\n\n\n\npred5 = k4.predict(obs4.unsqueeze(0), mask5, torch.zeros(1,n_obs,1))\n\n\npred_df5 = NormalsDf(pd.DataFrame(pred5.mean.squeeze(0), index=time, columns=[\"var1\", \"var2\"]),\n                     pd.DataFrame(pred5.std.squeeze(0).squeeze(-1), index=time, columns=[\"var1\", \"var2\"])).tidy()\n\n\nobs_df5_0 = pd.DataFrame({'time': time, 'var1': obs4[:,0].numpy(),\n                        'is_present': mask5[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \nobs_df5_1 = pd.DataFrame({'time': time, 'var2': obs4[:,1].numpy(),\n                        'is_present': mask5[0,:,1].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nobs_df5 = pd.concat([obs_df5_0, obs_df5_1])\n\n\nplot_df5 = pd.merge(obs_df4, pred_df5, on=['time', 'variable'])\n\n\np5 = facet_variable(plot_df5, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 350, 'height': 220}, n_cols=1)\np5\n\n\n\n\n\n\n\nsave_plot(p5, \"var_no_corr\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/var_no_corr.vl.json')"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html",
    "href": "presentations/Hainich_results_pres18_jan.html",
    "title": "Hainich with ERA-Interim",
    "section": "",
    "text": "Manual fine tuning learning process for presentation 18 Jan 2023\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom sklearn.decomposition import PCA\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path64)\n\n\n# dls = imp_dataloader(hai64, hai_era64, var_sel = gen_var_sel(['TA', 'SW_IN', 'VPD']), block_len=200, gap_len=10, bs=20, control_lags=[1])\n\n\npc = PCA().fit(hai)\n\n\nd0 = hai.iloc[0:1]\n\n\ntr0 = pc.transform(d0)\n\n\ntr0\n\narray([[-121.11917652,   -7.06844313,    0.87975241]])\n\n\n\nt0 = np.random.randn(5,1) * 10\ntt = np.hstack([t0, 2 * t0 + np.random.randn(5,1), 3 * t0 + np.random.randn(5,1)])\n\n\ntt = np.vstack([tt, -tt])\n\n\ntt.mean(0)\n\narray([1.77635684e-16, 0.00000000e+00, 0.00000000e+00])\n\n\n\ndef pca(X):\n    # Data matrix X, assumes 0-centered\n    n, m = X.shape\n    assert np.allclose(X.mean(axis=0), np.zeros(m))\n    # Compute covariance matrix\n    C = np.dot(X.T, X) / (n-1)\n    # Eigen decomposition\n    eigen_vals, eigen_vecs = np.linalg.eig(C)\n    # Project X onto PC space\n    X_pca = np.dot(X, eigen_vecs)\n    return X_pca, eigen_vecs\n\n\npca(tt)\n\n(array([[-9.15947487e+00, -3.03773243e-01,  3.81626348e-01],\n        [ 2.00045215e+01, -1.99595583e-02,  1.53745860e+00],\n        [ 1.33744310e+01,  2.01395206e-02, -3.19819214e-02],\n        [-8.99057811e+00,  1.24208069e-01,  9.57988123e-01],\n        [ 3.14616875e+01, -4.88141095e-02, -5.79117681e-01],\n        [ 9.15947487e+00,  3.03773243e-01, -3.81626348e-01],\n        [-2.00045215e+01,  1.99595583e-02, -1.53745860e+00],\n        [-1.33744310e+01, -2.01395206e-02,  3.19819214e-02],\n        [ 8.99057811e+00, -1.24208069e-01, -9.57988123e-01],\n        [-3.14616875e+01,  4.88141095e-02,  5.79117681e-01]]),\n array([[-0.26545847, -0.95659856,  0.12021234],\n        [-0.53518386,  0.04249433, -0.84366608],\n        [-0.80194142,  0.28829401,  0.52323659]]))\n\n\n\nsk_pc = PCA(2).fit(tt)\n\n\ntt @ sk_pc.components_.T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\ntr = sk_pc.transform(tt)\ntr\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\nsk_pc.components_.T\n\narray([[-0.26545847,  0.12021234],\n       [-0.53518386, -0.84366608],\n       [-0.80194142,  0.52323659]])\n\n\n\n(sk_pc.components_ @ tt.T).T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\ntt[0, None]\n\narray([[2.76792539, 4.56712931, 7.45746711]])\n\n\n\nsk_pc.components_.shape\n\n(3, 3)\n\n\n\ntt[0].shape\n\n(3,)\n\n\n\n(sk_pc.components_ @ tt[0])\n\narray([-9.15947487,  0.38162635])\n\n\n\ntt[0]\n\narray([2.76792539, 4.56712931, 7.45746711])\n\n\n\nsk_pc.components_.T @ tr[0]\n\narray([2.47733634, 4.58003795, 7.54504311])\n\n\n\nsk_pc.inverse_transform(tr[0])\n\narray([2.47733634, 4.58003795, 7.54504311])\n\n\n\nd0.to_numpy()\n\narray([[-0.6  ,  0.   ,  0.222]])\n\n\n\nhai.iloc[0]\n\nTA      -0.600\nSW_IN    0.000\nVPD      0.222\nName: 2000-01-01 00:30:00, dtype: float64\n\n\n\\[ x = y\\Lambda \\]\n\npc.components_\n\narray([[ 0.01681572,  0.99979324,  0.01143269],\n       [ 0.93010891, -0.01983747,  0.36674772],\n       [-0.36689868, -0.00446652,  0.93025018]])\n\n\n\nnp.linalg.inv(pc.components_)\n\narray([[ 0.01681572,  0.93010891, -0.36689868],\n       [ 0.99979324, -0.01983747, -0.00446652],\n       [ 0.01143269,  0.36674772,  0.93025018]])\n\n\nassuming that the control has the same dimensions of the observations then if we are doing a local slope model we have \\(B \\in \\mathbb{R}^{state \\times contr}\\): \\[ B = \\begin{bmatrix} -I & I \\\\ 0 & 0 \\end{bmatrix}\\]\n\nfrom torch import hstack, eye, vstack, ones, zeros, tensor\nfrom functools import partial\n\n\ndef set_dtype(*args, dtype=torch.float64):\n    return [partial(arg, dtype=dtype) for arg in args] \n\n\neye, ones, zeros, tensor = set_dtype(eye, ones, zeros, tensor)\n\n\ndef init_smart(n_dim_obs, n_dim_state, df, pca=True):\n    # n_dim_obs == n_dim_contr\n    if pca:\n        comp = PCA(n_dim_state).fit(df).components_\n        obs_matrix = tensor(comp.T) # transform state -&gt; obs\n        contr_matrix = tensor(comp) # transform obs -&gt; state\n    else:\n        obs_matrix, contr_matrix = eye(n_dim_obs), eye(n_dim_obs)\n        \n    return KalmanFilter(\n        trans_matrix =     vstack([hstack([eye(n_dim_state),                eye(n_dim_state)]),\n                                   hstack([zeros(n_dim_state, n_dim_state), eye(n_dim_state)])]),\n        trans_off =        zeros(n_dim_state * 2),        \n        trans_cov =        eye(n_dim_state * 2)*.1,        \n        obs_matrix =       hstack([obs_matrix, zeros(n_dim_obs, n_dim_state)]),\n        obs_off =          zeros(n_dim_obs),          \n        obs_cov =          eye(n_dim_obs)*.01,            \n        contr_matrix =     vstack([hstack([-contr_matrix,                  contr_matrix]),\n                                   hstack([ zeros(n_dim_state,n_dim_obs), zeros(n_dim_state, n_dim_obs)])]),\n        init_state_mean =  zeros(n_dim_state * 2),        \n        init_state_cov =   eye(n_dim_state * 2) * 3,\n    ) \n\n\nnp.hstack([np.eye(2), np.eye(2)])\n\narray([[1., 0., 1., 0.],\n       [0., 1., 0., 1.]])\n\n\n\ninit_smart(3,2, hai)\n\n\nKalman Filter (3 obs, 4 state, 6 contr)  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n1.0000\n0.0000\n1.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n1.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\n\n\n\n\ny_0\n0.0168\n0.9301\n0.0000\n0.0000\n\n\ny_1\n0.9998\n-0.0198\n0.0000\n0.0000\n\n\ny_2\n0.0114\n0.3667\n0.0000\n0.0000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0168\n-0.9998\n-0.0114\n0.0168\n0.9998\n0.0114\n\n\nx_1\n-0.9301\n0.0198\n-0.3667\n0.9301\n-0.0198\n0.3667\n\n\nx_2\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\n\n\n\n\n\n \n\n\n\nclass PersistentRecorder(Callback):\n    order = 70\n    name = \"per_recorder\"\n    attrs = ['lrs', 'iters', 'losses', 'values']\n    def before_fit(self):\n        \"Prepare state for training\"\n        for attr in self.attrs:\n            if not hasattr(self.per_recorder, attr): setattr(self.per_recorder, attr, [])\n\n    def after_batch(self):\n        for attr in self.attrs:\n            setattr(self.per_recorder, attr, getattr(self.recorder, attr))\n\n\nmodels = []\n\n\ndls = imp_dataloader(hai, hai_era, var_sel = ['TA', 'SW_IN', 'VPD'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nlen(dls.valid.items)\nitems = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n\n\ndef train_show_save(learn, n_iter, lr):\n    learn.fit(n_iter, lr)\n    models.append(learn.model.state_dict().copy())\n    learn.recorder.plot_loss()\n    items = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n    return show_results(learn, items = items, control=hai_control)\n\n\n\n\nmodel = init_smart(3,3,3, hai, pca=False).cuda()\n\n\nloss = loss_func=KalmanLoss(only_gap=False)\nlearn = Learner(dls, model, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\nlearn.model.use_smooth = True\n\n\nshow_results(learn, items=items, control=hai_control)\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-87.370314\n-96.212915\n0.069196\n0.210351\n0.946861\n-250571814513532732431918956544.000000\n02:37\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1489\n0.0042\n0.0120\n0.8404\n-0.0794\n-0.1127\n\n\nx_1\n-0.0954\n1.1464\n-0.0546\n0.1177\n0.8560\n0.0599\n\n\nx_2\n-0.0225\n0.0420\n1.1357\n-0.0717\n-0.1190\n0.8475\n\n\nx_3\n0.1201\n0.0661\n0.1170\n1.1840\n-0.1150\n-0.1073\n\n\nx_4\n0.0770\n0.1299\n0.0068\n-0.1847\n1.0987\n-0.1500\n\n\nx_5\n0.1084\n0.0358\n0.1133\n-0.1809\n-0.0281\n1.1533\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0341\n0.0493\n0.0361\n-0.0008\n-0.0312\n0.0026\n\n\nx_1\n0.0493\n0.1125\n0.0531\n0.0475\n0.0047\n0.0509\n\n\nx_2\n0.0361\n0.0531\n0.0399\n0.0001\n-0.0327\n0.0058\n\n\nx_3\n-0.0008\n0.0475\n0.0001\n0.0608\n0.0614\n0.0585\n\n\nx_4\n-0.0312\n0.0047\n-0.0327\n0.0614\n0.0901\n0.0554\n\n\nx_5\n0.0026\n0.0509\n0.0058\n0.0585\n0.0554\n0.0594\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0008\n\n\nx_1\n-0.0083\n\n\nx_2\n0.0067\n\n\nx_3\n0.0023\n\n\nx_4\n-0.0007\n\n\nx_5\n0.0005\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.8238\n-0.1897\n-0.1014\n-0.0841\n0.1914\n0.1286\n\n\ny_1\n0.0814\n0.8353\n0.1048\n0.1040\n0.0127\n0.0689\n\n\ny_2\n-0.0742\n-0.1502\n0.8296\n0.1282\n0.1811\n-0.0746\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0084\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0084\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0084\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0017\n\n\ny_1\n-0.0046\n\n\ny_2\n0.0026\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0290\n-0.0485\n-0.0022\n0.9297\n0.0792\n-0.0247\n\n\nx_1\n-0.0267\n-0.8778\n-0.0233\n0.0113\n0.9099\n0.0261\n\n\nx_2\n-0.0018\n-0.0592\n-1.0044\n-0.0310\n0.0674\n0.9204\n\n\nx_3\n-0.0503\n-0.0312\n-0.0556\n-0.0440\n-0.0529\n-0.0587\n\n\nx_4\n-0.0375\n-0.0537\n-0.0246\n-0.0412\n-0.0352\n-0.0338\n\n\nx_5\n-0.0653\n-0.0179\n-0.0575\n-0.0655\n-0.0307\n-0.0449\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0393\n\n\nx_1\n0.0044\n\n\nx_2\n-0.0092\n\n\nx_3\n0.0207\n\n\nx_4\n-0.0094\n\n\nx_5\n-0.0021\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.5500\n-0.1402\n-0.0082\n0.5233\n0.3011\n0.0882\n\n\nx_1\n-0.1402\n2.4578\n0.1502\n0.0355\n0.5714\n-0.0459\n\n\nx_2\n-0.0082\n0.1502\n2.5045\n0.2739\n0.0634\n0.5195\n\n\nx_3\n0.5233\n0.0355\n0.2739\n2.3814\n-0.2201\n-0.1842\n\n\nx_4\n0.3011\n0.5714\n0.0634\n-0.2201\n2.3880\n-0.1162\n\n\nx_5\n0.0882\n-0.0459\n0.5195\n-0.1842\n-0.1162\n2.3874\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-108.023250\n-113.113034\n0.073622\n0.198386\n0.956828\n-105578612000578019271909572608.000000\n02:37\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1440\n-0.0057\n0.0107\n0.8262\n-0.0845\n-0.1205\n\n\nx_1\n-0.0534\n1.2089\n-0.0223\n0.0948\n0.7937\n0.0494\n\n\nx_2\n-0.0289\n0.0176\n1.1417\n-0.0702\n-0.1154\n0.8351\n\n\nx_3\n0.1277\n0.0589\n0.1074\n1.1787\n-0.1203\n-0.1024\n\n\nx_4\n0.0654\n0.1560\n0.0043\n-0.1781\n1.1087\n-0.1415\n\n\nx_5\n0.0959\n0.0439\n0.1197\n-0.1725\n-0.0340\n1.1496\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0207\n0.0254\n0.0210\n-0.0052\n-0.0232\n-0.0033\n\n\nx_1\n0.0254\n0.0548\n0.0255\n0.0259\n0.0034\n0.0281\n\n\nx_2\n0.0210\n0.0255\n0.0222\n-0.0056\n-0.0241\n-0.0023\n\n\nx_3\n-0.0052\n0.0259\n-0.0056\n0.0473\n0.0504\n0.0465\n\n\nx_4\n-0.0232\n0.0034\n-0.0241\n0.0504\n0.0695\n0.0473\n\n\nx_5\n-0.0033\n0.0281\n-0.0023\n0.0465\n0.0473\n0.0477\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0004\n\n\nx_1\n-0.0055\n\n\nx_2\n0.0048\n\n\nx_3\n0.0027\n\n\nx_4\n-0.0033\n\n\nx_5\n0.0021\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7902\n-0.2032\n-0.1217\n-0.0986\n0.2164\n0.1090\n\n\ny_1\n0.0160\n0.7417\n0.0555\n0.1016\n0.1041\n0.0374\n\n\ny_2\n-0.1103\n-0.1594\n0.7916\n0.1185\n0.2096\n-0.0949\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0077\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0077\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0077\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0009\n\n\ny_1\n-0.0053\n\n\ny_2\n-0.0021\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0277\n-0.0595\n-0.0019\n0.9181\n0.0838\n-0.0194\n\n\nx_1\n-0.0414\n-0.8661\n-0.0323\n0.0110\n0.8741\n0.0262\n\n\nx_2\n-0.0029\n-0.0647\n-1.0045\n-0.0319\n0.0757\n0.8957\n\n\nx_3\n-0.0678\n-0.0368\n-0.0564\n-0.0559\n-0.0610\n-0.0639\n\n\nx_4\n-0.0374\n-0.0690\n-0.0209\n-0.0485\n-0.0406\n-0.0354\n\n\nx_5\n-0.0591\n-0.0267\n-0.0753\n-0.0633\n-0.0407\n-0.0555\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0396\n\n\nx_1\n-0.0009\n\n\nx_2\n-0.0049\n\n\nx_3\n0.0153\n\n\nx_4\n-0.0139\n\n\nx_5\n-0.0094\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.6364\n-0.1456\n-0.0020\n0.5695\n0.3986\n0.1628\n\n\nx_1\n-0.1456\n2.3292\n0.1064\n0.2074\n0.7591\n0.1475\n\n\nx_2\n-0.0020\n0.1064\n2.4205\n0.3489\n0.1488\n0.6143\n\n\nx_3\n0.5695\n0.2074\n0.3489\n2.2351\n-0.4355\n-0.3277\n\n\nx_4\n0.3986\n0.7591\n0.1488\n-0.4355\n2.1625\n-0.3445\n\n\nx_5\n0.1628\n0.1475\n0.6143\n-0.3277\n-0.3445\n2.2518\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps\")\n\nPath('models/17_jan_all_gaps.pth')\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-122.620241\n-126.621236\n0.077944\n0.202366\n0.948695\n-115274990280631988956159803392.000000\n02:38\n\n\n1\n-134.455010\n-139.768441\n0.074152\n0.188695\n0.959962\n-103653315974387814635399020544.000000\n02:31\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1347\n-0.0147\n0.0100\n0.8021\n-0.1122\n-0.1444\n\n\nx_1\n-0.0388\n1.2480\n-0.0185\n0.0773\n0.7576\n0.0414\n\n\nx_2\n-0.0257\n-0.0018\n1.1500\n-0.0814\n-0.1272\n0.8203\n\n\nx_3\n0.1332\n0.0451\n0.0866\n1.1612\n-0.1103\n-0.0917\n\n\nx_4\n0.0559\n0.1720\n0.0056\n-0.1625\n1.1052\n-0.1248\n\n\nx_5\n0.0721\n0.0457\n0.1226\n-0.1523\n-0.0385\n1.1357\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0146\n0.0183\n0.0142\n-0.0081\n-0.0202\n-0.0067\n\n\nx_1\n0.0183\n0.0445\n0.0187\n0.0148\n-0.0014\n0.0168\n\n\nx_2\n0.0142\n0.0187\n0.0143\n-0.0065\n-0.0187\n-0.0044\n\n\nx_3\n-0.0081\n0.0148\n-0.0065\n0.0340\n0.0393\n0.0343\n\n\nx_4\n-0.0202\n-0.0014\n-0.0187\n0.0393\n0.0550\n0.0378\n\n\nx_5\n-0.0067\n0.0168\n-0.0044\n0.0343\n0.0378\n0.0361\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0014\n\n\nx_1\n-0.0059\n\n\nx_2\n0.0039\n\n\nx_3\n0.0016\n\n\nx_4\n-0.0036\n\n\nx_5\n0.0032\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7310\n-0.1642\n-0.0983\n-0.1030\n0.2236\n0.0907\n\n\ny_1\n0.0158\n0.6777\n0.0705\n0.1196\n0.1642\n0.0316\n\n\ny_2\n-0.1124\n-0.1249\n0.7399\n0.1211\n0.2193\n-0.0846\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0064\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0065\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0065\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0019\n\n\ny_1\n-0.0046\n\n\ny_2\n-0.0026\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0316\n-0.0708\n-0.0019\n0.8851\n0.0973\n-0.0094\n\n\nx_1\n-0.0727\n-0.8659\n-0.0584\n0.0075\n0.7964\n0.0142\n\n\nx_2\n0.0098\n-0.0734\n-1.0045\n-0.0156\n0.0845\n0.8489\n\n\nx_3\n-0.0977\n-0.0434\n-0.0574\n-0.0723\n-0.0760\n-0.0730\n\n\nx_4\n-0.0309\n-0.1114\n-0.0185\n-0.0559\n-0.0634\n-0.0420\n\n\nx_5\n-0.0503\n-0.0424\n-0.1056\n-0.0644\n-0.0571\n-0.0729\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0585\n\n\nx_1\n-0.0141\n\n\nx_2\n-0.0070\n\n\nx_3\n0.0239\n\n\nx_4\n-0.0043\n\n\nx_5\n-0.0069\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.9257\n0.1245\n0.1987\n0.4475\n0.2435\n0.0893\n\n\nx_1\n0.1245\n2.4393\n0.3641\n0.2107\n0.7802\n0.1895\n\n\nx_2\n0.1987\n0.3641\n2.4862\n0.2959\n0.0089\n0.6053\n\n\nx_3\n0.4475\n0.2107\n0.2959\n2.1058\n-0.5771\n-0.4644\n\n\nx_4\n0.2435\n0.7802\n0.0089\n-0.5771\n1.9818\n-0.5064\n\n\nx_5\n0.0893\n0.1895\n0.6053\n-0.4644\n-0.5064\n2.0952\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-149.288297\n-152.804403\n0.072760\n0.179190\n0.956827\n-61340364986143763995601928192.000000\n02:35\n\n\n1\n-160.099637\n-165.002097\n0.071263\n0.181697\n0.963587\n-48387608947773502947627892736.000000\n02:37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps_final\")\n\nPath('models/17_jan_all_gaps_final.pth')\n\n\n\np = show_results(learn, control=hai_control, items = [items[i] for i in [0,5,4]])\n\n\n\n\n\n\n\n\n\n\ndls2 = imp_dataloader(hai, hai_era, var_sel = ['TA'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nmodel2 = init_smart(3,3, hai, pca=True).cuda()\n\n\nloss2 = loss_func=KalmanLoss(only_gap=False)\nlearn2 = Learner(dls2, model2, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\ntrain_show_save(learn2, 1, 1e-3)\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/91 00:00&lt;?]\n    \n    \n\n\nNotImplementedError: Module [KalmanFilter] is missing the required \"forward\" function\n\n\nDisable smoother\n\nmodel.use_smooth = False\n\n\ntrain_show_save(learn, 3, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n534.159578\n465.194522\n0.284379\n0.329570\n-1.919198\n-241825956681560181426503024640.000000\n02:24\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.6761\n0.1157\n-0.0095\n0.5895\n0.0728\n0.0850\n\n\nx_1\n0.0309\n0.6675\n0.1639\n0.2296\n0.5542\n-0.1129\n\n\nx_2\n0.1836\n-0.1476\n0.6498\n0.1684\n-0.2142\n0.5899\n\n\nx_3\n-0.1246\n0.0361\n0.0138\n0.6844\n-0.0830\n0.1501\n\n\nx_4\n-0.0167\n-0.0399\n0.0044\n0.0089\n0.7537\n0.0580\n\n\nx_5\n0.0341\n0.0300\n-0.0436\n-0.0362\n0.2878\n0.6487\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.5106\n0.3768\n0.4383\n-0.1720\n-0.2863\n-0.1749\n\n\nx_1\n0.3768\n0.5687\n0.1931\n-0.2781\n0.0461\n0.0798\n\n\nx_2\n0.4383\n0.1931\n0.4406\n-0.0734\n-0.3650\n-0.2384\n\n\nx_3\n-0.1720\n-0.2781\n-0.0734\n0.5150\n0.3608\n0.3418\n\n\nx_4\n-0.2863\n0.0461\n-0.3650\n0.3608\n0.8281\n0.6975\n\n\nx_5\n-0.1749\n0.0798\n-0.2384\n0.3418\n0.6975\n0.6156\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0166\n\n\nx_1\n-0.0196\n\n\nx_2\n-0.0446\n\n\nx_3\n0.0033\n\n\nx_4\n-0.0022\n\n\nx_5\n0.0107\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n-0.3096\n0.5414\n-0.0652\n0.4906\n-0.0643\n-0.3834\n\n\ny_1\n0.6288\n-0.3366\n-0.3681\n0.0321\n-0.2062\n0.3634\n\n\ny_2\n-0.2813\n0.0500\n0.4738\n-0.0601\n0.2872\n-0.1334\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.3678\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.3707\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.3693\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0197\n\n\ny_1\n-0.0452\n\n\ny_2\n-0.0451\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9410\n-0.5681\n-0.8722\n0.9998\n1.2958\n1.0828\n\n\nx_1\n-0.7019\n-0.7354\n-0.9277\n1.2348\n1.1344\n0.9620\n\n\nx_2\n-0.9322\n-0.7328\n-0.8307\n1.0310\n1.1112\n1.1017\n\n\nx_3\n0.0914\n0.2592\n0.0286\n0.0638\n0.1644\n-0.0007\n\n\nx_4\n0.2175\n-0.0398\n0.0226\n0.2072\n-0.1205\n-0.0117\n\n\nx_5\n-0.0578\n0.1485\n0.0908\n-0.0613\n0.0627\n0.0792\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0290\n\n\nx_1\n-0.0320\n\n\nx_2\n-0.0494\n\n\nx_3\n0.0296\n\n\nx_4\n0.0387\n\n\nx_5\n0.1010\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n3.7171\n2.7054\n2.8366\n2.0078\n1.3904\n2.3275\n\n\nx_1\n2.7054\n4.5189\n2.1010\n1.4436\n1.8804\n2.8057\n\n\nx_2\n2.8366\n2.1010\n3.3027\n1.3323\n0.9307\n2.0280\n\n\nx_3\n2.0078\n1.4436\n1.3323\n3.3867\n2.1461\n3.0821\n\n\nx_4\n1.3904\n1.8804\n0.9307\n2.1461\n2.5441\n2.6815\n\n\nx_5\n2.3275\n2.8057\n2.0280\n3.0821\n2.6815\n3.7653\n\n\n\n\n\n \n\n\nIndexError: list index out of range\n\n\n\ntrain_show_save(learn, 1, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n272.526685\n207.314376\n0.279615\n0.290300\n-3.962859\n-113462721102480788295669252096.000000\n06:35\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5320\n0.0984\n0.2567\n\n\nx_1\n0.5386\n-0.1615\n-0.1019\n\n\nx_2\n0.4759\n0.4640\n0.4202\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.4043\n0.5126\n1.8864\n\n\nx_1\n0.5126\n0.1871\n0.6885\n\n\nx_2\n1.8864\n0.6885\n2.5339\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0972\n\n\nx_1\n-0.2735\n\n\nx_2\n0.0298\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1792\n0.7014\n-0.3164\n\n\ny_1\n-0.5045\n0.1311\n0.3455\n\n\ny_2\n-0.0962\n0.0705\n0.1246\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0190\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.1624\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0297\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1664\n\n\ny_1\n-0.0433\n\n\ny_2\n0.0232\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0331\n-0.4960\n0.2924\n0.5946\n-0.1127\n-0.1933\n\n\nx_1\n0.5991\n0.3053\n0.2248\n0.7649\n0.9446\n0.0394\n\n\nx_2\n0.0060\n0.0013\n-0.0667\n-0.1143\n0.7781\n0.6409\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0627\n\n\nx_1\n-0.6392\n\n\nx_2\n-1.2803\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n9.0224\n3.6789\n10.5469\n\n\nx_1\n3.6789\n2.3654\n5.9687\n\n\nx_2\n10.5469\n5.9687\n21.9915\n\n\n\n\n\n \n\n\nNameError: name 'learn1' is not defined\n\n\n\ntrain_show_save(learn, 1, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n160.675568\n150.361449\n0.266633\n0.269324\n-1.959343\n-53151966368643636872563654656.000000\n05:30\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5363\n0.1070\n0.2723\n\n\nx_1\n0.5375\n-0.1534\n-0.0948\n\n\nx_2\n0.4909\n0.4638\n0.4301\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.3408\n0.4970\n1.7922\n\n\nx_1\n0.4970\n0.1842\n0.6643\n\n\nx_2\n1.7922\n0.6643\n2.3955\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0948\n\n\nx_1\n-0.2568\n\n\nx_2\n0.0393\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1742\n0.6954\n-0.3174\n\n\ny_1\n-0.5094\n0.1551\n0.3429\n\n\ny_2\n-0.1147\n0.0583\n0.1163\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0121\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.1100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0190\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1697\n\n\ny_1\n-0.0123\n\n\ny_2\n0.0387\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0363\n-0.5225\n0.3103\n0.5948\n-0.0692\n-0.1628\n\n\nx_1\n0.5955\n0.3523\n0.1332\n0.7587\n0.9820\n-0.0509\n\n\nx_2\n-0.0159\n0.0573\n-0.0471\n-0.1409\n0.7802\n0.6457\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0500\n\n\nx_1\n-0.6014\n\n\nx_2\n-1.2993\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n10.9221\n3.8061\n12.4742\n\n\nx_1\n3.8061\n1.7574\n5.6427\n\n\nx_2\n12.4742\n5.6427\n24.9669\n\n\n\n\n\n \n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n115.738528\n104.632719\n0.257947\n0.259673\n-1.679687\n-24519279833493970234084687872.000000\n05:09\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5303\n0.1124\n0.2795\n\n\nx_1\n0.5333\n-0.1470\n-0.0908\n\n\nx_2\n0.4963\n0.4602\n0.4322\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.2399\n0.4645\n1.6526\n\n\nx_1\n0.4645\n0.1740\n0.6191\n\n\nx_2\n1.6526\n0.6191\n2.2026\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0883\n\n\nx_1\n-0.2505\n\n\nx_2\n0.0440\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1715\n0.6898\n-0.3162\n\n\ny_1\n-0.5142\n0.1782\n0.3403\n\n\ny_2\n-0.1173\n0.0543\n0.1176\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0079\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0732\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0124\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1699\n\n\ny_1\n-0.0058\n\n\ny_2\n0.0383\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0449\n-0.5621\n0.3391\n0.5937\n0.0285\n-0.1154\n\n\nx_1\n0.5860\n0.3917\n0.0584\n0.7468\n1.0129\n-0.1247\n\n\nx_2\n-0.0479\n0.1664\n-0.0280\n-0.1814\n0.7807\n0.6422\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0488\n\n\nx_1\n-0.5958\n\n\nx_2\n-1.3038\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n12.1503\n4.1501\n13.5248\n\n\nx_1\n4.1501\n1.7544\n5.9730\n\n\nx_2\n13.5248\n5.9730\n27.6423\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nlearn.model.use_smooth = True\nshow_results(learn, items = [learn.dls.valid.items[i] for i in [10, 100, 300, 500, 700, 1000]])\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n30.829145\n26.059960\n0.201186\n0.215770\n-0.034140\n-48287381455002891037683220480.000000\n06:32\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5309\n0.1159\n0.2797\n\n\nx_1\n0.5313\n-0.1385\n-0.0881\n\n\nx_2\n0.4961\n0.4560\n0.4324\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.1473\n0.4769\n1.5832\n\n\nx_1\n0.4769\n0.2008\n0.6682\n\n\nx_2\n1.5832\n0.6682\n2.2249\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0894\n\n\nx_1\n-0.2426\n\n\nx_2\n0.0426\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1711\n0.6883\n-0.3146\n\n\ny_1\n-0.5151\n0.1887\n0.3414\n\n\ny_2\n-0.1155\n0.0454\n0.1145\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0063\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0583\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0097\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1747\n\n\ny_1\n0.0044\n\n\ny_2\n0.0287\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0475\n-0.5920\n0.3650\n0.5948\n0.0563\n-0.0801\n\n\nx_1\n0.5812\n0.4322\n-0.0088\n0.7377\n1.0076\n-0.1962\n\n\nx_2\n-0.0461\n0.1875\n-0.0228\n-0.1820\n0.7675\n0.6408\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0477\n\n\nx_1\n-0.5959\n\n\nx_2\n-1.2990\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n12.4282\n4.5316\n13.6935\n\n\nx_1\n4.5316\n2.1406\n6.4461\n\n\nx_2\n13.6935\n6.4461\n27.6478\n\n\n\n\n\n \n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n2.643850\n-2.886783\n0.193556\n0.212050\n0.165402\n-29540765662186335322106757120.000000\n06:27\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5339\n0.1210\n0.2783\n\n\nx_1\n0.5270\n-0.1285\n-0.0862\n\n\nx_2\n0.4976\n0.4502\n0.4350\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.0913\n0.4682\n1.5174\n\n\nx_1\n0.4682\n0.2037\n0.6628\n\n\nx_2\n1.5174\n0.6628\n2.1595\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0873\n\n\nx_1\n-0.2397\n\n\nx_2\n0.0401\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1690\n0.6841\n-0.3145\n\n\ny_1\n-0.5175\n0.1944\n0.3405\n\n\ny_2\n-0.1099\n0.0372\n0.1142\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0051\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0467\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0078\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1766\n\n\ny_1\n0.0070\n\n\ny_2\n0.0210\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0614\n-0.6233\n0.4026\n0.5869\n0.0964\n-0.0301\n\n\nx_1\n0.5842\n0.4624\n-0.0654\n0.7358\n0.9837\n-0.2582\n\n\nx_2\n-0.0390\n0.2132\n-0.0130\n-0.1775\n0.7580\n0.6438\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0447\n\n\nx_1\n-0.5957\n\n\nx_2\n-1.2961\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n13.1380\n4.8428\n14.4531\n\n\nx_1\n4.8428\n2.3677\n6.8288\n\n\nx_2\n14.4531\n6.8288\n28.0978\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nlearn.model.use_smooth = True\nlearn.loss_func = KalmanLoss(only_gap=True)\n\n\ntrain_show_save(learn, 1,  5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-3.892116\n-2.872949\n0.120294\n0.204105\n-2.923830\n-85436991145187843135533744128.000000\n04:26\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.2027\n0.3761\n0.2969\n\n\nx_1\n0.3594\n0.3686\n0.3606\n\n\nx_2\n0.4679\n0.2295\n0.2852\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0220\n0.0113\n-0.0224\n\n\nx_1\n0.0113\n0.0064\n-0.0097\n\n\nx_2\n-0.0224\n-0.0097\n0.0290\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.3025\n\n\nx_1\n-0.5231\n\n\nx_2\n0.2064\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.1489\n0.3517\n-0.0053\n\n\ny_1\n0.2922\n0.1890\n-0.4105\n\n\ny_2\n0.3920\n-0.0638\n0.2473\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0001\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0054\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0003\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.2038\n\n\ny_1\n0.1323\n\n\ny_2\n-0.1988\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9856\n0.6178\n0.4694\n-0.5368\n0.5367\n-0.0159\n\n\nx_1\n0.6667\n0.5327\n-0.3882\n0.6907\n-0.0209\n-0.2365\n\n\nx_2\n0.0330\n-0.7175\n-0.1714\n0.2048\n-0.6491\n0.1349\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0922\n\n\nx_1\n-0.6262\n\n\nx_2\n0.2368\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n8.1062\n12.7070\n6.0908\n\n\nx_1\n12.7070\n21.6312\n10.2860\n\n\nx_2\n6.0908\n10.2860\n5.5679\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nlearn1.model.use_smooth = True\n\n\ntrain_show_save(learn1, 1,  5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-415.543654\n-449.643886\n0.055635\n0.200575\n0.759219\n-45104963579554009935054372864.000000\n07:43\n\n\n\n\n\n\n\n\n\n  trans matrix (A) \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.1815\n0.3765\n0.3138\n\n\nx_1\n0.3401\n0.3500\n0.3764\n\n\nx_2\n0.4904\n0.2297\n0.2438\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0491\n0.0170\n-0.0614\n\n\nx_1\n0.0170\n0.0127\n-0.0243\n\n\nx_2\n-0.0614\n-0.0243\n0.0889\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.3080\n\n\nx_1\n-0.5210\n\n\nx_2\n0.2044\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.1630\n0.3552\n-0.0202\n\n\ny_1\n0.3143\n0.1963\n-0.4623\n\n\ny_2\n0.4022\n-0.0653\n0.2466\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0001\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0054\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0003\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.2096\n\n\ny_1\n0.1459\n\n\ny_2\n-0.2079\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0006\n0.5988\n0.4673\n-0.5447\n0.5317\n-0.0122\n\n\nx_1\n0.6825\n0.5156\n-0.3620\n0.7188\n-0.0186\n-0.1805\n\n\nx_2\n0.0303\n-0.7130\n-0.1672\n0.2100\n-0.6278\n0.1642\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0933\n\n\nx_1\n-0.6332\n\n\nx_2\n0.2392\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n8.2929\n13.0004\n6.2311\n\n\nx_1\n13.0004\n22.1310\n10.5205\n\n\nx_2\n6.2311\n10.5205\n5.6921\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n# learn1.save(\"model_16_jan1\")\n\nPath('models/model_16_jan1.pth')"
  },
  {
    "objectID": "exploration/log_likelihood.html",
    "href": "exploration/log_likelihood.html",
    "title": "Exploration of Loglikelihood computations",
    "section": "",
    "text": "This notebook is not running yet\n\nX[0]\n\ntensor(1)\n\n\n\nk.loglikelihood(X, smooth=False)\n\ntensor(-3.7474)\n\n\n\nfrom scipy import linalg\n\n\ndef log_multivariate_normal_density(X, means, covars, min_covar=1.e-7):\n    \"\"\"Log probability for full covariance matrices. \"\"\"\n    if hasattr(linalg, 'solve_triangular'):\n        # only in scipy since 0.9\n        solve_triangular = linalg.solve_triangular\n    else:\n        # slower, but works\n        solve_triangular = linalg.solve\n    n_samples, n_dim = X.shape\n    nmix = len(means)\n    log_prob = np.empty((n_samples, nmix))\n    for c, (mu, cv) in enumerate(zip(means, covars)):\n        try:\n            cv_chol = linalg.cholesky(cv, lower=True)\n        except linalg.LinAlgError:\n            # The model is most probabily stuck in a component with too\n            # few observations, we need to reinitialize this components\n            cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),\n                                      lower=True)\n        cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))\n        cv_sol = solve_triangular(cv_chol, (X - mu).T, lower=True).T\n        log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) + \\\n                                     n_dim * np.log(2 * np.pi) + cv_log_det)\n\n    return log_prob\n\n\nlog_multivariate_normal_density(np.ones((1,1)), np.zeros(1), np.eye(1))\n\narray([[-1.41893853]])\n\n\n\nMultivariateNormal(torch.zeros(1), torch.eye(1)).log_prob(torch.ones((1)))\n\ntensor(-1.4189)\n\n\n\nlog_multivariate_normal_density(np.ones((1, 2)), np.zeros((1,2)), np.expand_dims(np.eye(2), 0))\n\narray([[-2.83787707]])\n\n\n\nMultivariateNormal(torch.zeros(2), torch.eye(2)).log_prob(torch.ones((2)))\n\ntensor(-2.8379)\n\n\npytorch and pykalman have the same results for computing the loglikelihood of a gaussian distribution\n\ndef _torch_loglikelihoods(observation_matrices, observation_offsets,\n                    observation_covariance, predicted_state_means,\n                    predicted_state_covariances, observations):\n    \n    n_timesteps = observations.shape[0]\n    loglikelihoods = np.zeros(n_timesteps)\n    for t in range(n_timesteps):\n        observation = observations[t]\n        observation_matrix = _last_dims(observation_matrices, t)\n        observation_offset = _last_dims(observation_offsets, t, ndims=1)\n        predicted_state_mean = _last_dims(\n            predicted_state_means, t, ndims=1\n        )\n        predicted_state_covariance = _last_dims(\n            predicted_state_covariances, t\n        )\n\n        predicted_observation_mean = (\n            observation_matrix @ predicted_state_mean\n            + observation_offset\n        )\n        predicted_observation_covariance = (\n            observation_matrix @ predicted_state_covariance @ observation_matrix.T\n            + observation_covariance\n        )\n\n        loglikelihoods[t] = MultivariateNormal(\n            predicted_observation_mean,\n            predicted_observation_covariance\n        ).log_prob(observation.unsqueeze(0))\n    return loglikelihoods.sum()\n\n\nstate = k.filter(X)\n\n\npyk.loglikelihood??\n\n\nSignature: pyk.loglikelihood(X)\nSource:   \n    def loglikelihood(self, X):\n        \"\"\"Calculate the log likelihood of all observations\n        Parameters\n        ----------\n        X : [n_timesteps, n_dim_obs] array\n            observations for time steps [0...n_timesteps-1]\n        Returns\n        -------\n        likelihood : float\n            likelihood of all observations\n        \"\"\"\n        Z = np.array(self._parse_observations(X))\n        # initialize parameters\n        (transition_matrices, transition_offsets,\n         transition_covariance, observation_matrices,\n         observation_offsets, observation_covariance,\n         initial_state_mean, initial_state_covariance) = (\n            self._initialize_parameters()\n        )\n        # apply the Kalman Filter\n        (predicted_state_means, predicted_state_covariances,\n         kalman_gains, filtered_state_means,\n         filtered_state_covariances) = (\n            _filter(\n                transition_matrices, observation_matrices,\n                transition_covariance, observation_covariance,\n                transition_offsets, observation_offsets,\n                initial_state_mean, initial_state_covariance,\n                Z\n            )\n        )\n        # get likelihoods for each time step\n        loglikelihoods = _loglikelihoods(\n          observation_matrices, observation_offsets, observation_covariance,\n          predicted_state_means, predicted_state_covariances, Z\n        )\n        return np.sum(loglikelihoods)\nFile:      ~/anaconda3/envs/data-science/lib/python3.10/site-packages/pykalman/standard.py\nType:      method\n\n\n\n\n\nnX = X.numpy()\n\n\npred_mean, pred_cov = pyk.filter(nX)\n\n\npyk.loglikelihood(nX)\n\n-5.231597970652478\n\n\n\nZ = np.array(pyk._parse_observations(nX))\n\n\n(transition_matrices, transition_offsets,\n         transition_covariance, observation_matrices,\n         observation_offsets, observation_covariance,\n         initial_state_mean, initial_state_covariance) = (\n            pyk._initialize_parameters()\n        )\n\n\n(predicted_state_means, predicted_state_covariances,\n         kalman_gains, filtered_state_means,\n         filtered_state_covariances) = (\n            pykalman.standard._filter(\n                transition_matrices, observation_matrices,\n                transition_covariance, observation_covariance,\n                transition_offsets, observation_offsets,\n                initial_state_mean, initial_state_covariance,\n                Z\n            )\n        )\n\n\npykalman.standard._loglikelihoods(\n observation_matrices, observation_offsets, observation_covariance,\n          pred_mean, pred_cov, Z\n).sum()\n\n-3.7473859589713614\n\n\n\n_torch_loglikelihoods(\n    k.obs_matrices,\n    k.obs_offsets,\n    k.obs_cov,\n    state.mean,\n    state.cov,\n    X\n)\n\n-3.747385859489441\n\n\n\npykalman.standard._loglikelihoods(\n    k.obs_matrices.numpy(),\n    k.obs_offsets.numpy(),\n    k.obs_cov.numpy(),\n    state.mean.numpy(),\n    state.cov.numpy(),\n    Z\n).sum()\n\n-3.747385949780805\n\n\n\npyk.loglikelihood(Z)\n\n-5.231597970652478\n\n\n\ntest_close(pyk.filter(nX), k.filter(X))\n\n\npyk.loglikelihood(nX)\n\n-5.231597970652478\n\n\n\n\n\nfrom torch.distributions import MultivariateNormal\nimport torch\nfrom meteo_imp.gaussian import to_posdef\n\n\nn = 5\n\n\ncov = to_posdef(torch.rand(n,n))\nmean = torch.rand(n)\n\n\ndist = MultivariateNormal(mean, cov)\n\n\nobs = dist.sample()\n\n\nobs\n\ntensor([1.0237, 0.9482, 0.9448, 1.5264, 1.2376])\n\n\n\ndist.log_prob(obs)\n\ntensor(-3.6350)\n\n\n\ndist2 = MultivariateNormal(mean, torch.diag(cov.diag()))\n\n\ndist2.covariance_matrix\n\ntensor([[0.9555, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 1.1688, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 1.7292, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 2.1713, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 1.0093]])\n\n\n\ndist2.log_prob(obs)\n\ntensor(-6.6010)\n\n\nso we see that if there is a diagonal covariance the likelihood is the same of the sum of the likelihoods of the individual variables\n\ntot = 0\nfor i in range(n):\n    dist_n = MultivariateNormal(mean[i:i+1], cov[i:i+1, i:i+1])\n    tot += dist_n.log_prob(obs[i:i+1])\n\n\ntot\n\ntensor(-6.6010)\n\n\n\ndist_n.log_prob(obs[i:i+1])\n\ntensor(-1.3723)\n\n\nperformance big ll vs individual\n\nn = 5000\n\n\nstd = torch.rand(n)\nmean = torch.rand(n)\n\n\ndist = MultivariateNormal(mean, torch.diag(std))\n\n\nobs = dist.sample()\n\n\n%timeit dist.log_prob(obs)\n\n2.61 ms ± 134 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\ndist.log_prob(obs)\n\ntensor(-4552.8486)\n\n\n\ndef single_ll():\n    tot = 0\n    for i in range(n):\n        dist_n = MultivariateNormal(mean[i:i+1], torch.diag(std[i:i+1]))\n        tot += dist_n.log_prob(obs[i:i+1])\n\n\ntot\n\ntensor(-6.6010)\n\n\n\n%timeit single_ll()\n\n1.49 s ± 164 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\ndist = MultivariateNormal(mean.cuda(), torch.diag(std).cuda())\n\n\nobs = dist.sample()\n\n\n%timeit dist.log_prob(obs)\n\n630 µs ± 3.43 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "exploration/pytorch_constraints.html",
    "href": "exploration/pytorch_constraints.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "need to be sure that the matrix is a positive semi-definite, so we can use choleshy decomposition where if \\(A\\) is positive semidefinite it can be decomposed as \\(A=LL^T\\) where \\(L\\) is a lower triangular matrix. Therefore the idea is to have as the raw parameter \\(L\\) which we’ll enforce to be a lower triangular matrix\n\nimport torch\n\n\nR = torch.rand(3,3, requires_grad=True) \n\n\nL = torch.tril(R)\n\n\nL.sum().backward() \n\n\nL\n\ntensor([[0.6097, 0.0000, 0.0000],\n        [0.6872, 0.5714, 0.0000],\n        [0.6097, 0.9560, 0.4841]], grad_fn=&lt;TrilBackward0&gt;)\n\n\n\ntorch.distributions.MultivariateNormal(torch.zeros(3), R @ R.T)\n\nMultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3]))\n\n\n\nfor i in range(10_000):\n    R = torch.rand(3,3)\n    cov = R @ R.T + (torch.eye(3) * 1e-7)\n    torch.distributions.MultivariateNormal(torch.zeros(3), cov)\n\n\nfor i in range(10_000):\n    R = torch.rand(3,3)\n    L = torch.tril(R)\n    torch.distributions.MultivariateNormal(torch.zeros(3), L @ L.T)\n\nValueError: Expected parameter covariance_matrix (Tensor of shape (3, 3)) of distribution MultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[0.3166, 0.4638, 0.4075],\n        [0.4638, 0.6800, 0.6019],\n        [0.4075, 0.6019, 0.5586]])\n\n\n\nfrom gpytorch.constraints import Positive\nimport torch\n\n\ncov = torch.eye(3)\n\n\nconstraint = Positive()\n\n\nraw = constraint.inverse_transform(cov)\nraw\n\ntensor([[0.5413,   -inf,   -inf],\n        [  -inf, 0.5413,   -inf],\n        [  -inf,   -inf, 0.5413]])\n\n\n\nconstraint.transform(raw)\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n\n\n\nconstraint.inverse_transform(cov + torch.fill(cov, 1e-7))\n\ntensor([[  0.5413, -16.1181, -16.1181],\n        [-16.1181,   0.5413, -16.1181],\n        [-16.1181, -16.1181,   0.5413]])\n\n\n\nraw2 = -torch.ones(3,3)\n\n\nraw2\n\ntensor([[-1., -1., -1.],\n        [-1., -1., -1.],\n        [-1., -1., -1.]])\n\n\n\nconstraint.transform(raw2)\n\ntensor([[0.3133, 0.3133, 0.3133],\n        [0.3133, 0.3133, 0.3133],\n        [0.3133, 0.3133, 0.3133]])\n\n\n\n\n\nfrom meteo_imp.kalman.filter import *\n\n\nimport torch\n\n\nA = torch.rand(100, 100)\n\n\nA.dtype\n\ntorch.float32\n\n\n\nsymmetric_upto(A * A.T, -10)\n\n-10\n\n\n\ntorch.tril(A) * torch.tril(A).T\n\ntensor([[0.0088, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.1427, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.5120,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.1121, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3522, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2634]])"
  }
]