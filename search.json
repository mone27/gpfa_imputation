[
  {
    "objectID": "Init_parameters_effect.html",
    "href": "Init_parameters_effect.html",
    "title": "Init Lambda",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\nfrom meteo_imp.results import _display_as_row\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\nfrom IPython.display import HTML\n\n\ndef reset_seed():\n    torch.manual_seed(27);\n    np.random.seed(27);\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\ncache = True\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n302.475\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n\n\n...\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n330.202\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n330.202\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n330.202\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n\n\n\n\n\n\nreset_seed()\ndata = GPFADataTest(hai[:150]).add_random_missing()\n\n\nimp = GPFAImputationExplorer(hai[:20], latent_dims=2)\n\n\nimp\n\nGPFA Imputation Explorer:\n    N obs: 20\n    N features 4 (TA, SW_IN, LW_IN, VPD)\n    N missing observations 0\n    N latent: 2\n\n\n\nimp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.ones(4,2))\n\nimp.fit()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nGPFA Imputation Explorer:\n    N obs: 20\n    N features 4 (TA, SW_IN, LW_IN, VPD)\n    N missing observations 0\n    N latent: 2\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n\npca = PCA(2).fit(data.data_complete)\n\n\nPCA(2).fit(data.data_complete).components_\n\narray([[ 2.64510596e-03,  9.72190612e-01, -2.34163954e-01,\n         2.37925628e-03],\n       [-8.47181130e-03, -2.34134422e-01, -9.72167258e-01,\n        -3.50525961e-04]])\n\n\n\nimp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.tensor(PCA(2).fit(data.data_complete).components_, dtype=torch.float).T)\n\n\nimp.fit()\n\n\n\n\nGPFA Imputation Explorer:\n    N obs: 20\n    N features 4 (TA, SW_IN, LW_IN, VPD)\n    N missing observations 0\n    N latent: 2\n\n\n\n\n\n\ncache = here() / \".cache\" / \"hai_test_init_values.pickle\"\n# cache.unlink()\n\n\n@cache_disk(cache)\ndef compute():\n    out = {}\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    out[\"normal\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.ones(4,2))\n    out[\"ones\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.zeros(4,2))\n    out[\"zeros\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.rand(4,2))\n    out[\"rand1\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.rand(4,2))\n    out[\"rand2\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.rand(4,2))\n    out[\"rand3\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    imp = GPFAImputationExplorer(data.data, latent_dims=2)\n    imp.learner.model.covar_module.Lambda = torch.nn.Parameter(torch.tensor(PCA(2).fit(data.data_complete).components_, dtype=torch.float).T)\n    out[\"pca\"] = imp.fit().to_result(data.data_compl_tidy, units=units)\n    \n    return out\n\n\nresults = compute()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor key, result in results.items():\n    display(HTML(f\"<h4>{key}</h4>\"))\n    _display_as_row({'r2': result.r2(), **result.model_info})\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1858\n\n\nLW_IN\n0.9682\n\n\nVPD\n0.5829\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.5849\n0.8590\n\n\nSW_IN\n0.3494\n-0.0086\n\n\nLW_IN\n-0.6155\n0.4553\n\n\nVPD\n0.5600\n0.5320\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.3226\n\n\nz1\n7.1896\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.7995\n\n\nLW_IN\n0.0192\n\n\nVPD\n0.4386\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0053\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6200\n\n\nSW_IN\n0.1293\n\n\nLW_IN\n0.0110\n\n\nVPD\n0.9652\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.5621\n0.5621\n\n\nSW_IN\n0.2967\n0.2967\n\n\nLW_IN\n-0.1298\n-0.1298\n\n\nVPD\n0.7420\n0.7420\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.5764\n\n\nz1\n5.5764\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4161\n\n\nSW_IN\n0.8069\n\n\nLW_IN\n0.9339\n\n\nVPD\n0.0106\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0226\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-0.0176\n\n\nSW_IN\n-0.0140\n\n\nLW_IN\n-0.0032\n\n\nVPD\n-0.0225\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.0000\n0.0000\n\n\nSW_IN\n0.0000\n0.0000\n\n\nLW_IN\n0.0000\n0.0000\n\n\nVPD\n0.0000\n0.0000\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n0.6931\n\n\nz1\n0.6931\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4942\n\n\nSW_IN\n0.4942\n\n\nLW_IN\n0.4942\n\n\nVPD\n0.4942\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.4943\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9916\n\n\nSW_IN\n0.9688\n\n\nLW_IN\n0.1685\n\n\nVPD\n0.6162\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.1113\n0.8822\n\n\nSW_IN\n-1.2775\n0.4726\n\n\nLW_IN\n0.5127\n-0.0920\n\n\nVPD\n-0.2842\n0.7279\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.2701\n\n\nz1\n6.5753\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0011\n\n\nSW_IN\n0.0177\n\n\nLW_IN\n0.8398\n\n\nVPD\n0.4049\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1856\n\n\nLW_IN\n0.9680\n\n\nVPD\n0.5827\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.9612\n0.3015\n\n\nSW_IN\n0.0887\n0.3397\n\n\nLW_IN\n0.2557\n-0.7323\n\n\nVPD\n0.6497\n0.3773\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.0300\n\n\nz1\n7.5588\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.8078\n\n\nLW_IN\n0.0191\n\n\nVPD\n0.4389\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.1840\n\n\nLW_IN\n0.9684\n\n\nVPD\n0.5822\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.2080\n0.9447\n\n\nSW_IN\n-0.2592\n0.2189\n\n\nLW_IN\n0.7847\n-0.0903\n\n\nVPD\n-0.0126\n0.7127\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.8303\n\n\nz1\n6.8435\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.7976\n\n\nLW_IN\n0.0190\n\n\nVPD\n0.4412\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.1845\n\n\nLW_IN\n0.9683\n\n\nVPD\n0.5823\n\n\n\n\nLambda\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.9431\n-0.1357\n\n\nSW_IN\n0.2026\n0.2780\n\n\nLW_IN\n-0.0436\n-0.7956\n\n\nVPD\n0.7012\n0.0679\n\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.8095\n\n\nz1\n7.8881\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0008\n\n\nSW_IN\n0.7966\n\n\nLW_IN\n0.0195\n\n\nVPD\n0.4391\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0052\n\n\n\n\n\n\n\n\n\n\nfor key, result in results.items():\n    display(HTML(f\"<h4>{key}</h4>\"))\n    result.display_results()\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1858\n\n\nLW_IN\n0.9682\n\n\nVPD\n0.5829\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0863\nÂ°C\n\n\nSW_IN\n33.5182\nW m-2\n\n\nLW_IN\n3.3613\nW m-2\n\n\nVPD\n0.1651\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9819\n\n\nSW_IN\n0.1773\n\n\nLW_IN\n0.9495\n\n\nVPD\n0.5814\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1205\nÂ°C\n\n\nSW_IN\n43.8162\nW m-2\n\n\nLW_IN\n4.6838\nW m-2\n\n\nVPD\n0.1803\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.5849\n0.8590\n\n\nSW_IN\n0.3494\n-0.0086\n\n\nLW_IN\n-0.6155\n0.4553\n\n\nVPD\n0.5600\n0.5320\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.3226\n\n\nz1\n7.1896\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.7995\n\n\nLW_IN\n0.0192\n\n\nVPD\n0.4386\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0053\n\n\n\n\n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6200\n\n\nSW_IN\n0.1293\n\n\nLW_IN\n0.0110\n\n\nVPD\n0.9652\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.5693\nÂ°C\n\n\nSW_IN\n34.6616\nW m-2\n\n\nLW_IN\n18.7311\nW m-2\n\n\nVPD\n0.0477\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6607\n\n\nSW_IN\n0.0861\n\n\nLW_IN\n-0.0338\n\n\nVPD\n0.9568\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.5220\nÂ°C\n\n\nSW_IN\n46.1813\nW m-2\n\n\nLW_IN\n21.2015\nW m-2\n\n\nVPD\n0.0579\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.5621\n0.5621\n\n\nSW_IN\n0.2967\n0.2967\n\n\nLW_IN\n-0.1298\n-0.1298\n\n\nVPD\n0.7420\n0.7420\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.5764\n\n\nz1\n5.5764\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4161\n\n\nSW_IN\n0.8069\n\n\nLW_IN\n0.9339\n\n\nVPD\n0.0106\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0226\n\n\n\n\n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-0.0176\n\n\nSW_IN\n-0.0140\n\n\nLW_IN\n-0.0032\n\n\nVPD\n-0.0225\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.9315\nÂ°C\n\n\nSW_IN\n37.4040\nW m-2\n\n\nLW_IN\n18.8651\nW m-2\n\n\nVPD\n0.2585\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-0.1121\n\n\nSW_IN\n-0.0709\n\n\nLW_IN\n-0.0378\n\n\nVPD\n-0.1479\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.9450\nÂ°C\n\n\nSW_IN\n49.9906\nW m-2\n\n\nLW_IN\n21.2424\nW m-2\n\n\nVPD\n0.2985\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.0000\n0.0000\n\n\nSW_IN\n0.0000\n0.0000\n\n\nLW_IN\n0.0000\n0.0000\n\n\nVPD\n0.0000\n0.0000\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n0.6931\n\n\nz1\n0.6931\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4942\n\n\nSW_IN\n0.4942\n\n\nLW_IN\n0.4942\n\n\nVPD\n0.4942\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.4943\n\n\n\n\n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9916\n\n\nSW_IN\n0.9688\n\n\nLW_IN\n0.1685\n\n\nVPD\n0.6162\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0847\nÂ°C\n\n\nSW_IN\n6.5572\nW m-2\n\n\nLW_IN\n17.1756\nW m-2\n\n\nVPD\n0.1584\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9825\n\n\nSW_IN\n0.9670\n\n\nLW_IN\n0.2287\n\n\nVPD\n0.6275\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1185\nÂ°C\n\n\nSW_IN\n8.7722\nW m-2\n\n\nLW_IN\n18.3136\nW m-2\n\n\nVPD\n0.1700\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.1113\n0.8822\n\n\nSW_IN\n-1.2775\n0.4726\n\n\nLW_IN\n0.5127\n-0.0920\n\n\nVPD\n-0.2842\n0.7279\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.2701\n\n\nz1\n6.5753\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0011\n\n\nSW_IN\n0.0177\n\n\nLW_IN\n0.8398\n\n\nVPD\n0.4049\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1856\n\n\nLW_IN\n0.9680\n\n\nVPD\n0.5827\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0859\nÂ°C\n\n\nSW_IN\n33.5210\nW m-2\n\n\nLW_IN\n3.3695\nW m-2\n\n\nVPD\n0.1651\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9820\n\n\nSW_IN\n0.1771\n\n\nLW_IN\n0.9495\n\n\nVPD\n0.5812\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1203\nÂ°C\n\n\nSW_IN\n43.8197\nW m-2\n\n\nLW_IN\n4.6882\nW m-2\n\n\nVPD\n0.1803\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.9612\n0.3015\n\n\nSW_IN\n0.0887\n0.3397\n\n\nLW_IN\n0.2557\n-0.7323\n\n\nVPD\n0.6497\n0.3773\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.0300\n\n\nz1\n7.5588\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.8078\n\n\nLW_IN\n0.0191\n\n\nVPD\n0.4389\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.1840\n\n\nLW_IN\n0.9684\n\n\nVPD\n0.5822\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0852\nÂ°C\n\n\nSW_IN\n33.5551\nW m-2\n\n\nLW_IN\n3.3494\nW m-2\n\n\nVPD\n0.1652\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9822\n\n\nSW_IN\n0.1742\n\n\nLW_IN\n0.9509\n\n\nVPD\n0.5801\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1194\nÂ°C\n\n\nSW_IN\n43.8981\nW m-2\n\n\nLW_IN\n4.6204\nW m-2\n\n\nVPD\n0.1805\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.2080\n0.9447\n\n\nSW_IN\n-0.2592\n0.2189\n\n\nLW_IN\n0.7847\n-0.0903\n\n\nVPD\n-0.0126\n0.7127\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.8303\n\n\nz1\n6.8435\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nSW_IN\n0.7976\n\n\nLW_IN\n0.0190\n\n\nVPD\n0.4412\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0051\n\n\n\n\n\n\n\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.1845\n\n\nLW_IN\n0.9683\n\n\nVPD\n0.5823\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0852\nÂ°C\n\n\nSW_IN\n33.5447\nW m-2\n\n\nLW_IN\n3.3548\nW m-2\n\n\nVPD\n0.1652\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9822\n\n\nSW_IN\n0.1750\n\n\nLW_IN\n0.9508\n\n\nVPD\n0.5801\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1195\nÂ°C\n\n\nSW_IN\n43.8775\nW m-2\n\n\nLW_IN\n4.6273\nW m-2\n\n\nVPD\n0.1805\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.9431\n-0.1357\n\n\nSW_IN\n0.2026\n0.2780\n\n\nLW_IN\n-0.0436\n-0.7956\n\n\nVPD\n0.7012\n0.0679\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.8095\n\n\nz1\n7.8881\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0008\n\n\nSW_IN\n0.7966\n\n\nLW_IN\n0.0195\n\n\nVPD\n0.4391\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0052"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPFA Imputation",
    "section": "",
    "text": "GPFA Imputation\nThis is the homepage of the website that contains all the analysis. Use the sidebar for nagivation\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\nAdditional latent kernel\n\n\n\n\nAnalyze gaps fluxnet\n\n\n\n\nCode for presentation of 18th January\n\n\n\n\nCompare Loss functions\n\n\n\n\nExploration of Loglikelihood computations\n\n\n\n\nFill one day gap in SW_IN\n\n\n\n\nFilter loss - (float32)\n\n\n\n\nGap Length Variation\n\n\n\n\nHainich with ERA-Interim\n\n\n\n\nHainich with ERA-Interim\n\n\n\n\nHainich with ERA-Interim controls\n\n\n\n\nInit Lambda\n\n\n\n\nKalman Filter Hainich\n\n\n\n\nKernel visualization\n\n\n\n\nLocal Level Hainich\n\n\n\n\nLog Transform\n\n\n\n\nMeterological Time series Imputation using Kalman Filters\n\n\n\n\nMultiple Latent\n\n\n\n\nMultiple latent â¦\n\n\n\n\nNotes\n\n\n\n\nPositive semi-definite\n\n\n\n\nProcess Noise estimation\n\n\n\n\nResults\n\n\n\n\nSimple GP Hainich\n\n\n\n\nSingle latent\n\n\n\n\nTrain Kalman filter using Fastai using float64\n\n\n\n\nTrain multiple latents\n\n\n\n\nVariable distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Notes\nNotes: - Kernel Lengthscale: too short to handle big gaps - how to encode time? - normalization - parameters init\nnext steps:\n\n\n\nthings to try: - more variables - more kernels - basic variable dist - log - deploy to website\nThings to consider: - different latent kernel for different latent variable?\n\nTA\nSW_IN\nLW_IN\nVPD\nWS\nWD\nP\nPA\nSWC\nPPFD\nCO2 flux\nH02 flux\nLE\n\nCode improvements: - move normalization out of the learner class - use something like fastai pipelines for data transformation â¦"
  },
  {
    "objectID": "Train multiple latent.html",
    "href": "Train multiple latent.html",
    "title": "Train multiple latents",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\nfrom meteo_imp.results import _display_as_row \n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\n\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=300)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    #\"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-07 04:00:00\n3.48\n0.0\n0.065\n\n\n2000-01-07 04:30:00\n3.48\n0.0\n0.063\n\n\n2000-01-07 05:00:00\n3.47\n0.0\n0.063\n\n\n2000-01-07 05:30:00\n3.43\n0.0\n0.057\n\n\n2000-01-07 06:00:00\n3.41\n0.0\n0.055\n\n\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nn_obs = 200\nn_latents = range(1,4) \n\n\ndata = GPFADataTest(hai[:n_obs])\n\n\ncache_file_gaps = cache_path / \"hai_train_multi_latents.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\nmodel_save_dir = here() / \"analysis/trained_models\"\n\n\nimps = [GPFAImputation(data.data, latent_dims=i) for i in n_latents]\n\n\ntotal_iter = 0\n\n\ndef train_save(n_iter=10):\n    global total_iter\n    total_iter += n_iter\n    for imp, n_lat in zip(imps, n_latents):\n        imp.fit(n_iter)\n        imp.learner.save(model_save_dir / f\"GPFA_l_{n_lat}_train_{total_iter}_1ker_{n_obs}_obs.pickle\")\n\n\n\n\n\ntrain_save(100)\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_idx=0\nprint(\"n_latent: \" + str(imps[p_idx].learner.latent_dims))\ndisplay(imps[p_idx].learner.plot_progress())\n_display_as_row(imps[p_idx].learner.model.get_info(imps[p_idx].learner.var_names))\n\nn_latent: 1\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nz0\n\n\n\n\nTA\n0.7965\n\n\nSW_IN\n-0.0502\n\n\nVPD\n0.4993\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.5707\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0013\n\n\nSW_IN\n0.9871\n\n\nVPD\n0.6120\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0027\n\n\n\n\n\n\n\n\n\n\np_idx=1\nprint(\"n_latent: \" + str(imps[p_idx].learner.latent_dims))\ndisplay(imps[p_idx].learner.plot_progress())\n_display_as_row(imps[p_idx].learner.model.get_info(imps[p_idx].learner.var_names))\n\nn_latent: 2\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n-0.1784\n0.7830\n\n\nSW_IN\n0.9565\n0.3294\n\n\nVPD\n0.0957\n0.5738\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n4.7033\n\n\nz1\n6.8953\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0007\n\n\nSW_IN\n0.0261\n\n\nVPD\n0.5750\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0031\n\n\n\n\n\n\n\n\n\n\np_idx=2\nprint(\"n_latent: \" + str(imps[p_idx].learner.latent_dims))\ndisplay(imps[p_idx].learner.plot_progress())\n_display_as_row(imps[p_idx].learner.model.get_info(imps[p_idx].learner.var_names))\n\nn_latent: 3\n\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\nTA\n0.5749\n0.1098\n0.6824\n\n\nSW_IN\n-0.7796\n-0.0461\n0.6270\n\n\nVPD\n0.1813\n0.7577\n0.5405\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.2282\n\n\nz1\n1.8765\n\n\nz2\n4.0832\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0006\n\n\nSW_IN\n0.0285\n\n\nVPD\n0.0035\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0017"
  },
  {
    "objectID": "kalman/Hainich_Control.html",
    "href": "kalman/Hainich_Control.html",
    "title": "Hainich with ERA-Interim controls",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\n\n\nhai = pd.read_parquet(hai_path)\nhai64 = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path)\nhai_era64 = pd.read_parquet(hai_era_path64)\n\n\ndls = imp_dataloader(hai64, hai_era64, var_sel = gen_var_sel(['TA', 'SW_IN', 'VPD']), block_len=200, gap_len=gen_gap_len(10, min_v=3), bs=20, control_lags=[1])\n\n\n# dls = imp_dataloader(hai64, hai_era64, var_sel = gen_var_sel(['TA', 'SW_IN', 'VPD']), block_len=200, gap_len=10, bs=20, control_lags=[1])"
  },
  {
    "objectID": "kalman/Hainich_Control.html#simple-training",
    "href": "kalman/Hainich_Control.html#simple-training",
    "title": "Hainich with ERA-Interim controls",
    "section": "Simple training",
    "text": "Simple training\n\nloss_func_ng = loss_func=KalmanLoss(only_gap=False)\nlearn = Learner(dls, model, loss_func_ng, cbs=[ShowGraphCallback, Float64Callback], metrics=[rmse_mask, r2_mask])\n\n\nlearn.fit(10, 2e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n607.058369\n515.446269\n0.366365\n-18.953901\n01:39\n\n\n1\n475.921415\n359.786886\n0.180804\n-1.877576\n01:49\n\n\n2\n348.836041\n215.060710\n0.133364\n-0.778128\n01:59\n\n\n3\n213.805165\n71.989816\n0.132907\n-1.213813\n02:04\n\n\n4\n84.765372\n-51.247118\n0.119912\n-0.064529\n02:07\n\n\n5\n-41.113329\n-182.953342\n0.075950\n0.661874\n02:08\n\n\n6\n-169.925174\n-311.168974\n0.064809\n-0.035979\n02:04\n\n\n7\n-295.206480\n-432.871735\n0.063802\n0.936280\n02:04\n\n\n8\n-424.575996\n-562.614567\n0.048965\n0.908722\n01:59\n\n\n9\n-545.755660\n-679.265734\n0.045273\n0.648892\n02:00\n\n\n\n\n\n\n\n\n\nshow_results(learn)\n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nlearn.fit(10, 7e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n-701.508009\n-719.499257\n0.042491\n0.364471\n01:46\n\n\n1\n-733.036040\n-751.966214\n0.039660\n0.615754\n01:51\n\n\n2\n-766.875790\n-802.134089\n0.037965\n0.984316\n01:46\n\n\n3\n-802.225402\n-842.992511\n0.034145\n0.977250\n01:44\n\n\n4\n-840.689974\n-884.280286\n0.040979\n0.983124\n01:53\n\n\n5\n-880.406406\n-921.259744\n0.036642\n0.871848\n01:45\n\n\n6\n-920.637857\n-965.865153\n0.034881\n0.975765\n01:47\n\n\n7\n-960.791818\n-1002.702275\n0.034850\n0.629993\n01:49\n\n\n8\n-1003.434672\n-1048.419952\n0.031607\n0.975755\n01:48\n\n\n9\n-1035.639274\n-1066.565967\n0.040136\n0.435163\n01:50\n\n\n\n\n\n\n\n\n\nshow_results(learn)\n\n\n\n\n\n\ntrain only on gap\n\nlearn.loss_func =  KalmanLoss(only_gap=True)\n\n\nshow_results(learn)\n\n\n\n\n\n\n\nlearn.fit(10, 7e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n-0.005931\n-2.003364\n0.030327\n0.985116\n01:14\n\n\n1\n-1.177973\n-3.447589\n0.029919\n0.704350\n01:16\n\n\n2\n-1.061323\n0.247962\n0.034471\n0.881624\n01:22\n\n\n3\n-1.619825\n-0.819893\n0.031246\n0.990277\n01:17\n\n\n4\n-2.642889\n-2.878098\n0.031077\n-0.946337\n01:18\n\n\n5\n-3.325557\n2.120081\n0.038847\n0.991815\n01:20\n\n\n6\n-4.295325\n-1.668097\n0.035035\n0.985702\n01:18\n\n\n7\n-2.293203\n-0.351283\n0.033916\n0.910861\n01:19\n\n\n8\n-2.816982\n-2.604867\n0.031628\n0.669590\n01:19\n\n\n9\n-2.989582\n-2.357790\n0.032688\n0.983338\n01:17\n\n\n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nlearn.metrics = learn.metrics + [mk_metric(m) for m in [rmse_gap, r2_gap]]\n\n\nshow_results(learn)\n\n\n\n\n\n\n\nlearn\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.9932\n0.2895\n-0.2506\n\n\nx_1\n0.1582\n0.5455\n0.3679\n\n\nx_2\n-0.1023\n0.6036\n0.4589\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0317\n-0.0000\n0.0032\n\n\nx_1\n-0.0000\n0.0277\n0.0079\n\n\nx_2\n0.0032\n0.0079\n0.0268\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0705\n\n\nx_1\n0.0512\n\n\nx_2\n-0.1263\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.0415\n0.0734\n0.0369\n\n\ny_1\n0.1477\n-0.5466\n0.4599\n\n\ny_2\n0.1749\n0.0563\n-0.0193\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0000\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0000\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0000\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0593\n\n\ny_1\n0.1921\n\n\ny_2\n0.4388\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.4222\n0.6555\n-0.0202\n-0.9495\n0.4507\n0.1714\n\n\nx_1\n0.0009\n-0.0957\n0.1792\n0.3218\n-0.5409\n-0.2363\n\n\nx_2\n0.2752\n0.8957\n-0.1440\n0.1622\n0.1775\n-0.0837\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.3115\n\n\nx_1\n0.5003\n\n\nx_2\n0.5133\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n2.3021\n0.7211\n0.6520\n\n\nx_1\n0.7211\n3.7494\n3.8710\n\n\nx_2\n0.6520\n3.8710\n5.0761\n\n\n\n\n\n\n\n# learn.save(\"trained_hainich_control_9_jan_v1\")\n\nPath('models/trained_hainich_control_9_jan_v1.pth')"
  },
  {
    "objectID": "kalman/Hainich_Control.html#multiple-losses-training",
    "href": "kalman/Hainich_Control.html#multiple-losses-training",
    "title": "Hainich with ERA-Interim controls",
    "section": "Multiple losses training",
    "text": "Multiple losses training\n\ninp, targ = dls.one_batch()\ninp[0].eq(targ[0]).all()\ninp[1].all(1).all(1).any()\n\ntensor(False, device='cuda:0')\n\n\n\ninp[1].all(1).all(1)\n\ntensor([False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False, False],\n       device='cuda:0')\n\n\n\ninp[1][18].all()\n\ntensor(False, device='cuda:0')\n\n\n\nfor _ in range(100):\n    inp, targ = dls.one_batch()\n    assert not inp[1].all(1).all(1).any()\n\n\nloss_func_g = loss_func=KalmanLoss(only_gap=True)\nlearn = Learner(dls, model, loss_func_g, cbs=[ShowGraphCallback, Float64Callback], metrics=imp_metrics)\n\n\nlearn.fit(5, 2e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n35.681259\n25.316821\n0.569263\n0.675936\n-11.686040\n-4623607617801388104142699888640.000000\n01:11\n\n\n1\n28.467558\n23.233029\n0.524849\n0.610395\n-2.279983\n-2785450798303600881197612269568.000000\n01:12\n\n\n2\n24.512202\n21.556061\n0.508569\n0.522891\n-1.991859\n-1787686587625842508214908747776.000000\n01:17\n\n\n3\n22.205898\n18.522426\n0.504695\n0.497300\n-2.575984\n-2311579056314112802293087731712.000000\n01:14\n\n\n4\n19.906797\n16.373493\n0.532846\n0.453842\n-1.954186\n-2977305927889536455278742994944.000000\n01:13\n\n\n\n\n\n\n\n\n\nlearn\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0550\n0.8305\n0.9071\n\n\nx_1\n0.8076\n0.7187\n0.3710\n\n\nx_2\n1.0252\n0.9262\n0.4660\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.1091\n0.0170\n0.0334\n\n\nx_1\n0.0170\n0.0027\n0.0052\n\n\nx_2\n0.0334\n0.0052\n0.0103\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n1.0085\n\n\nx_1\n0.2683\n\n\nx_2\n0.1844\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.0586\n-0.0735\n0.2603\n\n\ny_1\n0.3055\n0.4040\n0.4354\n\n\ny_2\n0.2157\n0.9007\n0.4757\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0795\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.6049\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.1729\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0631\n\n\ny_1\n0.6257\n\n\ny_2\n0.8232\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n0.1065\n-0.1080\n-0.8654\n-0.3202\n-0.0629\n-0.2701\n\n\nx_1\n-1.0667\n-0.2948\n-0.0322\n-0.4331\n0.0113\n-0.1139\n\n\nx_2\n0.9766\n0.2488\n-0.2589\n0.5920\n0.3605\n-0.1070\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0949\n\n\nx_1\n0.0299\n\n\nx_2\n0.0380\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.1153\n-0.1604\n-0.1218\n\n\nx_1\n-0.1604\n0.4181\n0.8881\n\n\nx_2\n-0.1218\n0.8881\n3.4152\n\n\n\n\n\n\n\nshow_results(learn)\n\n\n\n\n\n\n\nlearn.model.use_smooth = False\n\n\nlearn.fit(5, 2e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n98.515476\n76.464946\n3.587664\n9.146169\n-556.182098\n-1785660235166445426744542481088512.000000\n00:53\n\n\n1\n65.572638\n42.698891\n1.761536\n4.240729\n-961.723482\n-978013546570620530329106643943424.000000\n00:56\n\n\n2\n55.393772\n45.721459\n1.547412\n3.812272\n-358.409783\n-482895226247397956117409612955648.000000\n00:58\n\n\n3\n50.176197\n45.659879\n1.704954\n4.215966\n-1291.738173\n-256935436439301020515829859483648.000000\n01:01\n\n\n4\n48.109062\n50.373996\n1.703977\n4.298851\n-94.260413\n-526513865521285616344450662924288.000000\n00:56\n\n\n\n\n\n\n\n\n\nlearn\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n-0.1561\n0.6142\n0.6362\n\n\nx_1\n0.5261\n0.4458\n0.1055\n\n\nx_2\n1.2212\n1.1215\n0.6900\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n3.4574\n3.0900\n2.8140\n\n\nx_1\n3.0900\n2.8125\n2.5484\n\n\nx_2\n2.8140\n2.5484\n2.3123\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.8462\n\n\nx_1\n0.1179\n\n\nx_2\n0.0779\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.0486\n-0.0819\n0.2397\n\n\ny_1\n0.2923\n0.3889\n0.4084\n\n\ny_2\n0.2078\n0.8587\n0.4657\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.1062\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.3648\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.3026\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.3044\n\n\ny_1\n0.6826\n\n\ny_2\n0.8804\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n0.1861\n-0.1349\n-0.7508\n-0.2323\n-0.0994\n-0.2119\n\n\nx_1\n-0.8815\n-0.2600\n0.1871\n-0.2865\n0.0373\n0.0849\n\n\nx_2\n1.0668\n0.1513\n-0.1757\n0.6918\n0.2565\n-0.0567\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.1463\n\n\nx_1\n-0.1932\n\n\nx_2\n-0.0464\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.4236\n0.0944\n0.3904\n\n\nx_1\n0.0944\n0.2095\n0.6389\n\n\nx_2\n0.3904\n0.6389\n6.4219\n\n\n\n\n\n\n\nlearn.use_smooth = True\n\n\nshow_results(learn)"
  },
  {
    "objectID": "kalman/kalman_float64.html",
    "href": "kalman/kalman_float64.html",
    "title": "Train Kalman filter using Fastai using float64",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nimport pandas as pd\nimport torch\nimport numpy as np\n\nNameError: name 'ListNormal' is not defined\n\n\n\n@cache_disk(\"full_hai\")\ndef load_data():\n    return read_fluxnet_csv(hai_path, None, num_dtype=np.float64)\n\nhai = load_data()\n\n\nmodel = KalmanFilter(**KalmanFilterTester(dtype=torch.float64).params)\n\n\ndls = make_dataloader(hai, 200, 10, bs=10) \n\n\nlen(hai) / 200 / 10 * .8\n\n91.1808\n\n\n\nlen(dls.train)\n\n91\n\n\n\nlearn = Learner(dls, model, loss_func=imp_ll_loss, cbs=[ShowGraphCallback, Float64Callback])\n\n\nlen(dls.train)\n\n91\n\n\n\n\n\nlearn.fit(10, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n\n\n\nNotImplementedError: Module [KalmanFilter] is missing the required \"forward\" function\n\n\n\nlearn.recorder.plot_loss()\n\n\n# learn.fit(10, 1e-2)\n\n\n# learn.recorder.plot_loss()\n\n\n# torch.save(learn.model, \"trained_model_20_dec_f64.pickle\")\n\n\ntrained_state = learn.model.state_dict()\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\n\n\ndef gap2res(var_sel, gap_len, gap_start, block_start=1000, block_end=1200):\n    data = MeteoDataTest(hai.iloc[block_start:block_end, :]).add_gap(gap_len, var_sel, gap_start)\n    imp = KalmanImputation(data.data)\n    imp.model.filter = learn.model # ensure float64 support\n    imp.model.filter.load_state_dict(trained_state)\n    return imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\ndisplay_as_row(learn.model.get_info())\n\n\ncheck_posdef(learn.model.obs_cov.to(torch.float32))\n\n\ngap2res(var_sel, gap_start=30, gap_len=15, block_start=500, block_end=700).display_results()\n\n\n\n\n\ncompute the loss for all predictions not only the gap\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False), cbs=[ShowGraphCallback, Float64Callback])\n\n\nlearn.fit(10, 2e-3)\n\n\nlearn.recorder.plot_loss()\n\nHorrible idea â¦ probably by smoothing is too easy to predict the points that the parameters of the model have basically no influence.\nNeed to try with filtering\n\n\n\ncompute the loss for all predictions not only the gap\n\nmodel = KalmanFilter(**KalmanFilterTester(dtype=torch.float64).params)\n\n\nmodel.use_smooth = False\n\n\nmodel.check_args = None\n\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False), cbs=[ShowGraphCallback, Float64Callback])\n\n\nlearn.fit(12, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n7412.622489\n6652.202241\n03:45\n\n\n1\n5049.731810\n2825.582725\n03:44\n\n\n2\n1962.345019\n903.253100\n03:47\n\n\n3\n318.255686\n-117.658645\n03:44\n\n\n4\n-323.398913\n-448.278246\n03:47\n\n\n5\n-654.908696\n-679.999570\n03:43\n\n\n6\n-803.473786\n-873.061342\n03:46\n\n\n7\n-1016.195842\n-1018.383004\n03:45\n\n\n8\n-1142.367950\n-1148.339634\n03:46\n\n\n9\n-1260.992943\n-1299.262096\n03:41\n\n\n10\n-1431.255912\n-1435.102765\n03:46\n\n\n11\n-1686.184436\n-1593.321410\n03:40\n\n\n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\nlearn.fit(10, 1e-3)\n\n\nlearn.recorder.plot_loss()\n\n\nlearn.fit(10, 5e-4)\n\n\nlearn.recorder.plot_loss()\n\n\ntrained_state = learn.model.state_dict()\n\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\n\n\ndef gap2res(var_sel, gap_len, gap_start, block_start=1000, block_end=1200):\n    data = MeteoDataTest(hai.iloc[block_start:block_end, :]).add_gap(gap_len, var_sel, gap_start)\n    imp = KalmanImputation(data.data)\n    imp.model.filter = learn.model # ensure float64 support\n    imp.model.filter.load_state_dict(trained_state)\n    return imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\nfrom ipywidgets import interact_manual, IntSlider"
  },
  {
    "objectID": "kalman/Loss_comparison.html",
    "href": "kalman/Loss_comparison.html",
    "title": "Compare Loss functions",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\n\nIn order to optimize the parameters of a KalmanFilter there are multiple ways, one is to use EM like pykalman another is to optimize using the likelihood like statsmodels is doing.\nStatsmodels doesnât support gap in the dataset\nThere are 2 main approach for the loss:\n\nuse only the filter. This means that the models runs only the kalman filter and for every observations tries to predict the next one. This doesnât consider the gaps and doesnât use the smoother. The loss is calculated for the whole period\npredict the gap after smoothing. Run the Kalman smoother and then compute the loglikelihood only for the gap\n\nin addition between different batches the loss can be: - averaged - summed\nThe model predictions are mean and stadard deviation for each observation in the dataset, with shape [n_batches, n_obs, n_variables]\nFor each variable at each observation if computes the log-likelihood of the univariate distribution, which is then summed across all the variables and all the observation in a batch`\n\n\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai = load_data()\nhai64 = load_data(np.float64)\n\n\ndef train_loss(reduction, use_smooth):\n    model = KalmanFilter.init_random(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1], dtype=torch.float64).cuda()\n    model.use_smooth = use_smooth\n    only_gap = use_smooth\n    dls = make_dataloader(hai64[:5*20_000], block_len=200, gap_len=10, bs=20) # about 5 year of data\n    loss_func = loss_func=KalmanLoss(only_gap=only_gap, reduction=reduction)\n    learn = Learner(dls, model, loss_func, cbs=[ShowGraphCallback, Float64Callback], metrics=[msk_rmse, msk_r2])\n    learn.fit(20, 1e-2)\n    return learn\n\nload from cache trained models\n\nimport dill\n\n\nwith open(\"models_loss_comparison_30dec.pickle\", 'rb') as f:\n    (l_sum_smooth, l_sum_n_smooth, l_mean_smooth, l_mean_n_smooth) = dill.load(f)\n\n\n\n\n\nl_sum_smooth = train_loss('sum', use_smooth=True)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n10799.557130\n7761.787664\n0.494636\n0.706476\n00:33\n\n\n1\n6721.645770\n1878.776931\n0.464390\n0.739616\n00:33\n\n\n2\n4177.969558\n509.344728\n0.412180\n0.793374\n00:33\n\n\n3\n2260.037030\n-1837.433033\n0.246094\n0.927043\n00:33\n\n\n4\n-719.709847\n-9555.917596\n0.199754\n0.952821\n00:33\n\n\n5\n-4476.204630\n-13089.933634\n0.149999\n0.973490\n00:34\n\n\n6\n-8455.999841\n-17691.481401\n0.131274\n0.979583\n00:33\n\n\n7\n-11678.127028\n-17865.444855\n0.147398\n0.972290\n00:34\n\n\n8\n-14031.962195\n-18773.763584\n0.137897\n0.976673\n00:34\n\n\n9\n-15678.972540\n-18799.284560\n0.141924\n0.975796\n00:35\n\n\n10\n-16803.071401\n-18882.290928\n0.112587\n0.985104\n00:36\n\n\n11\n-17558.598880\n-18924.760868\n0.105362\n0.986287\n00:33\n\n\n12\n-18067.875428\n-18897.525877\n0.093465\n0.989777\n00:33\n\n\n13\n-18430.414265\n-19040.231475\n0.090060\n0.990478\n00:35\n\n\n14\n-18694.256430\n-19050.077039\n0.092960\n0.989644\n00:33\n\n\n15\n-18885.238713\n-19185.962157\n0.080832\n0.992125\n00:33\n\n\n16\n-19050.434482\n-19113.437583\n0.114341\n0.984587\n00:33\n\n\n17\n-19185.244701\n-19220.514641\n0.082792\n0.992028\n00:33\n\n\n18\n-19271.169882\n-19315.973143\n0.066270\n0.994715\n00:33\n\n\n19\n-19338.463470\n-19355.567454\n0.068022\n0.994381\n00:33\n\n\n\n\n\n\n\n\n\nl_sum_smooth.recorder.plot_loss()\n\n\n\n\n\nshow_results(l_sum_smooth, [10, 50, 100])\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\ndisplay_as_row(l_sum_smooth.model.get_info(var_names=hai64.columns))\n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.8793\n0.1644\n-0.0996\n\n\nz_1\n0.0674\n0.9061\n0.0915\n\n\nz_2\n-0.1359\n0.1807\n0.8870\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.1691\n-0.1189\n-0.1084\n\n\nz_1\n-0.1189\n0.1337\n0.0929\n\n\nz_2\n-0.1084\n0.0929\n0.3573\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n0.0393\n\n\nz_1\n0.0015\n\n\nz_2\n0.1445\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.1359\n0.1995\n-0.0180\n\n\nSW_IN\n-0.0492\n0.3400\n0.0636\n\n\nVPD\n0.3182\n0.0204\n0.3036\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.0000\n0.0000\n-0.0000\n\n\nSW_IN\n0.0000\n0.0000\n-0.0000\n\n\nVPD\n-0.0000\n-0.0000\n0.0000\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.2448\n\n\nSW_IN\n0.3635\n\n\nVPD\n0.2137\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n0.2512\n\n\nz_1\n0.6348\n\n\nz_2\n0.9272\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.0244\n0.4137\n-0.1930\n\n\nz_1\n0.4137\n1.5316\n0.9529\n\n\nz_2\n-0.1930\n0.9529\n2.2897\n\n\n\n\n\n\n\n\n\n\nl_sum_n_smooth = train_loss('sum', use_smooth=False)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n93576.557506\n49094.936151\n40.175671\n-1609.196695\n00:22\n\n\n1\n55729.858957\n25465.931891\n13.037290\n-168.944314\n00:22\n\n\n2\n40901.222382\n23625.398854\n9.227300\n-81.796224\n00:23\n\n\n3\n32801.561799\n21803.994577\n6.553250\n-44.308130\n00:23\n\n\n4\n28134.386174\n20767.311986\n4.905621\n-25.004391\n00:23\n\n\n5\n25155.250779\n19650.187167\n3.522742\n-11.580414\n00:23\n\n\n6\n23134.458851\n18852.613585\n2.670879\n-6.114421\n00:24\n\n\n7\n21590.111416\n18724.717731\n2.463030\n-4.972687\n00:23\n\n\n8\n20488.952666\n18046.332419\n1.743591\n-2.261957\n00:24\n\n\n9\n19599.282756\n17686.631493\n1.554881\n-1.386024\n00:24\n\n\n10\n18864.805491\n17168.486468\n1.231224\n-0.576305\n00:23\n\n\n11\n18219.402631\n16717.755567\n0.993929\n0.016635\n00:24\n\n\n12\n17652.747846\n16358.387093\n0.948614\n0.086813\n00:23\n\n\n13\n17166.811259\n15819.057481\n0.743849\n0.461778\n00:22\n\n\n14\n16669.927076\n15380.745776\n0.729747\n0.474362\n00:23\n\n\n15\n16170.134417\n14886.565936\n0.694190\n0.532979\n00:24\n\n\n16\n15616.390677\n14280.504826\n0.653767\n0.585480\n00:25\n\n\n17\n15008.060510\n13473.336895\n0.579920\n0.672648\n00:23\n\n\n18\n14325.386795\n12561.507037\n0.566265\n0.686859\n00:23\n\n\n19\n13516.621992\n11399.619833\n0.543082\n0.710991\n00:23\n\n\n\n\n\n\n\n\n\nl_sum_n_smooth.recorder.plot_loss()\n\n\n\n\nfit a bit more\n\nl_sum_n_smooth.fit(5, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n10346.725174\n9452.293934\n0.518965\n0.736902\n00:23\n\n\n1\n9009.084750\n7346.894680\n0.498220\n0.756269\n00:23\n\n\n2\n7605.196719\n6058.022931\n0.528030\n0.725869\n00:23\n\n\n3\n6738.237530\n5312.245515\n0.501561\n0.751644\n00:23\n\n\n4\n5908.156025\n4639.401604\n0.500624\n0.751768\n00:23\n\n\n\n\n\n\n\n\n\nl_sum_n_smooth.fit(10, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n4075.124830\n4743.987561\n0.504186\n0.748768\n00:22\n\n\n1\n4534.079512\n5471.174814\n0.494402\n0.757260\n00:22\n\n\n2\n4162.734111\n4298.111266\n0.494588\n0.758349\n00:22\n\n\n3\n3856.497019\n4796.030030\n0.499238\n0.751841\n00:23\n\n\n4\n3702.818910\n3923.179320\n0.493548\n0.759028\n00:23\n\n\n5\n3538.964910\n4498.824727\n0.483678\n0.767376\n00:24\n\n\n6\n3410.967823\n3545.588596\n0.478077\n0.772404\n00:24\n\n\n7\n3237.693183\n3850.488723\n0.495080\n0.754614\n00:24\n\n\n8\n3111.747285\n4198.825180\n0.487525\n0.764516\n00:23\n\n\n9\n3083.048830\n3667.951800\n0.490212\n0.759747\n00:24\n\n\n\n\n\n\n\n\n\nshow_results(l_sum_n_smooth, [10, 50, 100])\n\n\n\n\n\n\nwith smoother enabled just for predictions\n\nl_sum_n_smooth.model.use_smooth = True\ndisplay(show_results(l_sum_n_smooth, [10, 50, 100]))\nl_sum_n_smooth.model.use_smooth = False\n\n\n\n\n\n\n\ndisplay_as_row(l_sum_n_smooth.model.get_info(var_names=hai64.columns))\n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.9738\n0.0646\n0.0733\n\n\nz_1\n0.3341\n0.5742\n0.3603\n\n\nz_2\n-0.3077\n0.4121\n0.6017\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.5887\n0.1503\n-0.3697\n\n\nz_1\n0.1503\n0.0416\n-0.0951\n\n\nz_2\n-0.3697\n-0.0951\n0.2325\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n0.0393\n\n\nz_1\n-0.0971\n\n\nz_2\n0.0904\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.4275\n0.1589\n0.7232\n\n\nSW_IN\n0.1958\n0.8560\n0.3983\n\n\nVPD\n0.3929\n0.3593\n0.7669\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.1843\n-0.0151\n-0.0008\n\n\nSW_IN\n-0.0151\n0.0013\n0.0001\n\n\nVPD\n-0.0008\n0.0001\n0.0000\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.5145\n\n\nSW_IN\n0.8793\n\n\nVPD\n0.6626\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n-0.0205\n\n\nz_1\n-0.6986\n\n\nz_2\n-0.6000\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.8298\n1.1307\n-0.6162\n\n\nz_1\n1.1307\n1.2855\n0.1820\n\n\nz_2\n-0.6162\n0.1820\n0.7764\n\n\n\n\n\n\n\n\n\n\nl_mean_smooth = train_loss('mean', use_smooth=True)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n697.162786\n630.356973\n0.503804\n0.739012\n00:36\n\n\n1\n635.893690\n554.948438\n0.397441\n0.837237\n00:33\n\n\n2\n573.158679\n438.329959\n0.397376\n0.839449\n00:33\n\n\n3\n443.483282\n104.015152\n0.415593\n0.825035\n00:34\n\n\n4\n279.839509\n-134.222046\n0.440517\n0.801995\n00:33\n\n\n5\n73.469916\n-312.444000\n0.405823\n0.831356\n00:33\n\n\n6\n-80.695937\n-384.546196\n0.409707\n0.827130\n00:33\n\n\n7\n-197.950462\n-445.921780\n0.409722\n0.827629\n00:33\n\n\n8\n-283.184402\n-451.063827\n0.409284\n0.828156\n00:34\n\n\n9\n-340.951170\n-454.172981\n0.406012\n0.830759\n00:34\n\n\n10\n-379.005242\n-456.886911\n0.399631\n0.835787\n00:33\n\n\n11\n-417.681836\n-581.048350\n0.137346\n0.980816\n00:33\n\n\n12\n-524.525933\n-843.416952\n0.122917\n0.984063\n00:33\n\n\n13\n-655.300585\n-940.494837\n0.112013\n0.987288\n00:33\n\n\n14\n-754.148642\n-954.246318\n0.106991\n0.988218\n00:33\n\n\n15\n-823.230645\n-964.190164\n0.093932\n0.990694\n00:33\n\n\n16\n-870.296853\n-964.511931\n0.097596\n0.990023\n00:33\n\n\n17\n-902.821560\n-966.711134\n0.083416\n0.992802\n00:33\n\n\n18\n-925.295414\n-974.465392\n0.070724\n0.994710\n00:36\n\n\n19\n-942.501159\n-981.922380\n0.061404\n0.996135\n00:34\n\n\n\n\n\n\n\n\n\nl_mean_smooth.recorder.plot_loss()\n\n\n\n\n\nshow_results(l_mean_smooth, [10, 50, 100])\n\n\n\n\n\n\n\ndisplay_as_row(l_mean_smooth.model.get_info(var_names=hai64.columns))\n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.0959\n-0.1656\n-0.0462\n\n\nz_1\n0.1858\n0.6684\n-0.1010\n\n\nz_2\n-0.0301\n0.0735\n0.9870\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.2404\n0.3451\n0.0008\n\n\nz_1\n0.3451\n0.5604\n-0.0763\n\n\nz_2\n0.0008\n-0.0763\n0.3327\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n0.5843\n\n\nz_1\n0.5746\n\n\nz_2\n0.3402\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.2666\n-0.1751\n-0.0240\n\n\nSW_IN\n-0.0178\n0.2368\n0.1899\n\n\nVPD\n0.3308\n-0.2863\n0.0118\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.0000\n0.0000\n0.0000\n\n\nSW_IN\n0.0000\n0.0000\n-0.0000\n\n\nVPD\n0.0000\n-0.0000\n0.0000\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.5414\n\n\nSW_IN\n0.4390\n\n\nVPD\n0.6652\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n0.7301\n\n\nz_1\n0.3346\n\n\nz_2\n0.2617\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.4340\n0.7932\n0.2754\n\n\nz_1\n0.7932\n1.1764\n0.2383\n\n\nz_2\n0.2754\n0.2383\n0.5386\n\n\n\n\n\n\n\n\n\n\nl_mean_n_smooth = train_loss('mean', use_smooth=False)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n1057.629457\n888.496646\n1.018840\n-0.055726\n00:22\n\n\n1\n923.925165\n796.762860\n0.674683\n0.544355\n00:22\n\n\n2\n851.088850\n747.840067\n0.596738\n0.645535\n00:23\n\n\n3\n798.429587\n705.430505\n0.606329\n0.634376\n00:23\n\n\n4\n754.539451\n668.123294\n0.616092\n0.622592\n00:23\n\n\n5\n715.262041\n636.254135\n0.611814\n0.628103\n00:23\n\n\n6\n681.105511\n615.883004\n0.617857\n0.620452\n00:23\n\n\n7\n651.706290\n591.641635\n0.601030\n0.640745\n00:23\n\n\n8\n625.154468\n570.554773\n0.592957\n0.650125\n00:23\n\n\n9\n593.910834\n477.784763\n0.467038\n0.783076\n00:23\n\n\n10\n542.761901\n414.206483\n0.429209\n0.817010\n00:23\n\n\n11\n488.455552\n339.528152\n0.422597\n0.821515\n00:24\n\n\n12\n404.451756\n144.976959\n0.433326\n0.812972\n00:23\n\n\n13\n308.818492\n107.266816\n0.447009\n0.800872\n00:23\n\n\n14\n228.120486\n59.730369\n0.401824\n0.838990\n00:23\n\n\n15\n164.371803\n39.778540\n0.411961\n0.830528\n00:23\n\n\n16\n121.075008\n31.160392\n0.421206\n0.823260\n00:23\n\n\n17\n84.520273\n4.141408\n0.420663\n0.823536\n00:24\n\n\n18\n57.648469\n4.640894\n0.418233\n0.825792\n00:24\n\n\n19\n36.316722\n3.784453\n0.421853\n0.822295\n00:23\n\n\n\n\n\n\n\n\n\nl_mean_n_smooth.recorder.plot_loss()\n\n\n\n\n\nshow_results(l_mean_n_smooth, [10, 50, 100])\n\n\n\n\n\n\n\nl_mean_n_smooth.model.use_smooth = True\ndisplay(show_results(l_mean_n_smooth, [10, 50, 100]))\nl_mean_n_smooth.model.use_smooth = False\n\n\n\n\n\n\n\ndisplay_as_row(l_mean_n_smooth.model.get_info(var_names=hai64.columns))\n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.8564\n0.3303\n0.2741\n\n\nz_1\n0.0411\n0.5239\n-0.3802\n\n\nz_2\n0.1810\n-0.3956\n0.6654\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.1339\n0.1567\n-0.1591\n\n\nz_1\n0.1567\n0.1839\n-0.1865\n\n\nz_2\n-0.1591\n-0.1865\n0.1893\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n-0.1275\n\n\nz_1\n0.0851\n\n\nz_2\n0.1455\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.3125\n0.3005\n0.5561\n\n\nSW_IN\n0.1724\n0.6300\n0.5023\n\n\nVPD\n0.1824\n0.6207\n0.7000\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.0000\n-0.0000\n0.0001\n\n\nSW_IN\n-0.0000\n0.0000\n-0.0016\n\n\nVPD\n0.0001\n-0.0016\n0.1801\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.4672\n\n\nSW_IN\n0.0952\n\n\nVPD\n0.2722\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n-1.0630\n\n\nz_1\n0.2947\n\n\nz_2\n-0.3536\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n3.4376\n0.9167\n0.0107\n\n\nz_1\n0.9167\n0.5092\n-0.3225\n\n\nz_2\n0.0107\n-0.3225\n0.4000\n\n\n\n\n\n\n\n\n\n\nimport dill\n\n\nwith open(\"models_loss_comparison_30dec.pickle\", 'wb') as f:\n    # dill.dump([l_sum_smooth, l_sum_n_smooth, l_mean_smooth, l_mean_n_smooth], f)\n\n\n\n\ninteract_results(l_sum_smooth, hai64)\n\n\n\n\n<function meteo_imp.kalman.fastai.interact_results.<locals>._inner(gap_len, items_idx, block_len, **var_names)>"
  },
  {
    "objectID": "kalman/KalmanFilter_PyTorch.html",
    "href": "kalman/KalmanFilter_PyTorch.html",
    "title": "Kalman Filter Hainich",
    "section": "",
    "text": "Kalman filter model on Hainich data using PyTorch\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.data import hai, units\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\nfrom meteo_imp.kalman.model import *\nfrom ipywidgets import interact, interact_manual, IntSlider\n\n\nhai\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n200001011712.0\n-0.60\n0.0\n0.222\n\n\n200001011712.0\n-0.65\n0.0\n0.122\n\n\n200001011712.0\n-0.58\n0.0\n0.090\n\n\n200001011712.0\n-0.51\n0.0\n0.110\n\n\n200001011712.0\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n200001044480.0\n4.74\n0.0\n1.191\n\n\n200001044480.0\n4.75\n0.0\n1.057\n\n\n200001044480.0\n4.76\n0.0\n0.935\n\n\n200001044480.0\n4.62\n0.0\n1.162\n\n\n200001044480.0\n4.51\n0.0\n1.636\n\n\n\n\n\n\ndata = MeteoDataTest(hai).add_gap(10, ['TA', 'SW_IN', 'VPD'], 30)\n\n\ndata.data\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n200001011712.0\n-0.60\n0.0\n0.222\n\n\n200001011712.0\n-0.65\n0.0\n0.122\n\n\n200001011712.0\n-0.58\n0.0\n0.090\n\n\n200001011712.0\n-0.51\n0.0\n0.110\n\n\n200001011712.0\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n200001044480.0\n4.74\n0.0\n1.191\n\n\n200001044480.0\n4.75\n0.0\n1.057\n\n\n200001044480.0\n4.76\n0.0\n0.935\n\n\n200001044480.0\n4.62\n0.0\n1.162\n\n\n200001044480.0\n4.51\n0.0\n1.636\n\n\n\n\n\n\nimport numpy as np\n\n\nimp = KalmanImputation(data.data)\n\n\nimp.fit(n_iter = 50)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat iteration 9 expected positive definite matrix. \ntensor([[ 1.8868, -0.4285, -1.4917],\n        [-0.4203,  1.1536, -0.5259],\n        [-1.4795, -0.5314,  1.7019]], grad_fn=<SubBackward0>)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat iteration 9 expected positive definite matrix. \ntensor([[ 0.2549, -0.1689, -0.1888],\n        [-0.1536,  0.4337, -0.2591],\n        [-0.1773, -0.2612,  0.2829]], grad_fn=<AddBackward0>)\nat iteration 9 expected positive definite matrix. \ntensor([[ 0.6063, -0.3421, -0.3056],\n        [-0.3361,  0.6454, -0.3535],\n        [-0.3007, -0.3570,  0.5838]], grad_fn=<AddBackward0>)\n\n\nValueError: Expected parameter covariance_matrix (Tensor of shape (3, 3)) of distribution MultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[ 0.6063, -0.3421, -0.3056],\n        [-0.3361,  0.6454, -0.3535],\n        [-0.3007, -0.3570,  0.5838]], grad_fn=<ExpandBackward0>)\n\n\n\nimp.model.plot_loss()\n\n\nimp.impute()\n\n\ndata.data.columns\n\n\nres = imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nres.display_results()\n\n\n\n\ndef gap2res(var_sel, gap_len, gap_start, n_iter):\n    data = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\n    return KalmanImputation(data.data).fit(n_iter=n_iter).to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10, 5)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]"
  },
  {
    "objectID": "kalman/kalman_fastai.html",
    "href": "kalman/kalman_fastai.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2"
  },
  {
    "objectID": "kalman/kalman_fastai.html#filter-loss---float32",
    "href": "kalman/kalman_fastai.html#filter-loss---float32",
    "title": "Meteo Imputation",
    "section": "Filter loss - (float32)",
    "text": "Filter loss - (float32)\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai = load_data()\n\n\nmodel = KalmanFilter.init_random(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1]).cuda()\n\n\nmodel.use_smooth = False\n\n\ndls = make_dataloader(hai[:5*20_000], block_len=200, gap_len=1, bs=30) # about 5 year of data \n\n\ndls.one_batch()[0][0].dtype, dls.one_batch()[0][0].device\n\n(torch.float32, device(type='cuda', index=0))\n\n\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False, reduction='sum'), cbs=[ShowGraphCallback], metrics=[msk_rmse, msk_r2])\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n28334.117188\n24884.890625\n1.044047\n-0.270700\n00:10\n\n\n1\n27505.285156\n23820.916016\n0.936844\n-0.022787\n00:09\n\n\n2\n26766.705078\n22994.548828\n0.868220\n0.122282\n00:09\n\n\n3\n26070.080078\n22316.949219\n0.818773\n0.219742\n00:09\n\n\n4\n25482.609375\n21709.919922\n0.778877\n0.294147\n00:09\n\n\n5\n24909.001953\n21147.484375\n0.743662\n0.356575\n00:09\n\n\n6\n24372.490234\n20604.521484\n0.716608\n0.402541\n00:09\n\n\n7\n23862.095703\n20079.589844\n0.694325\n0.439092\n00:09\n\n\n8\n23340.068359\n19563.205078\n0.676239\n0.467748\n00:09\n\n\n9\n22824.031250\n19030.230469\n0.661495\n0.490591\n00:09\n\n\n\n\n\n\n\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n20513.478516\n18492.871094\n0.646592\n0.512978\n00:09\n\n\n1\n20331.521484\n17940.644531\n0.636259\n0.528079\n00:09\n\n\n2\n20015.253906\n17363.677734\n0.626718\n0.541712\n00:09\n\n\n3\n19642.833984\n16716.089844\n0.616619\n0.556175\n00:10\n\n\n4\n19214.849609\n15884.863281\n0.600617\n0.578533\n00:09\n\n\n5\n18648.689453\n14677.888672\n0.547030\n0.651080\n00:10\n\n\n6\n17847.457031\n13356.031250\n0.496326\n0.713167\n00:09\n\n\n7\n17063.013672\n12896.752930\n0.455412\n0.758161\n00:09\n\n\n8\n16343.417969\n12419.463867\n0.433779\n0.779173\n00:10\n\n\n9\n15676.717773\n11876.413086\n0.415929\n0.795271\n00:13\n\n\n\n\n\n\n\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n13020.457031\n11358.107422\n0.402912\n0.806103\n00:11\n\n\n1\n12650.278320\n10791.625000\n0.392657\n0.815727\n00:10\n\n\n2\n12294.549805\n10184.606445\n0.383261\n0.823566\n00:10\n\n\n3\n11870.844727\n9569.536133\n0.376152\n0.828884\n00:10\n\n\n4\n11425.293945\n8862.309570\n0.366659\n0.839007\n00:11\n\n\n5\n10938.154297\n8113.016602\n0.363477\n0.842085\n00:11\n\n\n6\n10377.956055\n7253.085938\n0.363612\n0.841904\n00:10\n\n\n7\n9712.108398\n5912.989746\n0.365726\n0.841298\n00:10\n\n\n8\n8721.796875\n1818.676758\n0.379970\n0.830193\n00:11\n\n\n9\n6890.971191\n-1213.163086\n0.375065\n0.835149\n00:14\n\n\n\n\n\n\n\n\n\nlearn.fit(10, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nr2\ntime\n\n\n\n\n0\n991.025146\n-375.008484\n0.374033\n0.836182\n00:10\n\n\n1\n382.140930\n-1426.713867\n0.376774\n0.833246\n00:09\n\n\n2\n-10.901829\n-1699.835083\n0.374207\n0.835683\n00:09\n\n\n3\n-249.258392\n-1820.056519\n0.373657\n0.836395\n00:09\n\n\n4\n-377.162689\n-1793.390137\n0.375005\n0.834909\n00:10\n\n\n5\n-475.895691\n-1815.924683\n0.375007\n0.834904\n00:09\n\n\n6\n-518.640808\n-1835.481812\n0.374665\n0.835544\n00:09\n\n\n7\n-586.902527\n-1827.023193\n0.374206\n0.835841\n00:10\n\n\n8\n-655.092407\n-1729.346924\n0.375481\n0.834565\n00:10\n\n\n9\n-669.299683\n-1798.226685\n0.374862\n0.835274\n00:09\n\n\n\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\ndisplay_as_row(learn.model.get_info(var_names=hai.columns))\n\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.1146\n0.5802\n0.1363\n\n\nz_1\n0.1496\n0.6129\n0.3537\n\n\nz_2\n0.3043\n0.6128\n0.0236\n\n\n\n\ntrans_cov (Q)\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n0.7217\n0.1717\n0.0886\n\n\nz_1\n0.1717\n0.1250\n-0.0129\n\n\nz_2\n0.0886\n-0.0129\n0.0281\n\n\n\n\n\ntrans_off\n\n\n\nlatent\noffset\n\n\n\n\nz_0\n0.2728\n\n\nz_1\n-0.0485\n\n\nz_2\n0.4645\n\n\n\n\n\nobs_matrix (H)\n\n\n\nvariable\nz_0\nz_1\nz_2\n\n\n\n\nTA\n0.4154\n0.6550\n0.3770\n\n\nSW_IN\n0.0119\n0.5495\n0.5917\n\n\nVPD\n0.1603\n0.6821\n-0.0631\n\n\n\n\n\nobs_cov (R)\n\n\n\nvariable\nTA\nSW_IN\nVPD\n\n\n\n\nTA\n0.8515\n0.1525\n0.9209\n\n\nSW_IN\n0.1525\n0.2047\n0.2631\n\n\nVPD\n0.9209\n0.2631\n1.8438\n\n\n\n\n\nobs_off\n\n\n\nvariable\noffset\n\n\n\n\nTA\n0.6057\n\n\nSW_IN\n0.4861\n\n\nVPD\n0.1418\n\n\n\n\n\ninit_state_mean\n\n\n\nlatent\nmean\n\n\n\n\nz_0\n-0.0218\n\n\nz_1\n0.3442\n\n\nz_2\n0.0868\n\n\n\n\n\ninit_state_cov\n\n\n\nlatent\nz_0\nz_1\nz_2\n\n\n\n\nz_0\n1.1216\n1.0998\n0.7476\n\n\nz_1\n1.0998\n1.9714\n0.9649\n\n\nz_2\n0.7476\n0.9649\n0.6006\n\n\n\n\n\n\nThis is filtering! this is not smoothing\n\n# torch.save(learn.model, \"model_trained_30dec.pickle\")\n\n\nshow_results(learn, bind_interaction=False)\n\n\n\n\n\n\n\nlearn.model.use_smooth = True\nshow_results(learn, items=[10, 110, 130])\nlearn.model.use_smooth = False\n\n\n\n\n\n\n\nshow_results(learn, items=[10, 110, 130])\n\n\n\n\n\n\n\n# learn.fit(10, 1e-2)\n\n\n# learn.recorder.plot_loss()\n\n\nlearn.model.use_smooth = False\n\n\nshow_results(learn, items=[1,2,3])\n\n\n\n\n\n\n\n# torch.save(learn.model, \"trained_filter_29_dec_1.pickle\")"
  },
  {
    "objectID": "kalman/kalman_fastai.html#filter-loss---float64",
    "href": "kalman/kalman_fastai.html#filter-loss---float64",
    "title": "Meteo Imputation",
    "section": "Filter loss - (float64)",
    "text": "Filter loss - (float64)\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai64 = load_data(np.float64)\n\n\nmodel64 = KalmanFilter.init_random(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1], dtype=torch.float64).cuda()\n\n\nmodel64.use_smooth = False\n\n\ndls64 = make_dataloader(hai64[:5*20_000], block_len=200, gap_len=1, bs=10) # about 5 year of data \n\n\ndls64.one_batch()[0][0].dtype, dls64.one_batch()[0][0].device\n\n(torch.float64, device(type='cuda', index=0))\n\n\n\nlearn64 = Learner(dls, model, loss_func=KalmanLoss(only_gap=False, reduction='sum'), cbs=[ShowGraphCallback, Float16Callback], metrics=[msk_rmse, msk_r2])\n\nNameError: name 'Float16Callback' is not defined\n\n\n\nlearn64.fit(10, 2e-3)\n\n\nlearn64.recorder.plot_loss()\n\n\ndisplay_as_row(learn64.model.get_info(var_names=hai64.columns))\n\n\nshow_results(learn64, bind_interaction=False)\n\n\n# learn.fit(10, 1e-2)\n\n\n# learn.recorder.plot_loss()\n\n\nlearn.model.use_smooth = False\n\n\nshow_results(learn, items=[1,2,3])"
  },
  {
    "objectID": "kalman/kalman_obs_cov.html",
    "href": "kalman/kalman_obs_cov.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nshortcut for having randomly initialized and with correct type paramters\nmore sensible default to the obs_cov"
  },
  {
    "objectID": "kalman/kalman_obs_cov.html#results",
    "href": "kalman/kalman_obs_cov.html#results",
    "title": "Meteo Imputation",
    "section": "Results",
    "text": "Results\n\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\n\n\ndef gap2res(var_sel, gap_len, gap_start, block_start=1000, block_end=1200):\n    data = MeteoDataTest(hai.iloc[block_start:block_end, :]).add_gap(gap_len, var_sel, gap_start)\n    imp = KalmanImputation(data.data)\n    imp.model.filter.load_state_dict(trained_state)\n    return imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\ndata = MeteoDataTest(hai)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\nfrom ipywidgets import interact_manual, IntSlider"
  },
  {
    "objectID": "kalman/KalmanFilter_simple.html",
    "href": "kalman/KalmanFilter_simple.html",
    "title": "Local Level Hainich",
    "section": "",
    "text": "simple local level model on Hainich data\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nimport altair as alt\nalt.renderers.enable('mimetype')\n\nRendererRegistry.enable('mimetype')\n\n\n\nfrom meteo_imp.data import hai, units\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.imputation import KalmanImputation\nfrom meteo_imp.kalman.model import *\nfrom ipywidgets import interact, interact_manual, IntSlider\n\n\nhai\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n1.636\n\n\n\n\n\n\ndata = MeteoDataTest(hai).add_gap(10, ['TA', 'SW_IN', 'VPD'], 30)\n\n\ndata.data\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n1.636\n\n\n\n\n\nWarning WIP. Data is not standardized (at it should)\n\nimp = KalmanImputation(data.data, LocalLevelModel)\n\n\nimp.fit()\n\n<meteo_imp.kalman.imputation.KalmanImputation>\n\n\n\nimp.impute()\n\nTypeError: KalmanModel.predict() takes 2 positional arguments but 3 were given\n\n\n\ndata.data.columns\n\n\nres = imp.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nres.display_results()\n\n\n\n\ndef gap2res(var_sel, gap_len, gap_start, model, n_iter):\n    data = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\n    return KalmanImputation(data.data, model).fit(n_iter=n_iter).to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nvar_sel = data.data.columns\n\n\n%time gap2res(var_sel, 10, 10, LocalLevelModel, 5)\n\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\n\n\n\ngap_len = 30\ngap_start = 40\nvar_sel = ('TA', 'SW_IN', 'VPD')\nn_iter = 10\n\n\ndata = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\nimp_ls = KalmanImputation(data.data, LocalSlopeModel).fit(n_iter=n_iter)\nres_ls = imp_ls.to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nres_ls.display_results()\n\n\nimp\n\n\ndata = MeteoDataTest(hai).add_gap(gap_len, var_sel, gap_start)\nimp_ls = KalmanImputation(data.data, LocalSlopeModel).fit(n_iter=n_iter, smooth=False).to_result(data.data_compl_tidy, var_names= data.data.columns, units=units, pred_all=True)\n\n\nimp_ls.display_results()"
  },
  {
    "objectID": "kalman/filter_fill_one_day_gap_SW_IN.html",
    "href": "kalman/filter_fill_one_day_gap_SW_IN.html",
    "title": "Fill one day gap in SW_IN",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.fastai import _add_lags_df\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastai.vision.data import get_grid\nfrom fastcore.foundation import L\nfrom fastcore.transform import *\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\n\n\nfrom ipywidgets import widgets, HBox, VBox\nfrom typing import Sequence\n\n\nclass InteractiveSequence():\n    def __init__(self,s: Sequence, start=0):\n        self.s =s\n        self.i = start\n        self.output = widgets.Output()\n        self.button_next = widgets.Button(description=\"Next\", icon=\"arrow-right\")\n        self.button_prev = widgets.Button(description=\"Previous\", icon=\"arrow-left\", disabled=True)\n        self.button_next.on_click(self.on_next)\n        self.button_prev.on_click(self.on_prev)\n        self.label = widgets.Label(f\"of {len(self.s)-1}\")\n        self.slider = widgets.IntSlider(start, 0, len(s)-1, 1)\n        self.slider.observe(self.on_slide, names=\"value\")\n    def update_view(self):\n        self.button_enable()\n        self.slider.value = self.i\n        with self.output:\n            display(self.s[self.i])\n        self.output.clear_output(wait=True)\n    def button_enable(self):\n        if self.i < len(self.s) - 1:  self.button_next.disabled = False\n        else: self.button_next.disabled = True\n        \n        if self.i == 0: self.button_prev.disabled = True\n        else: self.button_prev.disabled = False\n        \n    def on_next(self, b):\n        self.i +=1\n        self.update_view()\n    def on_prev(self, b):\n        self.i -=1\n        self.update_view()\n    def on_slide(self, change):\n        self.i = change['new']\n        self.update_view()\n        \n    def __call__(self):\n        self.update_view()\n        display(VBox([HBox([self.slider, self.label]), HBox([self.button_prev, self.button_next])]), self.output)\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path64)\n\n\nhai[8_110:8_160].SW_IN.plot()\n\n<AxesSubplot: xlabel='time'>\n\n\n\n\n\n\ndf = hai[8_110-80:8_160+80]\ncontrol = _add_lags_df(hai_era.loc[df.index], 1)\n\n\ndf.plot()\n\n<AxesSubplot: xlabel='time'>\n\n\n\n\n\n\ncontrol.plot()\n\n<AxesSubplot: xlabel='time'>\n\n\n\n\n\n\ndf.columns\n\nIndex(['TA', 'SW_IN', 'VPD'], dtype='object')\n\n\n\nmask = df.astype(dtype=bool)\nmask.iloc[:,:] = True\nmask.iloc[100:115, 1] = False\n\n\nmask.sum(0)\n\nTA       210\nSW_IN    195\nVPD      210\ndtype: int64\n\n\n\npipe = Pipeline([MeteoImpDf2Tensor, MeteoImpNormalize(*get_stats(df), *get_stats(control)), ToTuple])\n\n\npipe[1]\n\n\n  \n    MeteoImpNormalize -- {'mean_data': tensor([ 18.2214, 355.5345,  13.8018], dtype=torch.float64), 'std_data': tensor([  7.3713, 336.5250,   8.4760], dtype=torch.float64), 'mean_control': tensor([ 16.4072, 325.8798,  10.0621,  16.3431, 324.6188,   9.9632],\n       dtype=torch.float64), 'std_control': tensor([  6.5264, 307.3572,   7.4125,   6.4755, 307.5501,   7.2901],\n       dtype=torch.float64)}\n  \n  (MeteoImpTensor,object) -> encodes\n\n  (MeteoImpTensor,object) -> decodes\n(NormalsParams,object) -> decodes\n\n\n\n\n\ndata = MeteoImpDf(df, mask, control)\n\n\ninput, targ = pipe(data)\n\ninput = input[0].unsqueeze(0), input[1].unsqueeze(0), input[2].unsqueeze(0), \n\ntarg = targ[0].unsqueeze(0), targ[1].unsqueeze(0), targ[2].unsqueeze(0), \n\n\n# input[0][~input[1]] = torch.nan \n\n\nk = KalmanFilter.init_local_slope_pca(3,3, df)\n\n\nk\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.0000\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n1.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\nx_4\n0.0000\n\n\nx_5\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.0050\n0.6587\n-0.7524\n0.0000\n0.0000\n0.0000\n\n\ny_1\n1.0000\n-0.0083\n-0.0006\n0.0000\n0.0000\n0.0000\n\n\ny_2\n0.0066\n0.7524\n0.6587\n0.0000\n0.0000\n0.0000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0050\n-1.0000\n-0.0066\n0.0050\n1.0000\n0.0066\n\n\nx_1\n-0.6587\n0.0083\n-0.7524\n0.6587\n-0.0083\n0.7524\n\n\nx_2\n0.7524\n0.0006\n-0.6587\n-0.7524\n-0.0006\n0.6587\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\nx_4\n0.0000\n\n\nx_5\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n3.0000\n\n\n\n\n\n\n\npred = k(input)\n\n\nloss_f = KalmanLoss(only_gap=True)\nloss_f_all = KalmanLoss(only_gap=False)\n\n\nloss_f(pred, targ) \n\ntensor(15.7478, dtype=torch.float64, grad_fn=<MeanBackward0>)\n\n\n\npipe1 = Pipeline(pipe.fs[-2,-1])\n\n\npipe1\n\nPipeline: MeteoImpNormalize -- {'mean_data': tensor([ 18.2214, 355.5345,  13.8018], dtype=torch.float64), 'std_data': tensor([  7.3713, 336.5250,   8.4760], dtype=torch.float64), 'mean_control': tensor([ 16.4072, 325.8798,  10.0621,  16.3431, 324.6188,   9.9632],\n       dtype=torch.float64), 'std_control': tensor([  6.5264, 307.3572,   7.4125,   6.4755, 307.5501,   7.2901],\n       dtype=torch.float64)} -> ToTuple\n\n\n\ndef plot_pred(pred, data, control=True):\n    pred_dec = preds2df([pipe.decode(pred)], [data])[0]\n\n    data_pred = pd.merge(data.tidy(hai_control), pred_dec.tidy(), on=[\"time\", \"variable\"])\n\n    # title = [f\"loss: {loss.item():.6f}\"] + [format_metric(name, val) for name, val in metrics.items()]\n    return plot_variable(data_pred, variable=\"SW_IN\", ys=[\"value\", \"mean\", \"control\"], error=True, control=control)\n\n\nplot_pred(pred, data)\n\n\n\n\n\n\n\nfrom tqdm.auto import tqdm\n\n\ndef train(k, data_gen, loss_f, n_iter, lr, control=True):\n    k.train()\n    data, input, targ = data_gen\n    optimizer = torch.optim.Adam(k.parameters(), lr=lr) \n\n    t_info = pd.DataFrame(columns=['loss', 'rmse_gap', 'rmse', 'plot'])\n    pd.DataFrame({k: pd.Series(dtype=t) for k, t in [('loss', float), ('rmse_gap', float), ('rmse', float), ('plot', object)]})\n    for i in tqdm(range(n_iter)):\n        # Zero gradients from previous iteration\n        optimizer.zero_grad()\n        # Output from model\n        pred= k(input)\n        loss = loss_f(pred, targ)\n        t_info.loc[i, 'loss'] = loss_f(pred, targ).item()\n        t_info.loc[i, 'loss_all'] = loss_f_all(pred, targ).item()\n        t_info.loc[i, 'rmse_gap'] = rmse_gap(pred, targ)\n        t_info.loc[i, 'rmse'] = rmse_mask(pred, targ)\n        t_info.loc[i, 'plot'] = plot_pred(pred, data, control)\n        # backpropagate gradients\n        loss.backward()\n        optimizer.step()\n    return t_info\n\n\ndef data_gen_one_gap(df, g_inter, df_control, device='cpu', all_gap=False):\n    control = _add_lags_df(df_control.loc[df.index], 1)\n    mask = df.astype(dtype=bool)\n    mask.iloc[:,:] = True\n    col_sel = [0,1,2] if all_gap else 1\n    mask.iloc[g_inter[0]:g_inter[1], col_sel] = False \n    pipe = Pipeline([MeteoImpDf2Tensor, MeteoImpNormalize(*get_stats(df), *get_stats(control)), ToTuple])\n    data = MeteoImpDf(df, mask, control)\n    input, targ = pipe(data)\n    input = input[0].unsqueeze(0).to(device), input[1].unsqueeze(0).to(device), input[2].unsqueeze(0).to(device), \n    targ = targ[0].unsqueeze(0).to(device), targ[1].unsqueeze(0).to(device), targ[2].unsqueeze(0).to(device), \n    return data, input, targ\n\n\ndef plot_train_info(t_info):\n    ax = get_grid(4,2,2, figsize=(8,6))\n\n    t_info.loss.plot(ax=ax[0], title=\"loss gap\")\n    t_info.loss_all.plot(ax=ax[1], title=\"loss all\")\n    t_info.rmse.plot(ax=ax[2], title=\"rmse\")\n    t_info.rmse_gap.plot(ax=ax[3], title=\"rmse gap\")\n    \n    InteractiveSequence(t_info['plot'], start=len(t_info)-1)()\n\n\n\n\ndata_gen0 = data_gen_one_gap(hai[8_110-80:8_160+80], (95, 120), hai_era )\n\n\nk0 = KalmanFilter.init_local_slope_pca(3,3, None)\nt_info0 = train(k0, data_gen0, loss_f, 4t0, 1e-2)\nplot_train_info(t_info0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk = KalmanFilter.init_local_slope_pca(3,3, None)\nt_info = train(k, (data, input, targ), loss_f, 40, 1e-2)\n\n\n\n\n\nax = get_grid(4,2,2, figsize=(8,6))\n\nt_info.loss.plot(ax=ax[0], title=\"loss gap\")\nt_info.loss_all.plot(ax=ax[1], title=\"loss all\")\nt_info.rmse.plot(ax=ax[2], title=\"rmse\")\nt_info.rmse_gap.plot(ax=ax[3], title=\"rmse gap\")\n\n<AxesSubplot: title={'center': 'rmse gap'}>\n\n\n\n\n\n\nInteractiveSequence(t_info['plot'])()\n\n\n\n\n\n\n\n\n\n\n\nk = KalmanFilter.init_local_slope_pca(3,3, df)\nt_info_all = train(k, loss_f_all, 00)\n\nTypeError: train() missing 2 required positional arguments: 'n_iter' and 'lr'\n\n\n\nax = get_grid(4,2,2, figsize=(8,6))\n\nt_info_all.loss.plot(ax=ax[0], title=\"loss gap\")\nt_info_all.loss_all.plot(ax=ax[1], title=\"loss all\")\nt_info_all.rmse.plot(ax=ax[2], title=\"rmse\")\nt_info_all.rmse_gap.plot(ax=ax[3], title=\"rmse gap\")\n\n<AxesSubplot: title={'center': 'rmse gap'}>\n\n\n\n\n\n\nint_s = InteractiveSequence(t_info_all['plot'])()\n\n\n\n\n\n\n\n\nk\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.2977\n-0.0050\n0.0446\n0.6500\n-0.1277\n-0.0619\n\n\nx_1\n0.1847\n1.3479\n-0.1871\n-0.1503\n0.5598\n0.0960\n\n\nx_2\n0.1825\n-0.0310\n1.4431\n-0.0475\n0.0819\n0.5295\n\n\nx_3\n0.1796\n0.0517\n-0.0185\n1.2563\n-0.0411\n-0.1260\n\n\nx_4\n-0.1511\n0.0151\n-0.1012\n0.1421\n1.2565\n0.0956\n\n\nx_5\n-0.0133\n-0.0707\n0.0052\n0.1753\n-0.0608\n1.0677\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0347\n0.0011\n-0.0098\n0.0215\n0.0097\n0.0134\n\n\nx_1\n0.0011\n0.0050\n0.0025\n-0.0008\n0.0062\n0.0056\n\n\nx_2\n-0.0098\n0.0025\n0.0045\n-0.0078\n-0.0008\n-0.0010\n\n\nx_3\n0.0215\n-0.0008\n-0.0078\n0.0218\n0.0171\n0.0089\n\n\nx_4\n0.0097\n0.0062\n-0.0008\n0.0171\n0.0305\n0.0135\n\n\nx_5\n0.0134\n0.0056\n-0.0010\n0.0089\n0.0135\n0.0113\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.1064\n\n\nx_1\n0.0018\n\n\nx_2\n0.0035\n\n\nx_3\n-0.0239\n\n\nx_4\n-0.0242\n\n\nx_5\n0.0017\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n-0.0208\n0.3174\n-0.1853\n0.0874\n-0.0197\n-0.0855\n\n\ny_1\n0.5384\n0.0888\n0.0757\n0.1413\n-0.2172\n-0.0909\n\n\ny_2\n0.0512\n0.2923\n0.1433\n0.2092\n-0.0656\n-0.0948\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0036\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0037\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0037\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0290\n\n\ny_1\n-0.0349\n\n\ny_2\n-0.0331\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.2431\n-0.8326\n-0.0653\n-0.1411\n0.6699\n0.0356\n\n\nx_1\n-0.7021\n-0.1554\n-0.6750\n0.4507\n-0.0686\n0.7311\n\n\nx_2\n0.7760\n-0.0489\n-0.7530\n-0.6168\n-0.0417\n0.4382\n\n\nx_3\n0.0063\n-0.1731\n0.0473\n-0.0227\n-0.1416\n-0.0458\n\n\nx_4\n-0.0078\n0.0901\n0.0212\n0.0446\n0.1084\n0.0354\n\n\nx_5\n0.0482\n0.1171\n-0.0781\n0.0202\n0.1159\n-0.0167\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.1068\n\n\nx_1\n-0.6191\n\n\nx_2\n-0.1057\n\n\nx_3\n0.2509\n\n\nx_4\n0.5050\n\n\nx_5\n0.0913\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.2409\n-0.4041\n-0.8458\n0.8184\n0.5894\n0.5092\n\n\nx_1\n-0.4041\n4.9780\n-0.1700\n-0.4352\n-1.5693\n0.1166\n\n\nx_2\n-0.8458\n-0.1700\n1.9952\n0.7648\n0.3642\n0.9504\n\n\nx_3\n0.8184\n-0.4352\n0.7648\n2.2167\n0.2907\n0.0321\n\n\nx_4\n0.5894\n-1.5693\n0.3642\n0.2907\n4.1926\n-0.2442\n\n\nx_5\n0.5092\n0.1166\n0.9504\n0.0321\n-0.2442\n2.1119\n\n\n\n\n\n\n\n\n\n\ndata_gen = data_gen_one_gap(hai[8_110-80:8_160+80], (95, 120), hai_era )\n\n\nk2 = KalmanFilter.init_local_slope_pca(3,3, None)\nt_info2 = train(k2, data_gen, loss_f_all, 20, 1e-2)\nplot_train_info(t_info2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt_data_gen = data_gen_one_gap(hai[8_110-80:8_160+80], (120, 130), hai_era )\nplot_pred(k2(t_data_gen[1]), t_data_gen[0])\n\n\n\n\n\n\n\nk2\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1488\n0.1318\n0.0117\n0.8294\n0.0579\n0.0394\n\n\nx_1\n-0.0694\n1.0401\n-0.0690\n-0.0152\n0.8974\n0.0485\n\n\nx_2\n-0.0035\n0.1165\n1.1399\n0.0882\n0.1157\n0.8688\n\n\nx_3\n0.0019\n-0.0930\n0.0165\n1.1456\n0.0597\n0.0641\n\n\nx_4\n-0.0392\n0.0136\n-0.0292\n-0.0728\n1.0049\n-0.0712\n\n\nx_5\n0.0152\n-0.1065\n-0.0113\n0.0561\n0.0680\n1.1262\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0435\n0.0198\n0.0202\n0.0338\n-0.0226\n0.0008\n\n\nx_1\n0.0198\n0.0564\n0.0327\n0.0119\n0.0281\n0.0555\n\n\nx_2\n0.0202\n0.0327\n0.0351\n-0.0086\n-0.0188\n0.0315\n\n\nx_3\n0.0338\n0.0119\n-0.0086\n0.0669\n0.0237\n-0.0106\n\n\nx_4\n-0.0226\n0.0281\n-0.0188\n0.0237\n0.0984\n0.0362\n\n\nx_5\n0.0008\n0.0555\n0.0315\n-0.0106\n0.0362\n0.0661\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0059\n\n\nx_1\n-0.0184\n\n\nx_2\n-0.0014\n\n\nx_3\n0.0074\n\n\nx_4\n-0.0052\n\n\nx_5\n0.0015\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7973\n0.1772\n0.1269\n0.0267\n0.0122\n0.1052\n\n\ny_1\n0.1159\n0.8294\n0.1053\n-0.0063\n0.0176\n0.0040\n\n\ny_2\n0.1204\n0.1186\n0.7994\n0.0530\n-0.0154\n0.0423\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0082\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0082\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0082\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0071\n\n\ny_1\n0.0203\n\n\ny_2\n0.0114\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9610\n-0.0213\n0.0496\n1.0203\n0.0002\n0.0263\n\n\nx_1\n-0.0594\n-0.9830\n-0.0566\n-0.0564\n0.9417\n-0.0531\n\n\nx_2\n0.0428\n-0.0166\n-0.9357\n0.0211\n0.0120\n1.0258\n\n\nx_3\n-0.0010\n0.0289\n0.0187\n0.0179\n0.0074\n0.0176\n\n\nx_4\n-0.0422\n-0.0518\n-0.0445\n-0.0411\n-0.0503\n-0.0433\n\n\nx_5\n0.0126\n0.0244\n-0.0080\n0.0115\n0.0224\n0.0109\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0285\n\n\nx_1\n-0.2098\n\n\nx_2\n0.1249\n\n\nx_3\n0.1098\n\n\nx_4\n0.2097\n\n\nx_5\n-0.1615\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.4789\n0.5056\n-0.0939\n0.5551\n-0.5125\n0.3705\n\n\nx_1\n0.5056\n2.5263\n0.5559\n-0.5701\n0.4644\n-0.5116\n\n\nx_2\n-0.0939\n0.5559\n2.4719\n-0.4477\n-0.5614\n0.5220\n\n\nx_3\n0.5551\n-0.5701\n-0.4477\n2.4713\n0.5625\n0.5205\n\n\nx_4\n-0.5125\n0.4644\n-0.5614\n0.5625\n2.5478\n0.5138\n\n\nx_5\n0.3705\n-0.5116\n0.5220\n0.5205\n0.5138\n2.4855\n\n\n\n\n\n\n\nk2 = KalmanFilter.init_local_slope_pca(3,3, None)\nt_info2 = train(k2, data_gen, loss_f, 20, 1e-2)\nplot_train_info(t_info2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_gen3 = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era )\n\n\nk3 = KalmanFilter.init_local_slope_pca(3,3, None)\ntrain(k3, data_gen3, loss_f_all, 5, 1e-2) # get to decent parameters\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info3 = train(k3, data_gen3, loss_f_all, 40, 1e-2)\n\n\n\n\n\n\n\n\nplot_train_info(t_info3)\n\n\n\n\n\n\n\n\n\n\n\nt_info3.iloc[-2:-1]\n\n\n\n\n\nloss\nrmse_gap\nrmse\nplot\nloss_all\n\n\n\n\n38\n-123.996457\n0.13724\n0.082401\nalt.LayerChart(...)\n-123.996457\n\n\n\n\n\n\nk3\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1488\n0.1318\n0.0117\n0.8294\n0.0579\n0.0394\n\n\nx_1\n-0.0694\n1.0401\n-0.0690\n-0.0152\n0.8974\n0.0485\n\n\nx_2\n-0.0035\n0.1165\n1.1399\n0.0882\n0.1157\n0.8688\n\n\nx_3\n0.0019\n-0.0930\n0.0165\n1.1456\n0.0597\n0.0641\n\n\nx_4\n-0.0392\n0.0136\n-0.0292\n-0.0728\n1.0049\n-0.0712\n\n\nx_5\n0.0152\n-0.1065\n-0.0113\n0.0561\n0.0680\n1.1262\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0435\n0.0198\n0.0202\n0.0338\n-0.0226\n0.0008\n\n\nx_1\n0.0198\n0.0564\n0.0327\n0.0119\n0.0281\n0.0555\n\n\nx_2\n0.0202\n0.0327\n0.0351\n-0.0086\n-0.0188\n0.0315\n\n\nx_3\n0.0338\n0.0119\n-0.0086\n0.0669\n0.0237\n-0.0106\n\n\nx_4\n-0.0226\n0.0281\n-0.0188\n0.0237\n0.0984\n0.0362\n\n\nx_5\n0.0008\n0.0555\n0.0315\n-0.0106\n0.0362\n0.0661\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0059\n\n\nx_1\n-0.0184\n\n\nx_2\n-0.0014\n\n\nx_3\n0.0074\n\n\nx_4\n-0.0052\n\n\nx_5\n0.0015\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7973\n0.1772\n0.1269\n0.0267\n0.0122\n0.1052\n\n\ny_1\n0.1159\n0.8294\n0.1053\n-0.0063\n0.0176\n0.0040\n\n\ny_2\n0.1204\n0.1186\n0.7994\n0.0530\n-0.0154\n0.0423\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0082\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0082\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0082\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0071\n\n\ny_1\n0.0203\n\n\ny_2\n0.0114\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9610\n-0.0213\n0.0496\n1.0203\n0.0002\n0.0263\n\n\nx_1\n-0.0594\n-0.9830\n-0.0566\n-0.0564\n0.9417\n-0.0531\n\n\nx_2\n0.0428\n-0.0166\n-0.9357\n0.0211\n0.0120\n1.0258\n\n\nx_3\n-0.0010\n0.0289\n0.0187\n0.0179\n0.0074\n0.0176\n\n\nx_4\n-0.0422\n-0.0518\n-0.0445\n-0.0411\n-0.0503\n-0.0433\n\n\nx_5\n0.0126\n0.0244\n-0.0080\n0.0115\n0.0224\n0.0109\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0285\n\n\nx_1\n-0.2098\n\n\nx_2\n0.1249\n\n\nx_3\n0.1098\n\n\nx_4\n0.2097\n\n\nx_5\n-0.1615\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.4789\n0.5056\n-0.0939\n0.5551\n-0.5125\n0.3705\n\n\nx_1\n0.5056\n2.5263\n0.5559\n-0.5701\n0.4644\n-0.5116\n\n\nx_2\n-0.0939\n0.5559\n2.4719\n-0.4477\n-0.5614\n0.5220\n\n\nx_3\n0.5551\n-0.5701\n-0.4477\n2.4713\n0.5625\n0.5205\n\n\nx_4\n-0.5125\n0.4644\n-0.5614\n0.5625\n2.5478\n0.5138\n\n\nx_5\n0.3705\n-0.5116\n0.5220\n0.5205\n0.5138\n2.4855\n\n\n\n\n\n\n\nk0.trans_cov, k3.trans_cov\n\n(tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000]], dtype=torch.float64,\n        grad_fn=<AddBackward0>),\n tensor([[ 0.0078,  0.0039,  0.0089,  0.0073, -0.0016, -0.0008],\n         [ 0.0039,  0.0145,  0.0031,  0.0098, -0.0030,  0.0138],\n         [ 0.0089,  0.0031,  0.0131,  0.0071,  0.0011, -0.0003],\n         [ 0.0073,  0.0098,  0.0071,  0.0125, -0.0059,  0.0101],\n         [-0.0016, -0.0030,  0.0011, -0.0059,  0.0163, -0.0110],\n         [-0.0008,  0.0138, -0.0003,  0.0101, -0.0110,  0.0318]],\n        dtype=torch.float64, grad_fn=<AddBackward0>))\n\n\n\nk0.trans_matrix, k3.trans_matrix\n\n(Parameter containing:\n tensor([[1., 0., 0., 1., 0., 0.],\n         [0., 1., 0., 0., 1., 0.],\n         [0., 0., 1., 0., 0., 1.],\n         [0., 0., 0., 1., 0., 0.],\n         [0., 0., 0., 0., 1., 0.],\n         [0., 0., 0., 0., 0., 1.]], dtype=torch.float64, requires_grad=True),\n Parameter containing:\n tensor([[ 1.0086,  0.0641, -0.0050,  1.0275, -0.0139,  0.0290],\n         [-0.0564,  1.0605, -0.0554,  0.0049,  1.0635, -0.0571],\n         [ 0.0377,  0.0598,  1.0373,  0.0907, -0.0394,  1.0221],\n         [ 0.0103,  0.1001,  0.0120,  0.9910,  0.0585,  0.0319],\n         [ 0.0513,  0.0618,  0.0555, -0.0553,  0.9462, -0.0534],\n         [-0.0606,  0.0597, -0.0501,  0.0845,  0.0556,  1.1050]],\n        dtype=torch.float64, requires_grad=True))\n\n\n\nk0 = KalmanFilter.init_local_slope_pca(3,3, None)\ndef par_diff(k0, k1):\n    out = {}\n    for (n,p0), (_,p1) in zip(k0.named_parameters(), k1.named_parameters()):\n        out[n] = (p0-p1).abs().mean().detach().item()\n    return pd.DataFrame(out,index=[0])\n\n\npar_diff(k0, k3)\n\n\n\n\n\ntrans_matrix\ntrans_off\ntrans_cov_raw\ncontr_matrix\nobs_matrix\nobs_off\nobs_cov_raw\ninit_state_mean\ninit_state_cov_raw\n\n\n\n\n0\n0.080702\n0.013349\n0.079071\n0.047649\n0.108142\n0.073074\n0.457733\n0.159152\n0.249438\n\n\n\n\n\n\npar_diff(k2, k3)\n\n\n\n\n\ntrans_matrix\ntrans_off\ntrans_cov_raw\ncontr_matrix\nobs_matrix\nobs_off\nobs_cov_raw\ninit_state_mean\ninit_state_cov_raw\n\n\n\n\n0\n0.077905\n0.018252\n0.070294\n0.057594\n0.09894\n0.085995\n0.257732\n0.148817\n0.109384\n\n\n\n\n\n\npar_diff(k0, k2)\n\n\n\n\n\ntrans_matrix\ntrans_off\ntrans_cov_raw\ncontr_matrix\nobs_matrix\nobs_off\nobs_cov_raw\ninit_state_mean\ninit_state_cov_raw\n\n\n\n\n0\n0.069312\n0.00665\n0.087398\n0.030413\n0.090035\n0.012921\n0.200001\n0.140695\n0.183713\n\n\n\n\n\n\n\n\n\ndata_gen_gall = data_gen_one_gap(hai[8_110-80:8_160+80], (90, 130), hai_era, all_gap=True)\n\n\nk_gall = KalmanFilter.init_local_slope_pca(3,3)\n\n\nt_info_gall = train(k_gall, data_gen_gall, loss_f_all, 40, 1e-2)\n\n\n\n\nAssertionError: \n\n\n\nplot_train_info(t_info_gall)\n\n\n\n\n\ndata_gen4 = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era)\n\n\nk4 = KalmanFilter.init_random(3,3,6, dtype=torch.float64)\n\n\nt_info4 = train(k4, data_gen4, loss_f_all, 100, 1e-2)\n\n\n\n\n\nplot_train_info(t_info4)\n\n\n\n\n\n\n\n\n\n\n\nt_info4 = train(k4, data_gen4, loss_f_all, 100, 3e-2)\n\n\n\n\n\nplot_train_info(t_info4)\n\n\n\n\n\n\n\n\n\n\n\nk4\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.9905\n-0.0861\n0.0445\n\n\nx_1\n0.9582\n0.4306\n0.7853\n\n\nx_2\n0.7496\n0.2322\n0.7931\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.9549\n1.1126\n1.2843\n\n\nx_1\n1.1126\n0.6754\n0.6295\n\n\nx_2\n1.2843\n0.6295\n1.0946\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.5174\n\n\nx_1\n0.3126\n\n\nx_2\n0.7365\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.0578\n0.2861\n0.3425\n\n\ny_1\n0.6259\n0.2344\n-0.0193\n\n\ny_2\n-0.0358\n0.3612\n0.2777\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.3975\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.6716\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.5781\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.5712\n\n\ny_1\n0.4196\n\n\ny_2\n0.6178\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n0.1983\n0.7550\n0.2785\n0.7371\n1.0945\n0.3676\n\n\nx_1\n0.4880\n0.1959\n0.3758\n-0.1466\n0.0925\n0.0745\n\n\nx_2\n0.7912\n0.0917\n0.1315\n0.3220\n-0.0424\n0.1066\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.5681\n\n\nx_1\n0.0938\n\n\nx_2\n0.1264\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.2809\n0.8443\n-0.2254\n\n\nx_1\n0.8443\n3.5571\n2.0384\n\n\nx_2\n-0.2254\n2.0384\n1.6601\n\n\n\n\n\n\n\n\n\n\n\n\ndata_gen_sls = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era)\n\n\nk_sls = KalmanFilter.init_local_slope_pca(3,1, hai)\n\n\nt_info_sls = train(k_sls, data_gen_sls, loss_f_all, 70, 2e-2)\n\n\n\n\n\nplot_train_info(t_info_sls)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_gen5 = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era)\n\n\nk5 = KalmanFilter.init_random(3,1,6, dtype=torch.float64)\n\n\nt_info5 = train(k5, data_gen5, loss_f_all, 200, 2e-2)\n\n\n\n\n\nplot_train_info(t_info5)\n\n\n\n\n\n\n\n\n\n\n\nt_info5 = train(k5, data_gen5, loss_f_all, 200, 2e-2)\n\n\n\n\n\nplot_train_info(t_info5)\n\n\n\n\n\n\n\n\n\n\n\nt_info5 = train(k5, data_gen5, loss_f_all, 500, 2e-2)\n\n\n\n\n\nplot_train_info(t_info5)\n\n\n\n\n\n\n\n\n\n\n\nt_info5 = train(k5, data_gen4, loss_f_all, 100, 3e-2)\n\n\n\n\n\nplot_train_info(t_info5)\n\n\n\n\n\n\n\n\n\n\n\nk4\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.9905\n-0.0861\n0.0445\n\n\nx_1\n0.9582\n0.4306\n0.7853\n\n\nx_2\n0.7496\n0.2322\n0.7931\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.9549\n1.1126\n1.2843\n\n\nx_1\n1.1126\n0.6754\n0.6295\n\n\nx_2\n1.2843\n0.6295\n1.0946\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.5174\n\n\nx_1\n0.3126\n\n\nx_2\n0.7365\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.0578\n0.2861\n0.3425\n\n\ny_1\n0.6259\n0.2344\n-0.0193\n\n\ny_2\n-0.0358\n0.3612\n0.2777\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.3975\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.6716\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.5781\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.5712\n\n\ny_1\n0.4196\n\n\ny_2\n0.6178\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n0.1983\n0.7550\n0.2785\n0.7371\n1.0945\n0.3676\n\n\nx_1\n0.4880\n0.1959\n0.3758\n-0.1466\n0.0925\n0.0745\n\n\nx_2\n0.7912\n0.0917\n0.1315\n0.3220\n-0.0424\n0.1066\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.5681\n\n\nx_1\n0.0938\n\n\nx_2\n0.1264\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.2809\n0.8443\n-0.2254\n\n\nx_1\n0.8443\n3.5571\n2.0384\n\n\nx_2\n-0.2254\n2.0384\n1.6601\n\n\n\n\n\n\n\n\n\n\n\ndata_gen6 = data_gen_one_gap(hai[8_110-80:8_160+80], (95, 120), hai_era )\n\n\nk6 = KalmanFilter.init_local_slope_pca(3,3, None)\nk6.use_smooth = False\n# train(k6, data_gen6, loss_f_all, 5, 1e-2) # get to decent parameters\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info6 = train(k6, data_gen5, loss_f_all, 25, 1e-2)\n\n\n\n\n\nplot_train_info(t_info6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_gen_nc = data_gen_one_gap(hai[8_110-80:8_160+80], (80, 150), hai_era.sample(frac=1).reset_index(drop=True).set_index(hai_era.index) )\n\n/tmp/ipykernel_55400/2861280523.py:5: FutureWarning: Slicing a positional slice with .loc is not supported, and will raise TypeError in a future version.  Use .loc with labels or .iloc with positions instead.\n  mask.loc[g_inter[0]:g_inter[1], gap_var] = False\n\n\n\nk_nc = KalmanFilter.init_local_slope_pca(3,3, None, use_control=False)\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info_nc = train(k_nc, data_gen_nc, loss_f_all, 20, 1e-2)\n\n\n\n\n\nplot_train_info(t_info_nc)\n\n\n\n\n\n\n\n\n\n\n\ndata, input, targ = data_gen_nc\n\n\nt_data_g = data_gen_one_gap(hai[8_110-80:8_160+80], (120, 200), hai_era) \n\n\nplot_pred(k_nc(t_data_g[1]), t_data_g[0])\n\n\n\n\n\n\n\n\n\n\ndata_gen_nc2 = data_gen_one_gap(hai[8_110-80:8_160+80], (95, 105),\n                               hai_era,#.sample(frac=1).reset_index(drop=True).set_index(hai_era.index),\n                              all_gap=True)\n\n\nk_nc2 = KalmanFilter.init_local_slope_pca(3,3, None, use_control=False)\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info_nc2 = train(k_nc2, data_gen_nc2, loss_f_all, 20, 1e-2)\n\n\n\n\n\nplot_train_info(t_info_nc2)\n\n\n\n\n\n\n\n\n\n\n\ndata_gen_nc3 = data_gen_one_gap(hai[8_110-80:8_160+80], (90, 130),\n                               hai_era.sample(frac=1).reset_index(drop=True).set_index(hai_era.index),\n                              all_gap=True)\n\n\nk_nc3 = KalmanFilter.init_local_slope_pca(3,3,None, use_control=False)\n# k3.trans_cov = k3.trans_cov * 0.01\nt_info_nc3 = train(k_nc3, data_gen_nc3, loss_f_all, 20, 1e-2, control=False)\n\n\n\n\n\nplot_train_info(t_info_nc3)"
  },
  {
    "objectID": "kalman/Process_noise.html",
    "href": "kalman/Process_noise.html",
    "title": "Process Noise estimation",
    "section": "",
    "text": "Process Noise estimation\n\nfor a local level model using data from Hainich\n\n\nfrom meteo_imp.data import hai\n\n\nta_diff = (hai.TA - hai.TA.shift(-1))\nta_diff.hist(bins=20)\n\n<AxesSubplot: >\n\n\n\n\n\nthis is the difference between TA and the previous time step which is not too far from a normal distribution\n\nta_diff.std()\n\n0.12497036855024059\n\n\nso This should be the value of the Q in the kalman filter"
  },
  {
    "objectID": "kalman/Hainich_results_pres18_jan.html",
    "href": "kalman/Hainich_results_pres18_jan.html",
    "title": "Hainich with ERA-Interim",
    "section": "",
    "text": "Manual fine tuning learning process for presentation 18 Jan 2023\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import  *\nfrom meteo_imp.data import  _def_meteo_vars, units\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom pyprojroot import here\n\nfrom sklearn.decomposition import PCA\n\n\nbase_dir = here(\"analysis/presentations/plots_18_jan\")\nbase_dir.mkdir(exist_ok=True)\n\n\ndef save_plot(p, path):\n    f_name = base_dir / (path + \".vl.json\")\n    with open(f_name, 'w') as f:\n        f.write(p.to_json())\n    return f_name\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path64)\n\n\n\n\npc = PCA().fit(hai)\n\n\nd0 = hai.iloc[0:1]\n\n\ntr0 = pc.transform(d0)\n\n\ntr0\n\narray([[-121.11917652,   -7.06844313,    0.87975241]])\n\n\n\nt0 = np.random.randn(5,1) * 10\ntt = np.hstack([t0, 2 * t0 + np.random.randn(5,1), 3 * t0 + np.random.randn(5,1)])\n\n\ntt = np.vstack([tt, -tt])\n\n\ntt.mean(0)\n\narray([1.77635684e-16, 0.00000000e+00, 0.00000000e+00])\n\n\n\ndef pca(X):\n    # Data matrix X, assumes 0-centered\n    n, m = X.shape\n    assert np.allclose(X.mean(axis=0), np.zeros(m))\n    # Compute covariance matrix\n    C = np.dot(X.T, X) / (n-1)\n    # Eigen decomposition\n    eigen_vals, eigen_vecs = np.linalg.eig(C)\n    # Project X onto PC space\n    X_pca = np.dot(X, eigen_vecs)\n    return X_pca, eigen_vecs\n\n\npca(tt)\n\n(array([[-9.15947487e+00, -3.03773243e-01,  3.81626348e-01],\n        [ 2.00045215e+01, -1.99595583e-02,  1.53745860e+00],\n        [ 1.33744310e+01,  2.01395206e-02, -3.19819214e-02],\n        [-8.99057811e+00,  1.24208069e-01,  9.57988123e-01],\n        [ 3.14616875e+01, -4.88141095e-02, -5.79117681e-01],\n        [ 9.15947487e+00,  3.03773243e-01, -3.81626348e-01],\n        [-2.00045215e+01,  1.99595583e-02, -1.53745860e+00],\n        [-1.33744310e+01, -2.01395206e-02,  3.19819214e-02],\n        [ 8.99057811e+00, -1.24208069e-01, -9.57988123e-01],\n        [-3.14616875e+01,  4.88141095e-02,  5.79117681e-01]]),\n array([[-0.26545847, -0.95659856,  0.12021234],\n        [-0.53518386,  0.04249433, -0.84366608],\n        [-0.80194142,  0.28829401,  0.52323659]]))\n\n\n\nsk_pc = PCA(2).fit(tt)\n\n\ntt @ sk_pc.components_.T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\ntr = sk_pc.transform(tt)\ntr\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\nsk_pc.components_.T\n\narray([[-0.26545847,  0.12021234],\n       [-0.53518386, -0.84366608],\n       [-0.80194142,  0.52323659]])\n\n\n\n(sk_pc.components_ @ tt.T).T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\ntt[0, None]\n\narray([[2.76792539, 4.56712931, 7.45746711]])\n\n\n\nsk_pc.components_.shape\n\n(3, 3)\n\n\n\ntt[0].shape\n\n(3,)\n\n\n\n(sk_pc.components_ @ tt[0])\n\narray([-9.15947487,  0.38162635])\n\n\n\ntt[0]\n\narray([2.76792539, 4.56712931, 7.45746711])\n\n\n\nsk_pc.components_.T @ tr[0]\n\narray([2.47733634, 4.58003795, 7.54504311])\n\n\n\nsk_pc.inverse_transform(tr[0])\n\narray([2.47733634, 4.58003795, 7.54504311])\n\n\n\nd0.to_numpy()\n\narray([[-0.6  ,  0.   ,  0.222]])\n\n\n\nhai.iloc[0]\n\nTA      -0.600\nSW_IN    0.000\nVPD      0.222\nName: 2000-01-01 00:30:00, dtype: float64\n\n\n\\[ x = y\\Lambda \\]\n\npc.components_\n\narray([[ 0.01681572,  0.99979324,  0.01143269],\n       [ 0.93010891, -0.01983747,  0.36674772],\n       [-0.36689868, -0.00446652,  0.93025018]])\n\n\n\nnp.linalg.inv(pc.components_)\n\narray([[ 0.01681572,  0.93010891, -0.36689868],\n       [ 0.99979324, -0.01983747, -0.00446652],\n       [ 0.01143269,  0.36674772,  0.93025018]])\n\n\n\n\n\nassuming that the control has the same dimensions of the observations then if we are doing a local slope model we have \\(B \\in \\mathbb{R}^{state \\times contr}\\): \\[ B = \\begin{bmatrix} -I & I \\\\ 0 & 0 \\end{bmatrix}\\]\n\nfrom torch import hstack, eye, vstack, ones, zeros, tensor\nfrom functools import partial\n\n\ndef set_dtype(*args, dtype=torch.float64):\n    return [partial(arg, dtype=dtype) for arg in args] \n\n\neye, ones, zeros, tensor = set_dtype(eye, ones, zeros, tensor)\n\n\ndef init_smart(n_dim_obs, n_dim_state, df, pca=True):\n    # n_dim_obs == n_dim_contr\n    if pca:\n        comp = PCA(n_dim_state).fit(df).components_\n        obs_matrix = tensor(comp.T) # transform state -> obs\n        contr_matrix = tensor(comp) # transform obs -> state\n    else:\n        obs_matrix, contr_matrix = eye(n_dim_obs), eye(n_dim_obs)\n        \n    return KalmanFilter(\n        trans_matrix =     vstack([hstack([eye(n_dim_state),                eye(n_dim_state)]),\n                                   hstack([zeros(n_dim_state, n_dim_state), eye(n_dim_state)])]),\n        trans_off =        zeros(n_dim_state * 2),        \n        trans_cov =        eye(n_dim_state * 2)*.1,        \n        obs_matrix =       hstack([obs_matrix, zeros(n_dim_obs, n_dim_state)]),\n        obs_off =          zeros(n_dim_obs),          \n        obs_cov =          eye(n_dim_obs)*.01,            \n        contr_matrix =     vstack([hstack([-contr_matrix,                  contr_matrix]),\n                                   hstack([ zeros(n_dim_state,n_dim_obs), zeros(n_dim_state, n_dim_obs)])]),\n        init_state_mean =  zeros(n_dim_state * 2),        \n        init_state_cov =   eye(n_dim_state * 2) * 3,\n    ) \n\n\nnp.hstack([np.eye(2), np.eye(2)])\n\narray([[1., 0., 1., 0.],\n       [0., 1., 0., 1.]])\n\n\n\ninit_smart(3,2, hai)\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n1.0000\n0.0000\n1.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n1.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\n\n\n\n\ny_0\n0.0168\n0.9301\n0.0000\n0.0000\n\n\ny_1\n0.9998\n-0.0198\n0.0000\n0.0000\n\n\ny_2\n0.0114\n0.3667\n0.0000\n0.0000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0168\n-0.9998\n-0.0114\n0.0168\n0.9998\n0.0114\n\n\nx_1\n-0.9301\n0.0198\n-0.3667\n0.9301\n-0.0198\n0.3667\n\n\nx_2\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\n\n\n\n\n\n\n\nclass PersistentRecorder(Callback):\n    order = 70\n    name = \"per_recorder\"\n    attrs = ['lrs', 'iters', 'losses', 'values']\n    def before_fit(self):\n        \"Prepare state for training\"\n        for attr in self.attrs:\n            if not hasattr(self.per_recorder, attr): setattr(self.per_recorder, attr, [])\n\n    def after_batch(self):\n        for attr in self.attrs:\n            setattr(self.per_recorder, attr, getattr(self.recorder, attr))\n\n\nmodels = []\n\n\ndls = imp_dataloader(hai, hai_era, var_sel = ['TA', 'SW_IN', 'VPD'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nlen(dls.valid.items)\nitems = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n\n\ndef train_show_save(learn, n_iter, lr):\n    learn.fit(n_iter, lr)\n    models.append(learn.model.state_dict().copy())\n    learn.recorder.plot_loss()\n    items = [learn.dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n    return show_results(learn, items = items, control=hai_control)\n\n\n\n\n\nmodel = init_smart(3,3, hai, pca=False).cuda()\nmodel.var_names = _def_meteo_vars.values()\n\n\nloss = loss_func=KalmanLoss(only_gap=False)\nlearn = Learner(dls, model, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\nlearn.model.use_smooth = True\n\n\nshow_results(learn, items=items, control=hai_control)\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-87.370314\n-96.212915\n0.069196\n0.210351\n0.946861\n-250571814513532732431918956544.000000\n02:37\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1489\n0.0042\n0.0120\n0.8404\n-0.0794\n-0.1127\n\n\nx_1\n-0.0954\n1.1464\n-0.0546\n0.1177\n0.8560\n0.0599\n\n\nx_2\n-0.0225\n0.0420\n1.1357\n-0.0717\n-0.1190\n0.8475\n\n\nx_3\n0.1201\n0.0661\n0.1170\n1.1840\n-0.1150\n-0.1073\n\n\nx_4\n0.0770\n0.1299\n0.0068\n-0.1847\n1.0987\n-0.1500\n\n\nx_5\n0.1084\n0.0358\n0.1133\n-0.1809\n-0.0281\n1.1533\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0341\n0.0493\n0.0361\n-0.0008\n-0.0312\n0.0026\n\n\nx_1\n0.0493\n0.1125\n0.0531\n0.0475\n0.0047\n0.0509\n\n\nx_2\n0.0361\n0.0531\n0.0399\n0.0001\n-0.0327\n0.0058\n\n\nx_3\n-0.0008\n0.0475\n0.0001\n0.0608\n0.0614\n0.0585\n\n\nx_4\n-0.0312\n0.0047\n-0.0327\n0.0614\n0.0901\n0.0554\n\n\nx_5\n0.0026\n0.0509\n0.0058\n0.0585\n0.0554\n0.0594\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0008\n\n\nx_1\n-0.0083\n\n\nx_2\n0.0067\n\n\nx_3\n0.0023\n\n\nx_4\n-0.0007\n\n\nx_5\n0.0005\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.8238\n-0.1897\n-0.1014\n-0.0841\n0.1914\n0.1286\n\n\ny_1\n0.0814\n0.8353\n0.1048\n0.1040\n0.0127\n0.0689\n\n\ny_2\n-0.0742\n-0.1502\n0.8296\n0.1282\n0.1811\n-0.0746\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0084\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0084\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0084\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0017\n\n\ny_1\n-0.0046\n\n\ny_2\n0.0026\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0290\n-0.0485\n-0.0022\n0.9297\n0.0792\n-0.0247\n\n\nx_1\n-0.0267\n-0.8778\n-0.0233\n0.0113\n0.9099\n0.0261\n\n\nx_2\n-0.0018\n-0.0592\n-1.0044\n-0.0310\n0.0674\n0.9204\n\n\nx_3\n-0.0503\n-0.0312\n-0.0556\n-0.0440\n-0.0529\n-0.0587\n\n\nx_4\n-0.0375\n-0.0537\n-0.0246\n-0.0412\n-0.0352\n-0.0338\n\n\nx_5\n-0.0653\n-0.0179\n-0.0575\n-0.0655\n-0.0307\n-0.0449\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0393\n\n\nx_1\n0.0044\n\n\nx_2\n-0.0092\n\n\nx_3\n0.0207\n\n\nx_4\n-0.0094\n\n\nx_5\n-0.0021\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.5500\n-0.1402\n-0.0082\n0.5233\n0.3011\n0.0882\n\n\nx_1\n-0.1402\n2.4578\n0.1502\n0.0355\n0.5714\n-0.0459\n\n\nx_2\n-0.0082\n0.1502\n2.5045\n0.2739\n0.0634\n0.5195\n\n\nx_3\n0.5233\n0.0355\n0.2739\n2.3814\n-0.2201\n-0.1842\n\n\nx_4\n0.3011\n0.5714\n0.0634\n-0.2201\n2.3880\n-0.1162\n\n\nx_5\n0.0882\n-0.0459\n0.5195\n-0.1842\n-0.1162\n2.3874\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-108.023250\n-113.113034\n0.073622\n0.198386\n0.956828\n-105578612000578019271909572608.000000\n02:37\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1440\n-0.0057\n0.0107\n0.8262\n-0.0845\n-0.1205\n\n\nx_1\n-0.0534\n1.2089\n-0.0223\n0.0948\n0.7937\n0.0494\n\n\nx_2\n-0.0289\n0.0176\n1.1417\n-0.0702\n-0.1154\n0.8351\n\n\nx_3\n0.1277\n0.0589\n0.1074\n1.1787\n-0.1203\n-0.1024\n\n\nx_4\n0.0654\n0.1560\n0.0043\n-0.1781\n1.1087\n-0.1415\n\n\nx_5\n0.0959\n0.0439\n0.1197\n-0.1725\n-0.0340\n1.1496\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0207\n0.0254\n0.0210\n-0.0052\n-0.0232\n-0.0033\n\n\nx_1\n0.0254\n0.0548\n0.0255\n0.0259\n0.0034\n0.0281\n\n\nx_2\n0.0210\n0.0255\n0.0222\n-0.0056\n-0.0241\n-0.0023\n\n\nx_3\n-0.0052\n0.0259\n-0.0056\n0.0473\n0.0504\n0.0465\n\n\nx_4\n-0.0232\n0.0034\n-0.0241\n0.0504\n0.0695\n0.0473\n\n\nx_5\n-0.0033\n0.0281\n-0.0023\n0.0465\n0.0473\n0.0477\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0004\n\n\nx_1\n-0.0055\n\n\nx_2\n0.0048\n\n\nx_3\n0.0027\n\n\nx_4\n-0.0033\n\n\nx_5\n0.0021\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7902\n-0.2032\n-0.1217\n-0.0986\n0.2164\n0.1090\n\n\ny_1\n0.0160\n0.7417\n0.0555\n0.1016\n0.1041\n0.0374\n\n\ny_2\n-0.1103\n-0.1594\n0.7916\n0.1185\n0.2096\n-0.0949\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0077\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0077\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0077\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0009\n\n\ny_1\n-0.0053\n\n\ny_2\n-0.0021\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0277\n-0.0595\n-0.0019\n0.9181\n0.0838\n-0.0194\n\n\nx_1\n-0.0414\n-0.8661\n-0.0323\n0.0110\n0.8741\n0.0262\n\n\nx_2\n-0.0029\n-0.0647\n-1.0045\n-0.0319\n0.0757\n0.8957\n\n\nx_3\n-0.0678\n-0.0368\n-0.0564\n-0.0559\n-0.0610\n-0.0639\n\n\nx_4\n-0.0374\n-0.0690\n-0.0209\n-0.0485\n-0.0406\n-0.0354\n\n\nx_5\n-0.0591\n-0.0267\n-0.0753\n-0.0633\n-0.0407\n-0.0555\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0396\n\n\nx_1\n-0.0009\n\n\nx_2\n-0.0049\n\n\nx_3\n0.0153\n\n\nx_4\n-0.0139\n\n\nx_5\n-0.0094\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.6364\n-0.1456\n-0.0020\n0.5695\n0.3986\n0.1628\n\n\nx_1\n-0.1456\n2.3292\n0.1064\n0.2074\n0.7591\n0.1475\n\n\nx_2\n-0.0020\n0.1064\n2.4205\n0.3489\n0.1488\n0.6143\n\n\nx_3\n0.5695\n0.2074\n0.3489\n2.2351\n-0.4355\n-0.3277\n\n\nx_4\n0.3986\n0.7591\n0.1488\n-0.4355\n2.1625\n-0.3445\n\n\nx_5\n0.1628\n0.1475\n0.6143\n-0.3277\n-0.3445\n2.2518\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps\")\n\nPath('models/17_jan_all_gaps.pth')\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-149.288297\n-152.804403\n0.072760\n0.179190\n0.956827\n-61340364986143763995601928192.000000\n02:35\n\n\n1\n-160.099637\n-165.002097\n0.071263\n0.181697\n0.963587\n-48387608947773502947627892736.000000\n02:37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps_final\")\n\nPath('models/17_jan_all_gaps_final.pth')\n\n\n\n\n\nlearn.load(\"17_jan_all_gaps_final\");\n\n\np = show_results(learn, control=hai_control, items = [items[i] for i in [0,5,4]], show_metric=False, units=list(units.values()))\np\n\n\n\n\n\n\n\np0 = show_results(learn, control=hai_control, items = [items[i] for i in [0]], n_cols=2, show_metric=False, props={'width': 350, 'height': 250},  units=list(units.values()))\np0\n\n\n\n\n\n\n\np1 = show_results(learn, control=hai_control, items = [items[i] for i in [5]], n_cols=2, show_metric=False, props={'width': 350, 'height': 250}, units=list(units.values()))\np1\n\n\n\n\n\n\n\np2 = show_results(learn, control=hai_control, items = [items[i] for i in [4]], n_cols=2, show_metric=False, props={'width': 350, 'height': 250}, units=list(units.values()))\np2\n\n\n\n\n\n\n\nsave_plot(p0, \"results_gap_all_vars_g0\")\nsave_plot(p1, \"results_gap_all_vars_g1\")\nsave_plot(p2, \"results_gap_all_vars_g2\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/analysis/presentations/plots_18_jan/results_gap_all_vars_g2.vl.json')\n\n\n\ninteract_results(learn, hai, hai_era)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n<function meteo_imp.kalman.fastai.interact_results.<locals>._inner(gap_len, items_idx, control_lags, block_len, shift, **var_names)>\n\n\n\n\n\n\n\ndls2 = imp_dataloader(hai, hai_era, var_sel = ['TA'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nmodel2 = init_smart(3,3, hai, pca=True).cuda()\n\n\nmodel2\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.0000\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n1.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\nx_4\n0.0000\n\n\nx_5\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.0168\n0.9301\n-0.3669\n0.0000\n0.0000\n0.0000\n\n\ny_1\n0.9998\n-0.0198\n-0.0045\n0.0000\n0.0000\n0.0000\n\n\ny_2\n0.0114\n0.3667\n0.9303\n0.0000\n0.0000\n0.0000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0168\n-0.9998\n-0.0114\n0.0168\n0.9998\n0.0114\n\n\nx_1\n-0.9301\n0.0198\n-0.3667\n0.9301\n-0.0198\n0.3667\n\n\nx_2\n0.3669\n0.0045\n-0.9303\n-0.3669\n-0.0045\n0.9303\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\nx_4\n0.0000\n\n\nx_5\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_4\n0.0000\n0.0000\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_5\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n3.0000\n\n\n\n\n\n\n\nloss2 = loss_func=KalmanLoss(only_gap=False)\nlearn2 = Learner(dls2, model2, loss2, cbs=[Float64Callback], metrics=imp_metrics)\n\n\ntrain_show_save(learn2, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-60.562467\n-78.105121\n0.041354\n0.095727\n0.974146\n-10.220484\n03:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn2, 3, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-99.062288\n-107.225395\n0.043633\n0.077547\n0.955371\n-5.651017\n02:56\n\n\n1\n-115.215551\n-122.628456\n0.048706\n0.075058\n0.960905\n-4.600422\n02:52\n\n\n2\n-130.109601\n-137.303035\n0.054807\n0.069069\n0.966896\n-3.014253\n02:58\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn2.loss_func = KalmanLoss(only_gap=True)\n\n\ntrain_show_save(learn2, 1, .5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-2.358371\n-2.383616\n0.058387\n0.077012\n0.958465\n-4.192017\n01:52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn2, 1, 2e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-2.387042\n-2.445100\n0.057919\n0.073482\n0.956432\n-3.904405\n01:51"
  },
  {
    "objectID": "GPFA Hainich.html",
    "href": "GPFA Hainich.html",
    "title": "Single latent",
    "section": "",
    "text": "First analysis of the Hainich data using GPFA for filling the gaps\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n302.475\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n\n\n...\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n330.202\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n330.202\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n330.202\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n\n\n\n\n\n\n\n\n\ngpfa_data = GPFADataTest(hai).add_random_missing()\n\n\ngpfa_hai = GPFAImputation(gpfa_data.data, gpfa_data.tidy_df(complete=True, is_missing=True))\n\nTypeError: rand(): argument 'size' must be tuple of ints, but found element of type DataFrame at pos 2\n\n\n\ngpfa_hai\n\n\n%time imputed = gpfa_hai.impute()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nCPU times: user 5min 57s, sys: 485 ms, total: 5min 57s\nWall time: 5min 59s\n\n\n\nimputed\n\n\n\n\n\ntime\nvariable\nmean\nstd\n\n\n\n\n0\n0.0\nTA\n-0.600000\nNaN\n\n\n1\n2.0\nTA\n-0.580000\nNaN\n\n\n2\n3.0\nTA\n-0.510000\nNaN\n\n\n3\n4.0\nTA\n-0.490000\nNaN\n\n\n4\n11.0\nTA\n-0.230000\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n403\n189.0\nVPD\n0.826632\n0.252326\n\n\n404\n190.0\nVPD\n0.827371\n0.252322\n\n\n405\n192.0\nVPD\n1.213000\n0.000000\n\n\n406\n193.0\nVPD\n0.826446\n0.252319\n\n\n407\n197.0\nVPD\n0.820434\n0.252332\n\n\n\n\n\n\nhai_plot = gpfa_hai.plot_pred(units=units, properties =  {'height': 190 , 'width': 380})\n\nhai_plot.save(\"plots/plot_hai_winter_4_var_200_obs_random_gaps_row_20_value_10.vl.json\")\nhai_plot\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n#gpfa_hai.plot_pred(complete= gpfa_data.tidy_df(complete=True, is_missing=True) )\n\n\ngpfa_hai.rmse()\n\nAttributeError: module 'sklearn' has no attribute 'metrics'\n\n\n\ngpfa_hai.r2()\n\n\nlosses = pd.DataFrame(gpfa_hai.learner.losses.cpu().numpy(), columns=['loss'])\n\np = losses.plot()\nplt.savefig(here('analysis/plots/loss_plot_hai_winter_4_var_200_obs_random_gaps_row_20_value_10.png'))\np\n\nLambda parameter, the latent variable is very similar to the\n\ngpfa_hai.data.corr()\n\nNameError: name 'gpfa_hai' is not defined\n\n\n\ngpfa_hai.learner.model.covar_module.Lambda.detach()\n\nthis is the value of the length scale of the RBF latent kernel\n\ngpfa_hai.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\ngpfa_hai.learner.model.covar_module.psi.detach()\n\n\n\nThe low correlation between SW_IN and TA is likely due to cloud cover, which is hard to predict with a dialy cycle. Hence we are looking at summer days and there is a much better correlation\n\nhai_raw2 = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows= 7 * 30 * 24 * 2)\n\nNameError: name 'pd' is not defined\n\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai2 = (hai_raw2\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai2\n\n\nhai2[-800:-500].SW_IN.plot()\n\n\nhai2[-800:-500].corr()\n\n\ngpdata2 = GPFADataTest(hai2[-800:-500].copy()).add_random_missing()\n\n\ngp_imp2 = GPFAImputation(gpdata2.data, gpdata2.tidy_df(complete=True, is_missing=True))\n\n\n%time data_imp2 = gp_imp2.impute()\n\n\ngp_imp2.plot_pred(units=units)\n\n\ndata_imp2\n\n\ngp_imp2.rmse()\n\n\ngpdata2.data.corr()\n\n\ngp_imp2.learner.model.covar_module.Lambda.detach()\n\n\ngp_imp2.learner.model.covar_module.psi.detach()\n\nthis is the value of the length scale of the RBF latent kernel\n\ngp_imp2.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\n\n\ngpdata3 = GPFADataTest(hai2[-800:-500].loc[:, [\"TA\", \"SW_IN\"]].copy()).add_random_missing()\n\n\ngp_imp3 = GPFAImputation(gpdata3.data, gpdata3.tidy_df(complete=True, is_missing=True))\n\n\n%time data_imp3 = gp_imp3.impute()\n\n\ngp_imp3.plot_pred(units=units, bind_interaction=False)\n\n\ndata_imp3\n\n\ngpdata3.data.corr()\n\n\ngp_imp3.learner.model.covar_module.Lambda.detach()\n\nthis is the value of the length scale of the RBF latent kernel\n\ngp_imp3.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\ngp_imp3.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\n\n\n\n\nTrying to see how the model works with a continous gap of 10% the length of the dataset for all variables\n\ngpd_gap = GPFADataTest(hai).add_gap(20, variables = ['TA', 'SW_IN', 'LW_IN', 'VPD'])\n\n\ngp_gap = GPFAImputation(gpd_gap.data, gpd_gap.tidy_df(complete=True, is_missing=True))\n\n\ngp_gap\n\n\n%time gp_gap.impute()\n\n\ngap_plot= gp_gap.plot_pred(units=units, properties =  {'height': 190 , 'width': 380})\n\ngap_plot.save(here(\"analysis/plots\") /\" plot_hai_winter_4_var_200_obs_gap_20.vl.json\")\ngap_plot\n\n\nprint(gp_gap.rmse().to_markdown(index=False))\n\n\nprint(pd.DataFrame(gp_gap.learner.model.covar_module.Lambda.detach().numpy()).to_markdown(index=False))\n\n\npsi = pd.DataFrame(gp_gap.learner.model.covar_module.psi.detach().numpy())\npsi.insert(0, \"variable\", meteo_vars.values())\nprint(psi.to_markdown(index=False))\n\n\ngp_gap.learner.model.covar_module.latent_kernel.lengthscale.detach()\n\n\nlosses = pd.DataFrame(gp_gap.learner.losses.cpu().numpy(), columns=['loss'])\n\np = losses.plot()\nplt.savefig(here('analysis/plots/') /'loss_plot_hai_winter_4_var_200_obs_gap_20.png')\np"
  },
  {
    "objectID": "gap length variation.html",
    "href": "gap length variation.html",
    "title": "Gap Length Variation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\nfrom meteo_imp.results import _display_as_row \n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\nfrom itertools import combinations, repeat, zip_longest\n\nfrom ipywidgets import interact\nfrom tqdm.auto import tqdm\n\nfrom multiprocessing import Pool\n\nimport pickle\n\nModuleNotFoundError: No module named 'meteo_imp.gpfa.data_preparation'\n\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    #\"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nn_obs = 200\nn_latent = 1\ntotal_iter = 100\n\n\nmodel_save_dir = here() / \"analysis/trained_models\"\n\nmodel_path = model_save_dir / f\"GPFA_l_{n_latent}_train_{total_iter}_1ker_{n_obs}_obs.pickle\"\n\n\ndata = GPFADataTest(hai[:n_obs])\n\n\n# inspired from https://datagy.io/python-combinations-of-a-list/\ndef all_comb(l):\n    list_combinations = []\n    for n in range(1, len(l) + 1):\n        list_combinations += list(combinations(l, n))\n    return list_combinations\n\n\nall_comb(meteo_vars.values())\n\n\ndef to_result_pretrained(gap_len, n_latent, var_sel, gap_start=None):\n    data = GPFADataTest(hai[:n_obs]).add_gap(gap_len, var_sel, gap_start)\n    imp = GPFAImputationExplorer(data.data, latent_dims = n_latent)\n    model_path = model_save_dir / f\"GPFA_l_{n_latent}_train_{total_iter}_1ker_{n_obs}_obs.pickle\"\n    imp.learner.load(model_path)\n    return imp.to_result(data.data_compl_tidy, units=units)\n\n\n# to_result_pretrained(10, 1, ['TA'])\n\n::: {.cell 0=âhâ 1=âiâ 2=âdâ 3=âeâ}\n# @cache_disk(here() / \".cache/diff_gaps\")\n# def diff_gaps(gap_start=30):\n#     return {n_lat:\n#             {var_sel:{ \n#                 gap_len: to_result_pretrained(GPFADataTest(hai[:n_obs]).add_gap(gap_len, ['TA'], gap_start), n_lat)\n#                 for gap_len in [2, 4, 5, 7, 10 , 15, 20, 30, 50, 100]\n#                 }\n#                 for var_sel in all_comb(meteo_vars.values())}\n#             for n_lat in range(1,4)}\n:::\n\ngaps = [2, 5, 7, 10, 20, 30, 50, 100]\ngap_starts = [0, 30, 60, 90]\n\n\npath_base = here() / \".cache/diff_gap_partial\"\n# path_base.rmdir()\n\n\ndef process_var_sel(args, path_base=path_base):\n    var_sel, n_lat = args # limitations in python map...\n    f_name = path_base / f\"{'-'.join(var_sel)}__l_{n_lat}.pickle\"\n    if f_name.exists(): return\n    out = {}\n    for gap_len in gaps:\n        out[gap_len] = {}\n        for gap_start in gap_starts:\n            out[gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start) \n    with open(f_name, \"wb\") as f:\n        pickle.dump(out, f)    \n\n::: {.cell 0=âhâ 1=âiâ 2=âdâ 3=âeâ}\n# # this is going to run on the process\n# # @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\n# def process_n_lat(n_lat):\n#     out = {}\n#     for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n#         out[var_sel] = {}\n#         for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n#             out[var_sel][gap_len] = {}\n#             for gap_start in tqdm(gap_starts, desc=f\"gap len: {gap_len}\"):\n#                 out[var_sel][gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n#     return out\n:::\n\n# @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\ndef compute_diff_gaps(gap_start=30):\n    for n_lat in tqdm(range(1,4)):\n        with Pool(processes=4) as pool:\n            list(pool.imap(process_var_sel, zip(all_comb(meteo_vars.values()), repeat(n_lat,))))\n\n\n\n\nthis is memory intensive! (maybe there is a leak to fix somewhere â¦)\n\n# compute_diff_gaps()\n\n::: {.cell 0=âhâ 1=âiâ 2=âdâ 3=âeâ}\n# @cache_disk(here() / \".cache/diff_gaps/diff_gaps\")\n# def diff_gaps(gap_start=30):\n#     with Pool(processes=4) as pool:\n#         out = pool.map(res, range(1,4)\n#         for n_lat in tqdm(range(1,4)):\n#             out[n_lat] = {}\n#             for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n#                 out[n_lat][var_sel] = {}\n#                 for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n#                     out[n_lat][var_sel][gap_len] = {}\n#                     for gap_start in tqdm(gap_starts, desc=f\"gap len: {gap_len}\"):\n#                         out[n_lat][var_sel][gap_len][gap_start] = to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n#     return out\n:::\n::: {.cell 0=âhâ 1=âiâ 2=âdâ 3=âeâ}\n# @cache_disk(here() / \".cache/diff_gaps\")\n# def diff_gaps(gap_start=30):\n#     with Pool(processes=4) as pool:\n#         out = {}  \n#         for n_lat in tqdm(range(1,4)):\n#             out[n_lat] = {}\n#             for var_sel in tqdm(all_comb(meteo_vars.values()), desc=f\"latent: {n_lat}\"):\n#                 out[n_lat][var_sel] = {}\n#                 for gap_len in tqdm(gaps, desc=f\"sel: {var_sel}\"):\n#                     out[n_lat][var_sel][gap_len] = {}\n#                     f = lambda gap_start: to_result_pretrained(gap_len, n_latent=n_lat, var_sel = var_sel, gap_start=gap_start)\n#                     results = pool.map(f, gap_starts)\n#                     for gap_start, res in zip(gap_starts, results):\n#                         out[n_lat][var_sel][gap_len][gap_start] = res\n#     return out\n:::\n::: {.cell 0=âhâ 1=âiâ 2=âdâ 3=âeâ}\n# diff_gaps_res = diff_gaps()\n:::\n\n# loads computations from disk\ndef load_diff_gaps():\n    out = {}\n    for n_lat in tqdm(range(1,4)):\n        out[n_lat] = {}\n        for var_sel in all_comb(meteo_vars.values()):\n            f_name = path_base / f\"{'-'.join(var_sel)}__l_{n_lat}.pickle\"\n            with open(f_name, \"rb\") as f:\n                out[n_lat][var_sel] = pickle.load(f)  \n    return out\n\n\ndiff_gaps_res = load_diff_gaps()\n\n\n\n\n\nWhat I am doing here:\n\ntake a dataset with 200 obs and 3 variables\ndistribution and correlation between vars\nfit the kernel parameters using gradient descend on whole dataset and save trained model notebook\ncreate a dataset with all combinations of gap_len, gap_start, n latents and variable missing\npredict the model for all 200 Obs also when there are no gaps!\nNote: in case there is a gap in not all variable, the variable with the gap have the (correct) prediction conditioned on the other variables, but the variables with no gap have the base model prediction (which is often bad), which should not be considered\n\n\n\nWhat we can see from this result:\n\n\n\n1 latent the Lambda is almost 1 for TA, 0 for SW_IN and .4 for VPD. Hence is good for TA, horrible for SW_IN and somehow okayish forVPD`\n2 latent2: good for TA and SW_IN, still limited for VPD\n3 latents: quite good fit for all 3 models\n\ncomments\n\ncorrelation between SW_IN and the others variables is pretty low\ntherefore with 1-2 latents the model cannot model accurately more then 1 variable\n\n\n\n\n\nwhen gaps are short <~10 the model works kind of well, but there are issues in some locations (eg: TA, len: 10 start: 60)\n\ncomments\nthe lengthscale of the kernel is quite small (3 latents):\n\n5.2 z0\n1.8 z1\n4.0 z2\n\nso for longer gaps (in only one var) the main driver for the predictions are observations from the other variables, otherwise the models predictions are contastant as there is no way to use more information (eg. gap_len: 50, gap_start: 30, gaps in all vars)\nnotes - when SW_IN and VPD are close to 0 the gap filling is not that great also for shorter gaps (eg. SW_IN, len: 7, start: 30, n_lat: 3)\n\n\n\nthe interesting aspect is when there is a long gap, but in only 1-2 variables\n\nfor gaps only in TA with len up to 50 the models manages to follow the variations in the measurements, but with an error\n\nthis is pretty similar if there are gaps also in SW_IN, but not if there are gaps in VPD\nwith gap len over 100 it get way worse\n\n\nfor gaps only in VP with len up to 50 the models overall manages to follow the variations in the measurements, but with a considerable error (measurements are still in error bar) and the models has a lot of variations which are not present in the data\n\nthe predictions for SW_IN are bad (underestimates a lot the values) during the day for long gaps\n\n\n\n\n\n\n\n\n\n\n\n\nmore kernels -> can have different timescales. However with 150 obs both kernels have the same timescale, should use more data but then there are computation issues (with 1500 it would take more then 20hours to do the training)\nlog transform\nmore variables\n\n\n\n\nmodel performance: at the moment it takes ~8 minutes to train with 200 obs and ~20 seconds for inference\n\nprofile current model\nuse SparseGP\nCUDA support\n\nparameters init\nlearning rate and stability of parameters over training\nvariable transformation:\n\nall vars are now normalized (0 mean, 1 std)\ntime is enconded as integer increasing at steps of 1. Maybe not a good idea?\n\n\n\n\n\n\n\nuse ERA5-Land (world-wide dataset with complete meteo vars, but a coarse spatial-temporal scale)\ncompare performance with state of art models\nmodel where relation between variables changes over time\nunderstand gap distribution in real world:\n\naverage gap len (tentative results are: a lot of short gaps(<10) and some pretty long gap (>10.0000)\ncorrelation between variable gaps\nsite distributions of gaps?\n\n\n\n\n\n\n\n%time r = to_result_pretrained(12, 3, ['SW_IN', 'TA'], gap_start=60)\n\n\nr.display_results()\n\n\ndata = GPFADataTest(hai[:n_obs]).add_gap(50, ['TA'], 30)\n\n\nimp = GPFAImputationExplorer(data.data, latent_dims = n_latent)\nimp.learner.load(model_path)\n\n\nresult_pretrained(data)"
  },
  {
    "objectID": "Log transform - Multi latent - Imputation GPFA - Hainich.html",
    "href": "Log transform - Multi latent - Imputation GPFA - Hainich.html",
    "title": "Log Transform",
    "section": "",
    "text": "Trying to lo transform\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import *\n\n\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'log_SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'log_VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n1.636\n\n\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).log_transform(['SW_IN', 'VPD']).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).log_transform(['SW_IN', 'VPD']).add_gap(15, units.keys())\n\n\ndata_r_gaps.data\n\n\n\n\n\nTA\nlog_SW_IN\nlog_VPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.200489\n\n\n2000-01-01 01:00:00\nNaN\nNaN\nNaN\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.086178\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.104360\n\n\n2000-01-01 02:30:00\nNaN\n0.0\n0.097127\n\n\n...\n...\n...\n...\n\n\n2000-01-04 01:00:00\nNaN\nNaN\nNaN\n\n\n2000-01-04 01:30:00\n2.13\nNaN\n0.530628\n\n\n2000-01-04 02:00:00\n2.10\n0.0\n0.594983\n\n\n2000-01-04 02:30:00\n2.19\n0.0\n0.668854\n\n\n2000-01-04 03:00:00\n2.27\n0.0\nNaN\n\n\n\n\n\n\ncache_file_gaps = cache_path / \"hai_lo_transform.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\n@cache_disk(cache_file_gaps)\ndef compute_multiple_latent():\n    hai_r_gaps = [GPFAImputationExplorer(\n        data_r_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_r_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    hai_c_gaps = [GPFAImputationExplorer(\n        data_c_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_c_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    return hai_r_gaps, hai_c_gaps # cc\n\n\nhai_r_gaps, hai_c_gaps = compute_multiple_latent()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhai_r_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9929\n\n\nlog_SW_IN\n0.9888\n\n\nlog_VPD\n0.9838\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0778\nÂ°C\n\n\nlog_SW_IN\n0.1859\nW m-2\n\n\nlog_VPD\n0.0244\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9858\n\n\nlog_SW_IN\n0.9867\n\n\nlog_VPD\n0.9841\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1064\nÂ°C\n\n\nlog_SW_IN\n0.2308\nW m-2\n\n\nlog_VPD\n0.0255\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\nTA\n-0.0231\n0.1967\n0.9051\n\n\nlog_SW_IN\n0.8899\n0.3733\n0.0379\n\n\nlog_VPD\n-0.1494\n0.7971\n0.5105\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n4.2271\n\n\nz1\n2.9433\n\n\nz2\n6.6462\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0013\n\n\nlog_SW_IN\n0.0102\n\n\nlog_VPD\n0.0141\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0041\n\n\n\n\n\n\n\nhai_r_gaps[1].units = units\nhai_r_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9919\n\n\nlog_SW_IN\n0.9906\n\n\nlog_VPD\n0.5973\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0832\nÂ°C\n\n\nlog_SW_IN\n0.1700\nW m-2\n\n\nlog_VPD\n0.1216\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9844\n\n\nlog_SW_IN\n0.9890\n\n\nlog_VPD\n0.6195\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1115\nÂ°C\n\n\nlog_SW_IN\n0.2099\nW m-2\n\n\nlog_VPD\n0.1251\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.0841\n-0.9069\n\n\nlog_SW_IN\n0.9343\n0.0371\n\n\nlog_VPD\n0.2455\n-0.6557\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n3.7979\n\n\nz1\n6.6354\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0013\n\n\nlog_SW_IN\n0.0098\n\n\nlog_VPD\n0.4257\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0053\n\n\n\n\n\n\n\nhai_r_gaps[0].units = units\nhai_r_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9917\n\n\nlog_SW_IN\n-0.0027\n\n\nlog_VPD\n0.5397\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0844\nÂ°C\n\n\nlog_SW_IN\n1.7565\nW m-2\n\n\nlog_VPD\n0.1300\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9834\n\n\nlog_SW_IN\n-0.0389\n\n\nlog_VPD\n0.5384\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1151\nÂ°C\n\n\nlog_SW_IN\n2.0372\nW m-2\n\n\nlog_VPD\n0.1378\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\n\n\n\n\nTA\n0.9099\n\n\nlog_SW_IN\n0.0142\n\n\nlog_VPD\n0.6696\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.5268\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0021\n\n\nlog_SW_IN\n0.9885\n\n\nlog_VPD\n0.4629\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0043\n\n\n\n\n\n\n\n\n\n\nhai_c_gaps[2].units = units\nhai_c_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9357\n\n\nlog_SW_IN\n0.9046\n\n\nlog_VPD\n0.9007\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.2342\nÂ°C\n\n\nlog_SW_IN\n0.5418\nW m-2\n\n\nlog_VPD\n0.0604\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-11.2260\n\n\nlog_SW_IN\n-2.0597\n\n\nlog_VPD\n-1027.6723\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.7308\nÂ°C\n\n\nlog_SW_IN\n1.6771\nW m-2\n\n\nlog_VPD\n0.1880\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\nTA\n0.2127\n-0.4678\n0.6172\n\n\nlog_SW_IN\n-0.0392\n0.5399\n0.7212\n\n\nlog_VPD\n0.6975\n-0.0231\n0.5030\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n2.3790\n\n\nz1\n3.0309\n\n\nz2\n4.2982\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0009\n\n\nlog_SW_IN\n0.0038\n\n\nlog_VPD\n0.0044\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0022\n\n\n\n\n\n\n\nhai_c_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9919\n\n\nlog_SW_IN\n0.9906\n\n\nlog_VPD\n0.5973\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0832\nÂ°C\n\n\nlog_SW_IN\n0.1700\nW m-2\n\n\nlog_VPD\n0.1216\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9844\n\n\nlog_SW_IN\n0.9890\n\n\nlog_VPD\n0.6195\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1115\nÂ°C\n\n\nlog_SW_IN\n0.2099\nW m-2\n\n\nlog_VPD\n0.1251\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\nz1\n\n\n\n\nTA\n0.0841\n-0.9069\n\n\nlog_SW_IN\n0.9343\n0.0371\n\n\nlog_VPD\n0.2455\n-0.6557\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n3.7979\n\n\nz1\n6.6354\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0013\n\n\nlog_SW_IN\n0.0098\n\n\nlog_VPD\n0.4257\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0053\n\n\n\n\n\n\n\nhai_c_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9917\n\n\nlog_SW_IN\n-0.0027\n\n\nlog_VPD\n0.5397\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0844\nÂ°C\n\n\nlog_SW_IN\n1.7565\nW m-2\n\n\nlog_VPD\n0.1300\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9834\n\n\nlog_SW_IN\n-0.0389\n\n\nlog_VPD\n0.5384\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1151\nÂ°C\n\n\nlog_SW_IN\n2.0372\nW m-2\n\n\nlog_VPD\n0.1378\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nz0\n\n\n\n\nTA\n0.9099\n\n\nlog_SW_IN\n0.0142\n\n\nlog_VPD\n0.6696\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.5268\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0021\n\n\nlog_SW_IN\n0.9885\n\n\nlog_VPD\n0.4629\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0043"
  },
  {
    "objectID": "Kernel exploration.html",
    "href": "Kernel exploration.html",
    "title": "Kernel visualization",
    "section": "",
    "text": "Kernel visualization\n\nfrom meteo_imp.gpfa.gpfa import GPFAKernel\nimport gpytorch\nimport torch\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport pandas as pd\n\n\nk = gpytorch.kernels.RBFKernel()\n\n\nk.lengthscale = 3\n\n\nk(torch.tensor([1]), torch.tensor([2, 3])).evaluate()\n\ntensor([[0.9460, 0.8007]], grad_fn=<RBFCovarianceBackward>)\n\n\n\nalt.renderers.enable('mimetype')\n\nRendererRegistry.enable('mimetype')\n\n\n\ndef plot_kernel(kernel, t_range = torch.arange(-10, 10)):\n    y = kernel(torch.tensor([0]), t_range).evaluate().detach().numpy().squeeze()\n    return alt.Chart(pd.DataFrame({'x': t_range, 'y': y})).mark_line().encode(x='x', y ='y')\n    #plt.plot(t_range, y\n    # return y\n\n\nplot_kernel(k)"
  },
  {
    "objectID": "Additional latent kernel.html",
    "href": "Additional latent kernel.html",
    "title": "Additional latent kernel",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.gpfa.data_preparation import *\nfrom meteo_imp.gpfa.results import *\nfrom meteo_imp.gpfa.gpfa import *\nfrom meteo_imp.gpfa.results import _display_as_row\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import *\nfrom gpytorch.kernels import *\nimport gpytorch\n\ncp = here() / \".cache\" / \"add_kernel.pickle\"\n\n\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=1000)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-21 18:00:00\n-0.81\n0.0\n1.892\n\n\n2000-01-21 18:30:00\n-0.81\n0.0\n1.744\n\n\n2000-01-21 19:00:00\n-0.90\n0.0\n2.118\n\n\n2000-01-21 19:30:00\n-1.09\n0.0\n2.528\n\n\n2000-01-21 20:00:00\n-1.23\n0.0\n2.677\n\n\n\n\n\n\n\n\n\ndef _get_lengthscale_info(kernel: RBFKernel, suffix=''):\n        ls = kernel.lengthscale.detach().item()\n        return pd.DataFrame({\n            'lengthscale'+suffix: [ls]\n        }) \n\n\ndef _get_outscale_info(kernel: ScaleKernel, suffix=''):\n        ls = kernel.outputscale.detach().item()\n        return pd.DataFrame({\n            'outscale'+suffix: [ls]\n        }) \n\n\nclass GPFAMultiRbf(GPFA):\n    latent_kernel = lambda x: AdditiveKernel(RBFKernel(), ScaleKernel(RBFKernel()))\n    \n    \n    def get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -> dict[str, pd.DataFrame]:\n        \"Model info for a GPFA with a RBFKernel\"\n        out = {}\n\n        latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n\n        out[\"Lambda\"] = pd.concat([\n            None if var_names is None else pd.Series(var_names),\n            pd.DataFrame(\n                self.covar_module.Lambda.detach().cpu().numpy(),\n                columns=latent_names)],\n            axis=1)\n\n        ls_all = []\n        l_kernels = self.covar_module.latent_kernels\n        for kernel in list(l_kernels):\n            ls_s = [_get_lengthscale_info(kernel.kernels[0], \"_k0\")]\n            ls_s.append(_get_lengthscale_info(kernel.kernels[1].base_kernel, \"_k1\")) # this is a scale kernel\n            ls_all.append(pd.concat(ls_s, axis=1)) # attach multiple columns\n\n        ls_all = pd.concat(ls_all)\n        ls_all.insert(0, 'latent', latent_names)\n        out[\"Lengthscale\"] = ls_all\n        \n        os_all = []\n        l_kernels = self.covar_module.latent_kernels\n        for kernel in list(l_kernels):\n            os_s = [pd.DataFrame({'outscale_k0': [1] })] # there is no scaling here\n            os_s.append(_get_outscale_info(kernel.kernels[1], \"_k1\"))\n            os_all.append(pd.concat(os_s, axis=1)) # attach multiple columns\n\n        os_all = pd.concat(os_all)\n        os_all.insert(0, 'latent', latent_names)\n        out[\"Outscale\"] = os_all\n\n        psi = self.covar_module.psi.detach().cpu().numpy()\n        out[\"Psi\"] = pd.DataFrame({\n            'variable': var_names,\n            'psi': psi \n        })\n\n        out[\"Likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.item()]})\n\n        return out\n\n\nk = GPFAMultiRbf(torch.tensor([1,2,3]), torch.tensor([1,2,3]), gpytorch.likelihoods.GaussianLikelihood(), 2)\n\n\nk\n\nGPFAMultiRbf(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): AdditiveKernel(\n        (kernels): ModuleList(\n          (0): RBFKernel(\n            (raw_lengthscale_constraint): Positive()\n          )\n          (1): ScaleKernel(\n            (base_kernel): RBFKernel(\n              (raw_lengthscale_constraint): Positive()\n            )\n            (raw_outputscale_constraint): Positive()\n          )\n        )\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\n\n_display_as_row(k.get_info())\n\n\n\n\nz0\n\n\n\n\n0.5875\n\n\n0.4568\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale_k0\nlengthscale_k1\n\n\n\n\nz0\n0.6931\n0.6931\n\n\n\n\n\nOutscale\n\n\n\nlatent\noutscale_k0\noutscale_k1\n\n\n\n\nz0\n1\n0.6931\n\n\n\n\n\nPsi\n\n\n\nvariable\npsi\n\n\n\n\nNone\n0.6931\n\n\nNone\n0.6931\n\n\n\n\n\nLikelihood\n\n\n\nnoise\n\n\n\n\n0.6932\n\n\n\n\n\n\n\nreset_seed()\ndata = GPFADataTest(hai[:150]).add_random_missing()\n\n\n\n\n\n@cache_disk(cp)\ndef compute_small():\n    reset_seed()\n    data = GPFADataTest(hai[:150]).add_random_missing()\n    imp = GPFAImputationExplorer(data.data, latent_dims=3, model=GPFAMultiRbf)\n    return imp.fit().to_result(data.data_compl_tidy)\n\n\nres_small = compute_small()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n@cache_disk(cp)\ndef compute_large():\n    reset_seed()\n    data = GPFADataTest(hai[:1500]).add_random_missing()\n    imp = GPFAImputationExplorer(data.data, latent_dims=2, model=GPFAMultiRbf)\n    return imp.fit(), data\n\n\n# imp_large = compute_large()\n\n\n\n\n\n\nres_small.display_results(plot_args={'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9917\n\n\nSW_IN\n0.9725\n\n\nVPD\n0.9708\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\n\n\n\n\nTA\n0.0842\n\n\nSW_IN\n6.1635\n\n\nVPD\n0.0437\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9833\n\n\nSW_IN\n0.9712\n\n\nVPD\n0.9618\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\n\n\n\n\nTA\n0.1155\n\n\nSW_IN\n8.3501\n\n\nVPD\n0.0538\n\n\n\n\n\n\n\n\n\n0\nz0\nz1\nz2\n\n\n\n\nTA\n-0.1227\n0.8634\n0.1707\n\n\nSW_IN\n0.8996\n0.5550\n-0.6289\n\n\nVPD\n0.5291\n0.7391\n0.5174\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale_k0\nlengthscale_k1\n\n\n\n\nz0\n5.7209\n5.1237\n\n\nz1\n6.9906\n6.2228\n\n\nz2\n4.5014\n4.3961\n\n\n\n\n\nOutscale\n\n\n\nlatent\noutscale_k0\noutscale_k1\n\n\n\n\nz0\n1\n0.1827\n\n\nz1\n1\n0.1025\n\n\nz2\n1\n0.2387\n\n\n\n\n\nPsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.0014\n\n\nSW_IN\n0.0344\n\n\nVPD\n0.0238\n\n\n\n\n\nLikelihood\n\n\n\nnoise\n\n\n\n\n0.0050"
  },
  {
    "objectID": "presentations/plots_for_presentation_18_jan_23.html",
    "href": "presentations/plots_for_presentation_18_jan_23.html",
    "title": "Code for presentation of 18th January",
    "section": "",
    "text": "::: {.cell 0=âhâ 1=âiâ 2=âdâ 3=âeâ}\n%load_ext autoreload\n%autoreload 2\n:::\n\nfrom meteo_imp.kalman.filter import *\nimport torch\nimport numpy as np\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.fastai import plot_variable\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport altair as alt\nfrom pyprojroot import here\n\n\nn_obs = 20\nbase_dir = here(\"presentations/plots_18_jan\")\nbase_dir.mkdir(exist_ok=True)\n\n\ndef save_plot(p, path):\n    f_name = base_dir / (path + \".vl.json\")\n    with open(f_name, 'w') as f:\n        f.write(p.to_json())\n    return f_name\n\n\nplt_props = {'width': 460, 'height': 400}\n\n\n\n\\[ x_t = A\\color{blue}{x_{-1}} + \\varepsilon\\]\n\ntorch.tensor([[0.]]).shape\n\ntorch.Size([1, 1])\n\n\n\nk0 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([0.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.3]]),\n    obs_cov = torch.tensor([[0.1]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.01]]),\n)\n\n\npred0 = k0.smooth(torch.ones(1,n_obs,1), torch.zeros(1,n_obs,1, dtype=bool), torch.zeros(1,n_obs,1))\n\n\ntime = pd.Index(range(n_obs), name=\"time\")\n\n\npred0.cov.shape\n\ntorch.Size([1, 20, 1, 1])\n\n\n\npred0_df = NormalsDf(pd.DataFrame(pred0.mean.squeeze(0), index=time, columns=[\"state\"]),\n                     pd.DataFrame(pred0.cov.squeeze(0).squeeze(-1), index=time, columns=[\"state\"]))\n\n\np0 = facet_variable(pred0_df.tidy(), ys=[\"mean\", \"mean\"], error=True, point=False, gap_area=False, props=plt_props)\np0\n\n\n\n\n\n\n\nsave_plot(p0, \"only_state\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/only_state.vl.json')\n\n\n\n\n\n\ntorch.tensor([[0.]]).shape\n\ntorch.Size([1, 1])\n\n\n\nk1 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nobs = 2 * torch.sin(torch.arange(n_obs) * 2 * torch.pi / (n_obs-2))\n\n\nplt.plot(obs)\n\n\n\n\n\nmask1 = torch.ones(1,n_obs,1, dtype=bool)\n\n\npred1 = k1.smooth(obs.unsqueeze(0).unsqueeze(-1), mask1, torch.zeros(1,n_obs,1))\n\n\ntime = pd.Index(range(n_obs), name=\"time\")\n\n\npred1.cov.shape\n\ntorch.Size([1, 20, 1, 1])\n\n\n\npred_df1 = NormalsDf(pd.DataFrame(pred1.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred1.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df1 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask1[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df1 = pd.merge(obs_df1, pred_df1, on=['time', 'variable'])\n\n\np1 = facet_variable(plot_df1, ys=[\"value\", \"mean\"], error=True, gap_area=False, props=plt_props)\np1\n\n\n\n\n\n\n\nsave_plot(p1, \"obs\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/obs.vl.json')\n\n\n\n\n\n\nk2 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([0.]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nmask2 = torch.ones(1,n_obs,1, dtype=bool)\nmask2[0,11:16,0] = False\nmask2\n\ntensor([[[ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False],\n         [ True],\n         [ True],\n         [ True],\n         [ True]]])\n\n\n\npred2 = k2.smooth(obs.unsqueeze(0).unsqueeze(-1), mask2, torch.zeros(1,n_obs,1))\n\n\npred_df2 = NormalsDf(pd.DataFrame(pred2.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred2.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df2 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask2[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df2 = pd.merge(obs_df2, pred_df2, on=['time', 'variable'])\n\n\np2 = facet_variable(plot_df2, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 750, 'height': 500})\np2\n\n\n\n\n\n\n\nsave_plot(p2, \"gaps\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/gaps.vl.json')\n\n\n\n\n\n\nk3 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([1.]),        \n    contr_matrix = torch.tensor([-1., 1]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.tensor([[0.3]]),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.001]]),\n)\n\n\nmask3 = torch.ones(1,n_obs,1, dtype=bool)\nmask3[0,11:16,0] = False\nmask3\n\ntensor([[[ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [ True],\n         [False],\n         [False],\n         [False],\n         [False],\n         [False],\n         [ True],\n         [ True],\n         [ True],\n         [ True]]])\n\n\n\ncontr = torch.stack([(obs + 0.5)*1.2, ((obs + 0.5)*1.2).roll(-1)], dim=-1)\n\n\ncontr.shape\n\ntorch.Size([20, 2])\n\n\n\nplt.plot(contr[:,0])\nplt.plot(contr[:,1])\n\n\n\n\n\npred3 = k3.smooth(obs.unsqueeze(0).unsqueeze(-1), mask3, contr.unsqueeze(0))\n\n\npred_df3 = NormalsDf(pd.DataFrame(pred3.mean.squeeze(0), index=time, columns=[\"var\"]),\n                     pd.DataFrame(pred3.cov.squeeze(0).squeeze(-1), index=time, columns=[\"var\"])).tidy()\n\n\nobs_df3 = pd.DataFrame({'time': time, 'var': obs.numpy(),\n                        'is_present': mask3[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nplot_df3 = pd.merge(obs_df3, pred_df3, on=['time', 'variable'])\n\n\np3 = (plot_variable(plot_df3, variable=\"var\", ys=[\"value\", \"mean\"], error=True, gap_area=False, props=plt_props) +\nalt.Chart(pd.DataFrame({'contr':contr[:,0], 'time': time, 'col': 'control'})).mark_line(strokeDash=[6,3], color='orange').encode(x='time', y='contr'))\np3\n\n\n\n\n\n\n\nsave_plot(p3, \"control\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/control.vl.json')\n\n\n\n\n\n\nk4 = KalmanFilter(\n    trans_matrix = torch.tensor([1.]),\n    obs_matrix = torch.tensor([[.7], [.3]]),        \n    contr_matrix = torch.tensor([[0.]]),\n    trans_cov = torch.tensor([[.5]]),\n    obs_cov = torch.diag(torch.tensor([0.1, 0.1])),\n    trans_off = torch.tensor([0.]),\n    obs_off = torch.tensor([0., 0.]),\n    init_state_mean = torch.tensor([0.]),\n    init_state_cov = torch.tensor([[0.01]]),\n)\n\n\nk4\n\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n0.5000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\n\n\n\n\ny_0\n0.7000\n\n\ny_1\n0.3000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\n\n\n\n\ny_0\n0.1000\n0.0000\n\n\ny_1\n0.0000\n0.1000\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\n\n\n\n\nx_0\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\n\n\n\n\nx_0\n0.0100\n\n\n\n\n\n\n\nmask4 = torch.ones(1,n_obs,2, dtype=bool)\nmask4[0,11:16,0] = False\nmask4\n\ntensor([[[ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [False,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True]]])\n\n\n\nobs4 = torch.stack([\n    obs *.7 + torch.randn_like(obs) * .1,\n    obs *.3 + torch.randn_like(obs) * .1\n], dim=-1)\n\n\nplt.plot(obs4)\nplt.plot(obs)\n\n\n\n\n\nobs4.shape\n\ntorch.Size([20, 2])\n\n\n\npred4 = k4.predict(obs4.unsqueeze(0), mask4, torch.zeros(1,n_obs,1))\n\n\npred_df4 = NormalsDf(pd.DataFrame(pred4.mean.squeeze(0), index=time, columns=[\"var1\", \"var2\"]),\n                     pd.DataFrame(pred4.std.squeeze(0).squeeze(-1), index=time, columns=[\"var1\", \"var2\"])).tidy()\n\n\nobs_df4_0 = pd.DataFrame({'time': time, 'var1': obs4[:,0].numpy(),\n                        'is_present': mask4[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \nobs_df4_1 = pd.DataFrame({'time': time, 'var2': obs4[:,1].numpy(),\n                        'is_present': mask4[0,:,1].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nobs_df4 = pd.concat([obs_df4_0, obs_df4_1])\n\n\nplot_df4 = pd.merge(obs_df4, pred_df4, on=['time', 'variable'])\n\n\np4 = facet_variable(plot_df4, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 350, 'height': 220}, n_cols=1)\np4\n\n\n\n\n\n\n\nsave_plot(p4, \"var_corr\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/var_corr.vl.json')\n\n\n\nmask5 = torch.ones(1,n_obs,2, dtype=bool)\nmask5[0,11:16,:] = False\nmask5\n\ntensor([[[ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [False, False],\n         [False, False],\n         [False, False],\n         [False, False],\n         [False, False],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True],\n         [ True,  True]]])\n\n\n\npred5 = k4.predict(obs4.unsqueeze(0), mask5, torch.zeros(1,n_obs,1))\n\n\npred_df5 = NormalsDf(pd.DataFrame(pred5.mean.squeeze(0), index=time, columns=[\"var1\", \"var2\"]),\n                     pd.DataFrame(pred5.std.squeeze(0).squeeze(-1), index=time, columns=[\"var1\", \"var2\"])).tidy()\n\n\nobs_df5_0 = pd.DataFrame({'time': time, 'var1': obs4[:,0].numpy(),\n                        'is_present': mask5[0,:,0].numpy()}).melt([\"time\", \"is_present\"]) \nobs_df5_1 = pd.DataFrame({'time': time, 'var2': obs4[:,1].numpy(),\n                        'is_present': mask5[0,:,1].numpy()}).melt([\"time\", \"is_present\"]) \n\n\nobs_df5 = pd.concat([obs_df5_0, obs_df5_1])\n\n\nplot_df5 = pd.merge(obs_df4, pred_df5, on=['time', 'variable'])\n\n\np5 = facet_variable(plot_df5, ys=[\"value\", \"mean\"], error=True, gap_area=False, props={'width': 350, 'height': 220}, n_cols=1)\np5\n\n\n\n\n\n\n\nsave_plot(p5, \"var_no_corr\")\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/presentations/plots_18_jan/var_no_corr.vl.json')"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#outline",
    "href": "presentations/presentation_bioclim_18_jan_23.html#outline",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\nKalman Filter\nPreliminary results\nNext steps"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#background",
    "href": "presentations/presentation_bioclim_18_jan_23.html#background",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Background",
    "text": "Background\n\nEC tower measures also meteorological variables (eg. air temperature, wind speed)\ntechnical issues (eg. broken sensor) result in meteo time series with gaps\nPresence of gaps is a problem in many EC data applications (eg. ecosystem modelling)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#dataset",
    "href": "presentations/presentation_bioclim_18_jan_23.html#dataset",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Dataset",
    "text": "Dataset\n\nFluxnet 2015 data from Hainich (more 20 years)\nGlobal meteo dataset (downscaled ERA-Interim from Fluxnet 2015)\nmeteorological measurements every 30 mins\nfocusing on 3 variables\n\nAir temperature: TA\nIncoming shortwave radiation: SW_IN\nVapour Pressure Deficit: VPD"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of gaps for all TA, SW_IN and VPD for all sites"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gap-len-distribution-in-fluxnet-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gap len distribution in Fluxnet",
    "text": "Gap len distribution in Fluxnet\n\nplot of distribution of small gaps (<1 week) for all TA, SW_IN and VPD for all sites"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-fill-gaps",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-fill-gaps",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to fill gaps",
    "text": "How to fill gaps\n\nuse previous and following measurements for one variable and temporal auto-correlation (eg. diurnal cycles)\ncorrelation with other variables measures (eg. solar radiation and temperature)\nother measurements of meteo variables (eg. nearby station)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#state-of-the-art",
    "href": "presentations/presentation_bioclim_18_jan_23.html#state-of-the-art",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "State of the art",
    "text": "State of the art\nOneFlux pipeline (Fluxnet + ICOS + AmeriFlux)\n\nShort and medium gaps using Marginal Distribution Sampling (MDS)\nLong gaps filled with ERA data (global meteo dataset) using linear transformation to reduce site bias"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-mds-marginal-distribution-sampling-works",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-mds-marginal-distribution-sampling-works",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How MDS (Marginal Distribution Sampling) works",
    "text": "How MDS (Marginal Distribution Sampling) works\n\n\n\ntake a time window (7 days) around the gap\nuse 3 predictors variables (TA, SW_IN and VPD) and divide them in n discrete bins\nfor each bin (combination of conditions) find the average value of the missing variable\nfor each gap find the closest condition and fill with the average value\nif necessary increase the time window\nquality flag depends on the time window size"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-ta",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-ta",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling TA",
    "text": "MDS - gap filling TA"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-sw_in",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-sw_in",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling SW_IN",
    "text": "MDS - gap filling SW_IN"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-vpd",
    "href": "presentations/presentation_bioclim_18_jan_23.html#mds---gap-filling-vpd",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "MDS - gap filling VPD",
    "text": "MDS - gap filling VPD"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#current-approaches-limitations",
    "href": "presentations/presentation_bioclim_18_jan_23.html#current-approaches-limitations",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Current approaches limitations",
    "text": "Current approaches limitations\n\nNo uncertainty for the predictions (only a quality flag)\nEither MDS (variable correlation) or ERA data, donât combine the information\ndonât consider the observations before and after the gap"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#thesis-goal",
    "href": "presentations/presentation_bioclim_18_jan_23.html#thesis-goal",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Thesis goal",
    "text": "Thesis goal\n\ndevelop model to impute missing data in meteorological time series\ninclude all 3 imputation approaches\nprovide an uncertainty of the predictions"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-kalman-filter-works",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-kalman-filter-works",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How Kalman Filter works",
    "text": "How Kalman Filter works\n\n\nModels over time a latent variable (we are not observing it), the state of the system.\nThe current state \\(x_t\\) depends using:\n\nthe previous state \\(\\color{blue}{x_{t-1}}\\)\ncurrent observation \\(\\color{green}{y_t}\\)\ncontrol variable \\(\\color{purple}{c_t}\\) (ERA data)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#previous-state",
    "href": "presentations/presentation_bioclim_18_jan_23.html#previous-state",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "1. Previous state",
    "text": "1. Previous state\n\n\n\\[ x_t = A\\color{blue}{x_{t-1}} + \\varepsilon \\] where:\n\n\\(x_{t}\\) is the current state\n\\(\\color{blue}{x_{t-1}}\\) is the previous state\n\\(A\\) is a linear transformation of \\(\\color{blue}{x_{t-1}}\\)\n\\(\\varepsilon\\) is the âprocessâ noise which is a random variable with a normal distribution with mean 0\n\n\n\n\n\n\nExample of Kalman Filter \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#current-observation",
    "href": "presentations/presentation_bioclim_18_jan_23.html#current-observation",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "2. Current Observation",
    "text": "2. Current Observation\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(\\color{green}{y_{t}}\\) is the current observation\n\\(H\\) is a linear transformation of \\(\\color{green}{y_{t}}\\)\n\\(\\nu\\) is the âobservationâ noise which is a random variable with a normal distribution with mean 0\n\nusing the rules of probabilistic inference if we observe \\(y_t\\) you can update the distribution of \\(x_t\\)\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#gaps",
    "href": "presentations/presentation_bioclim_18_jan_23.html#gaps",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Gaps",
    "text": "Gaps\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#control-variable",
    "href": "presentations/presentation_bioclim_18_jan_23.html#control-variable",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "3. Control variable",
    "text": "3. Control variable\n\n\n\\[ \\begin{aligned}x_t &= A\\color{blue}{x_{t-1}} + B\\color{purple}{c_t} + \\varepsilon \\\\\\color{green}{y_t} &= Hx_t + \\nu\\end{aligned}\\]\nwhere:\n\n\\(B\\) is a linear transformation of \\(\\color{purple}{c_t}\\)\n\nUse the difference between current and previous value of control variable\n\n\n\n\n\nExample of Kalman Filter \\(H=1\\) \\(A=1\\) \\(B=[-1,1]\\)"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#extra-variable-correlation",
    "href": "presentations/presentation_bioclim_18_jan_23.html#extra-variable-correlation",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Extra: Variable correlation",
    "text": "Extra: Variable correlation\n\n\nGap in two variables\n\n\n\n\n\n\n\nGap in only one variable"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-find-model-parameters",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-find-model-parameters",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to find model parameters",
    "text": "How to find model parameters\n\ncreate artificial gaps\npredicting gap in the model\ncompute the log likelihood of the predictions\nmaximise the log likelihood"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filter",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filter",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filter",
    "text": "Kalman Filter\npros:\n\nProbabilist model: the output of the model is a distribution of predictions, not a single value\nCombines all 3 approaches to gap filling in one model\ninterpretable paramters\ncomputationally efficient\n\ncons:\n\nkeeps tracks only of the local state"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #1",
    "text": "Kalman Filters gap #1"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-2",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-2",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #2",
    "text": "Kalman Filters gap #2"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-3",
    "href": "presentations/presentation_bioclim_18_jan_23.html#kalman-filters-gap-3",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Kalman Filters gap #3",
    "text": "Kalman Filters gap #3"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#what-is-missing-in-the-model-development",
    "href": "presentations/presentation_bioclim_18_jan_23.html#what-is-missing-in-the-model-development",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "What is missing in the model development",
    "text": "What is missing in the model development\n\nimprove numerical stability of model (work in progress)\nfind optimal settings for training and inference\n\nn observations before after/gap\nhow to best generate artificial gaps"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model?",
    "text": "How to assess the model?\nOpen questions:\n\nhow to choose gap lengths?\nhow to choose number of variables missing?\nwhich variable to focus on?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-metrics",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-metrics",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Metrics",
    "text": "How to assess the model? Metrics\n\nRMSE - interpretation difficult as itâs relative to the variable\nr2 - gaps are often too short to interpret properly\n?"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nTime series\n\n\n\n\n\n\nScatter plots"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures-1",
    "href": "presentations/presentation_bioclim_18_jan_23.html#how-to-assess-the-model-figures-1",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "How to assess the model? Figures",
    "text": "How to assess the model? Figures\n\n\nGap length / mean RMSE\n\n\n\n\n\n\nDistribution gaps vs filled"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#model-use",
    "href": "presentations/presentation_bioclim_18_jan_23.html#model-use",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Model use",
    "text": "Model use\n\nwhat is the importance of the model performance?\n\ncan optimize kalman filter performance\n\nwhat is the impact of better gap filling for data users?\n\nwhy better filling for short gaps is useful\nhow can the uncertainty be used"
  },
  {
    "objectID": "presentations/presentation_bioclim_18_jan_23.html#future-outlook",
    "href": "presentations/presentation_bioclim_18_jan_23.html#future-outlook",
    "title": "Meterological Time series Imputation using Kalman Filters",
    "section": "Future outlook",
    "text": "Future outlook\n\nprovide pre-trained model on Fluxnet 2015 and then to fine-tune to local site\nprovide web-service for filling gaps\nreprocess Fluxnet 2015 dataset"
  },
  {
    "objectID": "presentations/Hainich_results_pres18_jan.html",
    "href": "presentations/Hainich_results_pres18_jan.html",
    "title": "Hainich with ERA-Interim",
    "section": "",
    "text": "Manual fine tuning learning process for presentation 18 Jan 2023\n\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.kalman.fastai import *\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.data import *\n\nfrom fastai.tabular.learner import *\nfrom fastai.learner import *\nfrom fastai.callback.all import *\nfrom fastcore.foundation import L\n\nfrom meteo_imp.kalman.fastai import show_results\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom sklearn.decomposition import PCA\n\n\nreset_seed()\n\n\nhai = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path64)\n\n\n# dls = imp_dataloader(hai64, hai_era64, var_sel = gen_var_sel(['TA', 'SW_IN', 'VPD']), block_len=200, gap_len=10, bs=20, control_lags=[1])\n\n\npc = PCA().fit(hai)\n\n\nd0 = hai.iloc[0:1]\n\n\ntr0 = pc.transform(d0)\n\n\ntr0\n\narray([[-121.11917652,   -7.06844313,    0.87975241]])\n\n\n\nt0 = np.random.randn(5,1) * 10\ntt = np.hstack([t0, 2 * t0 + np.random.randn(5,1), 3 * t0 + np.random.randn(5,1)])\n\n\ntt = np.vstack([tt, -tt])\n\n\ntt.mean(0)\n\narray([1.77635684e-16, 0.00000000e+00, 0.00000000e+00])\n\n\n\ndef pca(X):\n    # Data matrix X, assumes 0-centered\n    n, m = X.shape\n    assert np.allclose(X.mean(axis=0), np.zeros(m))\n    # Compute covariance matrix\n    C = np.dot(X.T, X) / (n-1)\n    # Eigen decomposition\n    eigen_vals, eigen_vecs = np.linalg.eig(C)\n    # Project X onto PC space\n    X_pca = np.dot(X, eigen_vecs)\n    return X_pca, eigen_vecs\n\n\npca(tt)\n\n(array([[-9.15947487e+00, -3.03773243e-01,  3.81626348e-01],\n        [ 2.00045215e+01, -1.99595583e-02,  1.53745860e+00],\n        [ 1.33744310e+01,  2.01395206e-02, -3.19819214e-02],\n        [-8.99057811e+00,  1.24208069e-01,  9.57988123e-01],\n        [ 3.14616875e+01, -4.88141095e-02, -5.79117681e-01],\n        [ 9.15947487e+00,  3.03773243e-01, -3.81626348e-01],\n        [-2.00045215e+01,  1.99595583e-02, -1.53745860e+00],\n        [-1.33744310e+01, -2.01395206e-02,  3.19819214e-02],\n        [ 8.99057811e+00, -1.24208069e-01, -9.57988123e-01],\n        [-3.14616875e+01,  4.88141095e-02,  5.79117681e-01]]),\n array([[-0.26545847, -0.95659856,  0.12021234],\n        [-0.53518386,  0.04249433, -0.84366608],\n        [-0.80194142,  0.28829401,  0.52323659]]))\n\n\n\nsk_pc = PCA(2).fit(tt)\n\n\ntt @ sk_pc.components_.T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\ntr = sk_pc.transform(tt)\ntr\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\nsk_pc.components_.T\n\narray([[-0.26545847,  0.12021234],\n       [-0.53518386, -0.84366608],\n       [-0.80194142,  0.52323659]])\n\n\n\n(sk_pc.components_ @ tt.T).T\n\narray([[ -9.15947487,   0.38162635],\n       [ 20.00452148,   1.5374586 ],\n       [ 13.37443103,  -0.03198192],\n       [ -8.99057811,   0.95798812],\n       [ 31.46168754,  -0.57911768],\n       [  9.15947487,  -0.38162635],\n       [-20.00452148,  -1.5374586 ],\n       [-13.37443103,   0.03198192],\n       [  8.99057811,  -0.95798812],\n       [-31.46168754,   0.57911768]])\n\n\n\ntt[0, None]\n\narray([[2.76792539, 4.56712931, 7.45746711]])\n\n\n\nsk_pc.components_.shape\n\n(3, 3)\n\n\n\ntt[0].shape\n\n(3,)\n\n\n\n(sk_pc.components_ @ tt[0])\n\narray([-9.15947487,  0.38162635])\n\n\n\ntt[0]\n\narray([2.76792539, 4.56712931, 7.45746711])\n\n\n\nsk_pc.components_.T @ tr[0]\n\narray([2.47733634, 4.58003795, 7.54504311])\n\n\n\nsk_pc.inverse_transform(tr[0])\n\narray([2.47733634, 4.58003795, 7.54504311])\n\n\n\nd0.to_numpy()\n\narray([[-0.6  ,  0.   ,  0.222]])\n\n\n\nhai.iloc[0]\n\nTA      -0.600\nSW_IN    0.000\nVPD      0.222\nName: 2000-01-01 00:30:00, dtype: float64\n\n\n\\[ x = y\\Lambda \\]\n\npc.components_\n\narray([[ 0.01681572,  0.99979324,  0.01143269],\n       [ 0.93010891, -0.01983747,  0.36674772],\n       [-0.36689868, -0.00446652,  0.93025018]])\n\n\n\nnp.linalg.inv(pc.components_)\n\narray([[ 0.01681572,  0.93010891, -0.36689868],\n       [ 0.99979324, -0.01983747, -0.00446652],\n       [ 0.01143269,  0.36674772,  0.93025018]])\n\n\nassuming that the control has the same dimensions of the observations then if we are doing a local slope model we have \\(B \\in \\mathbb{R}^{state \\times contr}\\): \\[ B = \\begin{bmatrix} -I & I \\\\ 0 & 0 \\end{bmatrix}\\]\n\nfrom torch import hstack, eye, vstack, ones, zeros, tensor\nfrom functools import partial\n\n\ndef set_dtype(*args, dtype=torch.float64):\n    return [partial(arg, dtype=dtype) for arg in args] \n\n\neye, ones, zeros, tensor = set_dtype(eye, ones, zeros, tensor)\n\n\ndef init_smart(n_dim_obs, n_dim_state, df, pca=True):\n    # n_dim_obs == n_dim_contr\n    if pca:\n        comp = PCA(n_dim_state).fit(df).components_\n        obs_matrix = tensor(comp.T) # transform state -> obs\n        contr_matrix = tensor(comp) # transform obs -> state\n    else:\n        obs_matrix, contr_matrix = eye(n_dim_obs), eye(n_dim_obs)\n        \n    return KalmanFilter(\n        trans_matrix =     vstack([hstack([eye(n_dim_state),                eye(n_dim_state)]),\n                                   hstack([zeros(n_dim_state, n_dim_state), eye(n_dim_state)])]),\n        trans_off =        zeros(n_dim_state * 2),        \n        trans_cov =        eye(n_dim_state * 2)*.1,        \n        obs_matrix =       hstack([obs_matrix, zeros(n_dim_obs, n_dim_state)]),\n        obs_off =          zeros(n_dim_obs),          \n        obs_cov =          eye(n_dim_obs)*.01,            \n        contr_matrix =     vstack([hstack([-contr_matrix,                  contr_matrix]),\n                                   hstack([ zeros(n_dim_state,n_dim_obs), zeros(n_dim_state, n_dim_obs)])]),\n        init_state_mean =  zeros(n_dim_state * 2),        \n        init_state_cov =   eye(n_dim_state * 2) * 3,\n    ) \n\n\nnp.hstack([np.eye(2), np.eye(2)])\n\narray([[1., 0., 1., 0.],\n       [0., 1., 0., 1.]])\n\n\n\ninit_smart(3,2, hai)\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n1.0000\n0.0000\n1.0000\n0.0000\n\n\nx_1\n0.0000\n1.0000\n0.0000\n1.0000\n\n\nx_2\n0.0000\n0.0000\n1.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n1.0000\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n0.1000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n0.1000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n0.1000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.1000\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\n\n\n\n\ny_0\n0.0168\n0.9301\n0.0000\n0.0000\n\n\ny_1\n0.9998\n-0.0198\n0.0000\n0.0000\n\n\ny_2\n0.0114\n0.3667\n0.0000\n0.0000\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0100\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0100\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0000\n\n\ny_1\n0.0000\n\n\ny_2\n0.0000\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0168\n-0.9998\n-0.0114\n0.0168\n0.9998\n0.0114\n\n\nx_1\n-0.9301\n0.0198\n-0.3667\n0.9301\n-0.0198\n0.3667\n\n\nx_2\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0000\n\n\nx_1\n0.0000\n\n\nx_2\n0.0000\n\n\nx_3\n0.0000\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n3.0000\n0.0000\n0.0000\n0.0000\n\n\nx_1\n0.0000\n3.0000\n0.0000\n0.0000\n\n\nx_2\n0.0000\n0.0000\n3.0000\n0.0000\n\n\nx_3\n0.0000\n0.0000\n0.0000\n3.0000\n\n\n\n\n\n\n\nclass PersistentRecorder(Callback):\n    order = 70\n    name = \"per_recorder\"\n    attrs = ['lrs', 'iters', 'losses', 'values']\n    def before_fit(self):\n        \"Prepare state for training\"\n        for attr in self.attrs:\n            if not hasattr(self.per_recorder, attr): setattr(self.per_recorder, attr, [])\n\n    def after_batch(self):\n        for attr in self.attrs:\n            setattr(self.per_recorder, attr, getattr(self.recorder, attr))\n\n\nmodels = []\n\n\ndls = imp_dataloader(hai, hai_era, var_sel = ['TA', 'SW_IN', 'VPD'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nlen(dls.valid.items)\nitems = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n\n\ndef train_show_save(learn, n_iter, lr):\n    learn.fit(n_iter, lr)\n    models.append(learn.model.state_dict().copy())\n    learn.recorder.plot_loss()\n    items = [dls.valid.items[i] for i in [10, 50, 100, 200, 300, 400]]\n    return show_results(learn, items = items, control=hai_control)\n\n\n\n\nmodel = init_smart(3,3,3, hai, pca=False).cuda()\n\n\nloss = loss_func=KalmanLoss(only_gap=False)\nlearn = Learner(dls, model, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\nlearn.model.use_smooth = True\n\n\nshow_results(learn, items=items, control=hai_control)\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-87.370314\n-96.212915\n0.069196\n0.210351\n0.946861\n-250571814513532732431918956544.000000\n02:37\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1489\n0.0042\n0.0120\n0.8404\n-0.0794\n-0.1127\n\n\nx_1\n-0.0954\n1.1464\n-0.0546\n0.1177\n0.8560\n0.0599\n\n\nx_2\n-0.0225\n0.0420\n1.1357\n-0.0717\n-0.1190\n0.8475\n\n\nx_3\n0.1201\n0.0661\n0.1170\n1.1840\n-0.1150\n-0.1073\n\n\nx_4\n0.0770\n0.1299\n0.0068\n-0.1847\n1.0987\n-0.1500\n\n\nx_5\n0.1084\n0.0358\n0.1133\n-0.1809\n-0.0281\n1.1533\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0341\n0.0493\n0.0361\n-0.0008\n-0.0312\n0.0026\n\n\nx_1\n0.0493\n0.1125\n0.0531\n0.0475\n0.0047\n0.0509\n\n\nx_2\n0.0361\n0.0531\n0.0399\n0.0001\n-0.0327\n0.0058\n\n\nx_3\n-0.0008\n0.0475\n0.0001\n0.0608\n0.0614\n0.0585\n\n\nx_4\n-0.0312\n0.0047\n-0.0327\n0.0614\n0.0901\n0.0554\n\n\nx_5\n0.0026\n0.0509\n0.0058\n0.0585\n0.0554\n0.0594\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0008\n\n\nx_1\n-0.0083\n\n\nx_2\n0.0067\n\n\nx_3\n0.0023\n\n\nx_4\n-0.0007\n\n\nx_5\n0.0005\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.8238\n-0.1897\n-0.1014\n-0.0841\n0.1914\n0.1286\n\n\ny_1\n0.0814\n0.8353\n0.1048\n0.1040\n0.0127\n0.0689\n\n\ny_2\n-0.0742\n-0.1502\n0.8296\n0.1282\n0.1811\n-0.0746\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0084\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0084\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0084\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.0017\n\n\ny_1\n-0.0046\n\n\ny_2\n0.0026\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0290\n-0.0485\n-0.0022\n0.9297\n0.0792\n-0.0247\n\n\nx_1\n-0.0267\n-0.8778\n-0.0233\n0.0113\n0.9099\n0.0261\n\n\nx_2\n-0.0018\n-0.0592\n-1.0044\n-0.0310\n0.0674\n0.9204\n\n\nx_3\n-0.0503\n-0.0312\n-0.0556\n-0.0440\n-0.0529\n-0.0587\n\n\nx_4\n-0.0375\n-0.0537\n-0.0246\n-0.0412\n-0.0352\n-0.0338\n\n\nx_5\n-0.0653\n-0.0179\n-0.0575\n-0.0655\n-0.0307\n-0.0449\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0393\n\n\nx_1\n0.0044\n\n\nx_2\n-0.0092\n\n\nx_3\n0.0207\n\n\nx_4\n-0.0094\n\n\nx_5\n-0.0021\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.5500\n-0.1402\n-0.0082\n0.5233\n0.3011\n0.0882\n\n\nx_1\n-0.1402\n2.4578\n0.1502\n0.0355\n0.5714\n-0.0459\n\n\nx_2\n-0.0082\n0.1502\n2.5045\n0.2739\n0.0634\n0.5195\n\n\nx_3\n0.5233\n0.0355\n0.2739\n2.3814\n-0.2201\n-0.1842\n\n\nx_4\n0.3011\n0.5714\n0.0634\n-0.2201\n2.3880\n-0.1162\n\n\nx_5\n0.0882\n-0.0459\n0.5195\n-0.1842\n-0.1162\n2.3874\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/fastai/callback/core.py:69: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-108.023250\n-113.113034\n0.073622\n0.198386\n0.956828\n-105578612000578019271909572608.000000\n02:37\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1440\n-0.0057\n0.0107\n0.8262\n-0.0845\n-0.1205\n\n\nx_1\n-0.0534\n1.2089\n-0.0223\n0.0948\n0.7937\n0.0494\n\n\nx_2\n-0.0289\n0.0176\n1.1417\n-0.0702\n-0.1154\n0.8351\n\n\nx_3\n0.1277\n0.0589\n0.1074\n1.1787\n-0.1203\n-0.1024\n\n\nx_4\n0.0654\n0.1560\n0.0043\n-0.1781\n1.1087\n-0.1415\n\n\nx_5\n0.0959\n0.0439\n0.1197\n-0.1725\n-0.0340\n1.1496\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0207\n0.0254\n0.0210\n-0.0052\n-0.0232\n-0.0033\n\n\nx_1\n0.0254\n0.0548\n0.0255\n0.0259\n0.0034\n0.0281\n\n\nx_2\n0.0210\n0.0255\n0.0222\n-0.0056\n-0.0241\n-0.0023\n\n\nx_3\n-0.0052\n0.0259\n-0.0056\n0.0473\n0.0504\n0.0465\n\n\nx_4\n-0.0232\n0.0034\n-0.0241\n0.0504\n0.0695\n0.0473\n\n\nx_5\n-0.0033\n0.0281\n-0.0023\n0.0465\n0.0473\n0.0477\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0004\n\n\nx_1\n-0.0055\n\n\nx_2\n0.0048\n\n\nx_3\n0.0027\n\n\nx_4\n-0.0033\n\n\nx_5\n0.0021\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7902\n-0.2032\n-0.1217\n-0.0986\n0.2164\n0.1090\n\n\ny_1\n0.0160\n0.7417\n0.0555\n0.1016\n0.1041\n0.0374\n\n\ny_2\n-0.1103\n-0.1594\n0.7916\n0.1185\n0.2096\n-0.0949\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0077\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0077\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0077\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0009\n\n\ny_1\n-0.0053\n\n\ny_2\n-0.0021\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0277\n-0.0595\n-0.0019\n0.9181\n0.0838\n-0.0194\n\n\nx_1\n-0.0414\n-0.8661\n-0.0323\n0.0110\n0.8741\n0.0262\n\n\nx_2\n-0.0029\n-0.0647\n-1.0045\n-0.0319\n0.0757\n0.8957\n\n\nx_3\n-0.0678\n-0.0368\n-0.0564\n-0.0559\n-0.0610\n-0.0639\n\n\nx_4\n-0.0374\n-0.0690\n-0.0209\n-0.0485\n-0.0406\n-0.0354\n\n\nx_5\n-0.0591\n-0.0267\n-0.0753\n-0.0633\n-0.0407\n-0.0555\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0396\n\n\nx_1\n-0.0009\n\n\nx_2\n-0.0049\n\n\nx_3\n0.0153\n\n\nx_4\n-0.0139\n\n\nx_5\n-0.0094\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.6364\n-0.1456\n-0.0020\n0.5695\n0.3986\n0.1628\n\n\nx_1\n-0.1456\n2.3292\n0.1064\n0.2074\n0.7591\n0.1475\n\n\nx_2\n-0.0020\n0.1064\n2.4205\n0.3489\n0.1488\n0.6143\n\n\nx_3\n0.5695\n0.2074\n0.3489\n2.2351\n-0.4355\n-0.3277\n\n\nx_4\n0.3986\n0.7591\n0.1488\n-0.4355\n2.1625\n-0.3445\n\n\nx_5\n0.1628\n0.1475\n0.6143\n-0.3277\n-0.3445\n2.2518\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps\")\n\nPath('models/17_jan_all_gaps.pth')\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-122.620241\n-126.621236\n0.077944\n0.202366\n0.948695\n-115274990280631988956159803392.000000\n02:38\n\n\n1\n-134.455010\n-139.768441\n0.074152\n0.188695\n0.959962\n-103653315974387814635399020544.000000\n02:31\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n1.1347\n-0.0147\n0.0100\n0.8021\n-0.1122\n-0.1444\n\n\nx_1\n-0.0388\n1.2480\n-0.0185\n0.0773\n0.7576\n0.0414\n\n\nx_2\n-0.0257\n-0.0018\n1.1500\n-0.0814\n-0.1272\n0.8203\n\n\nx_3\n0.1332\n0.0451\n0.0866\n1.1612\n-0.1103\n-0.0917\n\n\nx_4\n0.0559\n0.1720\n0.0056\n-0.1625\n1.1052\n-0.1248\n\n\nx_5\n0.0721\n0.0457\n0.1226\n-0.1523\n-0.0385\n1.1357\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.0146\n0.0183\n0.0142\n-0.0081\n-0.0202\n-0.0067\n\n\nx_1\n0.0183\n0.0445\n0.0187\n0.0148\n-0.0014\n0.0168\n\n\nx_2\n0.0142\n0.0187\n0.0143\n-0.0065\n-0.0187\n-0.0044\n\n\nx_3\n-0.0081\n0.0148\n-0.0065\n0.0340\n0.0393\n0.0343\n\n\nx_4\n-0.0202\n-0.0014\n-0.0187\n0.0393\n0.0550\n0.0378\n\n\nx_5\n-0.0067\n0.0168\n-0.0044\n0.0343\n0.0378\n0.0361\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.0014\n\n\nx_1\n-0.0059\n\n\nx_2\n0.0039\n\n\nx_3\n0.0016\n\n\nx_4\n-0.0036\n\n\nx_5\n0.0032\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n0.7310\n-0.1642\n-0.0983\n-0.1030\n0.2236\n0.0907\n\n\ny_1\n0.0158\n0.6777\n0.0705\n0.1196\n0.1642\n0.0316\n\n\ny_2\n-0.1124\n-0.1249\n0.7399\n0.1211\n0.2193\n-0.0846\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0064\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0065\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0065\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0019\n\n\ny_1\n-0.0046\n\n\ny_2\n-0.0026\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0316\n-0.0708\n-0.0019\n0.8851\n0.0973\n-0.0094\n\n\nx_1\n-0.0727\n-0.8659\n-0.0584\n0.0075\n0.7964\n0.0142\n\n\nx_2\n0.0098\n-0.0734\n-1.0045\n-0.0156\n0.0845\n0.8489\n\n\nx_3\n-0.0977\n-0.0434\n-0.0574\n-0.0723\n-0.0760\n-0.0730\n\n\nx_4\n-0.0309\n-0.1114\n-0.0185\n-0.0559\n-0.0634\n-0.0420\n\n\nx_5\n-0.0503\n-0.0424\n-0.1056\n-0.0644\n-0.0571\n-0.0729\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-0.0585\n\n\nx_1\n-0.0141\n\n\nx_2\n-0.0070\n\n\nx_3\n0.0239\n\n\nx_4\n-0.0043\n\n\nx_5\n-0.0069\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n2.9257\n0.1245\n0.1987\n0.4475\n0.2435\n0.0893\n\n\nx_1\n0.1245\n2.4393\n0.3641\n0.2107\n0.7802\n0.1895\n\n\nx_2\n0.1987\n0.3641\n2.4862\n0.2959\n0.0089\n0.6053\n\n\nx_3\n0.4475\n0.2107\n0.2959\n2.1058\n-0.5771\n-0.4644\n\n\nx_4\n0.2435\n0.7802\n0.0089\n-0.5771\n1.9818\n-0.5064\n\n\nx_5\n0.0893\n0.1895\n0.6053\n-0.4644\n-0.5064\n2.0952\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 2, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-149.288297\n-152.804403\n0.072760\n0.179190\n0.956827\n-61340364986143763995601928192.000000\n02:35\n\n\n1\n-160.099637\n-165.002097\n0.071263\n0.181697\n0.963587\n-48387608947773502947627892736.000000\n02:37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# learn.save(\"17_jan_all_gaps_final\")\n\nPath('models/17_jan_all_gaps_final.pth')\n\n\n\np = show_results(learn, control=hai_control, items = [items[i] for i in [0,5,4]])\n\n\n\n\n\n\n\n\n\n\ndls2 = imp_dataloader(hai, hai_era, var_sel = ['TA'], block_len=200, gap_len=10, bs=20, control_lags=[1], n_rep=2)\n\n\nmodel2 = init_smart(3,3, hai, pca=True).cuda()\n\n\nloss2 = loss_func=KalmanLoss(only_gap=False)\nlearn2 = Learner(dls2, model2, loss, cbs=[Float64Callback], metrics=imp_metrics)\n\n\ntrain_show_save(learn2, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n\n\n\nNotImplementedError: Module [KalmanFilter] is missing the required \"forward\" function\n\n\nDisable smoother\n\nmodel.use_smooth = False\n\n\ntrain_show_save(learn, 3, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n534.159578\n465.194522\n0.284379\n0.329570\n-1.919198\n-241825956681560181426503024640.000000\n02:24\n\n\n\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.6761\n0.1157\n-0.0095\n0.5895\n0.0728\n0.0850\n\n\nx_1\n0.0309\n0.6675\n0.1639\n0.2296\n0.5542\n-0.1129\n\n\nx_2\n0.1836\n-0.1476\n0.6498\n0.1684\n-0.2142\n0.5899\n\n\nx_3\n-0.1246\n0.0361\n0.0138\n0.6844\n-0.0830\n0.1501\n\n\nx_4\n-0.0167\n-0.0399\n0.0044\n0.0089\n0.7537\n0.0580\n\n\nx_5\n0.0341\n0.0300\n-0.0436\n-0.0362\n0.2878\n0.6487\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n0.5106\n0.3768\n0.4383\n-0.1720\n-0.2863\n-0.1749\n\n\nx_1\n0.3768\n0.5687\n0.1931\n-0.2781\n0.0461\n0.0798\n\n\nx_2\n0.4383\n0.1931\n0.4406\n-0.0734\n-0.3650\n-0.2384\n\n\nx_3\n-0.1720\n-0.2781\n-0.0734\n0.5150\n0.3608\n0.3418\n\n\nx_4\n-0.2863\n0.0461\n-0.3650\n0.3608\n0.8281\n0.6975\n\n\nx_5\n-0.1749\n0.0798\n-0.2384\n0.3418\n0.6975\n0.6156\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0166\n\n\nx_1\n-0.0196\n\n\nx_2\n-0.0446\n\n\nx_3\n0.0033\n\n\nx_4\n-0.0022\n\n\nx_5\n0.0107\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\ny_0\n-0.3096\n0.5414\n-0.0652\n0.4906\n-0.0643\n-0.3834\n\n\ny_1\n0.6288\n-0.3366\n-0.3681\n0.0321\n-0.2062\n0.3634\n\n\ny_2\n-0.2813\n0.0500\n0.4738\n-0.0601\n0.2872\n-0.1334\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.3678\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.3707\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.3693\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n-0.0197\n\n\ny_1\n-0.0452\n\n\ny_2\n-0.0451\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9410\n-0.5681\n-0.8722\n0.9998\n1.2958\n1.0828\n\n\nx_1\n-0.7019\n-0.7354\n-0.9277\n1.2348\n1.1344\n0.9620\n\n\nx_2\n-0.9322\n-0.7328\n-0.8307\n1.0310\n1.1112\n1.1017\n\n\nx_3\n0.0914\n0.2592\n0.0286\n0.0638\n0.1644\n-0.0007\n\n\nx_4\n0.2175\n-0.0398\n0.0226\n0.2072\n-0.1205\n-0.0117\n\n\nx_5\n-0.0578\n0.1485\n0.0908\n-0.0613\n0.0627\n0.0792\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0290\n\n\nx_1\n-0.0320\n\n\nx_2\n-0.0494\n\n\nx_3\n0.0296\n\n\nx_4\n0.0387\n\n\nx_5\n0.1010\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\n\n\n\n\nx_0\n3.7171\n2.7054\n2.8366\n2.0078\n1.3904\n2.3275\n\n\nx_1\n2.7054\n4.5189\n2.1010\n1.4436\n1.8804\n2.8057\n\n\nx_2\n2.8366\n2.1010\n3.3027\n1.3323\n0.9307\n2.0280\n\n\nx_3\n2.0078\n1.4436\n1.3323\n3.3867\n2.1461\n3.0821\n\n\nx_4\n1.3904\n1.8804\n0.9307\n2.1461\n2.5441\n2.6815\n\n\nx_5\n2.3275\n2.8057\n2.0280\n3.0821\n2.6815\n3.7653\n\n\n\n\n\n\nIndexError: list index out of range\n\n\n\ntrain_show_save(learn, 1, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n272.526685\n207.314376\n0.279615\n0.290300\n-3.962859\n-113462721102480788295669252096.000000\n06:35\n\n\n\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5320\n0.0984\n0.2567\n\n\nx_1\n0.5386\n-0.1615\n-0.1019\n\n\nx_2\n0.4759\n0.4640\n0.4202\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.4043\n0.5126\n1.8864\n\n\nx_1\n0.5126\n0.1871\n0.6885\n\n\nx_2\n1.8864\n0.6885\n2.5339\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0972\n\n\nx_1\n-0.2735\n\n\nx_2\n0.0298\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1792\n0.7014\n-0.3164\n\n\ny_1\n-0.5045\n0.1311\n0.3455\n\n\ny_2\n-0.0962\n0.0705\n0.1246\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0190\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.1624\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0297\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1664\n\n\ny_1\n-0.0433\n\n\ny_2\n0.0232\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0331\n-0.4960\n0.2924\n0.5946\n-0.1127\n-0.1933\n\n\nx_1\n0.5991\n0.3053\n0.2248\n0.7649\n0.9446\n0.0394\n\n\nx_2\n0.0060\n0.0013\n-0.0667\n-0.1143\n0.7781\n0.6409\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0627\n\n\nx_1\n-0.6392\n\n\nx_2\n-1.2803\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n9.0224\n3.6789\n10.5469\n\n\nx_1\n3.6789\n2.3654\n5.9687\n\n\nx_2\n10.5469\n5.9687\n21.9915\n\n\n\n\n\n\nNameError: name 'learn1' is not defined\n\n\n\ntrain_show_save(learn, 1, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n160.675568\n150.361449\n0.266633\n0.269324\n-1.959343\n-53151966368643636872563654656.000000\n05:30\n\n\n\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5363\n0.1070\n0.2723\n\n\nx_1\n0.5375\n-0.1534\n-0.0948\n\n\nx_2\n0.4909\n0.4638\n0.4301\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.3408\n0.4970\n1.7922\n\n\nx_1\n0.4970\n0.1842\n0.6643\n\n\nx_2\n1.7922\n0.6643\n2.3955\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0948\n\n\nx_1\n-0.2568\n\n\nx_2\n0.0393\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1742\n0.6954\n-0.3174\n\n\ny_1\n-0.5094\n0.1551\n0.3429\n\n\ny_2\n-0.1147\n0.0583\n0.1163\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0121\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.1100\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0190\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1697\n\n\ny_1\n-0.0123\n\n\ny_2\n0.0387\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0363\n-0.5225\n0.3103\n0.5948\n-0.0692\n-0.1628\n\n\nx_1\n0.5955\n0.3523\n0.1332\n0.7587\n0.9820\n-0.0509\n\n\nx_2\n-0.0159\n0.0573\n-0.0471\n-0.1409\n0.7802\n0.6457\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0500\n\n\nx_1\n-0.6014\n\n\nx_2\n-1.2993\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n10.9221\n3.8061\n12.4742\n\n\nx_1\n3.8061\n1.7574\n5.6427\n\n\nx_2\n12.4742\n5.6427\n24.9669\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 2e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n115.738528\n104.632719\n0.257947\n0.259673\n-1.679687\n-24519279833493970234084687872.000000\n05:09\n\n\n\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5303\n0.1124\n0.2795\n\n\nx_1\n0.5333\n-0.1470\n-0.0908\n\n\nx_2\n0.4963\n0.4602\n0.4322\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.2399\n0.4645\n1.6526\n\n\nx_1\n0.4645\n0.1740\n0.6191\n\n\nx_2\n1.6526\n0.6191\n2.2026\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0883\n\n\nx_1\n-0.2505\n\n\nx_2\n0.0440\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1715\n0.6898\n-0.3162\n\n\ny_1\n-0.5142\n0.1782\n0.3403\n\n\ny_2\n-0.1173\n0.0543\n0.1176\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0079\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0732\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0124\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1699\n\n\ny_1\n-0.0058\n\n\ny_2\n0.0383\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0449\n-0.5621\n0.3391\n0.5937\n0.0285\n-0.1154\n\n\nx_1\n0.5860\n0.3917\n0.0584\n0.7468\n1.0129\n-0.1247\n\n\nx_2\n-0.0479\n0.1664\n-0.0280\n-0.1814\n0.7807\n0.6422\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0488\n\n\nx_1\n-0.5958\n\n\nx_2\n-1.3038\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n12.1503\n4.1501\n13.5248\n\n\nx_1\n4.1501\n1.7544\n5.9730\n\n\nx_2\n13.5248\n5.9730\n27.6423\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.model.use_smooth = True\nshow_results(learn, items = [learn.dls.valid.items[i] for i in [10, 100, 300, 500, 700, 1000]])\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n30.829145\n26.059960\n0.201186\n0.215770\n-0.034140\n-48287381455002891037683220480.000000\n06:32\n\n\n\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5309\n0.1159\n0.2797\n\n\nx_1\n0.5313\n-0.1385\n-0.0881\n\n\nx_2\n0.4961\n0.4560\n0.4324\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.1473\n0.4769\n1.5832\n\n\nx_1\n0.4769\n0.2008\n0.6682\n\n\nx_2\n1.5832\n0.6682\n2.2249\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0894\n\n\nx_1\n-0.2426\n\n\nx_2\n0.0426\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1711\n0.6883\n-0.3146\n\n\ny_1\n-0.5151\n0.1887\n0.3414\n\n\ny_2\n-0.1155\n0.0454\n0.1145\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0063\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0583\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0097\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1747\n\n\ny_1\n0.0044\n\n\ny_2\n0.0287\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0475\n-0.5920\n0.3650\n0.5948\n0.0563\n-0.0801\n\n\nx_1\n0.5812\n0.4322\n-0.0088\n0.7377\n1.0076\n-0.1962\n\n\nx_2\n-0.0461\n0.1875\n-0.0228\n-0.1820\n0.7675\n0.6408\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0477\n\n\nx_1\n-0.5959\n\n\nx_2\n-1.2990\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n12.4282\n4.5316\n13.6935\n\n\nx_1\n4.5316\n2.1406\n6.4461\n\n\nx_2\n13.6935\n6.4461\n27.6478\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_show_save(learn, 1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n2.643850\n-2.886783\n0.193556\n0.212050\n0.165402\n-29540765662186335322106757120.000000\n06:27\n\n\n\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.5339\n0.1210\n0.2783\n\n\nx_1\n0.5270\n-0.1285\n-0.0862\n\n\nx_2\n0.4976\n0.4502\n0.4350\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.0913\n0.4682\n1.5174\n\n\nx_1\n0.4682\n0.2037\n0.6628\n\n\nx_2\n1.5174\n0.6628\n2.1595\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n-0.0873\n\n\nx_1\n-0.2397\n\n\nx_2\n0.0401\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.1690\n0.6841\n-0.3145\n\n\ny_1\n-0.5175\n0.1944\n0.3405\n\n\ny_2\n-0.1099\n0.0372\n0.1142\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0051\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0467\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0078\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1766\n\n\ny_1\n0.0070\n\n\ny_2\n0.0210\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.0614\n-0.6233\n0.4026\n0.5869\n0.0964\n-0.0301\n\n\nx_1\n0.5842\n0.4624\n-0.0654\n0.7358\n0.9837\n-0.2582\n\n\nx_2\n-0.0390\n0.2132\n-0.0130\n-0.1775\n0.7580\n0.6438\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n-1.0447\n\n\nx_1\n-0.5957\n\n\nx_2\n-1.2961\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n13.1380\n4.8428\n14.4531\n\n\nx_1\n4.8428\n2.3677\n6.8288\n\n\nx_2\n14.4531\n6.8288\n28.0978\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.model.use_smooth = True\nlearn.loss_func = KalmanLoss(only_gap=True)\n\n\ntrain_show_save(learn, 1,  5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-3.892116\n-2.872949\n0.120294\n0.204105\n-2.923830\n-85436991145187843135533744128.000000\n04:26\n\n\n\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.2027\n0.3761\n0.2969\n\n\nx_1\n0.3594\n0.3686\n0.3606\n\n\nx_2\n0.4679\n0.2295\n0.2852\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0220\n0.0113\n-0.0224\n\n\nx_1\n0.0113\n0.0064\n-0.0097\n\n\nx_2\n-0.0224\n-0.0097\n0.0290\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.3025\n\n\nx_1\n-0.5231\n\n\nx_2\n0.2064\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.1489\n0.3517\n-0.0053\n\n\ny_1\n0.2922\n0.1890\n-0.4105\n\n\ny_2\n0.3920\n-0.0638\n0.2473\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0001\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0054\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0003\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.2038\n\n\ny_1\n0.1323\n\n\ny_2\n-0.1988\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-0.9856\n0.6178\n0.4694\n-0.5368\n0.5367\n-0.0159\n\n\nx_1\n0.6667\n0.5327\n-0.3882\n0.6907\n-0.0209\n-0.2365\n\n\nx_2\n0.0330\n-0.7175\n-0.1714\n0.2048\n-0.6491\n0.1349\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0922\n\n\nx_1\n-0.6262\n\n\nx_2\n0.2368\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n8.1062\n12.7070\n6.0908\n\n\nx_1\n12.7070\n21.6312\n10.2860\n\n\nx_2\n6.0908\n10.2860\n5.5679\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn1.model.use_smooth = True\n\n\ntrain_show_save(learn1, 1,  5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\nrmse_gap\nr2\nr2_gap\ntime\n\n\n\n\n0\n-415.543654\n-449.643886\n0.055635\n0.200575\n0.759219\n-45104963579554009935054372864.000000\n07:43\n\n\n\n\n\n\n\n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.1815\n0.3765\n0.3138\n\n\nx_1\n0.3401\n0.3500\n0.3764\n\n\nx_2\n0.4904\n0.2297\n0.2438\n\n\n\n\ntrans cov (Q)\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0491\n0.0170\n-0.0614\n\n\nx_1\n0.0170\n0.0127\n-0.0243\n\n\nx_2\n-0.0614\n-0.0243\n0.0889\n\n\n\n\n\ntrans off\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.3080\n\n\nx_1\n-0.5210\n\n\nx_2\n0.2044\n\n\n\n\n\nobs matrix (H)\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n-0.1630\n0.3552\n-0.0202\n\n\ny_1\n0.3143\n0.1963\n-0.4623\n\n\ny_2\n0.4022\n-0.0653\n0.2466\n\n\n\n\n\nobs cov (R)\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.0001\n0.0000\n0.0000\n\n\ny_1\n0.0000\n0.0054\n0.0000\n\n\ny_2\n0.0000\n0.0000\n0.0003\n\n\n\n\n\nobs off\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.2096\n\n\ny_1\n0.1459\n\n\ny_2\n-0.2079\n\n\n\n\n\ncontr matrix (B)\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n-1.0006\n0.5988\n0.4673\n-0.5447\n0.5317\n-0.0122\n\n\nx_1\n0.6825\n0.5156\n-0.3620\n0.7188\n-0.0186\n-0.1805\n\n\nx_2\n0.0303\n-0.7130\n-0.1672\n0.2100\n-0.6278\n0.1642\n\n\n\n\n\ninit state mean\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.0933\n\n\nx_1\n-0.6332\n\n\nx_2\n0.2392\n\n\n\n\n\ninit state cov\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n8.2929\n13.0004\n6.2311\n\n\nx_1\n13.0004\n22.1310\n10.5205\n\n\nx_2\n6.2311\n10.5205\n5.6921\n\n\n\n\n\n\n\n\n\n\n\n\n# learn1.save(\"model_16_jan1\")\n\nPath('models/model_16_jan1.pth')"
  },
  {
    "objectID": "var_distribution.html",
    "href": "var_distribution.html",
    "title": "Variable distribution",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\nimport altair as alt\nimport polars as pl\n\nfrom meteo_imp.fluxnet.gap_finder import scan_fluxnet_csv\n\nfrom meteo_imp.utils import cache_disk\n\n\n\nload Hainich dataset\n\nhai_path = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\"\n# hai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=20_000)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = scan_fluxnet_csv(hai_path, convert_dates=True).rename(meteo_vars).select([pl.col(\"end\").alias(\"time\"), *meteo_vars.values()])\n\nhai.fetch(10)\n\nshape: (10, 5)\ntime\nTA\nSW_IN\nLW_IN\nVPD\ndatetime[Î¼s]\nf64\nf64\nf64\nf64\n2000-01-01 00:30:00\n-0.6\n0.0\n302.475\n0.222\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.09\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.11\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n2000-01-01 03:00:00\n-0.4\n0.0\n301.677\n0.111\n2000-01-01 03:30:00\n-0.36\n0.0\n301.677\n0.109\n2000-01-01 04:00:00\n-0.35\n0.0\n301.677\n0.107\n2000-01-01 04:30:00\n-0.28\n0.0\n308.046\n0.122\n2000-01-01 05:00:00\n-0.27\n0.0\n308.046\n0.138\n\n\n\nhai_td = hai.melt('time')\n\n\nhai_td.fetch(3)\n\nshape: (12, 3)\ntime\nvariable\nvalue\ndatetime[Î¼s]\nstr\nf64\n2000-01-01 00:30:00\n\"TA\"\n-0.6\n2000-01-01 01:00:00\n\"TA\"\n-0.65\n2000-01-01 01:30:00\n\"TA\"\n-0.58\n2000-01-01 00:30:00\n\"SW_IN\"\n0.0\n2000-01-01 01:00:00\n\"SW_IN\"\n0.0\n2000-01-01 01:30:00\n\"SW_IN\"\n0.0\n2000-01-01 00:30:00\n\"LW_IN\"\n302.475\n2000-01-01 01:00:00\n\"LW_IN\"\n302.475\n2000-01-01 01:30:00\n\"LW_IN\"\n301.677\n2000-01-01 00:30:00\n\"VPD\"\n0.222\n2000-01-01 01:00:00\n\"VPD\"\n0.122\n2000-01-01 01:30:00\n\"VPD\"\n0.09\n\n\n\n\n\n\nhai.drop('time').collect().to_pandas().hist(figsize=(15,10));\n\n\n\n\n\n# should to the binning before the plot\n# alt.Chart(hai_td.collect().to_pandas()).mark_line().encode(\n#     x = 'value',\n#     y = 'density()',\n#     facet = alt.Facet('variable', columns=2)\n# )\n\n\n\n\nCode inspired from source: https://towardsdatascience.com/altair-plot-deconstruction-visualizing-the-correlation-structure-of-weather-data-38fb5668c5b1\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_p.corr()\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\n\n\nTA\n1.000000\n0.432321\n0.639556\n0.735412\n\n\nSW_IN\n0.432321\n1.000000\n0.126278\n0.533506\n\n\nLW_IN\n0.639556\n0.126278\n1.000000\n0.270424\n\n\nVPD\n0.735412\n0.533506\n0.270424\n1.000000\n\n\n\n\n\n\ndef corr_mask(size):\n    corr_mask = np.zeros((size,size), dtype=bool)\n    for i in range(size):\n        for j in range(size):\n            corr_mask[i,j] = True if i >= j else False\n    return corr_mask\n\n\ncorr_mask(len(hai_p.columns))\n\narray([[ True, False, False, False],\n       [ True,  True, False, False],\n       [ True,  True,  True, False],\n       [ True,  True,  True,  True]])\n\n\n\nhai_p = hai_p[sorted(hai_p.columns)] # need to properly plot half a corr matrix\n\n\ncor_hai = (hai_p\n              .corr().mask(~corr_mask(len(hai_p.columns))).stack()\n              .reset_index()     # The stacking results in an index on the correlation values, we need the index as normal columns for Altair\n              .rename(columns={0: 'correlation', 'level_0': 'variable', 'level_1': 'variable2'}))\ncor_hai['correlation_label'] = cor_hai['correlation'].map('{:.2f}'.format)  # Round to 2 decimal\ncor_hai\n\n\n\n\n\nvariable\nvariable2\ncorrelation\ncorrelation_label\n\n\n\n\n0\nLW_IN\nLW_IN\n1.000000\n1.00\n\n\n1\nSW_IN\nLW_IN\n0.126278\n0.13\n\n\n2\nSW_IN\nSW_IN\n1.000000\n1.00\n\n\n3\nTA\nLW_IN\n0.639556\n0.64\n\n\n4\nTA\nSW_IN\n0.432321\n0.43\n\n\n5\nTA\nTA\n1.000000\n1.00\n\n\n6\nVPD\nLW_IN\n0.270424\n0.27\n\n\n7\nVPD\nSW_IN\n0.533506\n0.53\n\n\n8\nVPD\nTA\n0.735412\n0.74\n\n\n9\nVPD\nVPD\n1.000000\n1.00\n\n\n\n\n\n\nimport numpy as np\n\ndef compute_2d_histogram(var1, var2, df, density=True, bins=20):\n    H, xedges, yedges = np.histogram2d(df[var1], df[var2], bins=bins, density=density)\n    H[H == 0] = np.nan\n\n    # Create a nice variable that shows the bin boundaries\n    \n    x_width = xedges[1] - xedges[0] # all bins have same width\n    xedges = pd.Series(xedges[:-1] + x_width /2)\n    \n    y_width = yedges[1] - yedges[0] # all bins have same width\n    yedges = pd.Series(yedges[:-1] + y_width /2)\n    \n    # Cast to long format using melt\n    res = pd.DataFrame(H, \n                       index=yedges, \n                       columns=xedges).reset_index().melt(\n                            id_vars='index'\n                       ).rename(columns={'index': 'value2', \n                                         'value': 'count',\n                                         'variable': 'value'})\n    \n\n    res['variable'] = var1\n    res['variable2'] = var2 \n    return res.dropna() # Drop all combinations for which no values where found\n\n\nh, xe, ye = np.histogram2d(hai_p['VPD'], hai_p['TA'])\n\n\nx_width = xe[1] - xe[0] # all bins have same width\nxe = pd.Series(xe + x_width /2)\n\n\nh.shape\n\n(10, 10)\n\n\n\nxe\n\n0      2.38335\n1      7.15005\n2     11.91675\n3     16.68345\n4     21.45015\n5     26.21685\n6     30.98355\n7     35.75025\n8     40.51695\n9     45.28365\n10    50.05035\ndtype: float64\n\n\n\nhai_binned = pd.concat([compute_2d_histogram(var1, var2, hai_p) for var1 in meteo_vars.values() for var2 in meteo_vars.values()])\nhai_binned.head()\n\n\n\n\n\nvalue2\nvalue\ncount\nvariable\nvariable2\n\n\n\n\n0\n-17.19025\n-17.19025\n0.000083\nTA\nTA\n\n\n21\n-14.47075\n-14.47075\n0.000224\nTA\nTA\n\n\n42\n-11.75125\n-11.75125\n0.000574\nTA\nTA\n\n\n63\n-9.03175\n-9.03175\n0.001188\nTA\nTA\n\n\n84\n-6.31225\n-6.31225\n0.003781\nTA\nTA\n\n\n\n\n\n\n# Define selector\nvar_sel_cor = alt.selection_single(fields=['variable', 'variable2'], clear=False, \n                                  init={'variable': 'TA', 'variable2': 'SW_IN'})\n\n# Define correlation heatmap\nbase = alt.Chart(cor_hai).encode(\n    x='variable2:O',\n    y='variable:O'    \n)\n\ntext = base.mark_text().encode(\n    text='correlation_label',\n    color=alt.condition(\n        alt.datum.correlation > 0.5, \n        alt.value('white'),\n        alt.value('black')\n    )\n)\n\ncor_plot = base.mark_rect().encode(\n    color=alt.condition(var_sel_cor, alt.value('pink'), 'correlation:Q')\n).add_selection(var_sel_cor)\n\n# Define 2d binned histogram plot\nscat_plot = alt.Chart(hai_binned).transform_filter(\n    var_sel_cor\n).mark_rect().encode(\n    alt.X('value:N', axis=alt.Axis(format=\".4\")), \n    alt.Y('value2:N', axis=alt.Axis(format=\".4\"), sort='descending'),\n    alt.Color('count:Q', scale=alt.Scale(scheme='blues'))\n)\n\n# Combine all plots. hconcat plots both side-by-side \nalt.hconcat((cor_plot + text).properties(width=350, height=350), scat_plot.properties(width=350, height=350)).resolve_scale(color='independent')\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nhai_p = hai.collect().to_pandas().set_index('time')\n\n\nhai_td_p = hai_td.collect().to_pandas().set_index('time')"
  },
  {
    "objectID": "analyze_gaps_fluxnet.html",
    "href": "analyze_gaps_fluxnet.html",
    "title": "Analyze gaps fluxnet",
    "section": "",
    "text": "from IPython.display import display\nfrom ipywidgets import widgets, interact\n\n\nfrom pathlib import Path\nimport polars as pl\nfrom datetime import datetime\nfrom fastcore.utils import * # support of ls for paths\nimport matplotlib.pyplot as plt\nimport altair as alt\n\n\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\n\n\nsite_info = pl.read_parquet(out_dir / \"../site_info.parquet\").select([\n    pl.col(\"start\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"end\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\"),\n    pl.col(\"site\").cast(pl.Categorical).sort()\n])\n\n\nsite_info.head()\n\nshape: (5, 3)\nstart\nend\nsite\ndatetime[Î¼s]\ndatetime[Î¼s]\ncat\n2009-01-01 00:30:00\n2012-01-01 00:00:00\n\"AR-SLu\"\n2009-01-01 00:30:00\n2013-01-01 00:00:00\n\"AR-Vir\"\n2002-01-01 00:30:00\n2013-01-01 00:00:00\n\"AT-Neu\"\n2007-01-01 00:30:00\n2010-01-01 00:00:00\n\"AU-Ade\"\n2010-01-01 00:30:00\n2015-01-01 00:00:00\n\"AU-ASM\"\n\n\n\ndef duration_n_obs(duration):\n    \"converts a duration into a n of fluxnet observations\"\n    return abs(int(duration.total_seconds() / (30 * 60)))\n\n\nduration_n_obs(site_info[1, \"start\"] - site_info[1, \"end\"])\n\n70127\n\n\n\nduration_n_obs(site_info[0, \"start\"] - site_info[0, \"end\"])\n\n52559\n\n\n\nsite_info.select((pl.col(\"end\")-pl.col(\"start\")).dt.minutes() // 30).sum()\n\nshape: (1, 1)\nend\ni64\n26175287\n\n\n\nsp = site_info.to_pandas()\n\n\n((sp.end - sp.start).dt.total_seconds() / (30*60)).astype(int).sum()\n\n26175287\n\n\n\n# maybe this code should actually go in 20_gap_finding\nfiles = out_dir.ls()\nfiles.sort() # need to sort to match the site_info\nsites = []\nfor i, path in enumerate(files):\n    sites.append(pl.scan_parquet(path).with_columns([\n        pl.lit(site_info[i, \"site\"]).alias(\"site\"),\n        pl.lit(duration_n_obs(site_info[i, \"start\"] -  site_info[i, \"end\"])).alias(\"total_obs\"),\n        pl.col(\"TIMESTAMP_END\").cast(pl.Utf8).str.strptime(pl.Datetime, \"%Y%m%d%H%M\").alias(\"end\"),\n    ]).drop(\"TIMESTAMP_END\"))\n\n\ngap_stat = pl.concat(sites)\n\n\ngap_stat.head().fetch(5)\n\nshape: (5, 5)\ngap_len\nvariable\nsite\ntotal_obs\nend\nu32\nstr\nstr\ni32\ndatetime[Î¼s]\n16992\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-01-01 00:30:00\n5\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-12-21 11:00:00\n1\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2009-12-21 17:00:00\n1\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2010-01-06 13:00:00\n3\n\"TA_F_MDS_QC\"\n\"AR-SLu\"\n52559\n2010-01-07 13:00:00\n\n\n\ndef filter_variables(variables = [\"TA_F_QC\", \"SW_IN_QC\", \"LW_IN_QC\", \"VPD_F_QC\"]):\n    expr = False\n    for var in variables:\n        expr |= pl.col(\"variable\") == var\n    return expr\n\n\ndef pl_in(col, values):\n    expr = False\n    for val in values:\n        expr |= pl.col(col) == val\n    return expr\n\nsome sites have a lot of data missing, with the avg gap length of several years, so is seems that the year can have an impact\nImportant! here the 3 possibles gap value of a QC variable are considered as one (null, 1, 2) we should co\n\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).groupby(\"site\").agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()).alias(\"frac_gap\")\n]).collect()\n\nshape: (205, 3)\nsite\nmean\nfrac_gap\nstr\nf64\nf64\n\"CA-NS7\"\n1015.733333\n0.217263\n\"GF-Guy\"\n53.333333\n0.00083\n\"US-Ivo\"\n261.12\n0.093088\n\"FI-Jok\"\n88.463235\n0.17156\n\"FI-Let\"\n392.4\n0.2518\n\"US-Me2\"\n3168.0\n0.097305\n\"AU-Lox\"\n10494.0\n0.59817\n\"US-Wi9\"\n1835.2\n0.261521\n\"CA-NS4\"\n527.933333\n0.338771\n\"AR-SLu\"\n962.96875\n0.586293\n\"GL-ZaF\"\n1074.03125\n0.490097\n\"AU-DaS\"\n22.25\n0.029006\n...\n...\n...\n\"US-Wi4\"\n43.63354\n0.400702\n\"NL-Hor\"\n379.25\n0.194688\n\"IT-Cp2\"\n55.25\n0.025206\n\"US-Me5\"\n1.5\n0.000114\n\"US-Goo\"\n250.1\n0.199744\n\"RU-Ha1\"\n3396.571429\n0.451955\n\"US-Me4\"\n214.563218\n0.212863\n\"US-ORv\"\n77.101796\n0.734973\n\"BR-Sa1\"\n120.345528\n0.168888\n\"DE-RuS\"\n561.659091\n0.352403\n\"AU-TTE\"\n1916.2\n0.182124\n\"FI-Lom\"\n1.0\n0.000038\n\n\n\n\n\nshort_gaps = gap_stat.filter(pl_in('variable', ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC'])).filter(pl.col('gap_len') < 200).groupby('variable').agg(pl.col('gap_len').mean()).collect()\n\n\nshort_gaps\n\nshape: (3, 2)\nvariable\ngap_len\nstr\nf64\n\"VPD_F_QC\"\n9.642444\n\"TA_F_QC\"\n11.33915\n\"SW_IN_F_QC\"\n5.726891\n\n\n\nprint(short_gaps.to_pandas().to_markdown())\n\n|    | variable   |   gap_len |\n|---:|:-----------|----------:|\n|  0 | VPD_F_QC   |   9.64244 |\n|  1 | TA_F_QC    |  11.3392  |\n|  2 | SW_IN_F_QC |   5.72689 |\n\n\n\ngaps_year_site_ta = gap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).with_column(\n  pl.col(\"end\").dt.year().alias(\"year\")  \n).groupby([\"site\", \"year\"]).agg([\n    pl.col(\"gap_len\").mean().alias(\"mean\"),\n    (pl.col(\"gap_len\").sum() / (48 * 365)).alias(\"frac_gap\")\n]).sort([\"site\", \"year\"]).collect()\n\n\ngaps_year_site_ta.describe()\n\nshape: (7, 5)\ndescribe\nsite\nyear\nmean\nfrac_gap\nstr\nstr\nf64\nf64\nf64\n\"count\"\n\"1232\"\n1232.0\n1232.0\n1232.0\n\"null_count\"\n\"0\"\n0.0\n0.0\n0.0\n\"mean\"\nnull\n2007.18263\n826.696712\n0.156607\n\"std\"\nnull\n4.633427\n3339.649175\n0.350794\n\"min\"\n\"AR-SLu\"\n1991.0\n1.0\n0.000057\n\"max\"\n\"ZM-Mon\"\n2015.0\n52608.0\n7.686073\n\"median\"\nnull\n2008.0\n42.166667\n0.030822\n\n\n\ngaps_year_site_ta.filter(pl.col(\"frac_gap\") >=1)\n\nshape: (15, 4)\nsite\nyear\nmean\nfrac_gap\nstr\ni32\nf64\nf64\n\"BE-Bra\"\n2003\n17738.0\n1.012443\n\"BR-Sa1\"\n2006\n4167.4\n1.189326\n\"CA-NS1\"\n2001\n1219.941176\n1.183733\n\"CA-NS3\"\n2002\n4580.8\n1.307306\n\"DE-Lnf\"\n2007\n52608.0\n3.00274\n\"IT-Cpz\"\n1998\n228.990566\n1.385445\n\"IT-Ro2\"\n2008\n1464.5\n1.504623\n\"RU-Cok\"\n2003\n411.729167\n1.128025\n\"SJ-Adv\"\n2011\n7235.0\n1.23887\n\"US-Blo\"\n1997\n292.784615\n1.086244\n\"US-CRT\"\n2004\n17545.0\n1.001427\n\"US-LWW\"\n2011\n52608.0\n3.00274\n\"US-Syv\"\n2009\n52560.0\n3.0\n\"US-WCr\"\n2007\n17520.0\n1.0\n\"ZM-Mon\"\n2000\n22443.333333\n7.686073\n\n\n\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).filter(pl_in(\"site\", [\"ZM-Mon\"])).collect()\n\nshape: (8, 5)\ngap_len\nvariable\nsite\ntotal_obs\nend\nu32\nstr\nstr\ni32\ndatetime[Î¼s]\n2913\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-01-01 00:30:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-02 17:30:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-04 04:30:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-04 09:00:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-10 11:30:00\n131743\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2000-03-11 02:00:00\n1\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2008-01-30 12:00:00\n7753\n\"TA_F_QC\"\n\"ZM-Mon\"\n175343\n2009-07-23 12:00:00\n\n\n\ngap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).filter(pl_in(\"site\", [\"DE-Lnf\"])).collect()\n\nshape: (6, 5)\ngap_len\nvariable\nsite\ntotal_obs\nend\nu32\nstr\nstr\ni32\ndatetime[Î¼s]\n5116\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2002-01-01 00:30:00\n52608\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2007-01-01 00:30:00\n1\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2010-04-15 10:00:00\n20\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2012-08-14 13:00:00\n45\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2012-10-09 14:00:00\n2\n\"TA_F_QC\"\n\"DE-Lnf\"\n192863\n2012-10-16 13:30:00\n\n\n\ndef visualize_by_site(df):\n    sites = df[\"site\"].unique()\n    for site in sites:\n        yield df.filter(pl.col(\"site\") == site)\n\n\nby_site = list(visualize_by_site(gaps_year_site_ta))\n\n\nbutton_next = widgets.Button(description=\"Next\", icon=\"arrow-right\")\nbutton_prev = widgets.Button(description=\"Previous\", icon=\"arrow-left\")\noutput = widgets.Output()\n\ndisplay(button_next, button_prev, output)\n\ni = 0\n\ndef update_view():\n    with output:\n        print(f\"{i} of {len(by_site)}\")\n        display(by_site[i])\n    output.clear_output(wait=True)\n\ndef on_next(b):\n    global i\n    if i < len(by_site):\n        i +=1\n    else:\n        button_next.disabled = True\n    update_view()\n\ndef on_prev(b):\n    global i\n    if i > 0:\n        i -=1\n    else:\n        button_prev.disabled = True\n    update_view()\n    \n\nbutton_next.on_click(on_next)\nbutton_prev.on_click(on_prev)\n\n\n\n\n\n\n\n\n\n\n\nta_gaps = gap_stat.filter(\n    pl.col(\"variable\") == \"TA_F_QC\" \n).collect()\n\n\nta_gaps.to_pandas().hist(column=\"gap_len\")\nplt.yscale('log')\n\n\n\n\n\n\n\n\nall_vars = gap_stat.select(pl.col(\"variable\").unique().sort()).collect()[\"variable\"]\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var):\n    ta_gaps = gap_stat.filter(\n        pl.col(\"variable\") == var \n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n\n\n\nall_sites = gap_stat.select(pl.col(\"site\").unique().sort()).collect()[\"site\"]\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.yscale('log') \n    plt.title(var)\n\n\n\n\n\n@interact(var = all_vars, site = all_sites)\ndef plot_var_dist(var, site, small=False):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var) & (pl.col(\"site\") == site)\n    ).filter(\n        pl.col(\"gap_len\") < 200 if small else True\n    ).collect().to_pandas().hist(\"gap_len\")\n    plt.title(f\"{site}: {var} - { 'gaps < 200' if small else 'all gaps'}\")\n\n\n\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var, small=False):\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") < 200 if small else True\n    ).collect().to_pandas().hist(\"gap_len\", bins=30)\n    plt.title(f\"{var} - { 'gaps < 200' if small else 'all gaps'}\")\n    plt.yscale('log')\n    plt.xscale('log') \n\n\n\n\n\nfrom fastai.vision.data import get_grid\nfrom pyprojroot import here\n\n\ndef plot_var_dist(var, small=False, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") < 200 if small else True\n    ).with_column(pl.col(\"gap_len\") / (24 *2 * 7)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - { 'gaps < 200' if small else 'all gaps'}\")\n    if not small: ax.set_yscale('log')\n    ax.set_xlabel(\"gap length (weeks)\")\n    ax.set_ylabel(f\"{'Log' if not small else ''} n gaps\")\n    # plt.xscale('log') \n\n\nfor ax, var in zip(get_grid(3,1,3, figsize=(15,8), sharey=True), ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC']):\n    plot_var_dist(var, ax=ax)\nplt.savefig(here(\"analysis/presentations/plots_18_jan/gap_len_dist.png\", warn=False))\n\n\n\n\n\ndef plot_var_dist_small(var, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") < (24 * 2 *7) \n    ).with_column(pl.col(\"gap_len\") / (24 *2)).collect().to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    ax.set_title(f\"{var} - gap len < 1 week\")\n    ax.set_xlabel(\"gap length (days)\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n\n\nfor ax, var in zip(get_grid(3,1,3, figsize=(15,8), sharey=True), ['TA_F_QC', 'SW_IN_F_QC', 'VPD_F_QC']):\n    plot_var_dist_small(var, ax=ax)\nplt.savefig(here(\"analysis/presentations/plots_18_jan/gap_len_dist_small.png\", warn=False))\n\n\n\n\n\ndef plot_var_dist_cum(var, ax=None):\n    if ax is None: ax = get_grid(1)[0]\n    \n    ta_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") < (24 * 2 *7) \n    ).collect() #.to_pandas().hist(\"gap_len\", bins=50, ax=ax)\n    bins = pl.cut(ta_gaps[\"gap_len\"], bins = pl.arange(0, 24 * 2 * 7, (24 * 2 * 7) // 50, eager=True))\n    return ta_gaps\n    ax.set_title(f\"{var} - gap len < 1 week\")\n    ax.set_xlabel(\"gap length (days)\")\n    ax.set_ylabel(f\"Log n gaps\")\n    ax.set_yscale('log') \n\n\n\n\n\nvar_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == 'TA_F_QC')\n    ).filter(\n        pl.col(\"gap_len\") < 20000\n    ).sort(pl.col(\"gap_len\")\n        \n    ).collect().to_pandas()\n\n\nalt.data_transformers.enable('data_server')\n\n\n \nalt.Chart(var_gaps).mark_boxplot().encode(\n    y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n    x='gap_len'\n)\n\n\nwidgets.IntSlider?\n\n\n@interact(var = all_vars)\ndef plot_var_dist(var, max_len=widgets.IntSlider(1000, 100, 20_000, 100)):\n    var_gaps = gap_stat.filter(\n        (pl.col(\"variable\") == var)\n    ).filter(\n        pl.col(\"gap_len\") < max_len\n    ).collect().to_pandas()\n    \n    display(alt.Chart(var_gaps).mark_boxplot().encode(\n        y=alt.Y('site', sort=alt.Sort(alt.EncodingSortField(field=\"y\", op=\"mean\", order=\"ascending\"))),\n        x='gap_len'\n    ))\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(pl.col(\"gap_len\").sum()).head().collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).groupby(\"variable\").agg(pl.col(\"gap_len\").mean()).collect()\n\n\ngap_stat.with_columns(\n    (pl.col(\"gap_len\") / pl.col(\"total_obs\")).alias(\"frac_gap\")\n).groupby([\"variable\", \"site\"]).agg(\n    pl.col(\"gap_len\").sum() / pl.col(\"total_obs\").first()\n).filter(pl.col(\"variable\") == \"TA_F_QC\").collect()"
  },
  {
    "objectID": "extract_gap_all_fluxnet.html",
    "href": "extract_gap_all_fluxnet.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.fluxnet.gap_finder import *\n\nlinks are obtained from this https://fluxnet.org/data-and-manifest/ page(requires login) by running in the browser console this code\nvar x = document.querySelectorAll(\"a\");\nvar myarray = []\nfor (var i=0; i<x.length; i++){\nvar nametext = x[i].textContent;\nvar cleantext = nametext.replace(/\\s+/g, ' ').trim();\nvar cleanlink = x[i].href;\nmyarray.push([cleantext,cleanlink]);\n};\nfunction make_table() {\n    var table = '<table><thead><th>Links</th></thead><tbody>';\n   for (var i=0; i<myarray.length; i++) {\n            table += '<tr><td>'+myarray[i][1]+'</td></tr>';\n    };\n \n    var w = window.open(\"\");\nw.document.write(table); \n}\nmake_table()\ncode inspired from https://towardsdatascience.com/quickly-extract-all-links-from-a-web-page-using-javascript-and-the-browser-console-49bb6f48127b\nand then doing some smart copy pasting\nacually download in parallel all files with, so is faster than download with python\nparallel -a fluxnet_parallel_wget.txt --jobs 10 wget\n\nfrom fluxnet_links import all_fluxnet_link\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\n\ntest_file = Path(\"../../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip\")\ntmp_dir = Path(\"/tmp\")\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\n\n\nsite_info = download_and_find_gaps(all_fluxnet_link, download_dir, out_dir, tmp_dir)\n\n\n\n\nFLX_AR-SLu_FLUXNET2015_FULLSET_HH_2009-2011_1-4\nFLX_AR-Vir_FLUXNET2015_FULLSET_HH_2009-2012_1-4\nFLX_AT-Neu_FLUXNET2015_FULLSET_HH_2002-2012_1-4\nFLX_AU-Ade_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_AU-ASM_FLUXNET2015_FULLSET_HH_2010-2014_2-4\nFLX_AU-Cpr_FLUXNET2015_FULLSET_HH_2010-2014_2-4\nFLX_AU-Cum_FLUXNET2015_FULLSET_HH_2012-2014_2-4\nFLX_AU-DaP_FLUXNET2015_FULLSET_HH_2007-2013_2-4\nFLX_AU-DaS_FLUXNET2015_FULLSET_HH_2008-2014_2-4\nFLX_AU-Dry_FLUXNET2015_FULLSET_HH_2008-2014_2-4\nFLX_AU-Emr_FLUXNET2015_FULLSET_HH_2011-2013_1-4\nFLX_AU-Fog_FLUXNET2015_FULLSET_HH_2006-2008_1-4\nFLX_AU-Gin_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_AU-GWW_FLUXNET2015_FULLSET_HH_2013-2014_1-4\nFLX_AU-How_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_AU-Lox_FLUXNET2015_FULLSET_HH_2008-2009_1-4\nFLX_AU-RDF_FLUXNET2015_FULLSET_HH_2011-2013_1-4\nFLX_AU-Rig_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_AU-Rob_FLUXNET2015_FULLSET_HH_2014-2014_1-4\nFLX_AU-Stp_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_AU-TTE_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_AU-Tum_FLUXNET2015_FULLSET_HR_2001-2014_2-4\nFLX_AU-Wac_FLUXNET2015_FULLSET_HH_2005-2008_1-4\nFLX_AU-Whr_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_AU-Wom_FLUXNET2015_FULLSET_HH_2010-2014_1-4\nFLX_AU-Ync_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_BE-Bra_FLUXNET2015_FULLSET_HH_1996-2014_2-4\nFLX_BE-Lon_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_BE-Vie_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_BR-Sa1_FLUXNET2015_FULLSET_HR_2002-2011_1-4\nFLX_BR-Sa3_FLUXNET2015_FULLSET_HH_2000-2004_1-4\nFLX_CA-Gro_FLUXNET2015_FULLSET_HH_2003-2014_1-4\nFLX_CA-Man_FLUXNET2015_FULLSET_HH_1994-2008_1-4\nFLX_CA-NS1_FLUXNET2015_FULLSET_HH_2001-2005_2-4\nFLX_CA-NS2_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-NS3_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-NS4_FLUXNET2015_FULLSET_HH_2002-2005_1-4\nFLX_CA-NS5_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-NS6_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-NS7_FLUXNET2015_FULLSET_HH_2002-2005_1-4\nFLX_CA-Oas_FLUXNET2015_FULLSET_HH_1996-2010_1-4\nFLX_CA-Obs_FLUXNET2015_FULLSET_HH_1997-2010_1-4\nFLX_CA-Qfo_FLUXNET2015_FULLSET_HH_2003-2010_1-4\nFLX_CA-SF1_FLUXNET2015_FULLSET_HH_2003-2006_1-4\nFLX_CA-SF2_FLUXNET2015_FULLSET_HH_2001-2005_1-4\nFLX_CA-SF3_FLUXNET2015_FULLSET_HH_2001-2006_1-4\nFLX_CA-TP1_FLUXNET2015_FULLSET_HH_2002-2014_2-4\nFLX_CA-TP2_FLUXNET2015_FULLSET_HH_2002-2007_1-4\nFLX_CA-TP3_FLUXNET2015_FULLSET_HH_2002-2014_1-4\nFLX_CA-TP4_FLUXNET2015_FULLSET_HH_2002-2014_1-4\nFLX_CA-TPD_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_CG-Tch_FLUXNET2015_FULLSET_HH_2006-2009_1-4\nFLX_CH-Cha_FLUXNET2015_FULLSET_HH_2005-2014_2-4\nFLX_CH-Dav_FLUXNET2015_FULLSET_HH_1997-2014_1-4\nFLX_CH-Fru_FLUXNET2015_FULLSET_HH_2005-2014_2-4\nFLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_CH-Oe1_FLUXNET2015_FULLSET_HH_2002-2008_2-4\nFLX_CH-Oe2_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_CN-Cha_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_CN-Cng_FLUXNET2015_FULLSET_HH_2007-2010_1-4\nFLX_CN-Dan_FLUXNET2015_FULLSET_HH_2004-2005_1-4\nFLX_CN-Din_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_CN-Du2_FLUXNET2015_FULLSET_HH_2006-2008_1-4\nFLX_CN-Du3_FLUXNET2015_FULLSET_HH_2009-2010_1-4\nFLX_CN-Ha2_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_CN-HaM_FLUXNET2015_FULLSET_HH_2002-2004_1-4\nFLX_CN-Qia_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_CN-Sw2_FLUXNET2015_FULLSET_HH_2010-2012_1-4\nFLX_CZ-BK1_FLUXNET2015_FULLSET_HH_2004-2014_2-4\nFLX_CZ-BK2_FLUXNET2015_FULLSET_HH_2004-2012_2-4\nFLX_CZ-wet_FLUXNET2015_FULLSET_HH_2006-2014_1-4\nFLX_DE-Akm_FLUXNET2015_FULLSET_HH_2009-2014_1-4\nFLX_DE-Geb_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_DE-Gri_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4\nFLX_DE-Kli_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_DE-Lkb_FLUXNET2015_FULLSET_HH_2009-2013_1-4\nFLX_DE-Lnf_FLUXNET2015_FULLSET_HH_2002-2012_1-4\nFLX_DE-Obe_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_DE-RuR_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_DE-RuS_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_DE-Seh_FLUXNET2015_FULLSET_HH_2007-2010_1-4\nFLX_DE-SfN_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_DE-Spw_FLUXNET2015_FULLSET_HH_2010-2014_1-4\nFLX_DE-Tha_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_DE-Zrk_FLUXNET2015_FULLSET_HH_2013-2014_2-4\nFLX_DK-Eng_FLUXNET2015_FULLSET_HH_2005-2008_1-4\nFLX_DK-Fou_FLUXNET2015_FULLSET_HH_2005-2005_1-4\nFLX_DK-Sor_FLUXNET2015_FULLSET_HH_1996-2014_2-4\nFLX_ES-Amo_FLUXNET2015_FULLSET_HH_2007-2012_1-4\nFLX_ES-LgS_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_ES-LJu_FLUXNET2015_FULLSET_HH_2004-2013_1-4\nFLX_ES-Ln2_FLUXNET2015_FULLSET_HH_2009-2009_1-4\nFLX_FI-Hyy_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_FI-Jok_FLUXNET2015_FULLSET_HH_2000-2003_1-4\nFLX_FI-Let_FLUXNET2015_FULLSET_HH_2009-2012_1-4\nFLX_FI-Lom_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_FI-Sod_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_FR-Fon_FLUXNET2015_FULLSET_HH_2005-2014_1-4\nFLX_FR-Gri_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_FR-LBr_FLUXNET2015_FULLSET_HH_1996-2008_1-4\nFLX_FR-Pue_FLUXNET2015_FULLSET_HH_2000-2014_2-4\nFLX_GF-Guy_FLUXNET2015_FULLSET_HH_2004-2014_2-4\nFLX_GH-Ank_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_GL-NuF_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_GL-ZaF_FLUXNET2015_FULLSET_HH_2008-2011_2-4\nFLX_GL-ZaH_FLUXNET2015_FULLSET_HH_2000-2014_2-4\nFLX_IT-BCi_FLUXNET2015_FULLSET_HH_2004-2014_2-4\nFLX_IT-CA1_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_IT-CA2_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_IT-CA3_FLUXNET2015_FULLSET_HH_2011-2014_2-4\nFLX_IT-Col_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_IT-Cp2_FLUXNET2015_FULLSET_HH_2012-2014_2-4\nFLX_IT-Cpz_FLUXNET2015_FULLSET_HH_1997-2009_1-4\nFLX_IT-Isp_FLUXNET2015_FULLSET_HH_2013-2014_1-4\nFLX_IT-La2_FLUXNET2015_FULLSET_HH_2000-2002_1-4\nFLX_IT-Lav_FLUXNET2015_FULLSET_HH_2003-2014_2-4\nFLX_IT-MBo_FLUXNET2015_FULLSET_HH_2003-2013_1-4\nFLX_IT-Noe_FLUXNET2015_FULLSET_HH_2004-2014_2-4\nFLX_IT-PT1_FLUXNET2015_FULLSET_HH_2002-2004_1-4\nFLX_IT-Ren_FLUXNET2015_FULLSET_HH_1998-2013_1-4\nFLX_IT-Ro1_FLUXNET2015_FULLSET_HH_2000-2008_1-4\nFLX_IT-Ro2_FLUXNET2015_FULLSET_HH_2002-2012_1-4\nFLX_IT-SR2_FLUXNET2015_FULLSET_HH_2013-2014_1-4\nFLX_IT-SRo_FLUXNET2015_FULLSET_HH_1999-2012_1-4\nFLX_IT-Tor_FLUXNET2015_FULLSET_HH_2008-2014_2-4\nFLX_JP-MBF_FLUXNET2015_FULLSET_HH_2003-2005_1-4\nFLX_JP-SMF_FLUXNET2015_FULLSET_HH_2002-2006_1-4\nFLX_MY-PSO_FLUXNET2015_FULLSET_HH_2003-2009_1-4\nFLX_NL-Hor_FLUXNET2015_FULLSET_HH_2004-2011_1-4\nFLX_NL-Loo_FLUXNET2015_FULLSET_HH_1996-2014_1-4\nFLX_PA-SPn_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_PA-SPs_FLUXNET2015_FULLSET_HH_2007-2009_1-4\nFLX_RU-Che_FLUXNET2015_FULLSET_HH_2002-2005_1-4\nFLX_RU-Cok_FLUXNET2015_FULLSET_HH_2003-2014_2-4\nFLX_RU-Fyo_FLUXNET2015_FULLSET_HH_1998-2014_2-4\nFLX_RU-Ha1_FLUXNET2015_FULLSET_HH_2002-2004_1-4\nFLX_SD-Dem_FLUXNET2015_FULLSET_HH_2005-2009_2-4\nFLX_SJ-Adv_FLUXNET2015_FULLSET_HH_2011-2014_1-4\nFLX_SJ-Blv_FLUXNET2015_FULLSET_HR_2008-2009_1-4\nFLX_SN-Dhr_FLUXNET2015_FULLSET_HH_2010-2013_1-4\nFLX_US-AR1_FLUXNET2015_FULLSET_HH_2009-2012_1-4\nFLX_US-AR2_FLUXNET2015_FULLSET_HH_2009-2012_1-4\nFLX_US-ARb_FLUXNET2015_FULLSET_HH_2005-2006_1-4\nFLX_US-ARc_FLUXNET2015_FULLSET_HH_2005-2006_1-4\nFLX_US-ARM_FLUXNET2015_FULLSET_HH_2003-2012_1-4\nFLX_US-Atq_FLUXNET2015_FULLSET_HH_2003-2008_1-4\nFLX_US-Blo_FLUXNET2015_FULLSET_HH_1997-2007_1-4\nFLX_US-Cop_FLUXNET2015_FULLSET_HR_2001-2007_1-4\nFLX_US-CRT_FLUXNET2015_FULLSET_HH_2011-2013_1-4\nFLX_US-GBT_FLUXNET2015_FULLSET_HH_1999-2006_1-4\nFLX_US-GLE_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_US-Goo_FLUXNET2015_FULLSET_HH_2002-2006_1-4\nFLX_US-Ha1_FLUXNET2015_FULLSET_HR_1991-2012_1-4\nFLX_US-IB2_FLUXNET2015_FULLSET_HH_2004-2011_1-4\nFLX_US-Ivo_FLUXNET2015_FULLSET_HH_2004-2007_1-4\nFLX_US-KS1_FLUXNET2015_FULLSET_HH_2002-2002_1-4\nFLX_US-KS2_FLUXNET2015_FULLSET_HH_2003-2006_1-4\nFLX_US-Lin_FLUXNET2015_FULLSET_HH_2009-2010_1-4\nFLX_US-Los_FLUXNET2015_FULLSET_HH_2000-2014_2-4\nFLX_US-LWW_FLUXNET2015_FULLSET_HH_1997-1998_1-4\nFLX_US-Me1_FLUXNET2015_FULLSET_HH_2004-2005_1-4\nFLX_US-Me2_FLUXNET2015_FULLSET_HH_2002-2014_1-4\nFLX_US-Me3_FLUXNET2015_FULLSET_HH_2004-2009_1-4\nFLX_US-Me4_FLUXNET2015_FULLSET_HH_1996-2000_1-4\nFLX_US-Me5_FLUXNET2015_FULLSET_HH_2000-2002_1-4\nFLX_US-Me6_FLUXNET2015_FULLSET_HH_2010-2014_2-4\nFLX_US-MMS_FLUXNET2015_FULLSET_HR_1999-2014_1-4\nFLX_US-Myb_FLUXNET2015_FULLSET_HH_2010-2014_2-4\nFLX_US-Ne1_FLUXNET2015_FULLSET_HR_2001-2013_1-4\nFLX_US-Ne2_FLUXNET2015_FULLSET_HR_2001-2013_1-4\nFLX_US-Ne3_FLUXNET2015_FULLSET_HR_2001-2013_1-4\nFLX_US-NR1_FLUXNET2015_FULLSET_HH_1998-2014_1-4\nFLX_US-Oho_FLUXNET2015_FULLSET_HH_2004-2013_1-4\nFLX_US-ORv_FLUXNET2015_FULLSET_HH_2011-2011_1-4\nFLX_US-PFa_FLUXNET2015_FULLSET_HR_1995-2014_1-4\nFLX_US-Prr_FLUXNET2015_FULLSET_HH_2010-2014_1-4\nFLX_US-SRC_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_US-SRG_FLUXNET2015_FULLSET_HH_2008-2014_1-4\nFLX_US-SRM_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_US-Sta_FLUXNET2015_FULLSET_HH_2005-2009_1-4\nFLX_US-Syv_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_US-Ton_FLUXNET2015_FULLSET_HH_2001-2014_1-4\nFLX_US-Tw1_FLUXNET2015_FULLSET_HH_2012-2014_1-4\nFLX_US-Tw2_FLUXNET2015_FULLSET_HH_2012-2013_1-4\nFLX_US-Tw3_FLUXNET2015_FULLSET_HH_2013-2014_2-4\nFLX_US-Tw4_FLUXNET2015_FULLSET_HH_2013-2014_1-4\nFLX_US-Twt_FLUXNET2015_FULLSET_HH_2009-2014_1-4\nFLX_US-UMB_FLUXNET2015_FULLSET_HR_2000-2014_1-4\nFLX_US-UMd_FLUXNET2015_FULLSET_HH_2007-2014_1-4\nFLX_US-Var_FLUXNET2015_FULLSET_HH_2000-2014_1-4\nFLX_US-WCr_FLUXNET2015_FULLSET_HH_1999-2014_1-4\nFLX_US-Whs_FLUXNET2015_FULLSET_HH_2007-2014_1-4\nFLX_US-Wi0_FLUXNET2015_FULLSET_HH_2002-2002_1-4\nFLX_US-Wi1_FLUXNET2015_FULLSET_HH_2003-2003_1-4\nFLX_US-Wi2_FLUXNET2015_FULLSET_HH_2003-2003_1-4\nFLX_US-Wi3_FLUXNET2015_FULLSET_HH_2002-2004_1-4\nFLX_US-Wi4_FLUXNET2015_FULLSET_HH_2002-2005_1-4\nFLX_US-Wi5_FLUXNET2015_FULLSET_HH_2004-2004_1-4\nFLX_US-Wi6_FLUXNET2015_FULLSET_HH_2002-2003_1-4\nFLX_US-Wi7_FLUXNET2015_FULLSET_HH_2005-2005_1-4\nFLX_US-Wi8_FLUXNET2015_FULLSET_HH_2002-2002_1-4\nFLX_US-Wi9_FLUXNET2015_FULLSET_HH_2004-2005_1-4\nFLX_US-Wkg_FLUXNET2015_FULLSET_HH_2004-2014_1-4\nFLX_US-WPT_FLUXNET2015_FULLSET_HH_2011-2013_1-4\nFLX_ZM-Mon_FLUXNET2015_FULLSET_HH_2000-2009_2-4\n\n\n\nsite_info\n\nshape: (206, 3)\nstart\nend\nsite\ni64\ni64\nstr\n200901010030\n201201010000\n\"AR-SLu\"\n200901010030\n201301010000\n\"AR-Vir\"\n200201010030\n201301010000\n\"AT-Neu\"\n200701010030\n201001010000\n\"AU-Ade\"\n201001010030\n201501010000\n\"AU-ASM\"\n201001010030\n201501010000\n\"AU-Cpr\"\n201201010030\n201501010000\n\"AU-Cum\"\n200701010030\n201401010000\n\"AU-DaP\"\n200801010030\n201501010000\n\"AU-DaS\"\n200801010030\n201501010000\n\"AU-Dry\"\n201101010030\n201401010000\n\"AU-Emr\"\n200601010030\n200901010000\n\"AU-Fog\"\n...\n...\n...\n200301010030\n200401010000\n\"US-Wi1\"\n200301010030\n200401010000\n\"US-Wi2\"\n200201010030\n200501010000\n\"US-Wi3\"\n200201010030\n200601010000\n\"US-Wi4\"\n200401010030\n200501010000\n\"US-Wi5\"\n200201010030\n200401010000\n\"US-Wi6\"\n200501010030\n200601010000\n\"US-Wi7\"\n200201010030\n200301010000\n\"US-Wi8\"\n200401010030\n200601010000\n\"US-Wi9\"\n200401010030\n201501010000\n\"US-Wkg\"\n201101010030\n201401010000\n\"US-WPT\"\n200001010030\n201001010000\n\"ZM-Mon\"\n\n\n\nsite_info.write_parquet(out_dir / \"../site_info.parquet\")"
  },
  {
    "objectID": "Simple GP Hainich.html",
    "href": "Simple GP Hainich.html",
    "title": "Simple GP Hainich",
    "section": "",
    "text": "imputation using simple GP\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.simple_gp_imputation import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import *\n\n\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n0.102\n\n\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n1.636\n\n\n\n\n\n\n\n\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(20, meteo_vars.values(), 60)\n\n\ndata = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values(), 60)\nres = SimpleGPImputationExplorer(data.data).fit().to_result(data.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres.display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9186\n\n\nSW_IN\n0.9267\n\n\nVPD\n0.9880\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.2635\nÂ°C\n\n\nSW_IN\n10.0560\nW m-2\n\n\nVPD\n0.0280\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-9.1010\n\n\nSW_IN\n0.7756\n\n\nVPD\n0.5680\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.8120\nÂ°C\n\n\nSW_IN\n30.2152\nW m-2\n\n\nVPD\n0.0796\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n4.7362\n\n\nSW_IN\n3.5688\n\n\nVPD\n2.6062\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.6055\n\n\nSW_IN\n1.4438\n\n\nVPD\n0.7713\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0057\n\n\nSW_IN\n0.0237\n\n\nVPD\n0.0046\n\n\n\n\n\n\n\ndata = GPFADataTest(hai[:150]).add_gap(20, meteo_vars.values(), 60)\nres = SimpleGPImputationExplorer(data.data).fit().to_result(data.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres.display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9600\n\n\nSW_IN\n0.3226\n\n\nVPD\n0.9833\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1846\nÂ°C\n\n\nSW_IN\n30.5714\nW m-2\n\n\nVPD\n0.0330\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-0.9281\n\n\nSW_IN\n-0.9669\n\n\nVPD\n0.5433\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.4883\nÂ°C\n\n\nSW_IN\n83.2934\nW m-2\n\n\nVPD\n0.0841\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n4.5675\n\n\nSW_IN\n3.7809\n\n\nVPD\n2.6307\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.6024\n\n\nSW_IN\n0.7026\n\n\nVPD\n0.7619\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0040\n\n\nSW_IN\n0.0249\n\n\nVPD\n0.0045\n\n\n\n\n\n\n\n\n\nres_r_gaps = SimpleGPImputationExplorer(data_r_gaps.data).fit().to_result(data_r_gaps.data_compl_tidy, units=units)\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n\n\n\n\n\n\n\n\n\nres_r_gaps.display_results(plot_args = {'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9911\n\n\nSW_IN\n0.9661\n\n\nVPD\n0.9675\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0873\nÂ°C\n\n\nSW_IN\n6.8369\nW m-2\n\n\nVPD\n0.0461\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9813\n\n\nSW_IN\n0.9574\n\n\nVPD\n0.9504\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1222\nÂ°C\n\n\nSW_IN\n10.1436\nW m-2\n\n\nVPD\n0.0613\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n6.4697\n\n\nSW_IN\n5.0863\n\n\nVPD\n5.3424\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.7881\n\n\nSW_IN\n1.6644\n\n\nVPD\n1.1420\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0068\n\n\nSW_IN\n0.0376\n\n\nVPD\n0.0305\n\n\n\n\n\n\n\n\n\n\nres_c_gaps = SimpleGPImputationExplorer(data_c_gaps.data).fit().to_result(data_c_gaps.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres_c_gaps.display_results(plot_args = {'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9500\n\n\nSW_IN\n0.9746\n\n\nVPD\n0.9401\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.2064\nÂ°C\n\n\nSW_IN\n5.9171\nW m-2\n\n\nVPD\n0.0626\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n-8.4622\n\n\nSW_IN\n0.1705\n\n\nVPD\n-1065.0841\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.6429\nÂ°C\n\n\nSW_IN\n10.5049\nW m-2\n\n\nVPD\n0.1929\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n3.0968\n\n\nSW_IN\n3.9848\n\n\nVPD\n2.5715\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.6266\n\n\nSW_IN\n0.7329\n\n\nVPD\n0.7777\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0028\n\n\nSW_IN\n0.0239\n\n\nVPD\n0.0064\n\n\n\n\n\n\n\n\n\n\ntry with a different random seed\n\nreset_seed(101)\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values())\n\n\n\n\nres_r_gaps = SimpleGPImputationExplorer(data_r_gaps.data).fit().to_result(data_r_gaps.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres_r_gaps.display_results(plot_args = {'bind_interaction': False})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.9671\n\n\nVPD\n0.9646\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0849\nÂ°C\n\n\nSW_IN\n6.7353\nW m-2\n\n\nVPD\n0.0481\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.9149\n\n\nVPD\n0.9433\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.0848\nÂ°C\n\n\nSW_IN\n9.2910\nW m-2\n\n\nVPD\n0.0680\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n6.4669\n\n\nSW_IN\n4.6214\n\n\nVPD\n4.9073\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.7311\n\n\nSW_IN\n0.9371\n\n\nVPD\n0.8954\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0094\n\n\nSW_IN\n0.0193\n\n\nVPD\n0.0215\n\n\n\n\n\n\n\n\n\n\nres_c_gaps = SimpleGPImputationExplorer(data_c_gaps.data).fit().to_result(data_c_gaps.data_compl_tidy, units=units)\n\n\n\n\n\n\n\n\n\n\n\nres_c_gaps.display_results(plot_args = {'bind_interaction': False, 'properties': {}})\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9849\n\n\nSW_IN\n0.9654\n\n\nVPD\n0.9291\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.1135\nÂ°C\n\n\nSW_IN\n6.9104\nW m-2\n\n\nVPD\n0.0680\nhPa\n\n\n\n\n\nr2 - Only GAP\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.2640\n\n\nSW_IN\n0.0000\n\n\nVPD\n-9.7505\n\n\n\n\n\nRMSE - Only GAP\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.3417\nÂ°C\n\n\nSW_IN\n14.0756\nW m-2\n\n\nVPD\n0.2107\nhPa\n\n\n\n\n\n\n\n\n\nvariable\nlengthscale\n\n\n\n\nTA\n3.0464\n\n\nSW_IN\n3.9805\n\n\nVPD\n2.5759\n\n\n\n\noutputscale\n\n\n\nvariable\noutputscale\n\n\n\n\nTA\n0.6008\n\n\nSW_IN\n0.7517\n\n\nVPD\n0.7584\n\n\n\n\n\nlikelihood\n\n\n\nvariable\nnoise\n\n\n\n\nTA\n0.0025\n\n\nSW_IN\n0.0284\n\n\nVPD\n0.0061"
  },
  {
    "objectID": "exploration/pytorch_constraints.html",
    "href": "exploration/pytorch_constraints.html",
    "title": "Meteo Imputation",
    "section": "",
    "text": "need to be sure that the matrix is a positive semi-definite, so we can use choleshy decomposition where if \\(A\\) is positive semidefinite it can be decomposed as \\(A=LL^T\\) where \\(L\\) is a lower triangular matrix. Therefore the idea is to have as the raw parameter \\(L\\) which weâll enforce to be a lower triangular matrix\n\nimport torch\n\n\nR = torch.rand(3,3, requires_grad=True) \n\n\nL = torch.tril(R)\n\n\nL.sum().backward() \n\n\nL\n\ntensor([[0.6097, 0.0000, 0.0000],\n        [0.6872, 0.5714, 0.0000],\n        [0.6097, 0.9560, 0.4841]], grad_fn=<TrilBackward0>)\n\n\n\ntorch.distributions.MultivariateNormal(torch.zeros(3), R @ R.T)\n\nMultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3]))\n\n\n\nfor i in range(10_000):\n    R = torch.rand(3,3)\n    cov = R @ R.T + (torch.eye(3) * 1e-7)\n    torch.distributions.MultivariateNormal(torch.zeros(3), cov)\n\n\nfor i in range(10_000):\n    R = torch.rand(3,3)\n    L = torch.tril(R)\n    torch.distributions.MultivariateNormal(torch.zeros(3), L @ L.T)\n\nValueError: Expected parameter covariance_matrix (Tensor of shape (3, 3)) of distribution MultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[0.3166, 0.4638, 0.4075],\n        [0.4638, 0.6800, 0.6019],\n        [0.4075, 0.6019, 0.5586]])\n\n\n\nfrom gpytorch.constraints import Positive\nimport torch\n\n\ncov = torch.eye(3)\n\n\nconstraint = Positive()\n\n\nraw = constraint.inverse_transform(cov)\nraw\n\ntensor([[0.5413,   -inf,   -inf],\n        [  -inf, 0.5413,   -inf],\n        [  -inf,   -inf, 0.5413]])\n\n\n\nconstraint.transform(raw)\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n\n\n\nconstraint.inverse_transform(cov + torch.fill(cov, 1e-7))\n\ntensor([[  0.5413, -16.1181, -16.1181],\n        [-16.1181,   0.5413, -16.1181],\n        [-16.1181, -16.1181,   0.5413]])\n\n\n\nraw2 = -torch.ones(3,3)\n\n\nraw2\n\ntensor([[-1., -1., -1.],\n        [-1., -1., -1.],\n        [-1., -1., -1.]])\n\n\n\nconstraint.transform(raw2)\n\ntensor([[0.3133, 0.3133, 0.3133],\n        [0.3133, 0.3133, 0.3133],\n        [0.3133, 0.3133, 0.3133]])\n\n\n\n\n\nfrom meteo_imp.kalman.filter import *\n\n\nimport torch\n\n\nA = torch.rand(100, 100)\n\n\nA.dtype\n\ntorch.float32\n\n\n\nsymmetric_upto(A * A.T, -10)\n\n-10\n\n\n\ntorch.tril(A) * torch.tril(A).T\n\ntensor([[0.0088, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.1427, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.5120,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.0000, 0.0000, 0.0000,  ..., 0.1121, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3522, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2634]])"
  },
  {
    "objectID": "exploration/log_likelihood.html",
    "href": "exploration/log_likelihood.html",
    "title": "Exploration of Loglikelihood computations",
    "section": "",
    "text": "This notebook is not running yet\n\nX[0]\n\ntensor(1)\n\n\n\nk.loglikelihood(X, smooth=False)\n\ntensor(-3.7474)\n\n\n\nfrom scipy import linalg\n\n\ndef log_multivariate_normal_density(X, means, covars, min_covar=1.e-7):\n    \"\"\"Log probability for full covariance matrices. \"\"\"\n    if hasattr(linalg, 'solve_triangular'):\n        # only in scipy since 0.9\n        solve_triangular = linalg.solve_triangular\n    else:\n        # slower, but works\n        solve_triangular = linalg.solve\n    n_samples, n_dim = X.shape\n    nmix = len(means)\n    log_prob = np.empty((n_samples, nmix))\n    for c, (mu, cv) in enumerate(zip(means, covars)):\n        try:\n            cv_chol = linalg.cholesky(cv, lower=True)\n        except linalg.LinAlgError:\n            # The model is most probabily stuck in a component with too\n            # few observations, we need to reinitialize this components\n            cv_chol = linalg.cholesky(cv + min_covar * np.eye(n_dim),\n                                      lower=True)\n        cv_log_det = 2 * np.sum(np.log(np.diagonal(cv_chol)))\n        cv_sol = solve_triangular(cv_chol, (X - mu).T, lower=True).T\n        log_prob[:, c] = - .5 * (np.sum(cv_sol ** 2, axis=1) + \\\n                                     n_dim * np.log(2 * np.pi) + cv_log_det)\n\n    return log_prob\n\n\nlog_multivariate_normal_density(np.ones((1,1)), np.zeros(1), np.eye(1))\n\narray([[-1.41893853]])\n\n\n\nMultivariateNormal(torch.zeros(1), torch.eye(1)).log_prob(torch.ones((1)))\n\ntensor(-1.4189)\n\n\n\nlog_multivariate_normal_density(np.ones((1, 2)), np.zeros((1,2)), np.expand_dims(np.eye(2), 0))\n\narray([[-2.83787707]])\n\n\n\nMultivariateNormal(torch.zeros(2), torch.eye(2)).log_prob(torch.ones((2)))\n\ntensor(-2.8379)\n\n\npytorch and pykalman have the same results for computing the loglikelihood of a gaussian distribution\n\ndef _torch_loglikelihoods(observation_matrices, observation_offsets,\n                    observation_covariance, predicted_state_means,\n                    predicted_state_covariances, observations):\n    \n    n_timesteps = observations.shape[0]\n    loglikelihoods = np.zeros(n_timesteps)\n    for t in range(n_timesteps):\n        observation = observations[t]\n        observation_matrix = _last_dims(observation_matrices, t)\n        observation_offset = _last_dims(observation_offsets, t, ndims=1)\n        predicted_state_mean = _last_dims(\n            predicted_state_means, t, ndims=1\n        )\n        predicted_state_covariance = _last_dims(\n            predicted_state_covariances, t\n        )\n\n        predicted_observation_mean = (\n            observation_matrix @ predicted_state_mean\n            + observation_offset\n        )\n        predicted_observation_covariance = (\n            observation_matrix @ predicted_state_covariance @ observation_matrix.T\n            + observation_covariance\n        )\n\n        loglikelihoods[t] = MultivariateNormal(\n            predicted_observation_mean,\n            predicted_observation_covariance\n        ).log_prob(observation.unsqueeze(0))\n    return loglikelihoods.sum()\n\n\nstate = k.filter(X)\n\n\npyk.loglikelihood??\n\n\nSignature: pyk.loglikelihood(X)\nSource:   \n    def loglikelihood(self, X):\n        \"\"\"Calculate the log likelihood of all observations\n        Parameters\n        ----------\n        X : [n_timesteps, n_dim_obs] array\n            observations for time steps [0...n_timesteps-1]\n        Returns\n        -------\n        likelihood : float\n            likelihood of all observations\n        \"\"\"\n        Z = np.array(self._parse_observations(X))\n        # initialize parameters\n        (transition_matrices, transition_offsets,\n         transition_covariance, observation_matrices,\n         observation_offsets, observation_covariance,\n         initial_state_mean, initial_state_covariance) = (\n            self._initialize_parameters()\n        )\n        # apply the Kalman Filter\n        (predicted_state_means, predicted_state_covariances,\n         kalman_gains, filtered_state_means,\n         filtered_state_covariances) = (\n            _filter(\n                transition_matrices, observation_matrices,\n                transition_covariance, observation_covariance,\n                transition_offsets, observation_offsets,\n                initial_state_mean, initial_state_covariance,\n                Z\n            )\n        )\n        # get likelihoods for each time step\n        loglikelihoods = _loglikelihoods(\n          observation_matrices, observation_offsets, observation_covariance,\n          predicted_state_means, predicted_state_covariances, Z\n        )\n        return np.sum(loglikelihoods)\nFile:      ~/anaconda3/envs/data-science/lib/python3.10/site-packages/pykalman/standard.py\nType:      method\n\n\n\n\n\nnX = X.numpy()\n\n\npred_mean, pred_cov = pyk.filter(nX)\n\n\npyk.loglikelihood(nX)\n\n-5.231597970652478\n\n\n\nZ = np.array(pyk._parse_observations(nX))\n\n\n(transition_matrices, transition_offsets,\n         transition_covariance, observation_matrices,\n         observation_offsets, observation_covariance,\n         initial_state_mean, initial_state_covariance) = (\n            pyk._initialize_parameters()\n        )\n\n\n(predicted_state_means, predicted_state_covariances,\n         kalman_gains, filtered_state_means,\n         filtered_state_covariances) = (\n            pykalman.standard._filter(\n                transition_matrices, observation_matrices,\n                transition_covariance, observation_covariance,\n                transition_offsets, observation_offsets,\n                initial_state_mean, initial_state_covariance,\n                Z\n            )\n        )\n\n\npykalman.standard._loglikelihoods(\n observation_matrices, observation_offsets, observation_covariance,\n          pred_mean, pred_cov, Z\n).sum()\n\n-3.7473859589713614\n\n\n\n_torch_loglikelihoods(\n    k.obs_matrices,\n    k.obs_offsets,\n    k.obs_cov,\n    state.mean,\n    state.cov,\n    X\n)\n\n-3.747385859489441\n\n\n\npykalman.standard._loglikelihoods(\n    k.obs_matrices.numpy(),\n    k.obs_offsets.numpy(),\n    k.obs_cov.numpy(),\n    state.mean.numpy(),\n    state.cov.numpy(),\n    Z\n).sum()\n\n-3.747385949780805\n\n\n\npyk.loglikelihood(Z)\n\n-5.231597970652478\n\n\n\ntest_close(pyk.filter(nX), k.filter(X))\n\n\npyk.loglikelihood(nX)\n\n-5.231597970652478\n\n\n\n\n\nfrom torch.distributions import MultivariateNormal\nimport torch\nfrom meteo_imp.gaussian import to_posdef\n\n\nn = 5\n\n\ncov = to_posdef(torch.rand(n,n))\nmean = torch.rand(n)\n\n\ndist = MultivariateNormal(mean, cov)\n\n\nobs = dist.sample()\n\n\nobs\n\ntensor([1.0237, 0.9482, 0.9448, 1.5264, 1.2376])\n\n\n\ndist.log_prob(obs)\n\ntensor(-3.6350)\n\n\n\ndist2 = MultivariateNormal(mean, torch.diag(cov.diag()))\n\n\ndist2.covariance_matrix\n\ntensor([[0.9555, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 1.1688, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 1.7292, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 2.1713, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 1.0093]])\n\n\n\ndist2.log_prob(obs)\n\ntensor(-6.6010)\n\n\nso we see that if there is a diagonal covariance the likelihood is the same of the sum of the likelihoods of the individual variables\n\ntot = 0\nfor i in range(n):\n    dist_n = MultivariateNormal(mean[i:i+1], cov[i:i+1, i:i+1])\n    tot += dist_n.log_prob(obs[i:i+1])\n\n\ntot\n\ntensor(-6.6010)\n\n\n\ndist_n.log_prob(obs[i:i+1])\n\ntensor(-1.3723)\n\n\nperformance big ll vs individual\n\nn = 5000\n\n\nstd = torch.rand(n)\nmean = torch.rand(n)\n\n\ndist = MultivariateNormal(mean, torch.diag(std))\n\n\nobs = dist.sample()\n\n\n%timeit dist.log_prob(obs)\n\n2.61 ms Â± 134 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n\n\n\ndist.log_prob(obs)\n\ntensor(-4552.8486)\n\n\n\ndef single_ll():\n    tot = 0\n    for i in range(n):\n        dist_n = MultivariateNormal(mean[i:i+1], torch.diag(std[i:i+1]))\n        tot += dist_n.log_prob(obs[i:i+1])\n\n\ntot\n\ntensor(-6.6010)\n\n\n\n%timeit single_ll()\n\n1.49 s Â± 164 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n\n\n\ndist = MultivariateNormal(mean.cuda(), torch.diag(std).cuda())\n\n\nobs = dist.sample()\n\n\n%timeit dist.log_prob(obs)\n\n630 Âµs Â± 3.43 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "Multi latent - GPFA Hainich.html",
    "href": "Multi latent - GPFA Hainich.html",
    "title": "Multiple Latent",
    "section": "",
    "text": "Trying to use more than 1 latent variable\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\nfrom meteo_imp.results import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\n\ndef reset_seed():\n    torch.manual_seed(27);\n    np.random.seed(27);\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n302.475\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n\n\n...\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n330.202\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n330.202\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n330.202\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n\n\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values())\n\n\ncache_file_gaps = cache_path / \"hai_diff_latents.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\n@cache_disk(cache_file_gaps)\ndef compute_multiple_latent():\n    hai_r_gaps = [GPFAImputation(\n        data_r_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_r_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    hai_c_gaps = [GPFAImputationExplorer(\n        data_c_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_c_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    return hai_r_gaps, hai_c_gaps\n\n\nhai_r_gaps, hai_c_gaps = compute_multiple_latent()\n\n\n\n\n\n\nhai_r_gaps\n\n[<gpfa_imputation.results.ImputationResult>,\n <gpfa_imputation.results.ImputationResult>,\n <gpfa_imputation.results.ImputationResult>]\n\n\n\nhai_r_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6601\n\n\nSW_IN\n0.0865\n\n\nLW_IN\n-0.0341\n\n\nVPD\n0.9569\n\n\n\n\nRMSE\n\n\n\nvariable\nrmse\nunits\n\n\n\n\nTA\n0.5224\nÂ°C\n\n\nSW_IN\n46.1695\nW m-2\n\n\nLW_IN\n21.2045\nW m-2\n\n\nVPD\n0.0578\nhPa\n\n\n\n\n\n\n\n\n\n0\nz0\n\n\n\n\nTA\n0.8015\n\n\nSW_IN\n0.4251\n\n\nLW_IN\n-0.1879\n\n\nVPD\n1.0596\n\n\n\n\nlengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.6057\n\n\n\n\n\npsi\n\n\n\nvariable\npsi\n\n\n\n\nTA\n0.4174\n\n\nSW_IN\n0.8049\n\n\nLW_IN\n0.9361\n\n\nVPD\n0.0103\n\n\n\n\n\nlikelihood\n\n\n\nnoise\n\n\n\n\n0.0224\n\n\n\n\n\n\n\nai_r_gaps[0].plot_pred()\n\n\nhai_r_gaps[1].display_results()\n\n\nhai_r_gaps[2].display_results()\n\n\n\n\n\nhai_c_gaps[0].display_results()\n\n\nhai_c_gaps[1].display_results()\n\n\nhai_c_gaps[2].display_results()"
  },
  {
    "objectID": "GPFA Hainich - multi latent var.html",
    "href": "GPFA Hainich - multi latent var.html",
    "title": "Multiple latent â¦",
    "section": "",
    "text": "Trying to use more than 1 latent variable\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\n\nfrom meteo_imp.gpfa.imputation import *\nfrom meteo_imp.data_preparation import *\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyprojroot import here\nimport matplotlib.pyplot as plt\n\nfrom meteo_imp.utils import cache_disk\n\n\ndef reset_seed():\n    torch.manual_seed(27);\n    np.random.seed(27);\n\n\ntorch.manual_seed(27);\nnp.random.seed(27);\ncache_path = here() / \".cache\"\n\n\n\ntake the first 200 rows from the Hainich dataset\n\nhai_path = Path(\"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\nhai_raw = pd.read_csv(here(\"data\") / hai_path, na_values=[\"-9999\", \"-9999.99\"], parse_dates=[0, 1], nrows=200)\n\n\nmeteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\nunits = {\n    'TA': 'Â°C',\n    'SW_IN': 'W m-2',\n    'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai = (hai_raw\n       .rename(columns=meteo_vars)\n       .set_index(\"TIMESTAMP_END\")\n       .loc[:, meteo_vars.values()])\nhai\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n302.475\n0.222\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.090\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.110\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n\n\n...\n...\n...\n...\n...\n\n\n2000-01-05 02:00:00\n4.74\n0.0\n330.202\n1.191\n\n\n2000-01-05 02:30:00\n4.75\n0.0\n330.202\n1.057\n\n\n2000-01-05 03:00:00\n4.76\n0.0\n330.202\n0.935\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n\n\n\n\n\n\n\nmakes here all the slow computations and cache them on disk\n\nreset_seed()\ndata_r_gaps = GPFADataTest(hai[:150]).add_random_missing()\ndata_c_gaps = GPFADataTest(hai[:150]).add_gap(15, meteo_vars.values())\n\n\ncache_file_gaps = cache_path / \"hai_diff_latents.pickle\"\n# cache_file_gaps.unlink() # uncomment this line to reset the cache\n\n\n@cache_disk(cache_file_gaps)\ndef compute_multiple_latent():\n    hai_r_gaps = [GPFAImputationExplorer(\n        data_r_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_r_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    hai_c_gaps = [GPFAImputationExplorer(\n        data_c_gaps.data, latent_dims=i)\n                  .fit()\n                  .to_result(data_c_gaps.data_compl_tidy, units=units)\n                  for i in range(1,4)]\n    return hai_r_gaps, hai_c_gaps\n\n\nhai_r_gaps, hai_c_gaps = compute_multiple_latent()\n\n\n\n\n\n\nhai_r_gaps\n\n[<gpfa_imputation.imputation.ImputationResult>,\n <gpfa_imputation.imputation.ImputationResult>,\n <gpfa_imputation.imputation.ImputationResult>]\n\n\n\nhai_r_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.6243\n\n\nSW_IN\n0.1244\n\n\nLW_IN\n0.0072\n\n\nVPD\n0.9597\n\n\n\n\nÎ\n\n\n\nvariable\nz0\n\n\n\n\ntime\n0.8015\n\n\nvariable\n0.4251\n\n\nmean\n-0.1879\n\n\nstd\n1.0596\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.6057\n\n\n\n\n\n\n\nhai_r_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9913\n\n\nSW_IN\n0.1884\n\n\nLW_IN\n0.9673\n\n\nVPD\n0.5811\n\n\n\n\nÎ\n\n\n\nvariable\nz0\nz1\n\n\n\n\ntime\n0.8553\n0.5997\n\n\nvariable\n0.3110\n-0.1375\n\n\nmean\n-0.3717\n0.6710\n\n\nstd\n0.7057\n0.2981\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n7.0575\n\n\nz1\n7.4311\n\n\n\n\n\n\n\nhai_r_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9915\n\n\nSW_IN\n0.9635\n\n\nLW_IN\n0.9679\n\n\nVPD\n0.6259\n\n\n\n\nÎ\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\ntime\n0.8547\n0.4558\n-0.0728\n\n\nvariable\n0.4540\n-0.3101\n1.2743\n\n\nmean\n-0.2829\n0.7159\n0.0165\n\n\nstd\n0.7268\n0.1550\n0.2461\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.8582\n\n\nz1\n7.3777\n\n\nz2\n5.3624\n\n\n\n\n\n\n\n\n\n\nhai_c_gaps[0].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9793\n\n\nSW_IN\n0.0152\n\n\nLW_IN\n-0.0507\n\n\nVPD\n0.5711\n\n\n\n\nÎ\n\n\n\nvariable\nz0\n\n\n\n\ntime\n0.8542\n\n\nvariable\n0.1202\n\n\nmean\n0.1860\n\n\nstd\n0.6229\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n6.0641\n\n\n\n\n\n\n\nhai_c_gaps[1].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9789\n\n\nSW_IN\n0.1882\n\n\nLW_IN\n0.9552\n\n\nVPD\n0.6114\n\n\n\n\nÎ\n\n\n\nvariable\nz0\nz1\n\n\n\n\ntime\n0.5927\n0.6728\n\n\nvariable\n0.4136\n-0.0936\n\n\nmean\n-0.5082\n0.5232\n\n\nstd\n0.5586\n0.4200\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n5.8459\n\n\nz1\n6.5770\n\n\n\n\n\n\n\nhai_c_gaps[2].display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\n\n\nvariable\nr2\n\n\n\n\nTA\n0.9785\n\n\nSW_IN\n0.9640\n\n\nLW_IN\n0.9562\n\n\nVPD\n0.6513\n\n\n\n\nÎ\n\n\n\nvariable\nz0\nz1\nz2\n\n\n\n\ntime\n-0.0527\n-0.4903\n0.7231\n\n\nvariable\n0.8765\n0.1260\n0.2360\n\n\nmean\n-0.0042\n-0.6728\n-0.3668\n\n\nstd\n0.2132\n-0.2787\n0.5878\n\n\n\n\n\nLengthscale\n\n\n\nlatent\nlengthscale\n\n\n\n\nz0\n4.5862\n\n\nz1\n6.9706\n\n\nz2\n5.3607"
  }
]