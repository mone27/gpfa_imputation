{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| default_exp gpfa.gpfa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gaussian Processes Factor Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Derivation of the equations to solve the Gaussian Processes Factor Analysis as described in: Yu, B.M., Cunningham, J.P., Santhanam, G., Ryu, S., Shenoy, K.V., Sahani, M., 2008. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity, in: Advances in Neural Information Processing Systems. Curran Associates, Inc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notation\n",
    "\n",
    "- $T$  Number of time steps\n",
    "- $N$  Number of variables observed\n",
    "- $K$  Number of dimensions of latent variable\n",
    "- $x_{:,t}$ vector of all the $N$ variables at time $t$, $\\in \\mathbb{R}^N$\n",
    "- $x_{n,:}$ vector of the $n$th variable at for time steps in $T$, $\\in \\mathbb{R}^T$\n",
    "- $x_{n,t}$ $n$th variable at time $t$, $\\in \\mathbb{R}$\n",
    "- $X_M = [x_{:,1}, ... x_{:, T}]$ Matrix with all the $N$ variables at all time steps $T$, $\\in \\mathbb{R}^{N \\times T}$\n",
    "- $X$ is a vector obtained by \"flattening\" $X_M$, by putting next to each other all variable at time $t$, $\\in \\mathbb{R}^{(N \\cdot T)}$\n",
    "- $t$ time step\n",
    "- $z_{i, t}$ $i$th latent variable at time $t$, $\\in \\mathbb{R}$\n",
    "- $Z = [z_1 , ... z_t]$ Vector with $z$ at all time steps in $T$, $\\in \\mathbb{R}^{K \\times T}$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian Processes Factor Analysis model\n",
    "\n",
    "We model the variables in this way\n",
    " $$x_{:,t} = \\Lambda z_{:,t} + \\epsilon $$\n",
    "where:\n",
    "\n",
    "- $\\Lambda$ is a Factor loading matrix that transforms $z_{:,t}$ into $x_{:,t}$, $\\in \\mathbb{R}^{N \\times K}$\n",
    "- $\\epsilon$ Random noise. The random noise is independent between the different time steps, $\\in \\mathbb{R}^N$:\n",
    "    - $p(\\epsilon) = \\mathcal{N}(0, \\psi)$ distribution of noise\n",
    "    - $\\psi$, covariance matrix of noise, it is a diagional matrix, $\\in \\mathbb{R}^{N \\times N}$\n",
    "\n",
    "The model consider $\\langle X \\rangle = 0$ (if $X$ doesn't have a 0 mean it can be easily transformed by substracting the mean)\n",
    "\n",
    "The latent variable $z$ is modelled over time using a Gaussian Process, one process for each dimension $k$\n",
    "for simplicity we assumed that $z$ has only one dimension ($k = 1$)\n",
    "\n",
    "$$p(Z) = \\mathcal{GP}(0, k(t, t \\prime))$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Derivation of $p(X)$\n",
    "\n",
    "$p(x_{:,t}|z_{:,t}) = \\mathcal{N}(\\Lambda z_{:,t}, \\psi)$ is easy to derive and then $p(x_{:,t})$ and $p(z_{:,t}|x_{:,t})$ can be obtained using the rules of Gaussian inference.\n",
    "\n",
    "However, what is interesting is to have the analytical form of $p(X)$, which models both the relations between $z$ and $x$ and the $z$ and $t$. The likelihood of $p(X)$ can then be maximized to obtain the parameters of the latent transformation and the kernel hyperparameter.\n",
    "\n",
    "$p(X)$ is a Guassian distribution with $T\\cdot N$ dimensions.\n",
    "\n",
    "$p(X) = \\mathcal{N}(\\langle X \\rangle, \\langle X X^T \\rangle)$\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Diagonal of the covariance matrix\n",
    "\n",
    "Let's start with the diagonal of the covariance matrix ($t = t \\prime$)\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t} + \\epsilon_{t})^T \\rangle$\n",
    "\n",
    "by multipling the two vectors together we obtain\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t}^T + \\epsilon_t \\Lambda^T z_{:,t}^T  + \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "The using the linearity of the [expectation](https://www.statlect.com/fundamentals-of-probability/expected-value-properties) we can:\n",
    "\n",
    "1) transform the expecations of a sum into a sum of expecations\n",
    "2) move the $\\Lambda$ out of the expecation, as it doesn't depend on $z$\n",
    "3) $\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle$ because $z_{:,t}$ and $\\epsilon_t$ are independent random variables\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T  \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "Then considering that $\\langle z_{:,t} \\rangle = 0$ and that $\\langle \\epsilon_t \\rangle = 0$\n",
    "the expression can be simplified as:\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "Then substituting:\n",
    "\n",
    "1) $\\langle z_{:,t} z_{:,t}^T\\rangle = k(t, t)$ as that is the covariance matrix of the Gaussian process\n",
    "2) $\\langle \\epsilon_t \\epsilon_t^T \\rangle= \\psi$\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda k(t,t)  \\Lambda^T + \\psi$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Off-diagonal\n",
    "\n",
    "similar to the steps of above\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t \\prime} + \\epsilon_{t \\prime})^T \\rangle$\n",
    "\n",
    "by multipling the two vectors together we obtain\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t \\prime}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t \\prime}^T + \\epsilon_t \\Lambda^T z_{:,t \\prime}^T  + \\epsilon_t \\epsilon_{t \\prime}^T \\rangle$\n",
    "\n",
    "Then using the linearity of the [expectation](https://www.statlect.com/fundamentals-of-probability/expected-value-properties) we can:\n",
    "\n",
    "1) transform the expecations of a sum into a sum of expecations\n",
    "2) move the $\\Lambda$ out of the expecatios, as it doesn't depend on t \n",
    "3) $\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle$ because $z_{:,t}$ and $\\epsilon_t$ are independent random variables\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T  \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "Then considering that $\\langle z_{:,t} \\rangle = 0$ and that $\\langle \\epsilon_t \\rangle = 0$\n",
    "the expression can be simplified as:\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t \\prime}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle$\n",
    "\n",
    "Then substituting:\n",
    "1) $\\langle z_{:,t} z_{:,t \\prime}^T\\rangle = k(t,t \\prime)$ as that is the covariance matrix of the Gaussian process\n",
    "2) $\\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle= 0$ as $\\epsilon_t$ and $\\epsilon_{t \\prime}$ are independent and $\\langle \\epsilon_t \\rangle = 0$\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Result\n",
    "\n",
    "The equation for the diagonal and off-diagonal element can be summarized as:\n",
    "\n",
    "$$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi$$\n",
    "\n",
    "where $\\delta(x) = \\begin{cases}\n",
    "                1 & if\\ x=0 \\\\\n",
    "                0    & if\\ x \\ne 0 \\\\\n",
    "            \\end{cases}$ \n",
    "\n",
    "Therefore $p(X)$ can be modelled as:\n",
    "\n",
    "$$p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{cccc}\n",
    "    \\Lambda k(t_1,t_1) \\Lambda^T + \\delta(1-1)\\psi & \\Lambda k(t_{1},t_{2}) \\Lambda^T + \\delta(1-2)\\psi& \\cdots & \\Lambda k(t_1 ,t_t) \\Lambda^T + \\delta(1-t)\\psi\\\\\n",
    "    \\Lambda k(t_{2},t_{1}) \\Lambda^T+ \\delta(2-1)\\psi &  \\Lambda k(t_{2},t_{2}) \\Lambda^T + \\delta(2-2)\\psi & \\cdots & \\Lambda k(t_{2},t_{t}) \\Lambda^T + \\delta(2-t)\\psi\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\Lambda k(t_{t}, t_{1}) \\Lambda^T+ \\delta(t-1)\\psi & \\Lambda k(t_{t},t_{2}) \\Lambda^T + \\delta(t-2)\\psi& \\cdots & \\Lambda k(t_{t},t_{t}) \\Lambda^T + \\delta(t-t)\\psi\\\\\n",
    "    \\end{array} } \\right )$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and this is also Gaussian Process with a \"special\" kernel. Multiplying kernel with a constant ($\\Lambda$) or adding a kernel ($\\delta$) yields another valid kernel\n",
    "\n",
    "\n",
    "If we define a new kernel as $$K(t,t \\prime) = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$ p(X) = \\mathcal{GP}(0, K(t, t\\prime))$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Latent variable with more than one dimension\n",
    "\n",
    "In order to have a latent variable with more than one-dimesions, we need to make small changes to the formula\n",
    "\n",
    "The starting point is the covariance matrix of the latent at time $t$, which is:\n",
    "\n",
    "$$\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} \\langle z_{1:,t} z_{1:,t}^T\\rangle & \\cdots & \\langle z_{1:,t} z_{k:,t}^T\\rangle\\\\ \\vdots & \\ddots & \\vdots \\\\ \\langle z_{k:,t} z_{1:,t}^T \\rangle & \\cdots & \\langle z_{k:,t} z_{k:,t}^T\\rangle\\end{array}\\right)$$ "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "since each dimension in $z$ is indipendent:\n",
    "\n",
    "$\\langle z_{k,t} z_{k\\prime,t} \\rangle = 0$\n",
    "\n",
    "each dimension in $z$ is modelled using a different kernel ($k_{z_k}(t,t\\prime)$), hence:\n",
    "\n",
    "$$\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} k_{z_1}(t, t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t)\\end{array}\\right)$$ \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "so the GPFA Kernel is:   \n",
    "$$K(t,t \\prime) = \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t, t \\prime) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t \\prime)\\end{array}\\right) \\Lambda^T + \\delta(t - t \\prime)\\psi$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "so `p(X)` is:\n",
    "\n",
    "$$p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{ccc}\n",
    "    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_t)\\psi\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_t)\\psi\\\\\n",
    "    \\end{array} } \\right )$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps\n",
    "\n",
    "- The parameters of the final GP ($\\Lambda, \\psi$ and the kernel hyperparameters) can be fitted by maximizing the likelihood of $p(X)$ using gradient descent\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "from fastcore.foundation import patch\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#| exports\n",
    "class GPFAKernel(gpytorch.kernels.Kernel):\n",
    "    \"\"\"\n",
    "    Kernel to implement Gaussian Processes Factor Analysis\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_features: int, # number of variables at each time step\n",
    "                 latent_kernel: gpytorch.kernels.Kernel, # func that returns any valid GPyTorch Kernel used to model the relationship over time of the latent\n",
    "                 latent_dims:int = 1,  # Number of latent dims\n",
    "                 Lambda: torch.tensor = None, #(n_features * latent_dims) initial value for factor loading matrix. If None init to one\n",
    "                 psi: torch.tensor = None, #(n_features) initial value for random noise covariance. Note this is only the diagonal matrix\n",
    "                 **kwargs):\n",
    "        super(GPFAKernel, self).__init__(**kwargs)\n",
    "        \n",
    "        # Number of features in the X for each time step\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.latent_dims = latent_dims\n",
    "        \n",
    "        # see GPyTorch Kernels\n",
    "        self.register_parameter(\n",
    "            name = \"Lambda\",\n",
    "            parameter = torch.nn.Parameter(torch.rand(self.n_features, self.latent_dims)))\n",
    "        \n",
    "        # each dim has it's own latent kernel\n",
    "        self.latent_kernels = torch.nn.ModuleList([latent_kernel() for _ in range(self.latent_dims)])\n",
    "        \n",
    "        self.register_parameter(\n",
    "            name = \"raw_psi_diag\",\n",
    "            parameter = torch.nn.Parameter(torch.zeros(self.n_features))) \n",
    "        self.register_constraint(\"raw_psi_diag\", gpytorch.constraints.Positive())\n",
    "        if psi is not None: self.psi = psi\n",
    "    \n",
    "    # Convenient getter and setter for psi, since there is the Positive() constraint\n",
    "    @property\n",
    "    def psi(self):\n",
    "        # when accessing the parameter, apply the constraint transform\n",
    "        return self.raw_psi_diag_constraint.transform(self.raw_psi_diag)\n",
    "\n",
    "    @psi.setter\n",
    "    def psi(self, value):\n",
    "        return self._set_psi(value)\n",
    "\n",
    "    def _set_psi(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_psi_diag)\n",
    "        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n",
    "        self.initialize(raw_psi_diag=self.raw_psi_diag_constraint.inverse_transform(value))\n",
    "    \n",
    "\n",
    "        \n",
    "    def forward(self, t1, t2, diag = False, last_dim_is_batch=False, **params):\n",
    "\n",
    "        # not implemented yet\n",
    "        assert diag is False\n",
    "        assert last_dim_is_batch is False\n",
    "\n",
    "        # take the number of observations from the input\n",
    "        n_obs = t1.shape[0]\n",
    "\n",
    "        # compute the latent kernel\n",
    "        kT = torch.stack([ kernel(t1, t2, diag, last_dim_is_batch, **params).evaluate() # this may make the whole thing slow as it breaks lazy evaluations\n",
    "                         for kernel in self.latent_kernels], dim=2)\n",
    "        return compute_gpfa_covariance(self.Lambda, kT, self.psi, self.n_features, n_obs)\n",
    "    \n",
    "    def num_outputs_per_input(self, x1,x2):\n",
    "        return self.n_features\n",
    "\n",
    "# this is a separate function, because torch script cannot take self as a parameter\n",
    "@torch.jit.script\n",
    "def compute_gpfa_covariance(Lambda, kT, psi, n_features, n_obs):\n",
    "    # pre allocate covariance matrix\n",
    "    X_cov = torch.empty(n_features * n_obs, n_features * n_obs, device=Lambda.device)\n",
    "    for i in torch.arange(n_obs):\n",
    "        for j in torch.arange(n_obs):\n",
    "            # i:i+1 is required to keep the number of dimensions\n",
    "            cov =  Lambda @ torch.diag(kT[i,j,:]) @ Lambda.T\n",
    "            # only diagonals add the noise\n",
    "            if i == j: cov += torch.diag(psi)\n",
    "            # add a block of size n_features*n_features to the covariance matrix\n",
    "            X_cov[i*n_features:(i*n_features + n_features),j*n_features:(j*n_features+n_features)] = cov\n",
    "    return X_cov"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpfa_k = GPFAKernel(n_features=2, latent_kernel=gpytorch.kernels.RBFKernel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parameters are correctly registered"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(gpfa_k.named_parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check that the Kernel is running"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpfa_k(torch.tensor((1, 2, 3))).evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPFA "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPFAZeroMean(gpytorch.means.Mean):\n",
    "    \"\"\"\n",
    "    Zero Mean function to be used in GPFA, as it takes into account the number of features\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, device):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.device = device\n",
    "    def forward(self, input):\n",
    "        shape = input.shape[0] * self.n_features\n",
    "        return torch.zeros(shape, device=self.device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "to change the latent_kernel you should subcall `GPFA`\n",
    "in this way the `get_info` function for the kernel can be changed to include the latent kernel details"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPFA(gpytorch.models.ExactGP):\n",
    "    latent_kernel = gpytorch.kernels.RBFKernel\n",
    "    def __init__(self, train_x, train_y, likelihood, n_features,latent_dims=1):\n",
    "        super(GPFA, self).__init__(train_x, train_y, likelihood)\n",
    "        self.likelihood = likelihood\n",
    "        self.mean_module = GPFAZeroMean(n_features, train_x.device) # gets device from train_x\n",
    "        self.covar_module = GPFAKernel(n_features, self.latent_kernel, latent_dims = latent_dims)\n",
    "\n",
    "    def forward(self, x, **params):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x, **params)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "make some very simple test data, to check that the model is working and can learn the parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "T = torch.arange(1,5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = torch.hstack([torch.arange(0,3) + 2* i for i in T]) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPFA(T, X, likelihood, n_features = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting the prior from the GP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model(T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fitting the parameters using gradient descend"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "training_iter = 10\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "losses = []\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(T)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, X)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f, Lambda: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.latent_kernels[0].lengthscale.item(),\n",
    "        model.covar_module.Lambda.mean().item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model is training!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-dimensional latent variable\n",
    "\n",
    "#### 2 dimensions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood_m = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_m = GPFA(T, X, likelihood, n_features = 3, latent_dims=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "check GP is running"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_m.covar_module.latent_kernels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_m.covar_module.Lambda"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likelihood(model_m(T))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5 dimensions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood_m = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_m = GPFA(T, X, likelihood, n_features = 3, latent_dims=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "check GP is running"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_m.covar_module.latent_kernels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_m.covar_module.Lambda"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likelihood(model_m(T))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get info\n",
    "\n",
    "this is to return the kernel info for printing, both during training and for the final results\n",
    "\n",
    "The general api is:\n",
    "\n",
    "`get_info(var_names=None) -> dict[str, pd.DataFrame]` where the string the title of a kernel parameter "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def get_info(self: GPFA,\n",
    "             var_names = None # Optional variable names for better printing\n",
    "            ) -> dict[str, pd.DataFrame]:\n",
    "    \"Model info for a GPFA with a RBFKernel\"\n",
    "    out = {}\n",
    "    \n",
    "    latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n",
    "    \n",
    "    out[\"Lambda\"] = pd.concat([\n",
    "        None if var_names is None else pd.Series(var_names, name='variable'),\n",
    "        pd.DataFrame(\n",
    "            self.covar_module.Lambda.detach().cpu().numpy(),\n",
    "            columns=latent_names)],\n",
    "        axis=1)\n",
    "    \n",
    "    ls = [self.covar_module.latent_kernels[i].lengthscale.detach().item() for i in range(self.covar_module.latent_dims)]\n",
    "    out[\"lengthscale\"] = pd.DataFrame({\n",
    "        'latent': latent_names,\n",
    "        'lengthscale': ls\n",
    "    })\n",
    "    \n",
    "    psi = self.covar_module.psi.detach().cpu().numpy()\n",
    "    out[\"psi\"] = pd.DataFrame({\n",
    "        'variable': var_names,\n",
    "        'psi': psi \n",
    "    })\n",
    "    \n",
    "    out[\"likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.detach().item()]})\n",
    "    \n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_m.get_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_m.get_info([\"a\", \"b\", \"c\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656852a7-1677-4a8e-88fc-27e4b22c54bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): RBFKernel(\n",
       "    (raw_lengthscale_constraint): Positive()\n",
       "  )\n",
       "  (1): RBFKernel(\n",
       "    (raw_lengthscale_constraint): Positive()\n",
       "  )\n",
       "  (2): RBFKernel(\n",
       "    (raw_lengthscale_constraint): Positive()\n",
       "  )\n",
       "  (3): RBFKernel(\n",
       "    (raw_lengthscale_constraint): Positive()\n",
       "  )\n",
       "  (4): RBFKernel(\n",
       "    (raw_lengthscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.covar_module.latent_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db6556-9d37-457d-86ca-2987685d5c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1433, 0.6319, 0.2940, 0.8762, 0.6009],\n",
       "        [0.7275, 0.7272, 0.2959, 0.3548, 0.8001],\n",
       "        [0.9632, 0.0958, 0.9202, 0.0039, 0.7309]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.covar_module.Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c46ff-14fc-482a-935f-975d6542845a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultivariateNormal(loc: torch.Size([12]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood(model_m(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d5e1f-425f-4428-92e6-eadb9d589648",
   "metadata": {},
   "source": [
    "## Get info\n",
    "\n",
    "this is to return the kernel info for printing, both during training and for the final results\n",
    "\n",
    "The general api is:\n",
    "\n",
    "`get_info(var_names=None) -> dict[str, pd.DataFrame]` where the string the title of a kernel parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d83f1f-f70a-4ec4-8229-53a2034dc24c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#| exports\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;129m@patch\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_info\u001B[39m(\u001B[38;5;28mself\u001B[39m: GPFA,\n\u001B[1;32m      4\u001B[0m              var_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;66;03m# Optional variable names for better printing\u001B[39;00m\n\u001B[1;32m      5\u001B[0m             ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, pd\u001B[38;5;241m.\u001B[39mDataFrame]:\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel info for a GPFA with a RBFKernel\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m     out \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[0;31mNameError\u001B[0m: name 'patch' is not defined"
     ]
    }
   ],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def get_info(self: GPFA,\n",
    "             var_names = None # Optional variable names for better printing\n",
    "            ) -> dict[str, pd.DataFrame]:\n",
    "    \"Model info for a GPFA with a RBFKernel\"\n",
    "    out = {}\n",
    "    \n",
    "    latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n",
    "    \n",
    "    out[\"Lambda\"] = pd.concat([\n",
    "        None if var_names is None else pd.Series(var_names, name='variable'),\n",
    "        pd.DataFrame(\n",
    "            self.covar_module.Lambda.detach().cpu().numpy(),\n",
    "            columns=latent_names)],\n",
    "        axis=1)\n",
    "    \n",
    "    ls = [self.covar_module.latent_kernels[i].lengthscale.detach().item() for i in range(self.covar_module.latent_dims)]\n",
    "    out[\"lengthscale\"] = pd.DataFrame({\n",
    "        'latent': latent_names,\n",
    "        'lengthscale': ls\n",
    "    })\n",
    "    \n",
    "    psi = self.covar_module.psi.detach().cpu().numpy()\n",
    "    out[\"psi\"] = pd.DataFrame({\n",
    "        'variable': var_names,\n",
    "        'psi': psi \n",
    "    })\n",
    "    \n",
    "    out[\"likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.detach().item()]})\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65c9e5-f335-4174-81e8-1b9a4be9c223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lambda':          z0        z1        z2        z3        z4\n",
       " 0  0.143287  0.631874  0.294008  0.876215  0.600861\n",
       " 1  0.727490  0.727183  0.295870  0.354760  0.800073\n",
       " 2  0.963249  0.095809  0.920186  0.003902  0.730877,\n",
       " 'lengthscale':   latent  lengthscale\n",
       " 0     z0     0.693147\n",
       " 1     z1     0.693147\n",
       " 2     z2     0.693147\n",
       " 3     z3     0.693147\n",
       " 4     z4     0.693147,\n",
       " 'psi':   variable       psi\n",
       " 0     None  0.693147\n",
       " 1     None  0.693147\n",
       " 2     None  0.693147,\n",
       " 'likelihood':       noise\n",
       " 0  1.155176}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31760c86-8f89-4d9d-a17c-233e0e58a142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lambda':    0        z0        z1        z2        z3        z4\n",
       " 0  a  0.143287  0.631874  0.294008  0.876215  0.600861\n",
       " 1  b  0.727490  0.727183  0.295870  0.354760  0.800073\n",
       " 2  c  0.963249  0.095809  0.920186  0.003902  0.730877,\n",
       " 'lengthscale':   latent  lengthscale\n",
       " 0     z0     0.693147\n",
       " 1     z1     0.693147\n",
       " 2     z2     0.693147\n",
       " 3     z3     0.693147\n",
       " 4     z4     0.693147,\n",
       " 'psi':   variable       psi\n",
       " 0        a  0.693147\n",
       " 1        b  0.693147\n",
       " 2        c  0.693147,\n",
       " 'likelihood':       noise\n",
       " 0  1.155176}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.get_info([\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746701df-3856-4083-81df-9e664bf33a40",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c41b65-9eb8-4d70-b4fc-05113783e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}