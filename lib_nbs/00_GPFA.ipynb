{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01888fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| default_exp gpfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f0a869-934a-4171-ab78-0d22aebf624c",
   "metadata": {},
   "source": [
    "# Gaussian Processes Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cdc8f7",
   "metadata": {},
   "source": [
    "Derivation of the equations to solve the Gaussian Processes Factor Analysis as described in: Yu, B.M., Cunningham, J.P., Santhanam, G., Ryu, S., Shenoy, K.V., Sahani, M., 2008. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity, in: Advances in Neural Information Processing Systems. Curran Associates, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0f969",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "- $T$  Number of time steps\n",
    "- $N$  Number of variables observed\n",
    "- $K$  Number of dimensions of latent variable\n",
    "- $x_{:,t}$ vector of all the $N$ variables at time $t$, $\\in \\mathbb{R}^N$\n",
    "- $x_{n,:}$ vector of the $n$th variable at for time steps in $T$, $\\in \\mathbb{R}^T$\n",
    "- $x_{n,t}$ $n$th variable at time $t$, $\\in \\mathbb{R}$\n",
    "- $X_M = [x_{:,1}, ... x_{:, T}]$ Matrix with all the $N$ variables at all time steps $T$, $\\in \\mathbb{R}^{N \\times T}$\n",
    "- $X$ is a vector obtained by \"flattening\" $X_M$, by putting next to each other all variable at time $t$, $\\in \\mathbb{R}^{(N \\cdot T)}$\n",
    "- $t$ time step\n",
    "- $z_{i, t}$ $i$th latent variable at time $t$, $\\in \\mathbb{R}$\n",
    "- $Z = [z_1 , ... z_t]$ Vector with $z$ at all time steps in $T$, $\\in \\mathbb{R}^{K \\times T}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc81df",
   "metadata": {},
   "source": [
    "## Gaussian Processes Factor Analysis model\n",
    "\n",
    "We model the variables in this way\n",
    " $$x_{:,t} = \\Lambda z_{:,t} + \\epsilon $$\n",
    "where:\n",
    "\n",
    "- $\\Lambda$ is a Factor loading matrix that transforms $z_{:,t}$ into $x_{:,t}$, $\\in \\mathbb{R}^{N \\times K}$\n",
    "- $\\epsilon$ Random noise. The random noise is independent between the different time steps, $\\in \\mathbb{R}^N$:\n",
    "    - $p(\\epsilon) = \\mathcal{N}(0, \\psi)$ distribution of noise\n",
    "    - $\\psi$, covariance matrix of noise, it is a diagional matrix, $\\in \\mathbb{R}^{N \\times N}$\n",
    "\n",
    "The model consider $\\langle X \\rangle = 0$ (if $X$ doesn't have a 0 mean it can be easily transformed by substracting the mean)\n",
    "\n",
    "The latent variable $z$ is modelled over time using a Gaussian Process, one process for each dimension $k$\n",
    "for simplicity we assumed that $z$ has only one dimension ($k = 1$)\n",
    "\n",
    "$$p(Z) = \\mathcal{GP}(0, k(t, t \\prime))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e65ea",
   "metadata": {},
   "source": [
    "## Derivation of $p(X)$\n",
    "\n",
    "$p(x_{:,t}|z_{:,t}) = \\mathcal{N}(\\Lambda z_{:,t}, \\psi)$ is easy to derive and then $p(x_{:,t})$ and $p(z_{:,t}|x_{:,t})$ can be obtained using the rules of Gaussian inference.\n",
    "\n",
    "However, what is interesting is to have the analytical form of $p(X)$, which models both the relations between $z$ and $x$ and the $z$ and $t$. The likelihood of $p(X)$ can then be maximized to obtain the parameters of the latent transformation and the kernel hyperparameter.\n",
    "\n",
    "$p(X)$ is a Guassian distribution with $T\\cdot N$ dimensions.\n",
    "\n",
    "$p(X) = \\mathcal{N}(\\langle X \\rangle, \\langle X X^T \\rangle)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7966cb",
   "metadata": {},
   "source": [
    "### Diagonal of the covariance matrix\n",
    "\n",
    "Let's start with the diagonal of the covariance matrix ($t = t \\prime$)\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t} + \\epsilon_{t})^T \\rangle$\n",
    "\n",
    "by multipling the two vectors together we obtain\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t}^T + \\epsilon_t \\Lambda^T z_{:,t}^T  + \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "The using the linearity of the [expectation](https://www.statlect.com/fundamentals-of-probability/expected-value-properties) we can:\n",
    "\n",
    "1) transform the expecations of a sum into a sum of expecations\n",
    "2) move the $\\Lambda$ out of the expecation, as it doesn't depend on $z$\n",
    "3) $\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle$ because $z_{:,t}$ and $\\epsilon_t$ are independent random variables\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T  \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "Then considering that $\\langle z_{:,t} \\rangle = 0$ and that $\\langle \\epsilon_t \\rangle = 0$\n",
    "the expression can be simplified as:\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "Then substituting:\n",
    "\n",
    "1) $\\langle z_{:,t} z_{:,t}^T\\rangle = k(t, t)$ as that is the covariance matrix of the Gaussian process\n",
    "2) $\\langle \\epsilon_t \\epsilon_t^T \\rangle= \\psi$\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda k(t,t)  \\Lambda^T + \\psi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf159d40",
   "metadata": {},
   "source": [
    "### Off-diagonal\n",
    "\n",
    "similar to the steps of above\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t \\prime} + \\epsilon_{t \\prime})^T \\rangle$\n",
    "\n",
    "by multipling the two vectors together we obtain\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t \\prime}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t \\prime}^T + \\epsilon_t \\Lambda^T z_{:,t \\prime}^T  + \\epsilon_t \\epsilon_{t \\prime}^T \\rangle$\n",
    "\n",
    "Then using the linearity of the [expectation](https://www.statlect.com/fundamentals-of-probability/expected-value-properties) we can:\n",
    "\n",
    "1) transform the expecations of a sum into a sum of expecations\n",
    "2) move the $\\Lambda$ out of the expecatios, as it doesn't depend on t \n",
    "3) $\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle$ because $z_{:,t}$ and $\\epsilon_t$ are independent random variables\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T  \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle$\n",
    "\n",
    "Then considering that $\\langle z_{:,t} \\rangle = 0$ and that $\\langle \\epsilon_t \\rangle = 0$\n",
    "the expression can be simplified as:\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t \\prime}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle$\n",
    "\n",
    "Then substituting:\n",
    "1) $\\langle z_{:,t} z_{:,t \\prime}^T\\rangle = k(t,t \\prime)$ as that is the covariance matrix of the Gaussian process\n",
    "2) $\\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle= 0$ as $\\epsilon_t$ and $\\epsilon_{t \\prime}$ are independent and $\\langle \\epsilon_t \\rangle = 0$\n",
    "\n",
    "$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d99835",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "The equation for the diagonal and off-diagonal element can be summarized as:\n",
    "\n",
    "$$\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi$$\n",
    "\n",
    "where $\\delta(x) = \\begin{cases}\n",
    "                1 & if\\ x=0 \\\\\n",
    "                0    & if\\ x \\ne 0 \\\\\n",
    "            \\end{cases}$ \n",
    "\n",
    "Therefore $p(X)$ can be modelled as:\n",
    "\n",
    "$$p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{cccc}\n",
    "    \\Lambda k(t_1,t_1) \\Lambda^T + \\delta(1-1)\\psi & \\Lambda k(t_{1},t_{2}) \\Lambda^T + \\delta(1-2)\\psi& \\cdots & \\Lambda k(t_1 ,t_t) \\Lambda^T + \\delta(1-t)\\psi\\\\\n",
    "    \\Lambda k(t_{2},t_{1}) \\Lambda^T+ \\delta(2-1)\\psi &  \\Lambda k(t_{2},t_{2}) \\Lambda^T + \\delta(2-2)\\psi & \\cdots & \\Lambda k(t_{2},t_{t}) \\Lambda^T + \\delta(2-t)\\psi\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\Lambda k(t_{t}, t_{1}) \\Lambda^T+ \\delta(t-1)\\psi & \\Lambda k(t_{t},t_{2}) \\Lambda^T + \\delta(t-2)\\psi& \\cdots & \\Lambda k(t_{t},t_{t}) \\Lambda^T + \\delta(t-t)\\psi\\\\\n",
    "    \\end{array} } \\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a137e94",
   "metadata": {},
   "source": [
    "and this is also Gaussian Process with a \"special\" kernel. Multiplying kernel with a constant ($\\Lambda$) or adding a kernel ($\\delta$) yields another valid kernel\n",
    "\n",
    "\n",
    "If we define a new kernel as $$K(t,t \\prime) = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$ p(X) = \\mathcal{GP}(0, K(t, t\\prime))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c06f2-5cbd-41f1-92cc-ca3f8da3ff5a",
   "metadata": {},
   "source": [
    "### Latent variable with more than one dimension\n",
    "\n",
    "In order to have a latent variable with more than one-dimesions, we need to make small changes to the formula\n",
    "\n",
    "The starting point is the covariance matrix of the latent at time $t$, which is:\n",
    "\n",
    "$$\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} \\langle z_{1:,t} z_{1:,t}^T\\rangle & \\cdots & \\langle z_{1:,t} z_{k:,t}^T\\rangle\\\\ \\vdots & \\ddots & \\vdots \\\\ \\langle z_{k:,t} z_{1:,t}^T \\rangle & \\cdots & \\langle z_{k:,t} z_{k:,t}^T\\rangle\\end{array}\\right)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb08803-e378-4226-8ca3-35f4d5c0d78e",
   "metadata": {},
   "source": [
    "since each dimension in $z$ is indipendent:\n",
    "\n",
    "$\\langle z_{k,t} z_{k\\prime,t} \\rangle = 0$\n",
    "\n",
    "each dimension in $z$ is modelled using a different kernel ($k_{z_k}(t,t\\prime)$), hence:\n",
    "\n",
    "$$\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} k_{z_1}(t, t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t)\\end{array}\\right)$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd3d6f9-c512-4ede-ad68-d86c47bef42a",
   "metadata": {},
   "source": [
    "so the GPFA Kernel is:   \n",
    "$$K(t,t \\prime) = \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t, t \\prime) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t \\prime)\\end{array}\\right) \\Lambda^T + \\delta(t - t \\prime)\\psi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f24e2-27b0-4e81-9ca1-f1e3b2bf6372",
   "metadata": {},
   "source": [
    "so `p(X)` is:\n",
    "\n",
    "$$p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{ccc}\n",
    "    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_t)\\psi\\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_t)\\psi\\\\\n",
    "    \\end{array} } \\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92208cae",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- The parameters of the final GP ($\\Lambda, \\psi$ and the kernel hyperparameters) can be fitted by maximizing the likelihood of $p(X)$ using gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af93cc0",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563b28b",
   "metadata": {},
   "source": [
    "## Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29360a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class GPFAKernel(gpytorch.kernels.Kernel):\n",
    "    \"\"\"\n",
    "    Kernel to implement Gaussian Processes Factor Analysis\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_features: int, # number of variables at each time step\n",
    "                 latent_kernel: gpytorch.kernels.Kernel, # func that returns any valid GPyTorch Kernel used to model the relationship over time of the latent\n",
    "                 latent_dims:int = 1,  # Number of latent dims\n",
    "                 Lambda: torch.tensor = None, #(n_features * latent_dims) initial value for factor loading matrix. If None init to one\n",
    "                 psi: torch.tensor = None, #(n_features) initial value for random noise covariance. Note this is only the diagonal matrix\n",
    "                 **kwargs):\n",
    "        super(GPFAKernel, self).__init__(**kwargs)\n",
    "        \n",
    "        # Number of features in the X for each time step\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.latent_dims = latent_dims\n",
    "        \n",
    "        # see GPyTorch Kernels\n",
    "        self.register_parameter(\n",
    "            name = \"Lambda\",\n",
    "            parameter = torch.nn.Parameter(torch.rand(self.n_features, self.latent_dims)))\n",
    "        \n",
    "        # each dim has it's own latent kernel\n",
    "        self.latent_kernels = torch.nn.ModuleList([latent_kernel() for _ in range(self.latent_dims)])\n",
    "        \n",
    "        self.register_parameter(\n",
    "            name = \"raw_psi_diag\",\n",
    "            parameter = torch.nn.Parameter(torch.zeros(self.n_features))) \n",
    "        self.register_constraint(\"raw_psi_diag\", gpytorch.constraints.Positive())\n",
    "        if psi is not None: self.psi = psi\n",
    "    \n",
    "    # Convenient getter and setter for psi, since there is the Positive() constraint\n",
    "    @property\n",
    "    def psi(self):\n",
    "        # when accessing the parameter, apply the constraint transform\n",
    "        return self.raw_psi_diag_constraint.transform(self.raw_psi_diag)\n",
    "\n",
    "    @psi.setter\n",
    "    def psi(self, value):\n",
    "        return self._set_psi(value)\n",
    "\n",
    "    def _set_psi(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_psi_diag)\n",
    "        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n",
    "        self.initialize(raw_psi_diag=self.raw_psi_diag_constraint.inverse_transform(value))\n",
    "    \n",
    "\n",
    "        \n",
    "    def forward(self, t1, t2, diag = False, last_dim_is_batch=False, **params):\n",
    "\n",
    "        # not implemented yet\n",
    "        assert diag is False\n",
    "        assert last_dim_is_batch is False\n",
    "\n",
    "        # take the number of observations from the input\n",
    "        n_obs = t1.shape[0]\n",
    "\n",
    "        # compute the latent kernel\n",
    "        kT = torch.stack([ kernel(t1, t2, diag, last_dim_is_batch, **params).evaluate() # this may make the whole thing slow as it breaks lazy evaluations\n",
    "                         for kernel in self.latent_kernels], dim=2)\n",
    "        return compute_gpfa_covariance(self.Lambda, kT, self.psi, self.n_features, n_obs)\n",
    "    \n",
    "    def num_outputs_per_input(self, x1,x2):\n",
    "        return self.n_features\n",
    "\n",
    "# this is a separate function, because torch script cannot take self as a parameter\n",
    "@torch.jit.script\n",
    "def compute_gpfa_covariance(Lambda, kT, psi, n_features, n_obs):\n",
    "    # pre allocate covariance matrix\n",
    "    X_cov = torch.empty(n_features * n_obs, n_features * n_obs, device=Lambda.device)\n",
    "    for i in torch.arange(n_obs):\n",
    "        for j in torch.arange(n_obs):\n",
    "            # i:i+1 is required to keep the number of dimensions\n",
    "            cov =  Lambda @ torch.diag(kT[i,j,:]) @ Lambda.T\n",
    "            # only diagonals add the noise\n",
    "            if i == j: cov += torch.diag(psi)\n",
    "            # add a block of size n_features*n_features to the covariance matrix\n",
    "            X_cov[i*n_features:(i*n_features + n_features),j*n_features:(j*n_features+n_features)] = cov\n",
    "    return X_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b2a1a-b51c-483d-b67e-0368c1d70ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpfa_k = GPFAKernel(n_features=2, latent_kernel=gpytorch.kernels.RBFKernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03ef6e-f086-40fc-93d0-2d71867d641f",
   "metadata": {},
   "source": [
    "The parameters are correctly registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e350e3-893a-4717-a4cb-4a1fd718b025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lambda',\n",
       "  Parameter containing:\n",
       "  tensor([[1.],\n",
       "          [1.]], requires_grad=True)),\n",
       " ('raw_psi_diag',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0.], requires_grad=True)),\n",
       " ('latent_kernels.0.raw_lengthscale',\n",
       "  Parameter containing:\n",
       "  tensor([[0.]], requires_grad=True))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gpfa_k.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f7bbf-00bf-44cc-b992-686828f4fd67",
   "metadata": {},
   "source": [
    "Check that the Kernel is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2eae6-3ba2-4a71-812b-f70e70d99a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6931, 1.0000, 0.3532, 0.3532, 0.0156, 0.0156],\n",
       "        [1.0000, 1.6931, 0.3532, 0.3532, 0.0156, 0.0156],\n",
       "        [0.3532, 0.3532, 1.6931, 1.0000, 0.3532, 0.3532],\n",
       "        [0.3532, 0.3532, 1.0000, 1.6931, 0.3532, 0.3532],\n",
       "        [0.0156, 0.0156, 0.3532, 0.3532, 1.6931, 1.0000],\n",
       "        [0.0156, 0.0156, 0.3532, 0.3532, 1.0000, 1.6931]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpfa_k(torch.tensor((1, 2, 3))).evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61a3bf-4a27-49b3-b39e-d4e836b5e000",
   "metadata": {},
   "source": [
    "## GPFA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPFAZeroMean(gpytorch.means.Mean):\n",
    "    \"\"\"\n",
    "    Zero Mean function to be used in GPFA, as it takes into account the number of features\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, device):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.device = device\n",
    "    def forward(self, input):\n",
    "        shape = input.shape[0] * self.n_features\n",
    "        return torch.zeros(shape, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GPFA(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, n_features, latent_kernel, latent_dims=1):\n",
    "        super(GPFA, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = GPFAZeroMean(n_features, train_x.device) # gets device from train_x\n",
    "        self.covar_module = GPFAKernel(n_features, latent_kernel, latent_dims = latent_dims)\n",
    "\n",
    "    def forward(self, x, **params):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x, **params)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47b618-4465-4fe7-a021-43bb38ac1c0c",
   "metadata": {},
   "source": [
    "make some very simple test data, to check that the model is working and can learn the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149005d-40cc-49db-ad14-4d112eed9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.arange(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28a241-a7a8-426b-af31-0b3cf8717cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.hstack([torch.arange(0,3) + 2* i for i in T]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874dea7-a292-4e5c-9667-9f3ea3fec34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91c2b7-61e1-4d97-a4d4-43cab85d5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daab629-ae73-4fbf-9ccd-52f5abbc3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497fb5a-d735-4288-b50a-4eb4144acafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPFA(T, X, likelihood, n_features = 3, latent_kernel = gpytorch.kernels.RBFKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b38e71-011c-40ec-b90e-582aa8bfbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2bb10-3f00-4864-a3f2-532033807806",
   "metadata": {},
   "source": [
    "Getting the prior from the GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1b1b7-c718-40ae-9ce0-d83f8eda933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528dca19-bad4-4f87-838d-89285f4d5504",
   "metadata": {},
   "source": [
    "Fitting the parameters using gradient descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e656ff1-2ca9-40aa-87c7-a33fee3f5cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "training_iter = 10\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "losses = []\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(T)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, X)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f, Lambda: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.latent_kernels[0].lengthscale.item(),\n",
    "        model.covar_module.Lambda.mean().item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a4be3-a9b4-4270-8621-df02d3083a3d",
   "metadata": {},
   "source": [
    "The model is training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7708a7-51de-4f09-ac69-86a287a00c56",
   "metadata": {},
   "source": [
    "## Multi-dimensional latent variable\n",
    "\n",
    "#### 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f081977-dcf6-45ac-946f-2fd48bf31506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood_m = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_m = GPFA(T, X, likelihood, n_features = 3, latent_kernel = gpytorch.kernels.RBFKernel, latent_dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919daa31-f9da-4cad-ad15-04bd48525381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e143c8af-3f08-4613-bf42-1341316d45dc",
   "metadata": {},
   "source": [
    "check GP is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef87a1-0ae1-4530-a15d-3fae52060134",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.covar_module.latent_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eedb92-cee2-47e0-aa6f-5781c65dc975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.covar_module.Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527e3d6-baa5-4a2e-b57b-2bcf9ea93038",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood(model_m(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e23921-2643-48ee-91ce-b8c74ccf85c3",
   "metadata": {},
   "source": [
    "#### 5 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016a997-e438-4d64-ad40-2470f05ff3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood_m = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_m = GPFA(T, X, likelihood, n_features = 3, latent_kernel = gpytorch.kernels.RBFKernel, latent_dims=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f55e8-8d87-4c0d-92a0-20fc4bf45fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025b1f9-03e3-4492-a326-ca65f072d666",
   "metadata": {},
   "source": [
    "check GP is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656852a7-1677-4a8e-88fc-27e4b22c54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.covar_module.latent_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db6556-9d37-457d-86ca-2987685d5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_m.covar_module.Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c46ff-14fc-482a-935f-975d6542845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood(model_m(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c41b65-9eb8-4d70-b4fc-05113783e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
