@article{alavi_filling_2006,
  title = {Filling Gaps in Evapotranspiration Measurements for Water Budget Studies: {{Evaluation}} of a {{Kalman}} Filtering Approach},
  shorttitle = {Filling Gaps in Evapotranspiration Measurements for Water Budget Studies},
  author = {Alavi, Nasim and Warland, Jon S. and Berg, Aaron A.},
  date = {2006-12-06},
  journaltitle = {Agricultural and Forest Meteorology},
  shortjournal = {Agricultural and Forest Meteorology},
  volume = {141},
  number = {1},
  pages = {57--66},
  issn = {0168-1923},
  doi = {10.1016/j.agrformet.2006.09.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0168192306002814},
  urldate = {2022-11-11},
  abstract = {Missing data in long-term eddy covariance measurements of latent heat flux produce errors in the estimation of evapotranspiration and the water budget. Because no standard method of gap filling has been widely accepted, identification of optimal filling methods for gaps is crucial for determining total evapotranspiration. In this study we evaluate the application of a Kalman filter for filling gaps in latent heat flux data collected from an agricultural research station. The filtering approach was compared with several gap-filling methods including mean diurnal variation, multiple regressions, 2-week average Priestley–Taylor coefficient, and multiple imputation. The results demonstrated that a Kalman filtering approach developed using the relationship between latent heat flux, available energy, and vapour pressure deficit provides a closer approximation of the original data and introduces smaller errors than the other methods evaluated. Evaluation of the Kalman filter approach demonstrates the efficiency of this technique in replacing data in both small and large gaps of up to several days.},
  langid = {english},
  keywords = {Data filling,Eddy covariance,Missing data,Multiple imputation,Recursive estimation},
  file = {/home/simone/Zotero/storage/E2A325VQ/Alavi et al. - 2006 - Filling gaps in evapotranspiration measurements fo.pdf;/home/simone/Zotero/storage/E664S4U9/S0168192306002814.html}
}

@book{aubinet_eddy_2012-1,
  title = {Eddy {{Covariance}}: {{A Practical Guide}} to {{Measurement}} and {{Data Analysis}}},
  shorttitle = {Eddy {{Covariance}}},
  editor = {Aubinet, Marc and Vesala, Timo and Papale, Dario},
  date = {2012},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-007-2351-1},
  url = {https://link.springer.com/10.1007/978-94-007-2351-1},
  urldate = {2023-02-10},
  isbn = {978-94-007-2350-4 978-94-007-2351-1},
  langid = {english},
  file = {/home/simone/Zotero/storage/XC9YLBCQ/Aubinet et al. - 2012 - Eddy Covariance A Practical Guide to Measurement .pdf}
}

@article{balzarolo_evaluating_2014,
  title = {Evaluating the Potential of Large-Scale Simulations to Predict Carbon Fluxes of Terrestrial Ecosystems over a {{European Eddy Covariance}} Network},
  author = {Balzarolo, M. and Boussetta, S. and Balsamo, G. and Beljaars, A. and Maignan, F. and Calvet, J.-C. and Lafont, S. and Barbu, A. and Poulter, B. and Chevallier, F. and Szczypta, C. and Papale, D.},
  date = {2014-05-20},
  journaltitle = {Biogeosciences},
  volume = {11},
  number = {10},
  pages = {2661--2678},
  publisher = {{Copernicus GmbH}},
  issn = {1726-4170},
  doi = {10.5194/bg-11-2661-2014},
  url = {https://bg.copernicus.org/articles/11/2661/2014/},
  urldate = {2023-02-10},
  abstract = {This paper reports a comparison between large-scale simulations of three different land surface models (LSMs), ORCHIDEE, ISBA-A-gs and CTESSEL, forced with the same meteorological data, and compared with the carbon fluxes measured at 32 eddy covariance (EC) flux tower sites in Europe. The results show that the three simulations have the best performance for forest sites and the poorest performance for cropland and grassland sites. In addition, the three simulations have difficulties capturing the seasonality of Mediterranean and sub-tropical biomes, characterized by dry summers. This reduced simulation performance is also reflected in deficiencies in diagnosed light-use efficiency (LUE) and vapour pressure deficit (VPD) dependencies compared to observations. Shortcomings in the forcing data may also play a role. These results indicate that more research is needed on the LUE and VPD functions for Mediterranean and sub-tropical biomes. Finally, this study highlights the importance of correctly representing phenology (i.e. leaf area evolution) and management (i.e. rotation–irrigation for cropland, and grazing–harvesting for grassland) to simulate the carbon dynamics of European ecosystems and the importance of ecosystem-level observations in model development and validation.},
  langid = {english},
  file = {/home/simone/Zotero/storage/B6ESDWT4/Balzarolo et al. - 2014 - Evaluating the potential of large-scale simulation.pdf}
}

@article{banbura_maximum_2014,
  title = {Maximum {{Likelihood Estimation}} of {{Factor Models}} on {{Datasets}} with {{Arbitrary Pattern}} of {{Missing Data}}},
  author = {Bańbura, Marta and Modugno, Michele},
  date = {2014},
  journaltitle = {Journal of Applied Econometrics},
  volume = {29},
  number = {1},
  pages = {133--160},
  issn = {1099-1255},
  doi = {10.1002/jae.2306},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2306},
  urldate = {2022-11-11},
  abstract = {In this paper we modify the expectation maximization algorithm in order to estimate the parameters of the dynamic factor model on a dataset with an arbitrary pattern of missing data. We also extend the model to the case with a serially correlated idiosyncratic component. The framework allows us to handle efficiently and in an automatic manner sets of indicators characterized by different publication delays, frequencies and sample lengths. This can be relevant, for example, for young economies for which many indicators have been compiled only recently. We evaluate the methodology in a Monte Carlo experiment and we apply it to nowcasting of the euro area gross domestic product. Copyright © 2012 John Wiley \& Sons, Ltd.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.2306},
  file = {/home/simone/Zotero/storage/I6SBZCQU/Bańbura and Modugno - 2014 - Maximum Likelihood Estimation of Factor Models on .pdf;/home/simone/Zotero/storage/4UAK4EDY/jae.html}
}

@unpublished{bansal_missing_2021,
  title = {Missing {{Value Imputation}} on {{Multidimensional Time Series}}},
  author = {Bansal, Parikshit and Deshpande, Prathamesh and Sarawagi, Sunita},
  date = {2021-07-16},
  number = {arXiv:2103.01600},
  eprint = {2103.01600},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.01600},
  url = {http://arxiv.org/abs/2103.01600},
  urldate = {2022-05-14},
  abstract = {We present DeepMVI, a deep learning method for missing value imputation in multidimensional time-series datasets. Missing values are commonplace in decision support platforms that aggregate data over long time stretches from disparate sources, and reliable data analytics calls for careful handling of missing data. One strategy is imputing the missing values, and a wide variety of algorithms exist spanning simple interpolation, matrix factorization methods like SVD, statistical models like Kalman filters, and recent deep learning methods. We show that often these provide worse results on aggregate analytics compared to just excluding the missing data. DeepMVI uses a neural network to combine fine-grained and coarse-grained patterns along a time series, and trends from related series across categorical dimensions. After failing with off-the-shelf neural architectures, we design our own network that includes a temporal transformer with a novel convolutional window feature, and kernel regression with learned embeddings. The parameters and their training are designed carefully to generalize across different placements of missing blocks and data characteristics. Experiments across nine real datasets, four different missing scenarios, comparing seven existing methods show that DeepMVI is significantly more accurate, reducing error by more than 50\% in more than half the cases, compared to the best existing method. Although slower than simpler matrix factorization methods, we justify the increased time overheads by showing that DeepMVI is the only option that provided overall more accurate analytics than dropping missing values.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/simone/Zotero/storage/EMEP4Y9S/Bansal et al. - 2021 - Missing Value Imputation on Multidimensional Time .pdf;/home/simone/Zotero/storage/GXPGC6GY/2103.html}
}

@article{beringer_technical_2017,
  title = {Technical Note: {{Dynamic INtegrated Gap-filling}} and Partitioning for {{OzFlux}} ({{DINGO}})},
  shorttitle = {Technical Note},
  author = {Beringer, Jason and McHugh, Ian and Hutley, Lindsay B. and Isaac, Peter and Kljun, Natascha},
  date = {2017-03-23},
  journaltitle = {Biogeosciences},
  volume = {14},
  number = {6},
  pages = {1457--1460},
  publisher = {{Copernicus GmbH}},
  issn = {1726-4170},
  doi = {10.5194/bg-14-1457-2017},
  url = {https://bg.copernicus.org/articles/14/1457/2017/},
  urldate = {2022-05-12},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Standardised, quality-controlled and robust data from flux networks underpin the understanding of ecosystem processes and tools necessary to support the management of natural resources, including water, carbon and nutrients for environmental and production benefits. The Australian regional flux network (OzFlux) currently has 23 active sites and aims to provide a continental-scale national research facility to monitor and assess Australia's terrestrial biosphere and climate for improved predictions. Given the need for standardised and effective data processing of flux data, we have developed a software suite, called the Dynamic INtegrated Gap-filling and partitioning for OzFlux (DINGO), that enables gap-filling and partitioning of the primary fluxes into ecosystem respiration (Fre) and gross primary productivity (GPP) and subsequently provides diagnostics and results. We outline the processing pathways and methodologies that are applied in DINGO (v13) to OzFlux data, including (1) gap-filling of meteorological and other drivers; (2) gap-filling of fluxes using artificial neural networks; (3) the \emph{u}* threshold determination; (4) partitioning into ecosystem respiration and gross primary productivity; (5) random, model and \emph{u}* uncertainties; and (6) diagnostic, footprint calculation, summary and results outputs. DINGO was developed for Australian data, but the framework is applicable to any flux data or regional network. Quality data from robust systems like DINGO ensure the utility and uptake of the flux data and facilitates synergies between flux, remote sensing and modelling.{$<$}/p{$>$}},
  langid = {english},
  file = {/home/simone/Zotero/storage/BII2QGZT/Beringer et al. - 2017 - Technical note Dynamic INtegrated Gap-filling and.pdf;/home/simone/Zotero/storage/VSHHQZNK/2017.html}
}

@article{bierman_new_1983,
  title = {A New Computationally Efficient Fixed-Interval, Discrete-Time Smoother},
  author = {Bierman, G. J.},
  date = {1983-09-01},
  journaltitle = {Automatica},
  shortjournal = {Automatica},
  volume = {19},
  number = {5},
  pages = {503--511},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(83)90005-5},
  url = {https://www.sciencedirect.com/science/article/pii/0005109883900055},
  urldate = {2023-01-24},
  abstract = {The Rauch-Tung-Streibel smoother recursion is used to derive a new smoother algorithm based upon a decomposition of the linear model dynamical equation and maximizing use of rank 1 matrix modification. This new algorithm, it turns out, parallels Bierman's forward recursive square-root information filter/backward recursive U-D factorized covariance algorithm. The new result features computational efficiency, reliance on numerically stable matrix modification algorithms, and reduced computer storage.},
  langid = {english},
  keywords = {computational methods,Kalman filters,Márkov processes,navigation,numerical methods,smoothing,state estimation,time-varying systems,tracking systems},
  file = {/home/simone/Zotero/storage/93ETWGS3/Bierman - 1983 - A new computationally efficient fixed-interval, di.pdf;/home/simone/Zotero/storage/HZIQTCYY/0005109883900055.html}
}

@article{bierman_numerical_1977,
  title = {Numerical Comparison of Kalman Filter Algorithms: {{Orbit}} Determination Case Study},
  shorttitle = {Numerical Comparison of Kalman Filter Algorithms},
  author = {Bierman, Gerald J. and Thornton, Catherine L.},
  date = {1977-01-01},
  journaltitle = {Automatica},
  shortjournal = {Automatica},
  volume = {13},
  number = {1},
  pages = {23--35},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(77)90006-1},
  url = {https://www.sciencedirect.com/science/article/pii/0005109877900061},
  urldate = {2023-01-12},
  abstract = {Numerical characteristics of various Kalman filter algorithms are illustrated with a realistic orbit determination study. The case study of this paper highlights the numerical deficiencies of the conventional and stabilized Kalman algorithms. Computational errors associated with these algorithms are found to be so large as to obscure important mismodeling effects and thus cause misleading estimates of filter accuracy. The positive result of this study is that the U-D covariance factorization algorithm has excellent numerical properties and is computationally efficient, having CPU costs that differ negligibly from the conventional Kalman costs. Accuracies of the U-D filter using single precision arithmetic consistently match the double precision reference results. Numerical stability of the U-D filter is further demonstrated by its insensitivity to variations in the a priori statistics.},
  langid = {english},
  keywords = {computational methods,computer software,data processing,discrete systems,Filtering,Kalman filters,optimal filtering,space vehicles,state estimation,stochastic systems},
  file = {/home/simone/Zotero/storage/D342WBRN/Bierman and Thornton - 1977 - Numerical comparison of kalman filter algorithms .pdf;/home/simone/Zotero/storage/JXCFPC8I/0005109877900061.html}
}

@book{bishop_pattern_2006,
  title = {Pattern Recognition and {{Machine Learning}}},
  author = {{Bishop}},
  date = {2006},
  file = {/home/simone/Zotero/storage/XTGVGHCE/Bishop - 2006 - Pattern recognition and Machine Learning.pdf}
}

@article{bonan_improving_2011-1,
  title = {Improving Canopy Processes in the {{Community Land Model}} Version 4 ({{CLM4}}) Using Global Flux Fields Empirically Inferred from {{FLUXNET}} Data},
  author = {Bonan, Gordon B. and Lawrence, Peter J. and Oleson, Keith W. and Levis, Samuel and Jung, Martin and Reichstein, Markus and Lawrence, David M. and Swenson, Sean C.},
  date = {2011},
  journaltitle = {Journal of Geophysical Research: Biogeosciences},
  volume = {116},
  number = {G2},
  issn = {2156-2202},
  doi = {10.1029/2010JG001593},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2010JG001593},
  urldate = {2023-02-10},
  abstract = {The Community Land Model version 4 (CLM4) overestimates gross primary production (GPP) compared with data-driven estimates and other process models. We use global, spatially gridded GPP and latent heat flux upscaled from the FLUXNET network of eddy covariance towers to evaluate and improve canopy processes in CLM4. We investigate differences in GPP and latent heat flux arising from model parameterizations (termed model structural error) and from uncertainty in the photosynthetic parameter Vcmax (termed model parameter uncertainty). Model structural errors entail radiative transfer, leaf photosynthesis and stomatal conductance, and canopy scaling of leaf processes. Model structural revisions reduce global GPP over the period 1982–2004 from 165 Pg C yr−1 to 130 Pg C yr−1, and global evapotranspiration decreases from 68,000 km3 yr−1 to 65,000 km3 yr−1, within the uncertainty of FLUXNET-based estimates. Colimitation of photosynthesis is a cause of the improvements, as are revisions to photosynthetic parameters and their temperature dependency. Improvements are seen in all regions and seasonally over the course of the year. Similar improvements occur in latent heat flux. Uncertainty in Vcmax produces effects of comparable magnitude as model structural errors, but of offsetting sign. This suggests that model structural errors can be compensated by parameter adjustment, and this may explain the lack of consensus in values for Vcmax used in terrestrial biosphere models. Our analyses show that despite inherent uncertainties global flux fields empirically inferred from FLUXNET data are a valuable tool to guide terrestrial biosphere model development and evaluation.},
  langid = {english},
  keywords = {biosphere model,evapotranspiration,FLUXNET,gross primary production,land surface model},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2010JG001593},
  file = {/home/simone/Zotero/storage/YV5J8SK5/Bonan et al. - 2011 - Improving canopy processes in the Community Land M.pdf;/home/simone/Zotero/storage/J2X2C3Q7/2010JG001593.html}
}

@article{buuren_mice_2011,
  title = {Mice: {{Multivariate Imputation}} by {{Chained Equations}} in {{R}}},
  shorttitle = {Mice},
  author = {van Buuren, Stef and Groothuis-Oudshoorn, Karin},
  date = {2011-12-12},
  journaltitle = {Journal of Statistical Software},
  volume = {45},
  pages = {1--67},
  issn = {1548-7660},
  doi = {10.18637/jss.v045.i03},
  url = {https://doi.org/10.18637/jss.v045.i03},
  urldate = {2023-02-18},
  abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
  langid = {english}
}

@misc{cao_brits_2018,
  title = {{{BRITS}}: {{Bidirectional Recurrent Imputation}} for {{Time Series}}},
  shorttitle = {{{BRITS}}},
  author = {Cao, Wei and Wang, Dong and Li, Jian and Zhou, Hao and Li, Lei and Li, Yitan},
  date = {2018-05-26},
  number = {arXiv:1805.10572},
  eprint = {1805.10572},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.10572},
  url = {http://arxiv.org/abs/1805.10572},
  urldate = {2022-06-16},
  abstract = {Time series are widely used as signals in many classification/regression tasks. It is ubiquitous that time series contains many missing values. Given multiple correlated time series data, how to fill in missing values and to predict their class labels? Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose BRITS, a novel method based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during the backpropagation.BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data.We evaluate our model on three real-world datasets, including an air quality dataset, a health-care data, and a localization data for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression accuracies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simone/Zotero/storage/A9RLV2K4/Cao et al. - 2018 - BRITS Bidirectional Recurrent Imputation for Time.pdf;/home/simone/Zotero/storage/KIZ6DKID/1805.html}
}

@article{cao_brits_nodate,
  title = {{{BRITS}}: {{Bidirectional Recurrent Imputation}} for {{Time Series}}},
  author = {Cao, Wei and Wang, Dong and Li, Jian and Zhou, Hao and Li, Lei and Li, Yitan},
  pages = {11},
  abstract = {Time series are ubiquitous in many classification/regression applications. However, the time series data in real applications may contain many missing values. Hence, given multiple (possibly correlated) time series data, it is important to fill in missing values and at the same time to predict their class labels. Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose a novel method, called BRITS, based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during backpropagation. BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data. We evaluate our model on three real-world datasets, including an air quality dataset, a healthcare dataset, and a localization dataset for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression.},
  langid = {english},
  file = {/home/simone/Zotero/storage/CFW39DYF/Cao et al. - BRITS Bidirectional Recurrent Imputation for Time.pdf}
}

@article{carlson_fast_1973,
  title = {Fast Triangular Formulation of the Square Root Filter.},
  author = {CARLSON, NEAL A.},
  date = {1973},
  journaltitle = {AIAA Journal},
  volume = {11},
  number = {9},
  pages = {1259--1265},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  issn = {0001-1452},
  doi = {10.2514/3.6907},
  url = {https://doi.org/10.2514/3.6907},
  urldate = {2023-02-15},
  annotation = {\_eprint: https://doi.org/10.2514/3.6907},
  file = {/home/simone/Zotero/storage/H3CEAURC/3.html}
}

@article{costa_gap_2021,
  title = {Gap {{Filling}} and {{Quality Control Applied}} to {{Meteorological Variables Measured}} in the {{Northeast Region}} of {{Brazil}}},
  author = {Costa, Rafaela Lisboa and Barros Gomes, Heliofábio and Cavalcante Pinto, David Duarte and da Rocha Júnior, Rodrigo Lins and dos Santos Silva, Fabrício Daniel and Barros Gomes, Helber and Lemos da Silva, Maria Cristina and Luís Herdies, Dirceu},
  options = {useprefix=true},
  date = {2021-10},
  journaltitle = {Atmosphere},
  volume = {12},
  number = {10},
  pages = {1278},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-4433},
  doi = {10.3390/atmos12101278},
  url = {https://www.mdpi.com/2073-4433/12/10/1278},
  urldate = {2022-05-14},
  abstract = {In this work, we used the MICE (Multivariate Imputation by Chained Equations) technique to impute missing daily data from six meteorological variables (precipitation, temperature, relative humidity, atmospheric pressure, wind speed and insolation) from 96 stations located in the northeast region of Brazil (NEB) for the period from 1961 to 2014. We then applied tests with a quality control system (QCS) developed for the detection, correction and possible replacement of suspicious data. Both the applied gap filling technique and the QCS showed that it was possible to solve two of the biggest problems found in time series of daily data measured in meteorological stations: the generation of plausible values for each variable of interest, in order to remedy the absence of observations, and how to detect and allow proper correction of suspicious values arising from observations.},
  issue = {10},
  langid = {english},
  keywords = {climate analysis,data quality control,missing values,time series,verification},
  file = {/home/simone/Zotero/storage/PXFKAHJY/Costa et al. - 2021 - Gap Filling and Quality Control Applied to Meteoro.pdf;/home/simone/Zotero/storage/2LGVL8TT/1278.html}
}

@book{dan_simon_optimal_2006,
  title = {Optimal {{State Estimation Kalman}}, {{H}} and {{Nonlinear Approaches}}},
  author = {{Dan Simon}},
  date = {2006},
  isbn = {100-47 1-70858-5},
  file = {/home/simone/Documents/uni/Thesis/books/Optimate state estimation.pdf}
}

@incollection{dong_filling_2021,
  title = {Filling {{Gaps}} in {{Micro-meteorological Data}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Applied Data Science}} and {{Demo Track}}},
  author = {Richard, Antoine and Fine, Lior and Rozenstein, Offer and Tanny, Josef and Geist, Matthieu and Pradalier, Cedric},
  editor = {Dong, Yuxiao and Ifrim, Georgiana and Mladenić, Dunja and Saunders, Craig and Van Hoecke, Sofie},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12461},
  pages = {101--117},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-67670-4_7},
  url = {https://link.springer.com/10.1007/978-3-030-67670-4_7},
  urldate = {2022-05-14},
  abstract = {Filling large data-gaps in Micro-Meteorological data has mostly been done using interpolation techniques based on a marginal distribution sampling. Those methods work well but need a large horizon of the previous events to achieve good results since they do not model the system but only rely on previously encountered iterations. In this paper, we propose to use multi-head deep attention networks to fill gaps in Micro-Meteorological Data. This methodology couples large-scale information extraction with modeling capabilities that cannot be achieved by interpolation-like techniques. Unlike Bidirectional RNNs, our architecture is not recurrent, it is simple to tune and our data efficiency is higher. We apply our architecture to real-life data and clearly show its applicability in agriculture, furthermore, we show that it could be used to solve related problems such as filling gaps in cyclic-multivariate-time-series.},
  isbn = {978-3-030-67669-8 978-3-030-67670-4},
  langid = {english},
  file = {/home/simone/Zotero/storage/RGEEFQ66/Richard et al. - 2021 - Filling Gaps in Micro-meteorological Data.pdf}
}

@unpublished{du_saits_2022,
  title = {{{SAITS}}: {{Self-Attention-based Imputation}} for {{Time Series}}},
  shorttitle = {{{SAITS}}},
  author = {Du, Wenjie and Cote, David and Liu, Yan},
  date = {2022-05-09},
  number = {arXiv:2202.08516},
  eprint = {2202.08516},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.08516},
  url = {http://arxiv.org/abs/2202.08516},
  urldate = {2022-05-14},
  abstract = {Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-series imputation task efficiently and reveal SAITS' potential to improve the learning performance of pattern recognition models on incomplete time-series data from the real world.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simone/Zotero/storage/IMCRV64Z/Du et al. - 2022 - SAITS Self-Attention-based Imputation for Time Se.pdf;/home/simone/Zotero/storage/QEKTBAF5/2202.html}
}

@misc{du_saits_2022-1,
  title = {{{SAITS}}: {{Self-Attention-based Imputation}} for {{Time Series}}},
  shorttitle = {{{SAITS}}},
  author = {Du, Wenjie and Cote, David and Liu, Yan},
  date = {2022-05-09},
  number = {arXiv:2202.08516},
  eprint = {2202.08516},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.08516},
  urldate = {2022-05-14},
  abstract = {Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weightedcombination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-series imputation task efficiently and reveal SAITS’ potential to improve the learning performance of pattern recognition models on incomplete time-series data from the real world.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simone/Zotero/storage/T6JN5ASV/Du et al. - 2022 - SAITS Self-Attention-based Imputation for Time Se.pdf}
}

@incollection{durbin_further_2012,
  title = {Further Computational Aspects},
  booktitle = {Time {{Series Analysis}} by {{State Space Methods}}},
  author = {Durbin, J. and Koopman, S.J.},
  editor = {Durbin, James and Koopman, Siem Jan},
  date = {2012-05-03},
  pages = {0},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199641178.003.0006},
  url = {https://doi.org/10.1093/acprof:oso/9780199641178.003.0006},
  urldate = {2023-01-14},
  abstract = {This chapter continues the discussion of the computational aspects of filtering and smoothing. It explains the estimation of a regression component of the model and intervention components; the square root filter and smoother which may be used when the Kalman filter and smoother show signs of numerical instability; how multivariate time series can be treated as univariate series by bringing elements of the observational vectors into the system one at a time, with computational savings relative to the multivariate treatment in some cases; and further modifications where the observation vector is high-dimensional. The chapter concludes by discussing computer packages for state space methods.},
  isbn = {978-0-19-964117-8},
  file = {/home/simone/Zotero/storage/RGUE4AJA/Durbin and Koopman - 2012 - Further computational aspects.pdf;/home/simone/Zotero/storage/A2TG7R3Z/173393244.html}
}

@incollection{durbin_maximum_2012,
  title = {Maximum Likelihood Estimation of Parameters},
  booktitle = {Time {{Series Analysis}} by {{State Space Methods}}},
  author = {Durbin, J. and Koopman, S.J.},
  editor = {Durbin, James and Koopman, Siem Jan},
  date = {2012-05-03},
  pages = {0},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199641178.003.0007},
  url = {https://doi.org/10.1093/acprof:oso/9780199641178.003.0007},
  urldate = {2023-01-14},
  abstract = {This chapter discusses maximum likelihood estimation of parameters both for the case where the distribution of the initial state vector is known and for the case where at least some elements of the vector are diffuse or are treated as fixed and unknown. For the linear Gaussian model it shows that the likelihood can be calculated by a routine application of the Kalman filter, even when the initial state vector is fully or partially diffuse. It details the computation of the likelihood when the univariate treatment of multivariate observations is adopted. It considers how the loglikelihood can be maximised by means of iterative numerical procedures. An important part in this process is played by the score vector. The chapter shows how this is calculated, both for the case where the initial state vector has a known distribution and for the diffuse case. A useful device for maximisation of the loglikelihood in some cases, particularly in the early stages of maximisation, is the EM algorithm; details are provided for the linear Gaussian model. The chapter also considers biases in estimates due to errors in parameter estimation and ends with a discussion of some questions of goodness-of-fit and diagnostic checks.},
  isbn = {978-0-19-964117-8},
  file = {/home/simone/Zotero/storage/G73FT33R/Durbin and Koopman - 2012 - Maximum likelihood estimation of parameters.pdf;/home/simone/Zotero/storage/P2MFEAY8/173394006.html}
}

@book{durbin_time_2012,
  title = {Time {{Series Analysis}} by {{State Space Methods}}},
  author = {Durbin, James and Koopman, Siem Jan},
  date = {2012-03-02},
  url = {https://doi.org/10.1093/acprof:oso/9780199641178.001.0001},
  abstract = {This book presents a comprehensive treatment of the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as being made up of distinct components such as trend, seasonal, regression elements and disturbance elements, each of which is modelled separately. The techniques that emerge from this approach are very flexible. Part I presents a full treatment of the construction and analysis of linear Gaussian state space models. The methods are based on the Kalman filter and are appropriate for a wide range of problems in practical time series analysis. The analysis can be carried out from both classical and Bayesian perspectives. Part I then presents illustrations to real series and exercises are provided for a selection of chapters. Part II discusses approximate and exact approaches for handling broad classes of non-Gaussian and nonlinear state space models. Approximate methods include the extended Kalman filter and the more recently developed unscented Kalman filter. The book shows that exact treatments become feasible when simulation-based methods such as importance sampling and particle filtering are adopted. Bayesian treatments based on simulation methods are also explored.}
}

@book{durbin_time_2012-1,
  title = {Time {{Series Analysis}} by {{State Space Methods}}: {{Second Edition}}},
  shorttitle = {Time {{Series Analysis}} by {{State Space Methods}}},
  author = {Durbin, James and Koopman, Siem Jan},
  date = {2012-05-03},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199641178.001.0001},
  url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199641178.001.0001/acprof-9780199641178},
  urldate = {2022-11-13},
  isbn = {978-0-19-964117-8},
  langid = {english},
  file = {/home/simone/Zotero/storage/PCNYPB2V/Durbin and Koopman - 2012 - Time Series Analysis by State Space Methods Secon.pdf}
}

@article{falge_gap_2001,
  title = {Gap Filling Strategies for Long Term Energy Flux Data Sets},
  author = {Falge, Eva and Baldocchi, Dennis and Olson, Richard and Anthoni, Peter and Aubinet, Marc and Bernhofer, Christian and Burba, George and Ceulemans, Reinhart and Clement, Robert and Dolman, Han and Granier, André and Gross, Patrick and Grünwald, Thomas and Hollinger, David and Jensen, Niels-Otto and Katul, Gabriel and Keronen, Petri and Kowalski, Andrew and Ta Lai, Chun and Law, Beverley E and Meyers, Tilden and Moncrieff, John and Moors, Eddy and William Munger, J and Pilegaard, Kim and Rannik, Üllar and Rebmann, Corinna and Suyker, Andrew and Tenhunen, John and Tu, Kevin and Verma, Shashi and Vesala, Timo and Wilson, Kell and Wofsy, Steve},
  date = {2001-03-01},
  journaltitle = {Agricultural and Forest Meteorology},
  shortjournal = {Agricultural and Forest Meteorology},
  volume = {107},
  number = {1},
  pages = {71--77},
  issn = {0168-1923},
  doi = {10.1016/S0168-1923(00)00235-5},
  url = {https://www.sciencedirect.com/science/article/pii/S0168192300002355},
  urldate = {2022-05-14},
  abstract = {At present a network of over 100 field sites are measuring carbon dioxide, water vapor and sensible heat fluxes between the biosphere and atmosphere, on a nearly continuous basis. Gaps in the long term measurements of evaporation and sensible heat flux must be filled before these data can be used for hydrological and meteorological applications. We adapted methods of gap filling for NEE (net ecosystem exchange of carbon) to energy fluxes and applied them to data sets available from the EUROFLUX and AmeriFlux eddy covariance databases. The average data coverage for the sites selected was 69\% and 75\% for latent heat (λE) and sensible heat (H). The methods were based on mean diurnal variations (half-hourly binned means of fluxes based on previous and subsequent days, MDV) and look-up tables for fluxes during assorted meteorological conditions (LookUp), and the impact of different gap filling methods on the annual sum of λE and H is investigated. The difference between annual λE filled by MDV and λE filled by LookUp ranged from −120 to 210MJm−2 per year, i.e. −48 to +86mm per year, or −13 to +39\% of the annual sum. For annual sums of H differences between −140 and +140MJm−2 per year or −12 to +19\% of the annual sum were found.},
  langid = {english},
  keywords = {AmeriFlux,Data filling,Eddy covariance,EUROFLUX,FLUXNET,Interpolation techniques,Latent heat,Sensible heat},
  file = {/home/simone/Zotero/storage/MDU8C4VI/Falge et al. - 2001 - Gap filling strategies for long term energy flux d.pdf;/home/simone/Zotero/storage/SEJJFKXA/S0168192300002355.html}
}

@article{falge_gap_2001-1,
  title = {Gap Filling Strategies for Defensible Annual Sums of Net Ecosystem Exchange},
  author = {Falge, Eva and Baldocchi, Dennis and Olson, Richard and Anthoni, Peter and Aubinet, Marc and Bernhofer, Christian and Burba, George and Ceulemans, Reinhart and Clement, Robert and Dolman, Han and Granier, André and Gross, Patrick and Grünwald, Thomas and Hollinger, David and Jensen, Niels-Otto and Katul, Gabriel and Keronen, Petri and Kowalski, Andrew and Lai, Chun Ta and Law, Beverley E. and Meyers, Tilden and Moncrieff, John and Moors, Eddy and Munger, J. William and Pilegaard, Kim and Rannik, Üllar and Rebmann, Corinna and Suyker, Andrew and Tenhunen, John and Tu, Kevin and Verma, Shashi and Vesala, Timo and Wilson, Kell and Wofsy, Steve},
  date = {2001-03-01},
  journaltitle = {Agricultural and Forest Meteorology},
  shortjournal = {Agricultural and Forest Meteorology},
  volume = {107},
  number = {1},
  pages = {43--69},
  issn = {0168-1923},
  doi = {10.1016/S0168-1923(00)00225-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0168192300002252},
  urldate = {2022-05-12},
  abstract = {Heightened awareness of global change issues within both science and political communities has increased interest in using the global network of eddy covariance flux towers to more fully understand the impacts of natural and anthropogenic phenomena on the global carbon balance. Comparisons of net ecosystem exchange (FNEE) responses are being made among biome types, phenology patterns, and stress conditions. The comparisons are usually performed on annual sums of FNEE; however, the average data coverage during a year is only 65\%. Therefore, robust and consistent gap filling methods are required. We review several methods of gap filling and apply them to data sets available from the EUROFLUX and AmeriFlux databases. The methods are based on mean diurnal variation (MDV), look-up tables (LookUp), and nonlinear regressions (Regr.), and the impact of different gap filling methods on the annual sum of FNEE is investigated. The difference between annual FNEE filled by MDV compared to FNEE filled by Regr. ranged from −45 to +200gCm−2 per year (MDV−Regr.). Comparing LookUp and Regr. methods resulted in a difference (LookUp−Regr.) ranging from −30 to +150gCm−2 per year. We also investigated the impact of replacing measurements at night, when turbulent mixing is insufficient. The nighttime correction for low friction velocities (u∗) shifted annual FNEE on average by +77gCm−2 per year, but in certain cases as much as +185gCm−2 per year. Our results emphasize the need to standardize gap filling-methods for improving the comparability of flux data products from regional and global flux networks.},
  langid = {english},
  keywords = {AmeriFlux,Data filling,Eddy covariance,EUROFLUX,FLUXNET,Interpolation techniques},
  file = {/home/simone/Zotero/storage/KLVQKU2U/Falge et al. - 2001 - Gap filling strategies for defensible annual sums .pdf}
}

@unpublished{fang_time_2020-1,
  title = {Time {{Series Data Imputation}}: {{A Survey}} on {{Deep Learning Approaches}}},
  shorttitle = {Time {{Series Data Imputation}}},
  author = {Fang, Chenguang and Wang, Chen},
  date = {2020-11-23},
  eprint = {2011.11347},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2011.11347},
  urldate = {2022-05-12},
  abstract = {Time series are all around in real-world applications. However, unexpected accidents for example broken sensors or missing of the signals will cause missing values in time series, making the data hard to be utilized. It then does harm to the downstream applications such as traditional classification or regression, sequential data integration and forecasting tasks, thus raising the demand for data imputation. Currently, time series data imputation is a well-studied problem with different categories of methods. However, these works rarely take the temporal relations among the observations and treat the time series as normal structured data, losing the information from the time data. In recent, deep learning models have raised great attention. Time series methods based on deep learning have made progress with the usage of models like RNN, since it captures time information from data. In this paper, we mainly focus on time series imputation technique with deep learning methods, which recently made progress in this field. We will review and discuss their model architectures, their pros and cons as well as their effects to show the development of the time series imputation methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/simone/Zotero/storage/Y3PEVSW8/Fang and Wang - 2020 - Time Series Data Imputation A Survey on Deep Lear.pdf}
}

@misc{fortuin_gp-vae_2020,
  title = {{{GP-VAE}}: {{Deep Probabilistic Time Series Imputation}}},
  shorttitle = {{{GP-VAE}}},
  author = {Fortuin, Vincent and Baranchuk, Dmitry and Rätsch, Gunnar and Mandt, Stephan},
  date = {2020-02-20},
  number = {arXiv:1907.04155},
  eprint = {1907.04155},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.04155},
  url = {http://arxiv.org/abs/1907.04155},
  urldate = {2022-07-28},
  abstract = {Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simone/Zotero/storage/7MIR996R/Fortuin et al. - 2020 - GP-VAE Deep Probabilistic Time Series Imputation.pdf;/home/simone/Zotero/storage/C4M3XGNN/1907.html}
}

@article{friend_fluxnet_2007-1,
  title = {{{FLUXNET}} and Modelling the Global Carbon Cycle},
  author = {Friend, Andrew D. and Arneth, Almut and Kiang, Nancy Y. and Lomas, Mark and Ogée, Jérôme and Rödenbeck, Christian and Running, Steven W. and Santaren, Jean-Diego and Sitch, Stephen and Viovy, Nicolas and Ian Woodward, F. and Zaehle, Sönke},
  date = {2007},
  journaltitle = {Global Change Biology},
  volume = {13},
  number = {3},
  pages = {610--633},
  issn = {1365-2486},
  doi = {10.1111/j.1365-2486.2006.01223.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2486.2006.01223.x},
  urldate = {2023-02-10},
  abstract = {Measurements of the net CO2 flux between terrestrial ecosystems and the atmosphere using the eddy covariance technique have the potential to underpin our interpretation of regional CO2 source–sink patterns, CO2 flux responses to forcings, and predictions of the future terrestrial C balance. Information contained in FLUXNET eddy covariance data has multiple uses for the development and application of global carbon models, including evaluation/validation, calibration, process parameterization, and data assimilation. This paper reviews examples of these uses, compares global estimates of the dynamics of the global carbon cycle, and suggests ways of improving the utility of such data for global carbon modelling. Net ecosystem exchange of CO2 (NEE) predicted by different terrestrial biosphere models compares favourably with FLUXNET observations at diurnal and seasonal timescales. However, complete model validation, particularly over the full annual cycle, requires information on the balance between assimilation and decomposition processes, information not readily available for most FLUXNET sites. Site history, when known, can greatly help constrain the model-data comparison. Flux measurements made over four vegetation types were used to calibrate the land-surface scheme of the Goddard Institute for Space Studies global climate model, significantly improving simulated climate and demonstrating the utility of diurnal FLUXNET data for climate modelling. Land-surface temperatures in many regions cool due to higher canopy conductances and latent heat fluxes, and the spatial distribution of CO2 uptake provides a significant additional constraint on the realism of simulated surface fluxes. FLUXNET data are used to calibrate a global production efficiency model (PEM). This model is forced by satellite-measured absorbed radiation and suggests that global net primary production (NPP) increased 6.2\% over 1982–1999. Good agreement is found between global trends in NPP estimated by the PEM and a dynamic global vegetation model (DGVM), and between the DGVM and estimates of global NEE derived from a global inversion of atmospheric CO2 measurements. Combining the PEM, DGVM, and inversion results suggests that CO2 fertilization is playing a major role in current increases in NPP, with lesser impacts from increasing N deposition and growing season length. Both the PEM and the inversion identify the Amazon basin as a key region for the current net terrestrial CO2 uptake (i.e. 33\% of global NEE), as well as its interannual variability. The inversion's global NEE estimate of −1.2 Pg [C] yr−1 for 1982–1995 is compatible with the PEM- and DGVM-predicted trends in NPP. There is, thus, a convergence in understanding derived from process-based models, remote-sensing-based observations, and inversion of atmospheric data. Future advances in field measurement techniques, including eddy covariance (particularly concerning the problem of night-time fluxes in dense canopies and of advection or flow distortion over complex terrain), will result in improved constraints on land-atmosphere CO2 fluxes and the rigorous attribution of mechanisms to the current terrestrial net CO2 uptake and its spatial and temporal heterogeneity. Global ecosystem models play a fundamental role in linking information derived from FLUXNET measurements to atmospheric CO2 variability. A number of recommendations concerning FLUXNET data are made, including a request for more comprehensive site data (particularly historical information), more measurements in undisturbed ecosystems, and the systematic provision of error estimates. The greatest value of current FLUXNET data for global carbon cycle modelling is in evaluating process representations, rather than in providing an unbiased estimate of net CO2 exchange.},
  langid = {english},
  keywords = {CO2 exchange,eddy covariance,FLUXNET,global carbon cycle,inversion,model},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2486.2006.01223.x},
  file = {/home/simone/Zotero/storage/UNLXF76S/Friend et al. - 2007 - FLUXNET and modelling the global carbon cycle.pdf;/home/simone/Zotero/storage/IY23XHR3/j.1365-2486.2006.01223.html}
}

@misc{garnelo_conditional_2018,
  title = {Conditional {{Neural Processes}}},
  author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J. and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J. and Eslami, S. M. Ali},
  date = {2018-07-04},
  number = {arXiv:1807.01613},
  eprint = {1807.01613},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.01613},
  url = {http://arxiv.org/abs/1807.01613},
  urldate = {2022-06-23},
  abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simone/Zotero/storage/2KWI4I2B/chapter 12 - Particle filtering - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/2WT32PC7/chapter 8 - Illustration of the use of linear model - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/5QWILWR7/chpater 11 - Importance sampling for smoothing.pdf;/home/simone/Zotero/storage/5Y23RW3T/Chapter 7 - Maximium likelihood estimation of parameters - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/BFM2VHAA/chapter 1 - Introduction - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/BUINT5A3/chapter 13 - Bayesan estimantors - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/CEWWJ4EL/chapter 10 - Aproximate filtering and smoothing - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/DVGTCTUS/chapter 14 - Non-guassian non-linear illustration - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/EBNWLBSU/chapter 9 - special case of non-linear and non-guassian models - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/GGM2PC9D/Chapter 5 - initialization of filter and smoother.pdf;/home/simone/Zotero/storage/JPXX3RH3/Garnelo et al. - 2018 - Conditional Neural Processes.pdf;/home/simone/Zotero/storage/PP3CAXL2/Chapter 2 - Local level model - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/SE8LUDLL/Chapter 5 - further computation aspects - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/UCTEMRUH/Chapter 3 - Linear state space models - Time Series Analysis by State Space Methods.pdf;/home/simone/Zotero/storage/WHG9UQ6E/1807.html}
}

@misc{garnelo_neural_2018,
  title = {Neural {{Processes}}},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  date = {2018-07-04},
  number = {arXiv:1807.01622},
  eprint = {1807.01622},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.01622},
  url = {http://arxiv.org/abs/1807.01622},
  urldate = {2022-06-23},
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simone/Zotero/storage/4F5XTHVB/Garnelo et al. - 2018 - Neural Processes.pdf;/home/simone/Zotero/storage/S78MUD4Q/1807.html}
}

@book{golub_matrix_2013,
  title = {Matrix {{Computations}}},
  author = {Golub, Gene H. and Loan, Charles F. Van},
  date = {2013},
  eprint = {X5YfsuCWpxMC},
  eprinttype = {googlebooks},
  publisher = {{JHU Press}},
  abstract = {A comprehensive treatment of numerical linear algebra from the standpoint of both theory and practice.The fourth edition of Gene H. Golub and Charles F. Van Loan's classic is an essential reference for computational scientists and engineers in addition to researchers in the numerical linear algebra community. Anyone whose work requires the solution to a matrix problem and an appreciation of its mathematical properties will find this book to be an indispensible tool. This revision is a cover-to-cover expansion and renovation of the third edition. It now includes an introduction to tensor computations and brand new sections on • fast transforms• parallel LU• discrete Poisson solvers• pseudospectra• structured linear equation problems• structured eigenvalue problems• large-scale SVD methods• polynomial eigenvalue problems Matrix Computations is packed with challenging problems, insightful derivations, and pointers to the literature—everything needed to become a matrix-savvy developer of numerical methods and software. The second most cited math book of 2012 according to MathSciNet, the book has placed in the top 10 for since 2005.},
  isbn = {978-1-4214-0794-4},
  langid = {english},
  pagetotal = {781},
  keywords = {Mathematics / Algebra / Linear,Mathematics / Applied,Mathematics / General},
  file = {/home/simone/Documents/uni/Thesis/books/(Johns Hopkins Studies in the Mathematical Sciences) Gene H. Golub, Charles F. Van Loan - Matrix Computations-Johns Hopkins University Press (2012).pdf}
}

@article{grewal_kalman_2010-1,
  title = {Kalman {{Filter Implementation With Improved Numerical Properties}}},
  author = {Grewal, Mohinder S. and Kain, James},
  date = {2010-09},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {55},
  number = {9},
  pages = {2058--2068},
  issn = {1558-2523},
  doi = {10.1109/TAC.2010.2042986},
  abstract = {This paper presents a new form of Kalman filter-the sigmaRho filter-useful for operational implementation in applications where stability and throughput requirements stress traditional implementations. The new mechanization has the benefits of square root filters in both promoting stability and reducing dynamic range of propagated terms. State standard deviations and correlation coefficients are propagated rather than covariance square root elements and these physically meaningful statistics are used to adapt the filtering for further ensuring reliable performance. Finally, all propagated variables can be scaled to predictable dynamic range so that fixed point procedures can be implemented for embedded applications. A sample problem from communications signal processing is presented that includes nonlinear state dynamics, extreme time-variation, and extreme range of system eigenvalues. The sigmaRho implementation is successfully applied at sample rates approaching 100 MHz to decode binary digital data from a 1.5-GHz carrier.},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {Dynamic range,Eigenvalues and eigenfunctions,Extended Kalman filter,Filtering,Kalman filters,Nonlinear dynamical systems,sigmaRho Kalman filter,Signal processing,square root Kalman filter,Stability,Statistics,Stress,Throughput},
  file = {/home/simone/Zotero/storage/RIP2WMJ2/Grewal and Kain - 2010 - Kalman Filter Implementation With Improved Numeric.pdf;/home/simone/Zotero/storage/CYXYMFHS/5411732.html}
}

@article{guen_shape_nodate,
  title = {Shape and {{Time Distortion Loss}} for {{Training Deep Time Series Forecasting Models}}},
  author = {Guen, Vincent LE and Thome, Nicolas},
  pages = {13},
  abstract = {This paper addresses the problem of time series forecasting for non-stationary signals and multiple future steps prediction. To handle this challenging task, we introduce DILATE (DIstortion Loss including shApe and TimE), a new objective function for training deep neural networks. DILATE aims at accurately predicting sudden changes, and explicitly incorporates two terms supporting precise shape and temporal change detection. We introduce a differentiable loss function suitable for training deep neural nets, and provide a custom back-prop implementation for speeding up optimization. We also introduce a variant of DILATE, which provides a smooth generalization of temporally-constrained Dynamic Time Warping (DTW). Experiments carried out on various non-stationary datasets reveal the very good behaviour of DILATE compared to models trained with the standard Mean Squared Error (MSE) loss function, and also to DTW and variants. DILATE is also agnostic to the choice of the model, and we highlight its benefit for training fully connected networks as well as specialized recurrent architectures, showing its capacity to improve over state-of-the-art trajectory forecasting approaches.},
  langid = {english},
  file = {/home/simone/Zotero/storage/8E8DTFEF/Guen and Thome - Shape and Time Distortion Loss for Training Deep T.pdf}
}

@article{hong_accuracy_2020,
  title = {Accuracy of Random-Forest-Based Imputation of Missing Data in the Presence of Non-Normality, Non-Linearity, and Interaction},
  author = {Hong, Shangzhi and Lynn, Henry S.},
  date = {2020-07-25},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {199},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-01080-1},
  url = {https://doi.org/10.1186/s12874-020-01080-1},
  urldate = {2022-05-17},
  abstract = {Missing data are common in statistical analyses, and imputation methods based on random forests (RF) are becoming popular for handling missing data especially in biomedical research. Unlike standard imputation approaches, RF-based imputation methods do not assume normality or require specification of parametric models. However, it is still inconclusive how they perform for non-normally distributed data or when there are non-linear relationships or interactions.},
  keywords = {Imputation accuracy,Missing data imputation,Random forest},
  file = {/home/simone/Zotero/storage/3KAXTWZA/Hong and Lynn - 2020 - Accuracy of random-forest-based imputation of miss.pdf;/home/simone/Zotero/storage/IGI24ZYX/s12874-020-01080-1.html}
}

@article{hui_gap-filling_2004,
  title = {Gap-Filling Missing Data in Eddy Covariance Measurements Using Multiple Imputation ({{MI}}) for Annual Estimations},
  author = {Hui, Dafeng and Wan, Shiqiang and Su, Bo and Katul, Gabriel and Monson, Russell and Luo, Yiqi},
  date = {2004-01-20},
  journaltitle = {Agricultural and Forest Meteorology},
  shortjournal = {Agricultural and Forest Meteorology},
  volume = {121},
  number = {1},
  pages = {93--111},
  issn = {0168-1923},
  doi = {10.1016/S0168-1923(03)00158-8},
  url = {https://www.sciencedirect.com/science/article/pii/S0168192303001588},
  urldate = {2022-05-12},
  abstract = {Missing data is a ubiquitous problem in evaluating long-term experimental measurements, such as those associated with the FluxNet project, due to the equipment failures, system maintenance, power-failure, and lightning strikes among other things. To estimate annual values of net ecosystem carbon exchange (NEE), latent heat flux (LE) and sensible heat flux (H), such gaps in the measured data must be filled or imputed. So far, no standardized method has been accepted and the imputation methods used are largely dependent on the researchers’ choice. Here, we used multiple imputation (MI) to gap-fill the missing data for annual estimations of NEE, LE and H at three flux sites associated with the FluxNet effort. MI is a Monte Carlo technique in which the missing values are replaced by several simulated values. Each data set imputed is a complete one where the observed values are the same as those in the original data set; only the missing values are different. Thus, the normal statistical analysis (e.g. annual total calculation) can be applied to each data set separately. The results of each analysis can be recombined into one summary. We applied the MI method to eddy covariance measurements collected from Walker Branch Watershed (WBW) site (a deciduous forest), Duke site (a coniferous forest) and Niwot site (a subalpine forest). Results showed that annual estimations of NEE, LE and H by MI were comparable to other imputation methods but MI was much easier to apply because of readily available software and standard algorithms. Besides the normal statistical analyses, MI also provided confidence intervals for each estimated parameter. This confidence interval is most useful when assessing energy, water, and carbon balance closures at a given tower site. Significant differences in annual NEE, LE and H were found among years at the three AmeriFlux sites. NEE at the Niwot Ridge site was lower and LE and H were higher than at the other two sites. With the available software and realistic gap-filling capability, MI has the potential to become a standardized method to gap-fill eddy covariance flux data for annual estimations and to improve the analysis of uncertainties associated with annual estimations of NEE, LE and H from regional and global flux networks.},
  langid = {english},
  keywords = {Eddy covariance,Latent heat,Missing data,Multiple imputation,Net ecosystem carbon exchange,Regression analysis,Sensible heat},
  file = {/home/simone/Zotero/storage/UQMXE27J/S0168192303001588.html}
}

@article{irvin_gap-filling_2021,
  title = {Gap-Filling Eddy Covariance Methane Fluxes: {{Comparison}} of Machine Learning Model Predictions and Uncertainties at {{FLUXNET-CH4}} Wetlands},
  shorttitle = {Gap-Filling Eddy Covariance Methane Fluxes},
  author = {Irvin, Jeremy and Zhou, Sharon and McNicol, Gavin and Lu, Fred and Liu, Vincent and Fluet-Chouinard, Etienne and Ouyang, Zutao and Knox, Sara Helen and Lucas-Moffat, Antje and Trotta, Carlo and Papale, Dario and Vitale, Domenico and Mammarella, Ivan and Alekseychik, Pavel and Aurela, Mika and Avati, Anand and Baldocchi, Dennis and Bansal, Sheel and Bohrer, Gil and Campbell, David I and Chen, Jiquan and Chu, Housen and Dalmagro, Higo J and Delwiche, Kyle B and Desai, Ankur R and Euskirchen, Eugenie and Feron, Sarah and Goeckede, Mathias and Heimann, Martin and Helbig, Manuel and Helfter, Carole and Hemes, Kyle S and Hirano, Takashi and Iwata, Hiroki and Jurasinski, Gerald and Kalhori, Aram and Kondrich, Andrew and Lai, Derrick YF and Lohila, Annalea and Malhotra, Avni and Merbold, Lutz and Mitra, Bhaskar and Ng, Andrew and Nilsson, Mats B and Noormets, Asko and Peichl, Matthias and Rey-Sanchez, A. Camilo and Richardson, Andrew D and Runkle, Benjamin RK and Schäfer, Karina VR and Sonnentag, Oliver and Stuart-Haëntjens, Ellen and Sturtevant, Cove and Ueyama, Masahito and Valach, Alex C and Vargas, Rodrigo and Vourlitis, George L and Ward, Eric J and Wong, Guan Xhuan and Zona, Donatella and Alberto, Ma. Carmelita R and Billesbach, David P and Celis, Gerardo and Dolman, Han and Friborg, Thomas and Fuchs, Kathrin and Gogo, Sébastien and Gondwe, Mangaliso J and Goodrich, Jordan P and Gottschalk, Pia and Hörtnagl, Lukas and Jacotot, Adrien and Koebsch, Franziska and Kasak, Kuno and Maier, Regine and Morin, Timothy H and Nemitz, Eiko and Oechel, Walter C and Oikawa, Patricia Y and Ono, Keisuke and Sachs, Torsten and Sakabe, Ayaka and Schuur, Edward A and Shortt, Robert and Sullivan, Ryan C and Szutu, Daphne J and Tuittila, Eeva-Stiina and Varlagin, Andrej and Verfaillie, Joeseph G and Wille, Christian and Windham-Myers, Lisamarie and Poulter, Benjamin and Jackson, Robert B},
  date = {2021-10-15},
  journaltitle = {Agricultural and Forest Meteorology},
  shortjournal = {Agricultural and Forest Meteorology},
  volume = {308--309},
  pages = {108528},
  issn = {0168-1923},
  doi = {10.1016/j.agrformet.2021.108528},
  url = {https://www.sciencedirect.com/science/article/pii/S0168192321002124},
  urldate = {2022-11-22},
  abstract = {Time series of wetland methane fluxes measured by eddy covariance require gap-filling to estimate daily, seasonal, and annual emissions. Gap-filling methane fluxes is challenging because of high variability and complex responses to multiple drivers. To date, there is no widely established gap-filling standard for wetland methane fluxes, with regards both to the best model algorithms and predictors. This study synthesizes results of different gap-filling methods systematically applied at 17 wetland sites spanning boreal to tropical regions and including all major wetland classes and two rice paddies. Procedures are proposed for: 1) creating realistic artificial gap scenarios, 2) training and evaluating gap-filling models without overstating performance, and 3) predicting half-hourly methane fluxes and annual emissions with realistic uncertainty estimates. Performance is compared between a conventional method (marginal distribution sampling) and four machine learning algorithms. The conventional method achieved similar median performance as the machine learning models but was worse than the best machine learning models and relatively insensitive to predictor choices. Of the machine learning models, decision tree algorithms performed the best in cross-validation experiments, even with a baseline predictor set, and artificial neural networks showed comparable performance when using all predictors. Soil temperature was frequently the most important predictor whilst water table depth was important at sites with substantial water table fluctuations, highlighting the value of data on wetland soil conditions. Raw gap-filling uncertainties from the machine learning models were underestimated and we propose a method to calibrate uncertainties to observations. The python code for model development, evaluation, and uncertainty estimation is publicly available. This study outlines a modular and robust machine learning workflow and makes recommendations for, and evaluates an improved baseline of, methane gap-filling models that can be implemented in multi-site syntheses or standardized products from regional and global flux networks (e.g., FLUXNET).},
  langid = {english},
  keywords = {flux,gap-filling,imputation,Machine learning,methane,time series,wetlands},
  file = {/home/simone/Zotero/storage/CPZD98PZ/Irvin et al. - 2021 - Gap-filling eddy covariance methane fluxes Compar.pdf;/home/simone/Zotero/storage/HK38X5IQ/S0168192321002124.html}
}

@article{isaac_ozflux_2017,
  title = {{{OzFlux}} Data: Network Integration from Collection to Curation},
  shorttitle = {{{OzFlux}} Data},
  author = {Isaac, Peter and Cleverly, James and McHugh, Ian and van Gorsel, Eva and Ewenz, Cacilia and Beringer, Jason},
  options = {useprefix=true},
  date = {2017-06-19},
  journaltitle = {Biogeosciences},
  volume = {14},
  number = {12},
  pages = {2903--2928},
  publisher = {{Copernicus GmbH}},
  issn = {1726-4170},
  doi = {10.5194/bg-14-2903-2017},
  url = {https://bg.copernicus.org/articles/14/2903/2017/},
  urldate = {2022-05-12},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Measurement of the exchange of energy and mass between the surface and the atmospheric boundary-layer by the eddy covariance technique has undergone great change in the last 2 decades. Early studies of these exchanges were confined to brief field campaigns in carefully controlled conditions followed by months of data analysis. Current practice is to run tower-based eddy covariance systems continuously over several years due to the need for continuous monitoring as part of a global effort to develop local-, regional-, continental- and global-scale budgets of carbon, water and energy. Efficient methods of processing the increased quantities of data are needed to maximise the time available for analysis and interpretation. Standardised methods are needed to remove differences in data processing as possible contributors to observed spatial variability. Furthermore, public availability of these data sets assists with undertaking global research efforts. The OzFlux data path has been developed (i) to provide a standard set of quality control and post-processing tools across the network, thereby facilitating inter-site integration and spatial comparisons; (ii) to increase the time available to researchers for analysis and interpretation by reducing the time spent collecting and processing data; (iii) to propagate both data and metadata to the final product; and (iv) to facilitate the use of the OzFlux data by adopting a standard file format and making the data available from web-based portals. Discovery of the OzFlux data set is facilitated through incorporation in FLUXNET data syntheses and the publication of collection metadata via the RIF-CS format. This paper serves two purposes. The first is to describe the data sets, along with their quality control and post-processing, for the other papers of this Special Issue. The second is to provide an example of one solution to the data collection and curation challenges that are encountered by similar flux tower networks worldwide.{$<$}/p{$>$}},
  langid = {english},
  file = {/home/simone/Zotero/storage/FPSGXH8K/Isaac et al. - 2017 - OzFlux data network integration from collection t.pdf;/home/simone/Zotero/storage/FND83KC4/2017.html}
}

@article{jing_multi-imputation_2022,
  title = {A {{Multi-imputation Method}} to {{Deal With Hydro-Meteorological Missing Values}} by {{Integrating Chain Equations}} and {{Random Forest}}},
  author = {Jing, Xin and Luo, Jungang and Wang, Jingmin and Zuo, Ganggang and Wei, Na},
  date = {2022-03-01},
  journaltitle = {Water Resources Management},
  shortjournal = {Water Resour Manage},
  volume = {36},
  number = {4},
  pages = {1159--1173},
  issn = {1573-1650},
  doi = {10.1007/s11269-021-03037-5},
  url = {https://doi.org/10.1007/s11269-021-03037-5},
  urldate = {2023-02-18},
  abstract = {Imputing hydro-meteorological missing values is essential in time series modeling. Imputation of missing values was traditionally performed after an observation period, which cannot effectively support water resources management in time. Therefore, it is necessary to deal with the missing data online. Moreover, traditional imputation methods usually consider only one observation variable and generate one set of imputations, which cannot describe the imputation uncertainty. Thus, a multiple-imputation method is proposed in this paper by integrating chain equations and random forest, namely, MICE-RF, to deal with the hydro-meteorological missing values. MICE-RF first simulates multiple imputation series to obtain the optimal imputations using the evaluation results of multiple imputation series. The traditional linear imputation, mean imputation, spline imputation, and k nearest neighbor imputation are compared to illustrate the robustness, reliability, and accuracy of the MICE-RF. According to the results, the MICE-RF provides the best imputation accuracy and can be easily implemented online.},
  langid = {english},
  keywords = {Hydro-meteorological time series,Missing data processing,Multiple imputation by chain equations,Random Forest},
  file = {/home/simone/Zotero/storage/HYQAVTWH/Jing et al. - 2022 - A Multi-imputation Method to Deal With Hydro-Meteo.pdf}
}

@article{kaminski_discrete_1971,
  title = {Discrete Square Root Filtering: {{A}} Survey of Current Techniques},
  shorttitle = {Discrete Square Root Filtering},
  author = {Kaminski, P. and Bryson, A. and Schmidt, S.},
  date = {1971-12},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {16},
  number = {6},
  pages = {727--736},
  issn = {1558-2523},
  doi = {10.1109/TAC.1971.1099816},
  abstract = {The conventional Kalman approach to discrete filtering involves propagation of a state estimate and an error covariance matrix from stage to stage. Alternate recursive relationships have been developed to propagate a state estimate and a square root error covariance instead. Although equivalent algebraically to the conventional approach, the square root filters exhibit improved numerical characteristics, particularly in ill-conditioned problems. In this paper, current techniques in square root filtering are surveyed and related by applying a duality association. Four efficient square root implementations are suggested, and compared with three common conventional implementations in terms of computational complexity and precision. The square root computational burden should not exceed the conventional by more than 50 percent in most practical problems. An examination of numerical conditioning predicts that the square root approach can yield twice the effective precision of the conventional filter in ill-conditioned problems. This prediction is verified in two examples. The excellent numerical characteristics and reasonable computation requirements of the square root approach make it a viable alternative to the conventional filter in many applications, particularly when computer word length is limited, or the estimation problem is badly conditioned.},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {Application software,Computer errors,Covariance matrix,Equations,Information filtering,Information filters,Kalman filters,Missiles,State estimation,Time measurement},
  file = {/home/simone/Zotero/storage/HKZXD4US/Kaminski et al. - 1971 - Discrete square root filtering A survey of curren.pdf}
}

@article{khayati_mind_2020,
  title = {Mind the {{Gap}}},
  author = {Khayati, Mourad and Lerner, Alberto and Tymchenko, Zakhar and Cudré-Mauroux, Philippe},
  date = {2020},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {13},
  pages = {768--782},
  doi = {10.14778/3377369.3377383},
  url = {https://folia.unifr.ch/global/documents/309429},
  urldate = {2022-05-14},
  abstract = {The SONAR project aims to create a scholarly archive that collects, promotes and preserves the publications of authors affiliated with Swiss public research institutions.},
  langid = {english},
  file = {/home/simone/Zotero/storage/JP6YW73U/Khayati et al. - 2020 - Mind the Gap.pdf;/home/simone/Zotero/storage/A3AE2YKK/309429.html}
}

@misc{kim_attentive_2019,
  title = {Attentive {{Neural Processes}}},
  author = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  date = {2019-07-09},
  number = {arXiv:1901.05761},
  eprint = {1901.05761},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1901.05761},
  url = {http://arxiv.org/abs/1901.05761},
  urldate = {2022-06-24},
  abstract = {Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/simone/Zotero/storage/2SISZIB7/Kim et al. - 2019 - Attentive Neural Processes.pdf;/home/simone/Zotero/storage/49DXUR9S/1901.html}
}

@article{kim_gap-filling_2020,
  title = {Gap-Filling Approaches for Eddy Covariance Methane Fluxes: {{A}} Comparison of Three Machine Learning Algorithms and a Traditional Method with Principal Component Analysis},
  shorttitle = {Gap-Filling Approaches for Eddy Covariance Methane Fluxes},
  author = {Kim, Yeonuk and Johnson, Mark S. and Knox, Sara H. and Black, T. Andrew and Dalmagro, Higo J. and Kang, Minseok and Kim, Joon and Baldocchi, Dennis},
  date = {2020},
  journaltitle = {Global Change Biology},
  volume = {26},
  number = {3},
  pages = {1499--1518},
  issn = {1365-2486},
  doi = {10.1111/gcb.14845},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/gcb.14845},
  urldate = {2022-11-22},
  abstract = {Methane flux (FCH4) measurements using the eddy covariance technique have increased over the past decade. FCH4 measurements commonly include data gaps, as is the case with CO2 and energy fluxes. However, gap-filling FCH4 data are more challenging than other fluxes due to its unique characteristics including multidriver dependency, variabilities across multiple timescales, nonstationarity, spatial heterogeneity of flux footprints, and lagged influence of biophysical drivers. Some researchers have applied a marginal distribution sampling (MDS) algorithm, a standard gap-filling method for other fluxes, to FCH4 datasets, and others have applied artificial neural networks (ANN) to resolve the challenging characteristics of FCH4. However, there is still no consensus regarding FCH4 gap-filling methods due to limited comparative research. We are not aware of the applications of machine learning (ML) algorithms beyond ANN to FCH4 datasets. Here, we compare the performance of MDS and three ML algorithms (ANN, random forest [RF], and support vector machine [SVM]) using multiple combinations of ancillary variables. In addition, we applied principal component analysis (PCA) as an input to the algorithms to address multidriver dependency of FCH4 and reduce the internal complexity of the algorithmic structures. We applied this approach to five benchmark FCH4 datasets from both natural and managed systems located in temperate and tropical wetlands and rice paddies. Results indicate that PCA improved the performance of MDS compared to traditional inputs. ML algorithms performed better when using all available biophysical variables compared to using PCA-derived inputs. Overall, RF was found to outperform other techniques for all sites. We found gap-filling uncertainty is much larger than measurement uncertainty in accumulated CH4 budget. Therefore, the approach used for FCH4 gap filling can have important implications for characterizing annual ecosystem-scale methane budgets, the accuracy of which is important for evaluating natural and managed systems and their interactions with global change processes.},
  langid = {english},
  keywords = {artificial neural network,comparison of gap-filling techniques,eddy covariance,machine learning,marginal distribution sampling,methane flux,random forest,support vector machine},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/gcb.14845},
  file = {/home/simone/Zotero/storage/PKU6MQNW/gcb.html}
}

@article{kramer_evaluation_2002,
  title = {Evaluation of Six Process-Based Forest Growth Models Using Eddy-Covariance Measurements of {{CO2}} and {{H2O}} Fluxes at Six Forest Sites in {{Europe}}},
  author = {Kramer, K. and Leinonen, I. and Bartelink, H. H. and Berbigier, P. and Borghetti, M. and Bernhofer, Ch and Cienciala, E. and Dolman, A. J. and Froer, O. and Gracia, C. A. and Granier, A. and Grünwald, T. and Hari, P. and Jans, W. and Kellomäki, S. and Loustau, D. and Magnani, F. and Markkanen, T. and Matteucci, G. and Mohren, G. M. J. and Moors, E. and Nissinen, A. and Peltola, H. and Sabaté, S. and Sanchez, A. and Sontag, M. and Valentini, R. and Vesala, T.},
  date = {2002},
  journaltitle = {Global Change Biology},
  volume = {8},
  number = {3},
  pages = {213--230},
  issn = {1365-2486},
  doi = {10.1046/j.1365-2486.2002.00471.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1046/j.1365-2486.2002.00471.x},
  urldate = {2023-01-18},
  abstract = {Reliable models are required to assess the impacts of climate change on forest ecosystems. Precise and independent data are essential to assess this accuracy. The flux measurements collected by the EUROFLUX project over a wide range of forest types and climatic regions in Europe allow a critical testing of the process-based models which were developed in the LTEEF project. The ECOCRAFT project complements this with a wealth of independent plant physiological measurements. Thus, it was aimed in this study to test six process-based forest growth models against the flux measurements of six European forest types, taking advantage of a large database with plant physiological parameters. The reliability of both the flux data and parameter values itself was not under discussion in this study. The data provided by the researchers of the EUROFLUX sites, possibly with local corrections, were used with a minor gap-filling procedure to avoid the loss of many days with observations. The model performance is discussed based on their accuracy, generality and realism. Accuracy was evaluated based on the goodness-of-fit with observed values of daily net ecosystem exchange, gross primary production and ecosystem respiration (gC m−2 d−1), and transpiration (kg H2O m−2 d−1). Moreover, accuracy was also evaluated based on systematic and unsystematic errors. Generality was characterized by the applicability of the models to different European forest ecosystems. Reality was evaluated by comparing the modelled and observed responses of gross primary production, ecosystem respiration to radiation and temperature. The results indicated that: Accuracy. All models showed similar high correlation with the measured carbon flux data, and also low systematic and unsystematic prediction errors at one or more sites of flux measurements. The results were similar in the case of several models when the water fluxes were considered. Most models fulfilled the criteria of sufficient accuracy for the ability to predict the carbon and water exchange between forests and the atmosphere. Generality. Three models of six could be applied for both deciduous and coniferous forests. Furthermore, four models were applied both for boreal and temperate conditions. However, no severe water-limited conditions were encountered, and no year-to-year variability could be tested. Realism. Most models fulfil the criterion of realism that the relationships between the modelled phenomena (carbon and water exchange) and environment are described causally. Again several of the models were able to reproduce the responses of measurable variables such as gross primary production (GPP), ecosystem respiration and transpiration to environmental driving factors such as radiation and temperature. Stomatal conductance appears to be the most critical process causing differences in predicted fluxes of carbon and water between those models that accurately describe the annual totals of GPP, ecosystem respiration and transpiration. As a conclusion, several process-based models are available that produce accurate estimates of carbon and water fluxes at several forest sites of Europe. This considerable accuracy fulfils one requirement of models to be able to predict the impacts of climate change on the carbon balance of European forests. However, the generality of the models should be further evaluated by expanding the range of testing over both time and space. In addition, differences in behaviour between models at the process level indicate requirement of further model testing, with special emphasis on modelling stomatal conductance realistically.},
  langid = {english},
  keywords = {accuracy,climate change,generality,process models,realism,water and carbon fluxes},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1046/j.1365-2486.2002.00471.x},
  file = {/home/simone/Zotero/storage/2TMMGCL2/j.1365-2486.2002.00471.html}
}

@article{kunwor_preserving_2017,
  title = {Preserving the Variance in Imputed Eddy-Covariance Measurements: {{Alternative}} Methods for Defensible Gap Filling},
  shorttitle = {Preserving the Variance in Imputed Eddy-Covariance Measurements},
  author = {Kunwor, Sujit and Starr, Gregory and Loescher, Henry W. and Staudhammer, Christina L.},
  date = {2017-01-15},
  journaltitle = {Agricultural and Forest Meteorology},
  shortjournal = {Agricultural and Forest Meteorology},
  volume = {232},
  pages = {635--649},
  issn = {0168-1923},
  doi = {10.1016/j.agrformet.2016.10.018},
  url = {https://www.sciencedirect.com/science/article/pii/S0168192316304105},
  urldate = {2022-05-12},
  abstract = {The high utility of eddy covariance (EC) data has made it the cornerstone of carbon dynamics research for more than two decades. However, a substantial number of measurements from EC data can be missing for various reasons. Robust gap-filling methods are required to estimate carbon budgets from net ecosystem exchange measurements of CO2 (NEE) with high precision and accuracy. While the gap-filled methods used have provided unbiased estimates of annual NEE, little research has been done on preserving the variance structures associated with gap-filled flux data. In this project, we used EC data from a longleaf pine ecosystem located in the southeast US to investigate variance preservation in gap-filling methods. We used three non-linear regression approaches to impute artificially created gaps of different sizes via light and temperature response curves: 1) “traditional” fixed monthly window, 2) moving window, and 3) moving window with parameter prediction using physiological drivers. The results of gap-filling simulations showed that the variability of NEE estimates made with moving window and parameter prediction methods were closer to that of observed NEE, whereas the traditional method had overall lower variability. The average root mean square errors (RMSE) of predictions was lower for moving window and parameter prediction (3.38 and 3.22, respectively), versus that of the traditional method (3.42) over one year, including both daytime and nighttime data across all gap sizes. The variances associated with moving window and parameter prediction methods were 52\% and 57\%, respectively, of the observed variance, versus that of the traditional (51\%), while the average of first-order autocorrelation coefficients was 0.76 for each method compared to 0.58 for observed. The results showed that the moving window approaches provided better estimates (lower RMSEs and more similar variance) at annual scales, yet underestimated the observed variance. These results contribute toward the development of a framework of standardized gap-filling approaches which maintain variation inherent in EC data. Moreover, these results call for further research on potential environmental drivers and their interactions for inclusion in gap-filling models, as well as exploration of sampling size of estimation windows and averaging time (half hour) of flux data to promote variance maintenance and decrease the autocorrelation of predictions.},
  langid = {english},
  keywords = {Eddy covariance,Gap-filling,Net ecosystem exchange,Orthonormal wavelet transformation,Parameter prediction,Variance preservation},
  file = {/home/simone/Zotero/storage/8LJCAGBQ/S0168192316304105.html}
}

@inproceedings{lawrence_gaussian_2003,
  title = {Gaussian {{Process Latent Variable Models}} for {{Visualisation}} of {{High Dimensional Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lawrence, Neil},
  date = {2003},
  volume = {16},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2003/hash/9657c1fffd38824e5ab0472e022e577e-Abstract.html},
  urldate = {2022-07-09},
  abstract = {In this paper we introduce a new underlying probabilistic model for prin- cipal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior’s covariance func- tion constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance func- tions which allow non-linear mappings. This more general Gaussian pro- cess latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets. Additionally our non-linear algorithm can be further kernelised leading to ‘twin kernel PCA’ in which a mapping between feature spaces occurs.},
  file = {/home/simone/Zotero/storage/FUIQ78VS/Lawrence - 2003 - Gaussian Process Latent Variable Models for Visual.pdf}
}

@unpublished{liu_deepfib_2021,
  title = {{{DeepFIB}}: {{Self-Imputation}} for {{Time Series Anomaly Detection}}},
  shorttitle = {{{DeepFIB}}},
  author = {Liu, Minhao and Xu, Zhijian and Xu, Qiang},
  date = {2021-12-12},
  number = {arXiv:2112.06247},
  eprint = {2112.06247},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.06247},
  url = {http://arxiv.org/abs/2112.06247},
  urldate = {2022-05-15},
  abstract = {Time series (TS) anomaly detection (AD) plays an essential role in various applications, e.g., fraud detection in finance and healthcare monitoring. Due to the inherently unpredictable and highly varied nature of anomalies and the lack of anomaly labels in historical data, the AD problem is typically formulated as an unsupervised learning problem. The performance of existing solutions is often not satisfactory, especially in data-scarce scenarios. To tackle this problem, we propose a novel self-supervised learning technique for AD in time series, namely \textbackslash emph\{DeepFIB\}. We model the problem as a \textbackslash emph\{Fill In the Blank\} game by masking some elements in the TS and imputing them with the rest. Considering the two common anomaly shapes (point- or sequence-outliers) in TS data, we implement two masking strategies with many self-generated training samples. The corresponding self-imputation networks can extract more robust temporal relations than existing AD solutions and effectively facilitate identifying the two types of anomalies. For continuous outliers, we also propose an anomaly localization algorithm that dramatically reduces AD errors. Experiments on various real-world TS datasets demonstrate that DeepFIB outperforms state-of-the-art methods by a large margin, achieving up to \$65.2\textbackslash\%\$ relative improvement in F1-score.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/simone/Zotero/storage/XXYSSPLZ/Liu et al. - 2021 - DeepFIB Self-Imputation for Time Series Anomaly D.pdf;/home/simone/Zotero/storage/D86ND7FB/2112.html}
}

@unpublished{liu_deepfib_2021-1,
  title = {{{DeepFIB}}: {{Self-Imputation}} for {{Time Series Anomaly Detection}}},
  shorttitle = {{{DeepFIB}}},
  author = {Liu, Minhao and Xu, Zhijian and Xu, Qiang},
  date = {2021-12-12},
  number = {arXiv:2112.06247},
  eprint = {2112.06247},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.06247},
  url = {http://arxiv.org/abs/2112.06247},
  urldate = {2022-05-14},
  abstract = {Time series (TS) anomaly detection (AD) plays an essential role in various applications, e.g., fraud detection in finance and healthcare monitoring. Due to the inherently unpredictable and highly varied nature of anomalies and the lack of anomaly labels in historical data, the AD problem is typically formulated as an unsupervised learning problem. The performance of existing solutions is often not satisfactory, especially in data-scarce scenarios. To tackle this problem, we propose a novel self-supervised learning technique for AD in time series, namely \textbackslash emph\{DeepFIB\}. We model the problem as a \textbackslash emph\{Fill In the Blank\} game by masking some elements in the TS and imputing them with the rest. Considering the two common anomaly shapes (point- or sequence-outliers) in TS data, we implement two masking strategies with many self-generated training samples. The corresponding self-imputation networks can extract more robust temporal relations than existing AD solutions and effectively facilitate identifying the two types of anomalies. For continuous outliers, we also propose an anomaly localization algorithm that dramatically reduces AD errors. Experiments on various real-world TS datasets demonstrate that DeepFIB outperforms state-of-the-art methods by a large margin, achieving up to \$65.2\textbackslash\%\$ relative improvement in F1-score.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/simone/Zotero/storage/AJNL53U3/Liu et al. - 2021 - DeepFIB Self-Imputation for Time Series Anomaly D.pdf;/home/simone/Zotero/storage/SUS74QXZ/2112.html}
}

@article{luttinen_gaussian-process_nodate,
  title = {Gaussian-Process Factor Analysis for Modeling Spatio-Temporal Data},
  author = {Luttinen, Jaakko},
  pages = {82},
  abstract = {The main theme of this thesis is analyzing and modeling large spatio-temporal datasets, such as global temperature measurements. The task is typically to extract relevant structure and features for predicting or studying the system. This can be a challenging problem because simple models are often not able to capture the complex structure suffiently well, and more sophisticated models can be computationally too expensive in practice. This thesis presents a novel spatio-temporal model which extends factor analysis by setting Gaussian process priors over the spatial and temporal components. In contrast to factor analysis, the presented model is capable of modeling complex spatial and temporal structure. Compared to standard Gaussian process regression over the spatio-temporal domain, the presented model gains substantial computational savings by operating only in the spatial or temporal domain at a time. Thus, it is feasible to model larger spatio-temporal datasets than with standard Gaussian process regression. The new model combines the modeling assumptions of several traditional techniques used for analyzing spatially and temporally distributed data: kriging is used for modeling spatial dependencies; empirical orthogonal functions reduce the dimensionality of the problem; and temporal smoothing finds relevant features from time series. The model is applied to reconstruct missing values in a historical sea surface temperature dataset. The results are promising and suggest that the proposed model may outperform the state-of-the-art reconstruction systems.},
  langid = {english},
  file = {/home/simone/Zotero/storage/HYMJMLT3/Luttinen - Gaussian-process factor analysis for modeling spat.pdf}
}

@article{luttinen_variational_nodate,
  title = {Variational {{Gaussian-process}} Factor Analysis for Modeling Spatio-Temporal Data},
  author = {Luttinen, Jaakko and Ihler, Alexander T},
  pages = {9},
  abstract = {We present a probabilistic factor analysis model which can be used for studying spatio-temporal datasets. The spatial and temporal structure is modeled by using Gaussian process priors both for the loading matrix and the factors. The posterior distributions are approximated using the variational Bayesian framework. High computational cost of Gaussian process modeling is reduced by using sparse approximations. The model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset. The results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.},
  langid = {english},
  file = {/home/simone/Zotero/storage/8BDMFVQC/Luttinen and Ihler - Variational Gaussian-process factor analysis for m.pdf}
}

@book{lyche_numerical_2020,
  title = {Numerical {{Linear Algebra}} and {{Matrix Factorizations}}},
  author = {Lyche, Tom},
  date = {2020},
  series = {Texts in {{Computational Science}} and {{Engineering}}},
  volume = {22},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-36468-7},
  url = {http://link.springer.com/10.1007/978-3-030-36468-7},
  urldate = {2023-01-13},
  isbn = {978-3-030-36467-0 978-3-030-36468-7},
  langid = {english},
  keywords = {eigenvalue problems,iterative measures,least squares,linear algebra,linear systems,MATLAB programming,matrix theory,nonlinear equations,numerical linear algebra,numerical stability,QR decomposition,scientific computing,singular value decomposition,tensor products},
  file = {/home/simone/Documents/uni/Thesis/books/Texts-in-Computational-Science-and-Engineering-Tom-Lyche-Numerical-Linear-Algebra-and-Matrix-Factorizations-Springer-Nature-2020.pdf}
}

@article{machens_functional_2010,
  title = {Functional, {{But Not Anatomical}}, {{Separation}} of "{{What}}" and "{{When}}" in {{Prefrontal Cortex}}},
  author = {Machens, C. K. and Romo, R. and Brody, C. D.},
  date = {2010-01-06},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {Journal of Neuroscience},
  volume = {30},
  number = {1},
  pages = {350--360},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3276-09.2010},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3276-09.2010},
  urldate = {2022-07-07},
  langid = {english},
  file = {/home/simone/Zotero/storage/84AHNFYU/Machens et al. - 2010 - Functional, But Not Anatomical, Separation of Wha.pdf}
}

@article{machens_functional_2010-1,
  title = {Functional, {{But Not Anatomical}}, {{Separation}} of "{{What}}" and "{{When}}" in {{Prefrontal Cortex}}},
  author = {Machens, C. K. and Romo, R. and Brody, C. D.},
  date = {2010-01-06},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {Journal of Neuroscience},
  volume = {30},
  number = {1},
  pages = {350--360},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3276-09.2010},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.3276-09.2010},
  urldate = {2022-07-07},
  langid = {english},
  file = {/home/simone/Zotero/storage/PXC28952/Machens et al. - 2010 - Functional, But Not Anatomical, Separation of Wha.pdf}
}

@article{medlyn_validation_2005,
  title = {On the Validation of Models of Forest {{CO2}} Exchange Using Eddy Covariance Data: Some Perils and Pitfalls},
  shorttitle = {On the Validation of Models of Forest {{CO2}} Exchange Using Eddy Covariance Data},
  author = {Medlyn, Belinda E. and Robinson, Andrew P. and Clement, Robert and McMurtrie, Ross E.},
  date = {2005-07-01},
  journaltitle = {Tree Physiology},
  shortjournal = {Tree Physiology},
  volume = {25},
  number = {7},
  pages = {839--857},
  issn = {0829-318X},
  doi = {10.1093/treephys/25.7.839},
  url = {https://doi.org/10.1093/treephys/25.7.839},
  urldate = {2023-01-18},
  abstract = {With the widespread application of eddy covariance technology, long-term records of hourly ecosystem mass and energy exchange are becoming available for forests around the world. These data sets hold great promise for testing and validation of models of forest function. However, model validation is not a straightforward task. The goals of this paper were to: (1) review some of the problems inherent in model validation; and (2) survey the tools available to modelers to improve validation procedures, with particular reference to eddy covariance data. A simple set of models applied to a data set of ecosystem CO2 exchange is used to illustrate our points.The major problems discussed are equifinality, insensitivity and uncertainty. Equifinality is the problem that different models, or different parameterizations of the same model, may yield similar results, making it difficult to distinguish which is correct. Insensitivity arises because the major sources of variation in eddy covariance data are the annual and diurnal cycles, which are represented by even the most basic models, and the size of the response to these cycles can mask effects of other driving variables. Uncertainty arises from three main sources: parameters, model structure and data, each of which is discussed in turn. Uncertainty is a particular issue with eddy covariance data because of the lack of replicated measurements and the potential for unquantified systematic errors such as flux loss due to advection.We surveyed several tools that improve model validation, including sensitivity analysis, uncertainty analysis, residual analysis and model comparison. Illustrative examples are used to demonstrate the use of each tool. We show that simplistic comparisons of model outputs with eddy covariance data are problematic, but use of these tools can greatly improve our confidence in model predictions.},
  file = {/home/simone/Zotero/storage/3IL4NKH2/Medlyn et al. - 2005 - On the validation of models of forest CO2 exchange.pdf;/home/simone/Zotero/storage/8QHLSTAV/1673665.html}
}

@article{moffat_comprehensive_2007,
  title = {Comprehensive Comparison of Gap-Filling Techniques for Eddy Covariance Net Carbon Fluxes},
  author = {Moffat, Antje M. and Papale, Dario and Reichstein, Markus and Hollinger, David Y. and Richardson, Andrew D. and Barr, Alan G. and Beckstein, Clemens and Braswell, Bobby H. and Churkina, Galina and Desai, Ankur R. and Falge, Eva and Gove, Jeffrey H. and Heimann, Martin and Hui, Dafeng and Jarvis, Andrew J. and Kattge, Jens and Noormets, Asko and Stauch, Vanessa J.},
  date = {2007-12-10},
  journaltitle = {Agricultural and Forest Meteorology},
  shortjournal = {Agricultural and Forest Meteorology},
  volume = {147},
  number = {3},
  pages = {209--232},
  issn = {0168-1923},
  doi = {10.1016/j.agrformet.2007.08.011},
  url = {https://www.sciencedirect.com/science/article/pii/S016819230700216X},
  urldate = {2022-05-12},
  abstract = {We review 15 techniques for estimating missing values of net ecosystem CO2 exchange (NEE) in eddy covariance time series and evaluate their performance for different artificial gap scenarios based on a set of 10 benchmark datasets from six forested sites in Europe. The goal of gap filling is the reproduction of the NEE time series and hence this present work focuses on estimating missing NEE values, not on editing or the removal of suspect values in these time series due to systematic errors in the measurements (e.g., nighttime flux, advection). The gap filling was examined by generating 50 secondary datasets with artificial gaps (ranging in length from single half-hours to 12 consecutive days) for each benchmark dataset and evaluating the performance with a variety of statistical metrics. The performance of the gap filling varied among sites and depended on the level of aggregation (native half-hourly time step versus daily), long gaps were more difficult to fill than short gaps, and differences among the techniques were more pronounced during the day than at night. The non-linear regression techniques (NLRs), the look-up table (LUT), marginal distribution sampling (MDS), and the semi-parametric model (SPM) generally showed good overall performance. The artificial neural network based techniques (ANNs) were generally, if only slightly, superior to the other techniques. The simple interpolation technique of mean diurnal variation (MDV) showed a moderate but consistent performance. Several sophisticated techniques, the dual unscented Kalman filter (UKF), the multiple imputation method (MIM), the terrestrial biosphere model (BETHY), but also one of the ANNs and one of the NLRs showed high biases which resulted in a low reliability of the annual sums, indicating that additional development might be needed. An uncertainty analysis comparing the estimated random error in the 10 benchmark datasets with the artificial gap residuals suggested that the techniques are already at or very close to the noise limit of the measurements. Based on the techniques and site data examined here, the effect of gap filling on the annual sums of NEE is modest, with most techniques falling within a range of ±25gCm−2year−1.},
  langid = {english},
  keywords = {Carbon flux,Eddy covariance,FLUXNET,Gap-filling comparison,Net ecosystem exchange (NEE),Review of gap-filling techniques},
  file = {/home/simone/Zotero/storage/XHTXWVRM/Moffat et al. - 2007 - Comprehensive comparison of gap-filling techniques.pdf;/home/simone/Documents/uni/Thesis/Sci-Hub Comprehensive comparison of gap-filling techniques for eddy covariance net carbon fluxes. Agricultural and Forest Meteorology, 147(3-4), 209–232 10.1016_j.agrformet.2007.08.011.html;/home/simone/Zotero/storage/75D5BEQB/Sci-Hub Comprehensive comparison of gap-filling techniques for eddy covariance net carbon fluxes. Agricultural and Forest Meteorology, 147(3-4), 209–232 10.1016_j.agrformet.2007.08.011.html;/home/simone/Zotero/storage/9DYLWCRU/S016819230700216X.html}
}

@book{mohinder_s_grewal_kalman_2001,
  title = {Kalman {{Filtering}}: {{Theory}} and {{Practice Using MATLAB}}, {{Second Edition}}},
  author = {{Mohinder S. Grewal} and {Angus P. Andrews}},
  date = {2001},
  isbn = {0-471-26638-8},
  file = {/home/simone/Documents/uni/Thesis/books/epdf.tips_kalman-filtering-theory-and-practice-using-matlab.pdf}
}

@online{noauthor_associated_2020,
  title = {Associated {{ICOS Ecosystem Station Labelling Report}} - {{Hainich}}},
  date = {2020},
  url = {https://data.icos-cp.eu/objects/_tFsWRgQcO7FkfvOq0OqIC8H},
  urldate = {2023-02-18}
}

@article{noauthor_interpretable_2021,
  title = {An Interpretable Self-Adaptive Deep Neural Network for Estimating Daily Spatially-Continuous {{PM2}}.5 Concentrations across {{China}}},
  date = {2021-05-10},
  journaltitle = {Science of The Total Environment},
  volume = {768},
  pages = {144724},
  publisher = {{Elsevier}},
  issn = {0048-9697},
  doi = {10.1016/j.scitotenv.2020.144724},
  url = {https://www.sciencedirect.com/science/article/pii/S0048969720382577},
  urldate = {2022-05-13},
  abstract = {Accurate estimation of daily spatially-continuous PM2.5 (fine particulate matter) concentration is a prerequisite to address environmental public heal…},
  langid = {english},
  file = {/home/simone/Zotero/storage/IHA4HV2V/S0048969720382577.html}
}

@online{noauthor_specification_nodate,
  title = {Specification - {{Vaisala HMP3 General Purpose Humidity}} and {{Temperature Probe}}},
  url = {https://docs.vaisala.com/v/u/B211826EN-C/en-US},
  urldate = {2023-02-18}
}

@online{noauthor_statsmodelstsastatespacekalman_filterkalmanfilter_nodate,
  title = {Statsmodels.Tsa.Statespace.Kalman\_filter.{{KalmanFilter}} — Statsmodels},
  url = {https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.kalman_filter.KalmanFilter.html#statsmodels.tsa.statespace.kalman_filter.KalmanFilter},
  urldate = {2023-02-19},
  file = {/home/simone/Zotero/storage/35JYBYMZ/statsmodels.tsa.statespace.kalman_filter.KalmanFilter.html}
}

@article{papale_ideas_2020,
  title = {Ideas and Perspectives: Enhancing the Impact of the {{FLUXNET}} Network of Eddy Covariance Sites},
  shorttitle = {Ideas and Perspectives},
  author = {Papale, Dario},
  date = {2020-11-17},
  journaltitle = {Biogeosciences},
  shortjournal = {Biogeosciences},
  volume = {17},
  number = {22},
  pages = {5587--5598},
  issn = {1726-4189},
  doi = {10.5194/bg-17-5587-2020},
  url = {https://bg.copernicus.org/articles/17/5587/2020/},
  urldate = {2023-01-18},
  abstract = {Abstract. In the last 20 years, the FLUXNET network provided unique measurements of CO2, energy and other greenhouse gas exchanges between ecosystems and atmosphere measured with the eddy covariance technique. These data have been widely used in different and heterogeneous applications, and FLUXNET became a reference source of information not only for ecological studies but also in modeling and remote sensing applications. The data are, in general, collected, processed and shared by regional networks or by single sites, and for this reason it is difficult for users interested in analyses involving multiple sites to easily access a coherent and standardized dataset. For this reason, periodic FLUXNET collections have been released in the last 15~years, every 5 to 10~years, with data standardized and shared under the same data use policy. However, the new tools available for data analysis and the need to constantly monitor the relations between ecosystem behavior and climate change require a reorganization of FLUXNET in order to increase the data interoperability, reduce the delay in the data sharing and facilitate the data use, all this while keeping in mind the great effort made by the site teams to collect these unique data and respecting the different regional and national network organizations and data policies. Here a proposal for a new organization of FLUXNET is presented with the aim of stimulating a discussion for the needed developments. In this new scheme, the regional and national networks become the pillars of the global initiative, organizing clusters and becoming responsible for the processing, preparation and distribution of datasets that users will be able to access in real time and with a machine-to-machine tool, obtaining always the most updated collection possible but keeping a high standardization and common data policy. This will also lead to an increase in the FAIRness (Findability, Accessibility, Interoperability and Reusability) of the FLUXNET data that will ensure a larger impact of the unique data produced and a proper data management and traceability.},
  langid = {english},
  file = {/home/simone/Zotero/storage/N3CGIMV3/Papale - 2020 - Ideas and perspectives enhancing the impact of th.pdf}
}

@unpublished{park_long-term_2022,
  title = {Long-{{Term Missing Value Imputation}} for {{Time Series Data Using Deep Neural Networks}}},
  author = {Park, Jangho and Muller, Juliane and Arora, Bhavna and Faybishenko, Boris and Pastorello, Gilberto and Varadharajan, Charuleka and Sahu, Reetik and Agarwal, Deborah},
  date = {2022-02-24},
  number = {arXiv:2202.12441},
  eprint = {2202.12441},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.12441},
  url = {http://arxiv.org/abs/2202.12441},
  urldate = {2022-05-14},
  abstract = {We present an approach that uses a deep learning model, in particular, a MultiLayer Perceptron (MLP), for estimating the missing values of a variable in multivariate time series data. We focus on filling a long continuous gap (e.g., multiple months of missing daily observations) rather than on individual randomly missing observations. Our proposed gap filling algorithm uses an automated method for determining the optimal MLP model architecture, thus allowing for optimal prediction performance for the given time series. We tested our approach by filling gaps of various lengths (three months to three years) in three environmental datasets with different time series characteristics, namely daily groundwater levels, daily soil moisture, and hourly Net Ecosystem Exchange. We compared the accuracy of the gap-filled values obtained with our approach to the widely-used R-based time series gap filling methods ImputeTS and mtsdi. The results indicate that using an MLP for filling a large gap leads to better results, especially when the data behave nonlinearly. Thus, our approach enables the use of datasets that have a large gap in one variable, which is common in many long-term environmental monitoring observations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Applications},
  file = {/home/simone/Zotero/storage/9TFCBLTU/Park et al. - 2022 - Long-Term Missing Value Imputation for Time Series.pdf;/home/simone/Zotero/storage/E8YW8FCQ/2202.html}
}

@article{park_new_1996,
  title = {New Square-Root Smoothing Algorithms},
  author = {Park, PooGyeon and Kailath, T.},
  date = {1996-05},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {41},
  number = {5},
  pages = {727--732},
  issn = {1558-2523},
  doi = {10.1109/9.489212},
  abstract = {This paper presents new square-root smoothing algorithms for the three best-known smoothing formulas: (1) Rauch-Tung-Striebel (RTS) formulas, (2) Desai-Weinert-Yusypchuk (DWY) formulas, called backward RTS formulas, and (3) Mayne-Fraser (MF) formulas, called two-filter formulas. The main feature of the new algorithms is that they use unitary rotations to replace all matrix inversion and backsubstitution steps common in earlier algorithms with unitary operations; this feature enables more efficient systolic array and parallel implementations and leads to algorithms with better numerical stability and conditioning properties.},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {Control system synthesis,Feedback,Information filtering,Information filters,Linear matrix inequalities,Numerical stability,Robust stability,Smoothing methods,Stability analysis,Stability criteria},
  file = {/home/simone/Zotero/storage/VPSHUV9J/Park and Kailath - 1996 - New square-root smoothing algorithms.pdf;/home/simone/Zotero/storage/U2ENFLT5/489212.html}
}

@article{park_square-root_1995,
  title = {Square-Root {{Bryson-Frazier}} Smoothing Algorithms},
  author = {Park, PooGyeon and Kailath, T.},
  date = {1995-04},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {40},
  number = {4},
  pages = {761--766},
  issn = {1558-2523},
  doi = {10.1109/9.376092},
  abstract = {Some new square-root algorithms for Bryson-Frazier smoothing formulas are suggested: square-root algorithms and a fast square-root (or so-called Chandrasekhar type) algorithm. The new square-root algorithms use square-root arrays composed of smoothed estimates and their error covariances. These algorithms provide many advantages over the conventional algorithms with respect to systolic array and parallel implementations as well as numerical stability and conditioning. For the case of constant-parameter systems, a fast square-root algorithm is suggested, which requires less computation than others.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {Covariance matrix,Filtering algorithms,Kalman filters,Monitoring,Numerical stability,Prediction algorithms,Riccati equations,Smoothing methods,State estimation,Systolic arrays},
  file = {/home/simone/Zotero/storage/PH6CNRJB/Park and Kailath - 1995 - Square-root Bryson-Frazier smoothing algorithms.pdf;/home/simone/Zotero/storage/GSIKAENL/376092.html}
}

@article{park_square-root_1995-1,
  title = {Square-Root {{RTS}} Smoothing Algorithms},
  author = {PARK, POOGYEON and KAILATH, THOMAS},
  date = {1995-11-01},
  journaltitle = {International Journal of Control},
  volume = {62},
  number = {5},
  pages = {1049--1060},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7179},
  doi = {10.1080/00207179508921582},
  url = {https://doi.org/10.1080/00207179508921582},
  urldate = {2023-01-24},
  abstract = {Two kinds of new square-root algorithms for Rauch-Tung-Striebel (RTS) smoothing formulas are suggested: stable square-root algorithms and a fast square-root (or so-called Chandrasekhar-type) algorithm. The new stable square-root algorithms use square-root arrays composed of smoothed estimates and their error covariances. These square-root algorithms provide many advantages over the conventional algorithms with respect to systolic array and parallel implementations as well as numerical stability and conditioning. For the case of constant-parameter systems, a fast square-root algorithm is suggested, which requires less computation than others.},
  annotation = {\_eprint: https://doi.org/10.1080/00207179508921582}
}

@article{park_square-root_1995-2,
  title = {Square-Root {{RTS}} Smoothing Algorithms},
  author = {PARK, POOGYEON and KAILATH, THOMAS},
  date = {1995-11-01},
  journaltitle = {International Journal of Control},
  volume = {62},
  number = {5},
  pages = {1049--1060},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7179},
  doi = {10.1080/00207179508921582},
  url = {https://doi.org/10.1080/00207179508921582},
  urldate = {2023-01-24},
  abstract = {Two kinds of new square-root algorithms for Rauch-Tung-Striebel (RTS) smoothing formulas are suggested: stable square-root algorithms and a fast square-root (or so-called Chandrasekhar-type) algorithm. The new stable square-root algorithms use square-root arrays composed of smoothed estimates and their error covariances. These square-root algorithms provide many advantages over the conventional algorithms with respect to systolic array and parallel implementations as well as numerical stability and conditioning. For the case of constant-parameter systems, a fast square-root algorithm is suggested, which requires less computation than others.},
  annotation = {\_eprint: https://doi.org/10.1080/00207179508921582},
  file = {/home/simone/Zotero/storage/AUPFVF3R/PARK and KAILATH - 1995 - Square-root RTS smoothing algorithms.pdf}
}

@article{park_square-root_2007,
  title = {Square-Root {{RTS}} Smoothing Algorithms},
  author = {PARK, POOGYEON and KAILATH, THOMAS},
  date = {2007-02-24},
  journaltitle = {International Journal of Control},
  publisher = {{Taylor \& Francis Group}},
  doi = {10.1080/00207179508921582},
  url = {https://www.tandfonline.com/doi/abs/10.1080/00207179508921582},
  urldate = {2023-01-24},
  abstract = {Two kinds of new square-root algorithms for Rauch-Tung-Striebel (RTS) smoothing formulas are suggested: stable square-root algorithms and a fast square-root (or so-called Chandrasekhar-type) algori...},
  langid = {english},
  file = {/home/simone/Zotero/storage/NHM26H79/00207179508921582.html}
}

@article{pastorello_fluxnet2015_2020,
  title = {The {{FLUXNET2015}} Dataset and the {{ONEFlux}} Processing Pipeline for Eddy Covariance Data},
  author = {Pastorello, Gilberto and Trotta, Carlo and Canfora, Eleonora and Chu, Housen and Christianson, Danielle and Cheah, You-Wei and Poindexter, Cristina and Chen, Jiquan and Elbashandy, Abdelrahman and Humphrey, Marty and Isaac, Peter and Polidori, Diego and Reichstein, Markus and Ribeca, Alessio and van Ingen, Catharine and Vuichard, Nicolas and Zhang, Leiming and Amiro, Brian and Ammann, Christof and Arain, M. Altaf and Ardö, Jonas and Arkebauer, Timothy and Arndt, Stefan K. and Arriga, Nicola and Aubinet, Marc and Aurela, Mika and Baldocchi, Dennis and Barr, Alan and Beamesderfer, Eric and Marchesini, Luca Belelli and Bergeron, Onil and Beringer, Jason and Bernhofer, Christian and Berveiller, Daniel and Billesbach, Dave and Black, Thomas Andrew and Blanken, Peter D. and Bohrer, Gil and Boike, Julia and Bolstad, Paul V. and Bonal, Damien and Bonnefond, Jean-Marc and Bowling, David R. and Bracho, Rosvel and Brodeur, Jason and Brümmer, Christian and Buchmann, Nina and Burban, Benoit and Burns, Sean P. and Buysse, Pauline and Cale, Peter and Cavagna, Mauro and Cellier, Pierre and Chen, Shiping and Chini, Isaac and Christensen, Torben R. and Cleverly, James and Collalti, Alessio and Consalvo, Claudia and Cook, Bruce D. and Cook, David and Coursolle, Carole and Cremonese, Edoardo and Curtis, Peter S. and D’Andrea, Ettore and da Rocha, Humberto and Dai, Xiaoqin and Davis, Kenneth J. and Cinti, Bruno De and de Grandcourt, Agnes and Ligne, Anne De and De Oliveira, Raimundo C. and Delpierre, Nicolas and Desai, Ankur R. and Di Bella, Carlos Marcelo and di Tommasi, Paul and Dolman, Han and Domingo, Francisco and Dong, Gang and Dore, Sabina and Duce, Pierpaolo and Dufrêne, Eric and Dunn, Allison and Dušek, Jiří and Eamus, Derek and Eichelmann, Uwe and ElKhidir, Hatim Abdalla M. and Eugster, Werner and Ewenz, Cacilia M. and Ewers, Brent and Famulari, Daniela and Fares, Silvano and Feigenwinter, Iris and Feitz, Andrew and Fensholt, Rasmus and Filippa, Gianluca and Fischer, Marc and Frank, John and Galvagno, Marta and Gharun, Mana and Gianelle, Damiano and Gielen, Bert and Gioli, Beniamino and Gitelson, Anatoly and Goded, Ignacio and Goeckede, Mathias and Goldstein, Allen H. and Gough, Christopher M. and Goulden, Michael L. and Graf, Alexander and Griebel, Anne and Gruening, Carsten and Grünwald, Thomas and Hammerle, Albin and Han, Shijie and Han, Xingguo and Hansen, Birger Ulf and Hanson, Chad and Hatakka, Juha and He, Yongtao and Hehn, Markus and Heinesch, Bernard and Hinko-Najera, Nina and Hörtnagl, Lukas and Hutley, Lindsay and Ibrom, Andreas and Ikawa, Hiroki and Jackowicz-Korczynski, Marcin and Janouš, Dalibor and Jans, Wilma and Jassal, Rachhpal and Jiang, Shicheng and Kato, Tomomichi and Khomik, Myroslava and Klatt, Janina and Knohl, Alexander and Knox, Sara and Kobayashi, Hideki and Koerber, Georgia and Kolle, Olaf and Kosugi, Yoshiko and Kotani, Ayumi and Kowalski, Andrew and Kruijt, Bart and Kurbatova, Julia and Kutsch, Werner L. and Kwon, Hyojung and Launiainen, Samuli and Laurila, Tuomas and Law, Bev and Leuning, Ray and Li, Yingnian and Liddell, Michael and Limousin, Jean-Marc and Lion, Marryanna and Liska, Adam J. and Lohila, Annalea and López-Ballesteros, Ana and López-Blanco, Efrén and Loubet, Benjamin and Loustau, Denis and Lucas-Moffat, Antje and Lüers, Johannes and Ma, Siyan and Macfarlane, Craig and Magliulo, Vincenzo and Maier, Regine and Mammarella, Ivan and Manca, Giovanni and Marcolla, Barbara and Margolis, Hank A. and Marras, Serena and Massman, William and Mastepanov, Mikhail and Matamala, Roser and Matthes, Jaclyn Hatala and Mazzenga, Francesco and McCaughey, Harry and McHugh, Ian and McMillan, Andrew M. S. and Merbold, Lutz and Meyer, Wayne and Meyers, Tilden and Miller, Scott D. and Minerbi, Stefano and Moderow, Uta and Monson, Russell K. and Montagnani, Leonardo and Moore, Caitlin E. and Moors, Eddy and Moreaux, Virginie and Moureaux, Christine and Munger, J. William and Nakai, Taro and Neirynck, Johan and Nesic, Zoran and Nicolini, Giacomo and Noormets, Asko and Northwood, Matthew and Nosetto, Marcelo and Nouvellon, Yann and Novick, Kimberly and Oechel, Walter and Olesen, Jørgen Eivind and Ourcival, Jean-Marc and Papuga, Shirley A. and Parmentier, Frans-Jan and Paul-Limoges, Eugenie and Pavelka, Marian and Peichl, Matthias and Pendall, Elise and Phillips, Richard P. and Pilegaard, Kim and Pirk, Norbert and Posse, Gabriela and Powell, Thomas and Prasse, Heiko and Prober, Suzanne M. and Rambal, Serge and Rannik, Üllar and Raz-Yaseef, Naama and Rebmann, Corinna and Reed, David and de Dios, Victor Resco and Restrepo-Coupe, Natalia and Reverter, Borja R. and Roland, Marilyn and Sabbatini, Simone and Sachs, Torsten and Saleska, Scott R. and Sánchez-Cañete, Enrique P. and Sanchez-Mejia, Zulia M. and Schmid, Hans Peter and Schmidt, Marius and Schneider, Karl and Schrader, Frederik and Schroder, Ivan and Scott, Russell L. and Sedlák, Pavel and Serrano-Ortíz, Penélope and Shao, Changliang and Shi, Peili and Shironya, Ivan and Siebicke, Lukas and Šigut, Ladislav and Silberstein, Richard and Sirca, Costantino and Spano, Donatella and Steinbrecher, Rainer and Stevens, Robert M. and Sturtevant, Cove and Suyker, Andy and Tagesson, Torbern and Takanashi, Satoru and Tang, Yanhong and Tapper, Nigel and Thom, Jonathan and Tomassucci, Michele and Tuovinen, Juha-Pekka and Urbanski, Shawn and Valentini, Riccardo and van der Molen, Michiel and van Gorsel, Eva and van Huissteden, Ko and Varlagin, Andrej and Verfaillie, Joseph and Vesala, Timo and Vincke, Caroline and Vitale, Domenico and Vygodskaya, Natalia and Walker, Jeffrey P. and Walter-Shea, Elizabeth and Wang, Huimin and Weber, Robin and Westermann, Sebastian and Wille, Christian and Wofsy, Steven and Wohlfahrt, Georg and Wolf, Sebastian and Woodgate, William and Li, Yuelin and Zampedri, Roberto and Zhang, Junhui and Zhou, Guoyi and Zona, Donatella and Agarwal, Deb and Biraud, Sebastien and Torn, Margaret and Papale, Dario},
  options = {useprefix=true},
  date = {2020-07-09},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {7},
  number = {1},
  pages = {225},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-020-0534-3},
  url = {https://www.nature.com/articles/s41597-020-0534-3},
  urldate = {2022-05-12},
  abstract = {The FLUXNET2015 dataset provides ecosystem-scale data on CO2, water, and energy exchange between the biosphere and the atmosphere, and other meteorological and biological measurements, from 212 sites around the globe (over 1500 site-years, up to and including year 2014). These sites, independently managed and operated, voluntarily contributed their data to create global datasets. Data were quality controlled and processed using uniform methods, to improve consistency and intercomparability across sites. The dataset is already being used in a number of applications, including ecophysiology studies, remote sensing studies, and development of ecosystem and Earth system models. FLUXNET2015 includes derived-data products, such as gap-filled time series, ecosystem respiration and photosynthetic uptake estimates, estimation of uncertainties, and metadata about the measurements, presented for the first time in this paper. In addition, 206 of these sites are for the first time distributed under a Creative Commons (CC-BY 4.0) license. This paper details this enhanced dataset and the processing methods, now made available as open-source codes, making the dataset more accessible, transparent, and reproducible.},
  issue = {1},
  langid = {english},
  keywords = {Carbon cycle,Climate sciences,Environmental sciences},
  file = {/home/simone/Zotero/storage/WQRB3WLI/Pastorello et al. - 2020 - The FLUXNET2015 dataset and the ONEFlux processing.pdf;/home/simone/Zotero/storage/3TL2G2C8/s41597-020-0534-3.html}
}

@incollection{potter_statistical_1963,
  title = {{{STATISTICAL FILTERING OF SPACE NAVIGATION MEASUREMENTS}}},
  booktitle = {Guidance and {{Control Conference}}},
  author = {POTTER, JAMES and STERN, ROBERT},
  date = {1963},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.1963-333},
  url = {https://arc.aiaa.org/doi/abs/10.2514/6.1963-333},
  urldate = {2023-01-13},
  annotation = {\_eprint: https://arc.aiaa.org/doi/pdf/10.2514/6.1963-333},
  file = {/home/simone/Zotero/storage/NPXBZ7TS/POTTER and STERN - STATISTICAL FILTERING OF SPACE NAVIGATION MEASUREM.pdf;/home/simone/Zotero/storage/IJAB4XMG/6.html}
}

@article{rauch_maximum_1965,
  title = {Maximum Likelihood Estimates of Linear Dynamic Systems},
  author = {RAUCH, H. E. and TUNG, F. and STRIEBEL, C. T.},
  date = {1965},
  journaltitle = {AIAA Journal},
  volume = {3},
  number = {8},
  pages = {1445--1450},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  issn = {0001-1452},
  doi = {10.2514/3.3166},
  url = {https://doi.org/10.2514/3.3166},
  urldate = {2023-02-19},
  annotation = {\_eprint: https://doi.org/10.2514/3.3166},
  file = {/home/simone/Zotero/storage/TVX4THZ3/3.html}
}

@article{reichstein_separation_2005-3,
  title = {On the Separation of Net Ecosystem Exchange into Assimilation and Ecosystem Respiration: Review and Improved Algorithm},
  shorttitle = {On the Separation of Net Ecosystem Exchange into Assimilation and Ecosystem Respiration},
  author = {Reichstein, Markus and Falge, Eva and Baldocchi, Dennis and Papale, Dario and Aubinet, Marc and Berbigier, Paul and Bernhofer, Christian and Buchmann, Nina and Gilmanov, Tagir and Granier, André and Grünwald, Thomas and Havránková, Katka and Ilvesniemi, Hannu and Janous, Dalibor and Knohl, Alexander and Laurila, Tuomas and Lohila, Annalea and Loustau, Denis and Matteucci, Giorgio and Meyers, Tilden and Miglietta, Franco and Ourcival, Jean-Marc and Pumpanen, Jukka and Rambal, Serge and Rotenberg, Eyal and Sanz, Maria and Tenhunen, John and Seufert, Günther and Vaccari, Francesco and Vesala, Timo and Yakir, Dan and Valentini, Riccardo},
  date = {2005},
  journaltitle = {Global Change Biology},
  volume = {11},
  number = {9},
  pages = {1424--1439},
  issn = {1365-2486},
  doi = {10.1111/j.1365-2486.2005.001002.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2486.2005.001002.x},
  urldate = {2022-05-12},
  abstract = {This paper discusses the advantages and disadvantages of the different methods that separate net ecosystem exchange (NEE) into its major components, gross ecosystem carbon uptake (GEP) and ecosystem respiration (Reco). In particular, we analyse the effect of the extrapolation of night-time values of ecosystem respiration into the daytime; this is usually done with a temperature response function that is derived from long-term data sets. For this analysis, we used 16 one-year-long data sets of carbon dioxide exchange measurements from European and US-American eddy covariance networks. These sites span from the boreal to Mediterranean climates, and include deciduous and evergreen forest, scrubland and crop ecosystems. We show that the temperature sensitivity of Reco, derived from long-term (annual) data sets, does not reflect the short-term temperature sensitivity that is effective when extrapolating from night- to daytime. Specifically, in summer active ecosystems the long-term temperature sensitivity exceeds the short-term sensitivity. Thus, in those ecosystems, the application of a long-term temperature sensitivity to the extrapolation of respiration from night to day leads to a systematic overestimation of ecosystem respiration from half-hourly to annual time-scales, which can reach {$>$}25\% for an annual budget and which consequently affects estimates of GEP. Conversely, in summer passive (Mediterranean) ecosystems, the long-term temperature sensitivity is lower than the short-term temperature sensitivity resulting in underestimation of annual sums of respiration. We introduce a new generic algorithm that derives a short-term temperature sensitivity of Reco from eddy covariance data that applies this to the extrapolation from night- to daytime, and that further performs a filling of data gaps that exploits both, the covariance between fluxes and meteorological drivers and the temporal structure of the fluxes. While this algorithm should give less biased estimates of GEP and Reco, we discuss the remaining biases and recommend that eddy covariance measurements are still backed by ancillary flux measurements that can reduce the uncertainties inherent in the eddy covariance data.},
  langid = {english},
  keywords = {carbon balance,computational methods,ecosystem respiration,eddy covariance,gross carbon uptake,temperature sensitivity of respiration},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2486.2005.001002.x},
  file = {/home/simone/Zotero/storage/UFS5CNLT/Reichstein et al. - 2005 - On the separation of net ecosystem exchange into a.pdf}
}

@report{ren_using_2019,
  type = {preprint},
  title = {Using {{Deep Learning}} to {{Fill Spatio-Temporal Data Gaps inHydrological Monitoring Networks}}},
  author = {Ren, Huiying and Cromwell, Erol and Kravitz, Ben and Chen, Xingyuan},
  date = {2019-05-16},
  institution = {{Groundwater hydrology/Stochastic approaches}},
  doi = {10.5194/hess-2019-196},
  url = {https://hess.copernicus.org/preprints/hess-2019-196/hess-2019-196.pdf},
  urldate = {2022-05-13},
  abstract = {Long-term spatio-temporal changes in subsurface hydrological flow are usually quantified through a network of wells; however, such observations often are spatially sparse and temporal gaps exist due to poor quality or instrument failure. In this study, we explore the ability of deep neural networks to fill in gaps in spatially distributed time-series data. We selected a location at the U.S. Department of Energy’s Hanford site to demonstrate and evaluate the new method, using a 10-year spatio5 temporal hydrological dataset of temperature, specific conductance, and groundwater table elevation from 42 wells that monitor the dynamic and heterogeneous hydrologic exchanges between the Columbia River and its adjacent groundwater aquifer. We employ a long short-term memory (LSTM)-based architecture, which is specially designed to address both spatial and temporal variations in the property fields.},
  langid = {english},
  file = {/home/simone/Zotero/storage/QP9IX647/Ren et al. - 2019 - Using Deep Learning to Fill Spatio-Temporal Data G.pdf}
}

@article{ricciuto_how_2009,
  title = {How Uncertainty in Gap-Filled Meteorological Input Forcing at Eddy Covariance Sites Impacts Modeled Carbon and Energy Flux ({{Invited}})},
  author = {Ricciuto, D. M. and Thornton, P. E. and Schaefer, K. and Cook, R. B. and Davis, K. J. and Synthesis, N.},
  date = {2009-12-01},
  volume = {2009},
  pages = {B54A-03},
  url = {https://ui.adsabs.harvard.edu/abs/2009AGUFM.B54A..03R},
  urldate = {2022-05-14},
  abstract = {In the North American Carbon Program (NACP) Site-level Interim Synthesis, gap-filled meteorological forcing data are provided to drive over 20 terrestrial carbon cycle models at over 40 eddy covariance towers using a standard simulation protocol. The meteorological gap-filling method ingests data from nearby towers or climate stations and uses simple interpolation techniques to fill missing half-hourly data values. These gap-filled forcing datasets are uncertain because of the gap-filling technique and because of potential biases in the tower observations such as the underestimation of precipitation or poor calibration of radiometers. Here we examine the effects of uncertainty in this forcing dataset at selected sites with two models. For each site, an ensemble of 10 forcing datasets is created using varying methods of bias correction, interpolation, and selection of nearby sites for gap-filling. We include the North American Regional Reanalysis (NARR) dataset as one of the ensemble members to quantify uncertainties caused by using reanalysis data. We examine the effects of forcing uncertainty on net ecosystem exchange (NEE), gross primary productivity (GPP), ecosystem respiration (Re) and latent heat (LE) at multiple timescales, with a particular focus on the interannual variability of these flux variables. We also examine the sensitivity of fluxes at each site to individual uncertainties in three variables: temperature, precipitation and photosynthetically active radiation (PAR).},
  eventtitle = {{{AGU Fall Meeting Abstracts}}},
  keywords = {0315 ATMOSPHERIC COMPOSITION AND STRUCTURE / Biosphere/atmosphere interactions,0414 BIOGEOSCIENCES / Biogeochemical cycles,0428 BIOGEOSCIENCES / Carbon cycling,0434 BIOGEOSCIENCES / Data sets,and modeling,processes},
  annotation = {ADS Bibcode: 2009AGUFM.B54A..03R}
}

@inproceedings{rutten_square-root_2013,
  title = {Square-Root Unscented Filtering and Smoothing},
  booktitle = {2013 {{IEEE Eighth International Conference}} on {{Intelligent Sensors}}, {{Sensor Networks}} and {{Information Processing}}},
  author = {Rutten, Mark G.},
  date = {2013-04},
  pages = {294--299},
  doi = {10.1109/ISSNIP.2013.6529805},
  abstract = {A square-root Kalman filter propagates the square-root (often the Cholesky factor) of the state covariance, rather than the full covariance matrix. Propagating these factors offers both computational efficiencies and greatly improved numerical properties. This paper introduces a new method of implementing the square-root unscented filter and the square-root unscented Rauch-Tung-Striebel smoother, which provide similar computational and numerical advantages over their traditional implementations. The new algorithms rely on the QR factorisation for calculating the covariance square-roots. A comparison with the previous development of the square-root unscented filter shows similar computational cost, while dramatically simplifying the implementation and improving numerical stability.},
  eventtitle = {2013 {{IEEE Eighth International Conference}} on {{Intelligent Sensors}}, {{Sensor Networks}} and {{Information Processing}}},
  keywords = {Computational efficiency,Covariance matrices,Kalman filters,Mathematical model,Matrix decomposition,Smoothing methods,Time measurement},
  file = {/home/simone/Zotero/storage/4NLZ74AX/Rutten - 2013 - Square-root unscented filtering and smoothing.pdf;/home/simone/Zotero/storage/3XQKLI6C/6529805.html}
}

@incollection{samaniego_bayesian_2011,
  title = {Bayesian vs. {{Classical Point Estimation}}: {{A Comparative Overview}}},
  shorttitle = {Bayesian vs. {{Classical Point Estimation}}},
  booktitle = {International {{Encyclopedia}} of {{Statistical Science}}},
  author = {Samaniego, Francisco J.},
  editor = {Lovric, Miodrag},
  date = {2011},
  pages = {136--138},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-04898-2_140},
  url = {https://doi.org/10.1007/978-3-642-04898-2_140},
  urldate = {2023-02-10},
  isbn = {978-3-642-04898-2},
  langid = {english},
  file = {/home/simone/Documents/uni/Thesis/books/ B Bayesian vs. Classical Point Estimation\: A Comparative Overview - samaniego2011.pdf}
}

@misc{shan_nrtsi_2021,
  title = {{{NRTSI}}: {{Non-Recurrent Time Series Imputation}}},
  shorttitle = {{{NRTSI}}},
  author = {Shan, Siyuan and Li, Yang and Oliva, Junier B.},
  date = {2021-05-27},
  number = {arXiv:2102.03340},
  eprint = {2102.03340},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.03340},
  url = {http://arxiv.org/abs/2102.03340},
  urldate = {2022-06-15},
  abstract = {Time series imputation is a fundamental task for understanding time series with missing data. Existing methods either do not directly handle irregularly-sampled data or degrade severely with sparsely observed data. In this work, we reformulate time series as permutation-equivariant sets and propose a novel imputation model NRTSI that does not impose any recurrent structures. Taking advantage of the permutation equivariant formulation, we design a principled and efficient hierarchical imputation procedure. In addition, NRTSI can directly handle irregularly-sampled time series, perform multiple-mode stochastic imputation, and handle data with partially observed dimensions. Empirically, we show that NRTSI achieves state-of-the-art performance across a wide range of time series imputation benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simone/Zotero/storage/YU9S5DUU/Shan et al. - 2021 - NRTSI Non-Recurrent Time Series Imputation.pdf;/home/simone/Zotero/storage/BTRQHF25/2102.html}
}

@inproceedings{suo_glima_2020,
  title = {{{GLIMA}}: {{Global}} and {{Local Time Series Imputation}} with {{Multi-directional Attention Learning}}},
  shorttitle = {{{GLIMA}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Suo, Qiuling and Zhong, Weida and Xun, Guangxu and Sun, Jianhui and Chen, Changyou and Zhang, Aidong},
  date = {2020-12},
  pages = {798--807},
  doi = {10.1109/BigData50022.2020.9378408},
  abstract = {Missing data, which commonly appears in multivariate time series, has been widely recognized as a key challenge in time series analysis. Many commonly used imputation methods either ignore the temporal dependencies of time series data, or do not adequately utilize the relationships among variables. State-ofthe-art methods on time series imputation are built on Recurrent Neural Networks (RNNs), which utilize the historical information to estimate current values sequentially. However, RNNs rely heavily on the output of nearby timestamps, which may lead to important information lost for long sequences. Moreover, individual variables typically present different dynamics and missingness patterns, which is neglected by the global RNN hidden states. In this paper, we propose an imputation framework to learn both global and local dependencies of multivariate time series, as well as a multi-dimensional self-attention to learn capture distant correlations across both time and feature. Extensive experiments show that the proposed framework outperforms the state-of-the-art methods in the imputation task, and benefits the downstream task.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  keywords = {Big Data,Conferences,Correlation,Missing Data,Recurrent Imputation,Recurrent neural networks,Self-Attention,Task analysis,Tensors,Time Series,Time series analysis},
  file = {/home/simone/Zotero/storage/WHBYVLDQ/9378408.html}
}

@article{tardivo_dynamic_2012,
  title = {A {{Dynamic Method}} for {{Gap Filling}} in {{Daily Temperature Datasets}}},
  author = {Tardivo, Gianmarco and Berti, Antonio},
  date = {2012-06-01},
  journaltitle = {Journal of Applied Meteorology and Climatology},
  volume = {51},
  number = {6},
  pages = {1079--1086},
  publisher = {{American Meteorological Society}},
  issn = {1558-8424, 1558-8432},
  doi = {10.1175/JAMC-D-11-0117.1},
  url = {https://journals.ametsoc.org/view/journals/apme/51/6/jamc-d-11-0117.1.xml},
  urldate = {2022-05-14},
  abstract = {Abstract A regression-based approach for temperature data reconstruction has been used to fill the gaps in the series of automatic temperature records obtained from the meteorological network of Veneto Region (northeastern Italy). The method presented is characterized by a dynamic selection of the reconstructing stations and of the coupling period that can precede or follow the missing data. Each gap is considered as a specific case, identifying the best set of stations and the period that minimizes the estimated reconstruction error for the gap, thus permitting a potentially better adaptation to time-dependent factors affecting the relationships between stations. The best sampling size is determined through an inference procedure, permitting a highly specific selection of the parameters used to fill each gap in the time series. With a proper selection of the parameters, the average errors of reconstruction are close to 0 and those corresponding to the 95th percentile are typically around 0.1°C. In comparison with similar regression-based approaches, the errors are lower, particularly for minimum temperatures, and the method limits inversions between the minimum, mean, and maximum temperatures.},
  langid = {english},
  file = {/home/simone/Zotero/storage/AYWPKKBH/Tardivo and Berti - 2012 - A Dynamic Method for Gap Filling in Daily Temperat.pdf;/home/simone/Zotero/storage/E44NPVVC/jamc-d-11-0117.1.html}
}

@article{ventura_mannga_2019,
  title = {{{MANNGA}}: {{A Robust Method}} for {{Gap Filling Meteorological Data}}},
  shorttitle = {{{MANNGA}}},
  author = {Ventura, Thiago Meirelles and Martins, Claudia Aparecida and de Figueiredo, Josiel Maimone and de Oliveira, Allan Gonçalves and Montanher, Johnata Rodrigo Pinheiro},
  date = {2019-08-05},
  journaltitle = {Revista Brasileira de Meteorologia},
  shortjournal = {Rev. bras. meteorol.},
  volume = {34},
  pages = {315--323},
  publisher = {{Sociedade Brasileira de Meteorologia}},
  issn = {0102-7786, 1982-4351},
  doi = {10.1590/0102-77863340035},
  url = {http://www.scielo.br/j/rbmet/a/dPKmVNkxrwhq3xLwCwxQG6p/abstract/?lang=en},
  urldate = {2022-05-14},
  abstract = {Abstract This paper presents Mannga (Multiple variables with Artificial Neural Network and Genetic Algorithm), a method designed for gap filling meteorological data. The main approach is to estimate the missing data based on values of other meteorological variables measured at the same time in the same local, since the meteorological variables are strongly related. Experimental tests showed the performance of Mannga compared with other two methods typically used by researches in this area. Good results were achieved, with high accuracy even for sequential failures, which is a big challenge for researchers. The core advantages of Mannga are the flexibility of handling different types of meteorological data, the ability of select the best variables to assist the gap filling and the capacity to deal with sequential failures. Moreover, the method is available to public use with the Java programming language.},
  langid = {english},
  keywords = {artificial neural network,genetic algorithm,multivariate data,open source software},
  file = {/home/simone/Zotero/storage/CFFD6X9E/Ventura et al. - 2019 - MANNGA A Robust Method for Gap Filling Meteorolog.pdf;/home/simone/Zotero/storage/65HVFWGD/abstract.html}
}

@article{verhaegen_numerical_1986,
  title = {Numerical Aspects of Different {{Kalman}} Filter Implementations},
  author = {Verhaegen, M. and Van Dooren, P.},
  date = {1986-10},
  journaltitle = {IEEE Transactions on Automatic Control},
  volume = {31},
  number = {10},
  pages = {907--917},
  issn = {1558-2523},
  doi = {10.1109/TAC.1986.1104128},
  abstract = {A theoretical analysis is made of the error propagation due to numerical roundoff for four different Kalman filter implementations: the conventional Kalman filter, the square root covariance filter, the square root information filter, and the Chandrasekhar square root filter. An experimental analysis is performed to validate the new insights gained by the theoretical analysis.},
  eventtitle = {{{IEEE Transactions}} on {{Automatic Control}}},
  keywords = {Analytical models,Covariance matrix,Enterprise resource planning,Information analysis,Information filtering,Information filters,Noise measurement,Performance analysis,Recursive estimation,Vectors},
  file = {/home/simone/Zotero/storage/9ZK9RK4J/Verhaegen and Van Dooren - 1986 - Numerical aspects of different Kalman filter imple.pdf;/home/simone/Zotero/storage/AGFZXASG/1104128.html}
}

@article{vitale_multiple_2018,
  title = {A {{Multiple Imputation Strategy}} for {{Eddy Covariance Data}}},
  author = {Vitale, D. and Bilancia, M. and Papale, D.},
  date = {2018-07},
  journaltitle = {Journal of Environmental Informatics},
  shortjournal = {J ENV INFORM},
  issn = {17262135, 16848799},
  doi = {10.3808/jei.201800391},
  url = {http://www.jeionline.org/index.php?journal=mys&page=article&op=view&path%5B%5D=201800391},
  urldate = {2022-06-26},
  abstract = {Half-hourly time series of net ecosystem exchange (NEE) of CO2, latent heat flux (LE) and sensible heat flux (H) measured through the micro-meteorological eddy covariance (EC) technique are noisy and show a high percentage of missing data. By using EC measurements that are part of the FLUXNET2015 dataset, we evaluate the performance of a multiple imputation (MI) strategy based on an efficient computational strategy introduced in Honaker and King (2010), combining the classic Expectation-Maximization (EM) algorithm with a bootstrap approach, in order to take draws from a suitable approximation of posterior distribution of model parameters. Armed with these instruments, we are able to introduce three new multiple imputation models, characterized by an increasing level of complexity, and built on top of multivariate normality assumption: 1) MLR, which imputes EC missing values using a static multiple linear regression of observed values of suitable input variables; 2) ADL, which enriches with dynamic properties the static specification of MLR, by considering an autoregressive distributed lag specification; 3) PADL, which adds further complexity by embedding the ADL model in a panel-data perspective. Under several artificial gap scenarios, we show that PADL has a better ability in modeling the complex dynamics of ecosystem fluxes and reconstructing missing data points, thus providing unbiased imputations and preserving the original sampling distribution. The added flexibility arising from the time series cross section structure of PADL warrants improved performances, outperforming those of other imputation methods, as well as of the marginal distribution sampling algorithm (MDS), a widely used gapfilling approach introduced by Reichstein et al. (2005), especially in the case of nighttime flux data. It is expected that the strategy proposed in this paper will become useful in creating multiple imputations for a variety of EC datasets, providing valid inferences for a broad range of scientific estimands (such as annual budgets).},
  langid = {english},
  file = {/home/simone/Zotero/storage/KK8HGGI4/A Multiple Imputation Strategy for Eddy Covariance.pdf}
}

@article{vuichard_filling_2015,
  title = {Filling the Gaps in Meteorological Continuous Data Measured at {{FLUXNET}} Sites with {{ERA-Interim}} Reanalysis},
  author = {Vuichard, N. and Papale, D.},
  date = {2015-07-13},
  journaltitle = {Earth System Science Data},
  volume = {7},
  number = {2},
  pages = {157--171},
  publisher = {{Copernicus GmbH}},
  issn = {1866-3508},
  doi = {10.5194/essd-7-157-2015},
  url = {https://essd.copernicus.org/articles/7/157/2015/},
  urldate = {2022-05-11},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Exchanges of carbon, water and energy between the land surface and the atmosphere are monitored by eddy covariance technique at the ecosystem level. Currently, the FLUXNET database contains more than 500 registered sites, and up to 250 of them share data (free fair-use data set). Many modelling groups use the FLUXNET data set for evaluating ecosystem models' performance, but this requires uninterrupted time series for the meteorological variables used as input. Because original in situ data often contain gaps, from very short (few hours) up to relatively long (some months) ones, we develop a new and robust method for filling the gaps in meteorological data measured at site level. Our approach has the benefit of making use of continuous data available globally (ERA-Interim) and a high temporal resolution spanning from 1989 to today. These data are, however, not measured at site level, and for this reason a method to downscale and correct the ERA-Interim data is needed. We apply this method to the level 4 data (L4) from the La Thuile collection, freely available after registration under a fair-use policy. The performance of the developed method varies across sites and is also function of the meteorological variable. On average over all sites, applying the bias correction method to the ERA-Interim data reduced the mismatch with the in situ data by 10 to 36 \%, depending on the meteorological variable considered. In comparison to the internal variability of the in situ data, the root mean square error (RMSE) between the in situ data and the unbiased ERA-I (ERA-Interim) data remains relatively large (on average over all sites, from 27 to 76 \% of the standard deviation of in situ data, depending on the meteorological variable considered). The performance of the method remains poor for the wind speed field, in particular regarding its capacity to conserve a standard deviation similar to the one measured at FLUXNET stations. {$<$}br{$><$}br{$>$} The ERA-Interim reanalysis data de-biased at FLUXNET sites can be downloaded from the PANGAEA data centre ({$<$}a href="http://doi.pangaea.de/10.1594/PANGAEA.838234" target="\_blank"{$>$}http://doi.pangaea.de/10.1594/PANGAEA.838234{$<$}/a{$>$}).{$<$}/p{$>$}},
  langid = {english},
  file = {/home/simone/Zotero/storage/E4K6LFW6/Vuichard and Papale - 2015 - Filling the gaps in meteorological continuous data.pdf;/home/simone/Zotero/storage/QWTEVIR4/2015.html}
}

@article{welch_introduction_2006,
  title = {An {{Introduction}} to the {{Kalman Filter}}},
  author = {Welch, Greg and Bishop, Gary},
  date = {2006},
  pages = {16},
  abstract = {In 1960, R.E. Kalman published his famous paper describing a recursive solution to the discrete-data linear filtering problem. Since that time, due in large part to advances in digital computing, the Kalman filter has been the subject of extensive research and application, particularly in the area of autonomous or assisted navigation.},
  langid = {english},
  file = {/home/simone/Zotero/storage/WKTWI3F4/Welch and Bishop - 2006 - An Introduction to the Kalman Filter.pdf}
}

@article{wutzler_basic_2018,
  title = {Basic and Extensible Post-Processing of Eddy Covariance Flux Data with {{REddyProc}}},
  author = {Wutzler, Thomas and Lucas-Moffat, Antje and Migliavacca, Mirco and Knauer, Jürgen and Sickel, Kerstin and Šigut, Ladislav and Menzer, Olaf and Reichstein, Markus},
  date = {2018-08-23},
  journaltitle = {Biogeosciences},
  volume = {15},
  number = {16},
  pages = {5015--5030},
  publisher = {{Copernicus GmbH}},
  issn = {1726-4170},
  doi = {10.5194/bg-15-5015-2018},
  url = {https://bg.copernicus.org/articles/15/5015/2018/},
  urldate = {2022-05-15},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} With the eddy covariance (EC) technique, net fluxes of carbon dioxide (CO\textsubscript{2}) and other trace gases as well as water and energy fluxes can be measured at the ecosystem level. These flux measurements are a main source for understanding biosphere–atmosphere interactions and feedbacks through cross-site analysis, model–data integration, and upscaling. The raw fluxes measured with the EC technique require extensive and laborious data processing. While there are standard tools\textsuperscript{1} available in an open-source environment for processing high-frequency (10 or 20\&thinsp;Hz) data into half-hourly quality-checked fluxes, there is a need for more usable and extensible tools for the subsequent post-processing steps. We tackled this need by developing the REddyProc package in the cross-platform language R that provides standard CO\textsubscript{2}-focused post-processing routines for reading (half-)hourly data from different formats, estimating the \emph{u}\textsubscript{*} threshold, as well as gap-filling, flux-partitioning, and visualizing the results. In addition to basic processing, the functions are extensible and allow easier integration in extended analysis than current tools. New features include cross-year processing and a better treatment of uncertainties. A comparison of REddyProc routines with other state-of-the-art tools resulted in no significant differences in monthly and annual fluxes across sites. Lower uncertainty estimates of both \emph{u}\textsubscript{*} and resulting gap-filled fluxes by 50\&thinsp;\% with the presented tool were achieved by an improved treatment of seasons during the bootstrap analysis. Higher estimates of uncertainty in daytime partitioning (about twice as high) resulted from a better accounting for the uncertainty in estimates of temperature sensitivity of respiration. The provided routines can be easily installed, configured, and used. Hence, the eddy covariance community will benefit from the REddyProc package, allowing easier integration of standard post-processing with extended analysis.{$<$}/p{$>$} {$<$}p{$>$}\textsuperscript{1}{$<$}a href="http://fluxnet.fluxdata.org/2017/10/10/toolbox-a-rolling-list-of-softwarepackages-for-flux-related-data-processing/" target="\_blank"{$>$}http://fluxnet.fluxdata.org/2017/10/10/toolbox-a-rolling-list-of-softwarepackages-for-flux-related-data-processing/{$<$}/a{$>$}, last access: 17 August 2018{$<$}/p{$>$}},
  langid = {english},
  file = {/home/simone/Zotero/storage/HM4QFLAR/bg-15-5015-2018-supplement_ReddyProc.pdf;/home/simone/Zotero/storage/Z7A6U2HN/Wutzler et al. - 2018 - Basic and extensible post-processing of eddy covar.pdf;/home/simone/Zotero/storage/R4V4R9G5/2018.html}
}

@article{xie_bilstm-i_2021,
  title = {{{BiLSTM-I}}: {{A Deep Learning-Based Long Interval Gap-Filling Method}} for {{Meteorological Observation Data}}},
  shorttitle = {{{BiLSTM-I}}},
  author = {Xie, Chuanjie and Huang, Chong and Zhang, Deqiang and He, Wei},
  date = {2021-01},
  journaltitle = {International Journal of Environmental Research and Public Health},
  volume = {18},
  number = {19},
  pages = {10321},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1660-4601},
  doi = {10.3390/ijerph181910321},
  url = {https://www.mdpi.com/1660-4601/18/19/10321},
  urldate = {2022-05-14},
  abstract = {Complete and high-resolution temperature observation data are important input parameters for agrometeorological disaster monitoring and ecosystem modelling. Due to the limitation of field meteorological observation conditions, observation data are commonly missing, and an appropriate data imputation method is necessary in meteorological data applications. In this paper, we focus on filling long gaps in meteorological observation data at field sites. A deep learning-based model, BiLSTM-I, is proposed to impute missing half-hourly temperature observations with high accuracy by considering temperature observations obtained manually at a low frequency. An encoder-decoder structure is adopted by BiLSTM-I, which is conducive to fully learning the potential distribution pattern of data. In addition, the BiLSTM-I model error function incorporates the difference between the final estimates and true observations. Therefore, the error function evaluates the imputation results more directly, and the model convergence error and the imputation accuracy are directly related, thus ensuring that the imputation error can be minimized at the time the model converges. The experimental analysis results show that the BiLSTM-I model designed in this paper is superior to other methods. For a test set with a time interval gap of 30 days, or a time interval gap of 60 days, the root mean square errors (RMSEs) remain stable, indicating the model’s excellent generalization ability for different missing value gaps. Although the model is only applied to temperature data imputation in this study, it also has the potential to be applied to other meteorological dataset-filling scenarios.},
  issue = {19},
  langid = {english},
  keywords = {data imputation,deep learning,meteorological observation data,time series},
  file = {/home/simone/Zotero/storage/KSX2HFPF/Xie et al. - 2021 - BiLSTM-I A Deep Learning-Based Long Interval Gap-.pdf;/home/simone/Zotero/storage/669PZM9D/10321.html}
}

@article{yang_adversarial_2020,
  title = {Adversarial {{Recurrent Time Series Imputation}}},
  author = {Yang, Shuo and Dong, Minjing and Wang, Yunhe and Xu, Chang},
  date = {2020},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--12},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3010524},
  abstract = {For the real-world time series analysis, data missing is a ubiquitously existing problem due to anomalies during data collecting and storage. If not treated properly, this problem will seriously hinder the classification, regression, or related tasks. Existing methods for time series imputation either impose too strong assumptions on the distribution of missing data or cannot fully exploit, even simply ignore, the informative temporal dependencies and feature correlations across different time steps. In this article, inspired by the idea of conditional generative adversarial networks, we propose a generative adversarial learning framework for time series imputation under the condition of observed data (as well as the labels, if possible). In our model, we employ a modified bidirectional RNN structure as the generator G, which is aimed at generating the missing values by taking advantage of the temporal and nontemporal information extracted from the observed time series. The discriminator D is designed to distinguish whether each value in a time series is generated or not so that it can help the generator to make an adjustment toward a more authentic imputation result. For an empirical verification of our model, we conduct imputation and classification experiments on several real-world time series data sets. The experimental results show an eminent improvement compared with state-of-the-art baseline models.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Correlation,Data models,Estimation,Gallium nitride,Generative adversarial learning,Generative adversarial networks,missing data imputation,Task analysis,Time series analysis,time series analysis.},
  file = {/home/simone/Zotero/storage/UVRI3ZIJ/Yang et al. - 2020 - Adversarial Recurrent Time Series Imputation.pdf;/home/simone/Zotero/storage/WQETXTIF/9158560.html}
}

@misc{yoon_estimating_2017,
  title = {Estimating {{Missing Data}} in {{Temporal Data Streams Using Multi-directional Recurrent Neural Networks}}},
  author = {Yoon, Jinsung and Zame, William R. and van der Schaar, Mihaela},
  options = {useprefix=true},
  date = {2017-11-23},
  number = {arXiv:1711.08742},
  eprint = {1711.08742},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1711.08742},
  urldate = {2022-06-16},
  abstract = {Missing data is a ubiquitous problem. It is especially challenging in medical settings because many streams of measurements are collected at different – and often irregular – times. Accurate estimation of those missing measurements is critical for many reasons, including diagnosis, prognosis and treatment. Existing methods address this estimation problem by interpolating within data streams or imputing across data streams (both of which ignore important information) or ignoring the temporal aspect of the data and imposing strong assumptions about the nature of the data-generating process and/or the pattern of missing data (both of which are especially problematic for medical data). We propose a new approach, based on a novel deep learning architecture that we call a Multi-directional Recurrent Neural Network (M-RNN) that interpolates within data streams and imputes across data streams. We demonstrate the power of our approach by applying it to five real-world medical datasets. We show that it provides dramatically improved estimation of missing measurements in comparison to 11 state-of-the-art benchmarks (including Spline and Cubic Interpolations, MICE, MissForest, matrix completion and several RNN methods); typical improvements in Root Mean Square Error are between 35\% - 50\%. Additional experiments based on the same five datasets demonstrate that the improvements provided by our method are extremely robust.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/simone/Zotero/storage/Q72JEMRZ/Yoon et al. - 2017 - Estimating Missing Data in Temporal Data Streams U.pdf}
}

@inproceedings{yu_gaussian-process_2008,
  title = {Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yu, Byron M and Cunningham, John P and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna V and Sahani, Maneesh},
  date = {2008},
  volume = {21},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2008/hash/ad972f10e0800b49d76fed33a21f6698-Abstract.html},
  urldate = {2022-07-07},
  abstract = {We consider the problem of extracting smooth low-dimensional neural trajectories'' that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials. Beyond the benefit of visualizing the high-dimensional noisy spiking activity in a compact denoised form, such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity. Current methods for extracting neural trajectories involve a two-stage process: the data are firstdenoised'' by smoothing over time, then a static dimensionality reduction technique is applied. We first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way, and account for spiking variability that may vary both across neurons and across time. We then present a novel method for extracting neural trajectories, Gaussian-process factor analysis (GPFA), which unifies the smoothing and dimensionality reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that GPFA provided a better characterization of the population activity than the two-stage methods. From the extracted single-trial neural trajectories, we directly observed a convergence in neural state during motor planning, an effect suggestive of attractor dynamics that was shown indirectly by previous studies.},
  file = {/home/simone/Zotero/storage/LTERTWHI/Yu et al. - 2008 - Gaussian-process factor analysis for low-dimension.pdf}
}

@article{zhang_dual-head_2021-2,
  title = {A Dual-Head Attention Model for Time Series Data Imputation},
  author = {Zhang, Yifan and Thorburn, Peter J.},
  date = {2021-10-01},
  journaltitle = {Computers and Electronics in Agriculture},
  shortjournal = {Computers and Electronics in Agriculture},
  volume = {189},
  pages = {106377},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2021.106377},
  url = {https://www.sciencedirect.com/science/article/pii/S016816992100394X},
  urldate = {2022-06-15},
  abstract = {Digital agriculture increasingly relies on the availability and accuracy of measurement data collected from various sensors. Of this data, water quality attracts great attention due to its intended use for crop irrigation, livestock, and other farming activities. Accurate and reliable water quality measurements enable farmers to understand the landscape comprehensively, optimising resource utilisation and reducing the negative impacts of agriculture on the environment. In practice, missing and incomplete data can create biased estimations and reduce the efficiency of many of the valuable applications provided by digital agriculture. The purpose of this paper is to propose a dual-head sequence-to-sequence imputation model (Dual-SSIM) designed to impute missing time series data in sensor networks, therefore reducing the negative consequences of missing and incomplete data. Unlike standard sequence-to-sequence architecture, the Dual-SSIM model features two encoders with gated recurrent units (GRUs) which are used to process temporal information before and after the missing gap separately. Furthermore, the attention mechanism is applied to two encoder outputs concurrently, in order to allow the model to focus on the high relative inputs when estimating missing data. The performance efficacy of Dual-SSIM has been investigated through the monitoring of water quality, sourced from an Australian water quality information system. Experimental results of this investigation indicate that Dual-SSIM outperforms associated alternatives based on the mean absolute error (MAE), root mean square error (RMSE), and dynamic time warping (DTW) scores in imputing two different water quality variables. Therefore, it can be concluded that Dual-SSIM provides an effective and promising approach for water quality data imputation.},
  langid = {english},
  keywords = {Data imputation,Deep learning,Sequence-to-sequence,Water quality},
  file = {/home/simone/Zotero/storage/94VE97MG/Zhang and Thorburn - 2021 - A dual-head attention model for time series data i.pdf;/home/simone/Zotero/storage/E69IS2HY/S016816992100394X.html}
}

@article{zhao_how_2012,
  title = {How Errors on Meteorological Variables Impact Simulated Ecosystem Fluxes: A Case Study for Six {{French}} Sites},
  shorttitle = {How Errors on Meteorological Variables Impact Simulated Ecosystem Fluxes},
  author = {Zhao, Y. and Ciais, P. and Peylin, P. and Viovy, N. and Longdoz, B. and Bonnefond, J. M. and Rambal, S. and Klumpp, K. and Olioso, A. and Cellier, P. and Maignan, F. and Eglin, T. and Calvet, J. C.},
  date = {2012-07-11},
  journaltitle = {Biogeosciences},
  volume = {9},
  number = {7},
  pages = {2537--2564},
  publisher = {{Copernicus GmbH}},
  issn = {1726-4170},
  doi = {10.5194/bg-9-2537-2012},
  url = {https://bg.copernicus.org/articles/9/2537/2012/},
  urldate = {2023-02-13},
  abstract = {We analyze how biases of meteorological drivers impact the calculation of ecosystem CO2, water and energy fluxes by models. To do so, we drive the same ecosystem model by meteorology from gridded products and by meteorology from local observation at eddy-covariance flux sites. The study is focused on six flux tower sites in France spanning across a climate gradient of 7–14 °C annual mean surface air temperature and 600–1040 mm mean annual rainfall, with forest, grassland and cropland ecosystems. We evaluate the results of the ORCHIDEE process-based model driven by meteorology from four different analysis data sets against the same model driven by site-observed meteorology. The evaluation is decomposed into characteristic time scales. The main result is that there are significant differences in meteorology between analysis data sets and local observation. The phase of seasonal cycle of air temperature, humidity and shortwave downward radiation is reproduced correctly by all meteorological models (average R2 = 0.90). At sites located in altitude, the misfit of meteorological drivers from analysis data sets and tower meteorology is the largest. We show that day-to-day variations in weather are not completely well reproduced by meteorological models, with R2 between analysis data sets and measured local meteorology going from 0.35 to 0.70. The bias of meteorological driver impacts the flux simulation by ORCHIDEE, and thus would have an effect on regional and global budgets. The forcing error, defined by the simulated flux difference resulting from prescribing modeled instead of observed local meteorology drivers to ORCHIDEE, is quantified for the six studied sites at different time scales. The magnitude of this forcing error is compared to that of the model error defined as the modeled-minus-observed flux, thus containing uncertain parameterizations, parameter values, and initialization. The forcing error is on average smaller than but still comparable to model error, with the ratio of forcing error to model error being the largest on daily time scale (86\%) and annual time scales (80\%). The forcing error incurred from using a gridded meteorological data set to drive vegetation models is therefore an important component of the uncertainty budget of regional CO2, water and energy fluxes simulations, and should be taken into consideration in up-scaling studies.},
  langid = {english},
  file = {/home/simone/Zotero/storage/Q4JD9FVR/Zhao et al. - 2012 - How errors on meteorological variables impact simu.pdf}
}

@article{zimmerman_machine_2018,
  title = {A Machine Learning Calibration Model Using Random Forests to Improve Sensor Performance for Lower-Cost Air Quality Monitoring},
  author = {Zimmerman, Naomi and Presto, Albert A. and Kumar, Sriniwasa P. N. and Gu, Jason and Hauryliuk, Aliaksei and Robinson, Ellis S. and Robinson, Allen L. and R. Subramanian},
  date = {2018-01-15},
  journaltitle = {Atmospheric Measurement Techniques},
  volume = {11},
  number = {1},
  pages = {291--313},
  publisher = {{Copernicus GmbH}},
  issn = {1867-1381},
  doi = {10.5194/amt-11-291-2018},
  url = {https://amt.copernicus.org/articles/11/291/2018/},
  urldate = {2021-06-02},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Low-cost sensing strategies hold the promise of denser air quality monitoring networks, which could significantly improve our understanding of personal air pollution exposure. Additionally, low-cost air quality sensors could be deployed to areas where limited monitoring exists. However, low-cost sensors are frequently sensitive to environmental conditions and pollutant cross-sensitivities, which have historically been poorly addressed by laboratory calibrations, limiting their utility for monitoring. In this study, we investigated different calibration models for the Real-time Affordable Multi-Pollutant (RAMP) sensor package, which measures CO, NO\textsubscript{2}, O\textsubscript{3}, and CO\textsubscript{2}. We explored three methods: (1) laboratory univariate linear regression, (2) empirical multiple linear regression, and (3) machine-learning-based calibration models using random forests (RF). Calibration models were developed for 16–19 RAMP monitors (varied by pollutant) using training and testing windows spanning August 2016 through February 2017 in Pittsburgh, PA, US. The random forest models matched (CO) or significantly outperformed (NO\textsubscript{2}, CO\textsubscript{2}, O\textsubscript{3}) the other calibration models, and their accuracy and precision were robust over time for testing windows of up to 16 weeks. Following calibration, average mean absolute error on the testing data set from the random forest models was 38 ppb for CO (14 \% relative error), 10 ppm for CO\textsubscript{2} (2 \% relative error), 3.5 ppb for NO\textsubscript{2} (29 \% relative error), and 3.4 ppb for O\textsubscript{3} (15 \% relative error), and Pearson \emph{r} versus the reference monitors exceeded 0.8 for most units. Model performance is explored in detail, including a quantification of model variable importance, accuracy across different concentration ranges, and performance in a range of monitoring contexts including the National Ambient Air Quality Standards (NAAQS) and the US EPA Air Sensors Guidebook recommendations of minimum data quality for personal exposure measurement. A key strength of the RF approach is that it accounts for pollutant cross-sensitivities. This highlights the importance of developing multipollutant sensor packages (as opposed to single-pollutant monitors); we determined this is especially critical for NO\textsubscript{2} and CO\textsubscript{2}. The evaluation reveals that only the RF-calibrated sensors meet the US EPA Air Sensors Guidebook recommendations of minimum data quality for personal exposure measurement. We also demonstrate that the RF-model-calibrated sensors could detect differences in NO\textsubscript{2} concentrations between a near-road site and a suburban site less than 1.5 km away. From this study, we conclude that combining RF models with carefully controlled state-of-the-art multipollutant sensor packages as in the RAMP monitors appears to be a very promising approach to address the poor performance that has plagued low-cost air quality sensors.{$<$}/p{$>$}},
  langid = {english},
  file = {/home/simone/Zotero/storage/SJ2H7KGF/Zimmerman et al. - 2018 - A machine learning calibration model using random .pdf;/home/simone/Zotero/storage/JLTREGZA/2018.html}
}
