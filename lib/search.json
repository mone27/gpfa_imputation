[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPFA Imputation",
    "section": "",
    "text": "This is developed by Simone Massaro as a master thesis in Ecosystem Analysis and Modelling at the University of Göttingen, Germany\nThe aim is to impute gaps into the meteorological time series for Eddy-Covariance applications.\nModels developed: - Gaussian Processes Factor Analysis (GPFA) - space state models (Kalman Filter)"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "GPFA Imputation",
    "section": "License",
    "text": "License\nThe code under model_imp.kalman.filter has the following license\n```All code contained except that in pykalman/utils.py is released under the license below. All code in pykalman/utils.py is released under the license contained therein.\nNew BSD License\nCopyright (c) 2012 Daniel Duckworth. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of Daniel Duckworth nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ```"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n cache_disk (base_file, rm_cache=False)\n\nDecorator to cache function output to disk\n\nimport time\nfrom tempfile import tempdir\n\n\ncp = Path(tempdir) / \"test_cache\"\n\n\n@cache_disk(cp)\ndef slow_add(a,b):\n    time.sleep(1)\n    return a + b\n\nthis time is the first time so not from the cache\n\n\n\nCPU times: user 4 µs, sys: 1 µs, total: 5 µs\nWall time: 13.8 µs\n\n\n3\n\n\nnow is much faster beacuse of the cache\n\n\n\nCPU times: user 3 µs, sys: 1 µs, total: 4 µs\nWall time: 4.77 µs\n\n\n3\n\n\nadding comments change the hash, so the function is still cached\n\n@cache_disk(cp)\ndef slow_add(a,b):\n    time.sleep(1)\n    # this is a comment\n    return a + b\n\n\n\n\nCPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 4.53 µs\n\n\n3\n\n\n\n\n\nCPU times: user 3 µs, sys: 1 µs, total: 4 µs\nWall time: 5.25 µs\n\n\n3\n\n\n\nsource\n\n\n\n\n reset_seed (seed=27)\n\n\nsource\n\n\n\n\n reset_seed (seed=27)"
  },
  {
    "objectID": "utils.html#normal-parameters",
    "href": "utils.html#normal-parameters",
    "title": "Utils",
    "section": "Normal Parameters",
    "text": "Normal Parameters\n\nsource\n\nListNormal.get_nth\n\n ListNormal.get_nth (n:int)\n\nGet the mean and cov for the nth Normal distribution in the list\n\nsource\n\n\nListNormal.detach\n\n ListNormal.detach ()\n\nDetach both mean and cov at once"
  },
  {
    "objectID": "utils.html#testing",
    "href": "utils.html#testing",
    "title": "Utils",
    "section": "Testing",
    "text": "Testing\n\nsource\n\ntest_close\n\n test_close (a, b, eps=1e-05)\n\ntest that a is within eps of b"
  },
  {
    "objectID": "utils.html#standard-scaler",
    "href": "utils.html#standard-scaler",
    "title": "Utils",
    "section": "Standard Scaler",
    "text": "Standard Scaler\nmake a standard scaler that can also inverse transfor standard deviations. see Standardizer for details of implementation\n\nreset_seed()\nxx = np.random.random((4, 10))\n\n\ns = StandardScaler().fit(xx)\n\n\ns.transform(xx)\n\narray([[ 0.07263978,  0.63279488, -0.9975139 ,  0.50899177,  0.15537652,\n         1.45555506,  1.56629646, -1.60237369,  1.51674974,  1.29584745],\n       [ 1.58579521,  0.83086419, -0.68281902,  0.51578245, -0.62395756,\n        -1.19720248, -0.43000476,  1.1539719 , -0.74724819, -0.85525414],\n       [-1.05809926, -1.69049694,  0.0895118 , -1.72684476, -1.08418417,\n         0.32617669, -1.16657374,  0.2345773 ,  0.26525847,  0.64349108],\n       [-0.60033573,  0.22683787,  1.59082112,  0.70207053,  1.55276521,\n        -0.58452927,  0.03028204,  0.21382449, -1.03476002, -1.08408439]])\n\n\n\ns.mean_\n\narray([0.40358703, 0.6758362 , 0.77934606, 0.70748673, 0.34417949,\n       0.62067044, 0.48500116, 0.54921643, 0.34604713, 0.3660338 ])\n\n\n\ns.scale_\n\narray([0.30471427, 0.21926148, 0.04405831, 0.31536161, 0.25229864,\n       0.24649441, 0.26061043, 0.21187396, 0.26093989, 0.22927816])\n\n\n\nsource\n\nStandardScaler.inverse_transform_std\n\n StandardScaler.inverse_transform_std (x_std)\n\n\n\n\n\nDetails\n\n\n\n\nx_std\nstandard deviations"
  },
  {
    "objectID": "utils.html#info-visualization",
    "href": "utils.html#info-visualization",
    "title": "Utils",
    "section": "Info visualization",
    "text": "Info visualization\n\nsource\n\narray2df\n\n array2df (x:torch.Tensor, row_names:Optional[Collection[str]],\n           col_names:Optional[Collection[str]], row_var:str)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nx\nTensor\n2d tensor\n\n\nrow_names\nOptional\nnames for the row\n\n\ncol_names\nOptional\nnames for the columns\n\n\nrow_var\nstr\nname of the first column (the one with row names). This should describe the values of row_name\n\n\n\n\nsource\n\n\ndisplay_as_row\n\n display_as_row (dfs:dict[str,pandas.core.frame.DataFrame], title='',\n                 styler=<function _style_df>)\n\ndisplay multiple dataframes in the same row\n\na = HTML(pd.DataFrame([1,2]).to_html(notebook=True))\n\n\ndisplay_as_row({\"test\": pd.DataFrame([1,2])}, \"hello\")\n\n\nhello  test \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n \n\n\n\ndisplay_as_row({f\"test{i}\": pd.DataFrame([1,2]) for i in range(10)})\n\n\n  test0 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n  test1 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n  test2 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n  test3 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n  test4 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n  test5 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n  test6 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n  test7 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n  test8 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2\n    \n  \n\n  test9 \n\n  \n    \n      0\n    \n  \n  \n    \n      1\n    \n    \n      2"
  },
  {
    "objectID": "utils.html#distributions",
    "href": "utils.html#distributions",
    "title": "Utils",
    "section": "Distributions",
    "text": "Distributions\n\nx = torch.stack([torch.eye(3)*i for i in  range(1,4)])\n\n\nx\n\ntensor([[[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]],\n\n        [[2., 0., 0.],\n         [0., 2., 0.],\n         [0., 0., 2.]],\n\n        [[3., 0., 0.],\n         [0., 3., 0.],\n         [0., 0., 3.]]])\n\n\n\ntorch.diagonal(x, dim1=1, dim2=2)\n\ntensor([[1., 1., 1.],\n        [2., 2., 2.],\n        [3., 3., 3.]])\n\n\n\nsource\n\ncov2std\n\n cov2std (x)\n\nconvert cov of array of covariances to array of stddev\n\n\n\n# from IPython.display import HTML\n# import docutils.core\n\n\n# def rstdoc(obj):\n#     \"Render `__doc__` as ReStructuredText\"\n#     html = docutils.core.publish_parts(obj.__doc__, writer_name='html5')['fragment']\n#     return HTML(html)\n\n\n# rstdoc(docutils.core.publish_parts)"
  },
  {
    "objectID": "utils.html#export",
    "href": "utils.html#export",
    "title": "Utils",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "Fluxnet/gap_finder.html",
    "href": "Fluxnet/gap_finder.html",
    "title": "Find Gaps in Fluxnet data",
    "section": "",
    "text": "test_file_zip = here() / Path(\"../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip\")\ntest_file = here() / Path(\"../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4/FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\ntmp_dir = Path(\"/tmp\")\nout_dir = here() / Path(\"../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\nunzip the file and load it lazily with polars\ncolumns selection, interested only in QC columns to find gaps\nThe goal is to find where the data is missing in the dataset (which means that it has been gap-filled) and find:\nwe filter out the rows where there is no gap (QC =0)\nthen find the start and end of gap by comparing with the original row number of the previous entry\nit works!\nsource"
  },
  {
    "objectID": "Fluxnet/gap_finder.html#find-gaps",
    "href": "Fluxnet/gap_finder.html#find-gaps",
    "title": "Find Gaps in Fluxnet data",
    "section": "Find gaps",
    "text": "Find gaps\n\ndef _find_gap_df(df, col_name):\n    \"Find gaps with a df with a single QC column\"\n    return df.filter(\n        pl.col(col_name) != 0\n    ).with_columns([\n        (pl.col(\"row_num\") - pl.col(\"row_num\").shift() ).alias(\"before\"),\n        (pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\")).alias(\"after\"),\n    ]).filter(\n        (pl.col(\"before\") != 1) | (pl.col(\"after\") != 1)\n    ).with_column(\n        (pl.when((pl.col(\"before\") != 1) & (pl.col(\"after\") != 1))\n        .then(pl.col(\"start\"))\n        .otherwise(pl.col(\"start\").shift(-1))\n        .alias(\"gap_end\"))\n    ).filter(\n        pl.col(\"before\") != 1\n    ).select(\n        [pl.col(\"start\").alias(\"gap_start\"), \"gap_end\"]\n    )\n\n\ndef find_gap_variable(df, col_name):\n    \n    # row numembering has to happen before filtering\n    df = df.with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    )\n    \n    # start with null values\n    dff = df.filter(\n            pl.col(col_name).is_null()\n        )\n    gaps = [\n        _find_gap_df(dff, col_name).with_columns(\n            pl.lit(None).alias(\"gap_value\")\n        )]\n    \n    \n    # all other values\n    # here the QC flags are merged together as we we not interested in the QC alg only if there is a gap\n    dff = df.filter(\n            ~pl.col(col_name).is_null()\n        )\n    gaps.append(\n        _find_gap_df(dff, col_name).with_columns(\n            pl.lit(1).alias(\"gap_value\")\n        ))\n    \n    return pl.concat(gaps)\n\n\nfind_gap_variable(df, \"SW_IN_F_QC\").collect().head()\n\nNotFoundError: start\n\n\n\ndef find_all_gaps(df):\n    return pl.concat(\n        [find_gap(df, col_name) for col_name in df.select(pl.col(\"^.*_QC$\")).columns]\n    )\n\n\nsource\n\nfind_all_gaps\n\n find_all_gaps (df)\n\n\ngaps_all = find_all_gaps(df).collect()\n\n\ngaps_all.groupby(\"variable\").agg(pl.col(\"gap_len\").sum() / df.collect().shape[0])\n\n\n\n\nshape: (45, 2)\n\n\n\n\nvariable\n\n\ngap_len\n\n\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"NEE_VUT_84_QC\"\n\n\n0.722569\n\n\n\n\n\"NEE_VUT_16_QC\"\n\n\n0.725254\n\n\n\n\n\"NEE_CUT_16_QC\"\n\n\n0.727048\n\n\n\n\n\"NEE_VUT_50_QC\"\n\n\n0.6978\n\n\n\n\n\"NEE_VUT_95_QC\"\n\n\n0.742042\n\n\n\n\n\"TA_F_MDS_QC\"\n\n\n0.012959\n\n\n\n\n\"H_F_MDS_QC\"\n\n\n0.198801\n\n\n\n\n\"LW_IN_F_QC\"\n\n\n0.243714\n\n\n\n\n\"TS_F_MDS_2_QC\"\n\n\n0.007857\n\n\n\n\n\"SWC_F_MDS_2_QC...\n\n\n0.242968\n\n\n\n\n\"CO2_F_MDS_QC\"\n\n\n0.097889\n\n\n\n\n\"NEE_CUT_75_QC\"\n\n\n0.709597\n\n\n\n\n...\n\n\n...\n\n\n\n\n\"SW_IN_F_MDS_QC...\n\n\n0.015802\n\n\n\n\n\"LW_IN_F_MDS_QC...\n\n\n0.243714\n\n\n\n\n\"SW_IN_F_QC\"\n\n\n0.015802\n\n\n\n\n\"NEE_CUT_95_QC\"\n\n\n0.745916\n\n\n\n\n\"LW_IN_JSB_QC\"\n\n\n0.022492\n\n\n\n\n\"PA_F_QC\"\n\n\n0.016109\n\n\n\n\n\"NEE_VUT_MEAN_Q...\n\n\n0.96898\n\n\n\n\n\"SWC_F_MDS_1_QC...\n\n\n0.015516\n\n\n\n\n\"TS_F_MDS_4_QC\"\n\n\n0.850741\n\n\n\n\n\"NEE_VUT_USTAR5...\n\n\n0.703832\n\n\n\n\n\"NEE_VUT_75_QC\"\n\n\n0.70968\n\n\n\n\n\"TS_F_MDS_1_QC\"\n\n\n0.006352\n\n\n\n\n\n\n\n\ndef download_and_find_gaps(urls, download_dir, out_dir, tmp_dir):\n    site_infos = []\n    for url in tqdm(urls):\n        file_zip = download_fluxnet(url, download_dir)\n        file, site_info = find_gaps_fluxnet_archive(file_zip, out_dir, tmp_dir)\n        site_infos.append(site_info)\n        print(file)\n        \n    return pl.concat(site_infos)\n\n\nsource\n\n\ndownload_and_find_gaps\n\n download_and_find_gaps (urls, download_dir, out_dir, tmp_dir)\n\n\nurls = [\"https://ftp.fluxdata.org/.fluxnet_downloads_86523/FLUXNET2015/FLX_AR-SLu_FLUXNET2015_FULLSET_2009-2011_1-4.zip?=mone27\",\n\"https://ftp.fluxdata.org/.fluxnet_downloads_86523/FLUXNET2015/FLX_AR-Vir_FLUXNET2015_FULLSET_2009-2012_1-4.zip?=mone27\"]\n\n\ndownload_and_find_gaps(urls, download_dir, out_dir, tmp_dir)\n\n\n\n\nFLX_AR-SLu_FLUXNET2015_FULLSET_HH_2009-2011_1-4\nFLX_AR-Vir_FLUXNET2015_FULLSET_HH_2009-2012_1-4\n\n\n\n\n\nshape: (2, 3)\n\n\n\n\nstart\n\n\nend\n\n\nsite\n\n\n\n\ni64\n\n\ni64\n\n\nstr\n\n\n\n\n\n\n200901010030\n\n\n201201010000\n\n\n\"AR-SLu\"\n\n\n\n\n200901010030\n\n\n201301010000\n\n\n\"AR-Vir\""
  },
  {
    "objectID": "Fluxnet/hainich.html",
    "href": "Fluxnet/hainich.html",
    "title": "Fluxnet Hainich",
    "section": "",
    "text": "_def_meteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai_path = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\"\n\n/opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/pyprojroot/pyprojroot.py:51: UserWarning: Path doesn't exist: /home/runner/work/meteo_imp/meteo_imp/data\n  warnings.warn(\"Path doesn't exist: {}\".format(path))\nsource"
  },
  {
    "objectID": "Fluxnet/hainich.html#export",
    "href": "Fluxnet/hainich.html#export",
    "title": "Fluxnet Hainich",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "simplegp_imputation.html",
    "href": "simplegp_imputation.html",
    "title": "Simple GP Imputation",
    "section": "",
    "text": "source\n\n\n\n SimpleGP (train_x, train_y, likelihood)\n\nExact GP implemnetation using GPyTorch\n\nk = SimpleGP(torch.tensor([1,2,3]), torch.tensor([1,2,3]), gpytorch.likelihoods.GaussianLikelihood())\n\n\nk\n\nSimpleGP(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): ConstantMean()\n  (covar_module): ScaleKernel(\n    (base_kernel): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n    (raw_outputscale_constraint): Positive()\n  )\n)\n\n\n\nk.covar_module.base_kernel.lengthscale.item()\n\n0.6931471824645996\n\n\n\nk.covar_module.outputscale.item()\n\n0.6931471824645996\n\n\n\nsource\n\n\n\n\n SimpleGP.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nk.get_info()\n\n{'lengthscale':    lengthscale\n 0     0.693147,\n 'outputscale':    outputscale\n 0     0.693147,\n 'likelihood':       noise\n 0  0.693247}"
  },
  {
    "objectID": "simplegp_imputation.html#learner",
    "href": "simplegp_imputation.html#learner",
    "title": "Simple GP Imputation",
    "section": "Learner",
    "text": "Learner\n\nsource\n\nSimpleGPLearner\n\n SimpleGPLearner (X:torch.Tensor, T:torch.Tensor=None)\n\nLearner for a simple GP process. It handles only 1 dimensional time series\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nTensor\n\n(n_obs) Univariate time series\n\n\nT\nTensor\nNone\n(n_obs) Vector of time of observations.\n\n\n\n\nX = torch.tensor([1.,2,3,4])\n\n\nl = SimpleGPLearner(X)\n\n\nl.train()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n\n\n\nl.predict(torch.tensor([5, 7]))\n\nNormalParameters(mean=tensor([5.5955, 6.4173]), std=tensor([0.1980, 0.6468]))\n\n\nImputation\nThe imputation using simple GPs make a separate GP process for each variable, which are completely independent\n\nsource\n\n\nSimpleGPImputationExplorer\n\n SimpleGPImputationExplorer (data:pandas.core.frame.DataFrame, cuda=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nobserved data with missing data as NA\n\n\ncuda\nbool\nFalse\nUse GPU?\n\n\n\n\nfd = GPFADataTest.generate(2, 7).add_gap(3, [\"x1\"]).add_gap(2, [\"x0\"])\n\n\ngp_imp = SimpleGPImputationExplorer(fd.data)\n\n\ngp_imp\n\nSimple GP Imputation Explorer:\n    N obs: 7\n    N features 2 (x0, x1)\n    N missing observations 5\n\n\n\ngp_imp.fit()\n\n\n\n\n\n\n\nSimple GP Imputation Explorer:\n    N obs: 7\n    N features 2 (x0, x1)\n    N missing observations 5\n\n\n\ngp_imp.learners[0].predict(torch.tensor([3.]))\n\nNormalParameters(mean=tensor([-0.1003]), std=tensor([0.2664]))\n\n\n\ngp_imp.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      0.322117\n      0.294301\n    \n    \n      1\n      -0.119194\n      0.224219\n    \n    \n      2\n      -0.193881\n      -0.017484\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n    \n    \n      5\n      0.540541\n      NaN\n    \n    \n      6\n      -0.410130\n      -0.271418\n    \n  \n\n\n\n\n\ngp_imp.train_data\n\ntensor([[ 0.3221,  0.2943],\n        [-0.1192,  0.2242],\n        [-0.1939, -0.0175],\n        [-0.4101, -0.2714]])\n\n\n\ngp_imp.predict()\n\n\n\n\n\n  \n    \n      \n      mean\n      time\n      std\n      variable\n    \n  \n  \n    \n      0\n      0.105723\n      0.0\n      0.232544\n      x0\n    \n    \n      1\n      -0.109509\n      1.0\n      0.232544\n      x0\n    \n    \n      2\n      -0.145935\n      2.0\n      0.232544\n      x0\n    \n    \n      3\n      -0.100289\n      3.0\n      0.266372\n      x0\n    \n    \n      4\n      -0.100289\n      4.0\n      0.266372\n      x0\n    \n    \n      5\n      -0.100289\n      5.0\n      0.266372\n      x0\n    \n    \n      6\n      -0.251402\n      6.0\n      0.232544\n      x0\n    \n    \n      0\n      0.294510\n      0.0\n      0.023458\n      x1\n    \n    \n      1\n      0.220237\n      1.0\n      0.023208\n      x1\n    \n    \n      2\n      -0.014538\n      2.0\n      0.023457\n      x1\n    \n    \n      3\n      -0.220033\n      3.0\n      0.083291\n      x1\n    \n    \n      4\n      -0.293843\n      4.0\n      0.150042\n      x1\n    \n    \n      5\n      -0.293429\n      5.0\n      0.122185\n      x1\n    \n    \n      6\n      -0.270590\n      6.0\n      0.023595\n      x1"
  },
  {
    "objectID": "simplegp_imputation.html#results",
    "href": "simplegp_imputation.html#results",
    "title": "Simple GP Imputation",
    "section": "results",
    "text": "results\n\nself = gp_imp\ninfos = [learner.model.get_info() for learner in self.learners]\n\n\ninfos\n\n[{'lengthscale':    lengthscale\n  0     0.192713,\n  'outputscale':    outputscale\n  0     0.366132,\n  'likelihood':       noise\n  0  0.384582},\n {'lengthscale':    lengthscale\n  0     1.679326,\n  'outputscale':    outputscale\n  0     0.910373,\n  'likelihood':       noise\n  0  0.004235}]\n\n\n\nsource\n\nSimpleGPImputationExplorer.model_info\n\n SimpleGPImputationExplorer.model_info ()\n\nCombine parameters of different kernels into one output\n\ngp_imp.model_info()\n\n{'lengthscale':   variable  lengthscale\n 0       x0     0.192713\n 0       x1     1.679326,\n 'outputscale':   variable  outputscale\n 0       x0     0.366132\n 0       x1     0.910373,\n 'likelihood':   variable     noise\n 0       x0  0.384582\n 0       x1  0.004235}\n\n\n\nsource\n\n\nSimpleGPImputationExplorer.to_result\n\n SimpleGPImputationExplorer.to_result (data_complete, units=None)\n\n\nImputationResult??\n\n\nInit signature:\nImputationResult(\n    data_imputed,\n    data_complete,\n    model_info,\n    units=None,\n    metrics_all_data=True,\n)\nDocstring:      <no docstring>\nSource:        \nclass ImputationResult:\n    def __init__(self,\n                 data_imputed, #imputed data in tidy format\n                 data_complete, # complete data in tidy format\n                 model_info, # learner for parameters display\n                 units = None, # units for plots\n                 metrics_all_data = True # Compute metrics only for gap or for all data?\n                ):\n        store_attr()\nFile:           ~/Documents/uni/Thesis/GPFA_imputation/gpfa_imputation/results.py\nType:           type\nSubclasses:     \n\n\n\n\n\ngp_imp.to_result(fd.data_compl_tidy).display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      x0\n      0.2123\n    \n    \n      x1\n      0.7596\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n    \n  \n  \n    \n      x0\n      0.2954\n    \n    \n      x1\n      0.1396\n    \n  \n\n \n\n\n\nModel Info  lengthscale \n\n  \n    \n      variable\n      lengthscale\n    \n  \n  \n    \n      x0\n      0.1927\n    \n    \n      x1\n      1.6793\n    \n  \n\n  outputscale \n\n  \n    \n      variable\n      outputscale\n    \n  \n  \n    \n      x0\n      0.3661\n    \n    \n      x1\n      0.9104\n    \n  \n\n  likelihood \n\n  \n    \n      variable\n      noise\n    \n  \n  \n    \n      x0\n      0.3846\n    \n    \n      x1\n      0.0042"
  },
  {
    "objectID": "kalman/kalman_imputation.html",
    "href": "kalman/kalman_imputation.html",
    "title": "Imputation Kalman Model",
    "section": "",
    "text": "source\n\n\n\n KalmanImputation (data:pandas.core.frame.DataFrame,\n                   model:meteo_imp.kalman.model.KalmanModel=<class\n                   'meteo_imp.kalman.model.KalmanModel'>)\n\nImputation using a kalman model\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\n\n\n\nmodel\nKalmanModel\nKalmanModel\na subclass of KalmanModel to be used as model\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\n\n\nreset_seed(1)\ndata = MeteoDataTest.generate_gpfa(2, 5).add_random_missing()\n\n\ndata.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      0.023263\n      NaN\n    \n    \n      1\n      0.219627\n      0.268028\n    \n    \n      2\n      -0.039892\n      0.063075\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      -0.645490\n      -0.144866\n    \n  \n\n\n\n\n\nk_imp = KalmanImputation(data.data)\n\n\nk_imp.train_data\n\ntensor([[ 0.3586,     nan],\n        [ 0.8847,  0.9976],\n        [ 0.1895,  0.0048],\n        [    nan,     nan],\n        [-1.4328, -1.0024]])\n\n\n\nk_imp.train_data[k_imp.train_idx]\n\ntensor([[ 0.8847,  0.9976],\n        [ 0.1895,  0.0048],\n        [-1.4328, -1.0024]])\n\n\n\nk_imp.fit(10, lr=0.1)\n\nstarting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<__main__.KalmanImputation>\n\n\n\nk_imp.impute()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0\n      x0\n      -0.031254\n      0.473841\n    \n    \n      1\n      1\n      x0\n      0.219627\n      0.000000\n    \n    \n      2\n      2\n      x0\n      -0.039892\n      0.000000\n    \n    \n      3\n      3\n      x0\n      -0.280452\n      0.514002\n    \n    \n      4\n      4\n      x0\n      -0.645490\n      0.000000\n    \n    \n      5\n      0\n      x1\n      0.109620\n      0.259775\n    \n    \n      6\n      1\n      x1\n      0.268028\n      0.000000\n    \n    \n      7\n      2\n      x1\n      0.063075\n      0.000000\n    \n    \n      8\n      3\n      x1\n      -0.000474\n      0.280925\n    \n    \n      9\n      4\n      x1\n      -0.144866\n      0.000000\n    \n  \n\n\n\n\n\ndata.data.shape\n\n\nk_imp.impute(pred_all=True)\n\n\n\n\nsource\n\n\n\n\n\n KalmanImputation.to_result (data_compl, var_names=None, units=None,\n                             pred_all=False)\n\n\nX = np.hstack([np.arange(0,3.), np.arange(3., 0, -1)]).reshape(6, 1)\n\n\nres = k_imp.to_result(data.data_compl_tidy)\n\n\nres.display_results()"
  },
  {
    "objectID": "kalman/kalman_imputation.html#debug",
    "href": "kalman/kalman_imputation.html#debug",
    "title": "Imputation Kalman Model",
    "section": "Debug",
    "text": "Debug\n\nfrom meteo_imp.data import hai\n\n/opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/pyprojroot/pyprojroot.py:51: UserWarning: Path doesn't exist: /home/runner/work/meteo_imp/meteo_imp/data\n  warnings.warn(\"Path doesn't exist: {}\".format(path))\n\n\n\ntd = MeteoDataTest(hai)\n\n\ntd.add_gap(10, 'TA', 10)\n\n\ni_hai = KalmanImputation(td.data)\n\n\ni_hai.fit()"
  },
  {
    "objectID": "kalman/kalman_imputation.html#export",
    "href": "kalman/kalman_imputation.html#export",
    "title": "Imputation Kalman Model",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/kalman_filter.html#utils",
    "href": "kalman/kalman_filter.html#utils",
    "title": "Kalman Filter",
    "section": "Utils",
    "text": "Utils\n\nPositive Definite Constraint\ntransform any matrix \\(A\\) into a positive definite matrix (\\(PD\\)) using the following formula\n\\(PD = A A^T + aI\\)\nwhere \\(AA^T\\) ensures that A is positive semi-definite and adding a small positive number of the diagonal \\(aI\\) ensures that is definite\nfor details see https://stackoverflow.com/a/58192215/8555045\nadd also inverse transformation using cholesky decomposition\nAPI inspired by gpytorch constraints\n\nsource\n\n\nis_pos_semidef\n\n is_pos_semidef (cov)\n\n\nsource\n\n\ncheck_is_pos_semidef\n\n check_is_pos_semidef (cov)\n\n\nsource\n\n\nPosDef\n\n PosDef (a=1e-05)\n\nPositive Definite Constraint for PyTorch parameters\n\nA = torch.rand(3,3)\nconstraint = PosDef()\n\nposdef = constraint.transform(A)\n\n\nA\n\ntensor([[0.9833, 0.0952, 0.4400],\n        [0.1624, 0.0813, 0.7096],\n        [0.8946, 0.8664, 0.3925]])\n\n\n\nposdef\n\ntensor([[0.9669, 0.1597, 0.8796],\n        [0.1597, 0.0330, 0.2158],\n        [0.8796, 0.2158, 1.7051]])\n\n\n\ntest_eq(torch.distributions.constraints.positive_definite.check(posdef), True)\n\n\nconstraint.inverse_transform(posdef)\n\ntensor([[0.9833, 0.0000, 0.0000],\n        [0.1624, 0.0813, 0.0000],\n        [0.8946, 0.8664, 0.3925]])\n\n\n\ntest_close(posdef, constraint.transform(constraint.inverse_transform(posdef)), eps=2e-5)\n\n\ntorch.set_printoptions(precision=10)\n\n\nposdef\n\ntensor([[0.9668961763, 0.1596965045, 0.8796463013],\n        [0.1596965045, 0.0329905860, 0.2157533318],\n        [0.8796463013, 0.2157533318, 1.7050620317]])\n\n\n\ntorch.isclose(posdef, posdef.mT, atol=1e-6).all(-2).all(-1)\n\ntensor(True)\n\n\n\nSymmetric\n\nsource\n\n\n\nis_symmetric\n\n is_symmetric (value, atol=1e-05)\n\n\nis_symmetric(A)\n\nFalse\n\n\n\nsource\n\n\nsymmetric_upto\n\n symmetric_upto (value, start=-8)\n\n\nsymmetric_upto(posdef)\n\ntensor(-8)\n\n\n\nsymmetric_upto(A)\n\ntensor(0)\n\n\n\nsource\n\n\nis_posdef\n\n is_posdef (cov)\n\n\nis_pos_semidef(posdef)\n\nTrue\n\n\n\nis_posdef(A)\n\nFalse\n\n\n\nis_posdef(posdef)\n\nTrue\n\n\n\ntorch.linalg.eigvalsh(A)\n\ntensor([-0.7837653160,  0.3598064184,  1.8811095953])\n\n\n\ntorch.linalg.eigvalsh(posdef)\n\ntensor([1.1096124072e-03, 3.8251259923e-01, 2.3213262558e+00])\n\n\ncheck if it is pos definite using eigenvalues\n\nMultivariateNormal(torch.stack([torch.ones(3)]*2), torch.stack([posdef, torch.eye(3)]))\n\nMultivariateNormal(loc: torch.Size([2, 3]), covariance_matrix: torch.Size([2, 3, 3]))\n\n\n\nsource\n\n\nis_posdef2\n\n is_posdef2 (cov)\n\n\nis_posdef2(posdef)\n\n(True, tensor([1.1096124072e-03, 3.8251259923e-01, 2.3213262558e+00]))\n\n\n\nis_posdef2(A)\n\n(False, tensor([-0.7837653160,  0.3598064184,  1.8811095953]))\n\n\n\nmask = torch.tril(torch.ones(3,3, dtype=torch.bool)).T\nmask\n\ntensor([[ True,  True,  True],\n        [False,  True,  True],\n        [False, False,  True]])\n\n\n\nA[mask]\n\ntensor([0.9833087921, 0.0952116251, 0.4400138855, 0.0813293457, 0.7096094489,\n        0.3925123215])\n\n\n\nA[mask] = torch.tril(A)[mask]\n\n\nsource\n\n\nmake_symmetric\n\n make_symmetric (value)\n\ndrops upper half to make matrix symmetric\n\nmake_symmetric(A)\n\ntensor([[0.9833087921, 0.1624072790, 0.0813293457],\n        [0.1624072790, 0.8945778608, 0.8664447665],\n        [0.8945778608, 0.8664447665, 0.3925123215]])\n\n\n\nis_symmetric(make_symmetric(A))\n\nFalse\n\n\n\nposdef_log = pd.DataFrame() # None # make it None to disable logging\n\n\nCheck pos def\n\nsource\n\n\n\ncheck_posdef\n\n check_posdef (value, name='', error=True, **extra_args)\n\n\ncheck_posdef(A, 'A').dtypes\n\n/tmp/ipykernel_13841/527603031.py:33: UserWarning: Matrix is not positive definite\n  warn(\"Matrix is not positive definite\")\n\n\nis_pd_eigv      bool\nis_pd_chol      bool\nis_sym          bool\nsym_upto       int64\neigv          object\nmatrix        object\nname          object\ndtype: object\n\n\n\nA\n\ntensor([[0.9833087921, 0.0000000000, 0.0000000000],\n        [0.1624072790, 0.0813293457, 0.0000000000],\n        [0.8945778608, 0.8664447665, 0.3925123215]])\n\n\n\nposdef_log.dtypes\n\nis_pd_eigv      bool\nis_pd_chol      bool\nis_sym          bool\nsym_upto       int64\neigv          object\nmatrix        object\nname          object\ndtype: object\n\n\n\nsymmetric_upto(A)\n\ntensor(0)\n\n\n\ncheck_posdef(A)\n\n/tmp/ipykernel_13841/527603031.py:33: UserWarning: Matrix is not positive definite\n  warn(\"Matrix is not positive definite\")\n\n\n\n\n\n\n  \n    \n      \n      is_pd_eigv\n      is_pd_chol\n      is_sym\n      sym_upto\n      eigv\n      matrix\n      name\n    \n  \n  \n    \n      0\n      False\n      False\n      False\n      0\n      [-0.7837653, 0.35980642, 1.8811096]\n      [[0.9833088, 0.0, 0.0], [0.16240728, 0.0813293...\n      \n    \n  \n\n\n\n\n\ncheck_posdef(posdef)\n\n\n\n\n\n  \n    \n      \n      is_pd_eigv\n      is_pd_chol\n      is_sym\n      sym_upto\n      eigv\n      matrix\n      name\n    \n  \n  \n    \n      0\n      True\n      True\n      True\n      -8\n      [0.0011096124, 0.3825126, 2.3213263]\n      [[0.9668962, 0.1596965, 0.8796463], [0.1596965...\n      \n    \n  \n\n\n\n\n\nsource\n\n\ncheck_posdef4\n\n check_posdef4 (value, error=True)\n\n\ndef check_posdef3(cov, error=True):\n    if not is_posdef(cov):\n        is_pd, eigv = is_posdef2(cov)\n        if is_pd:\n            print(f\"actually pos definite {eigv} \\n{cov}\")\n        else:\n            msg = f\"not pos definite {eigv} \\n {cov}\"\n            if error:\n                raise ValueError(msg)\n            else:\n                print(msg)\n\n\ndef check_posdef2(cov):\n    errors = []\n    for i in range(10):\n        if not is_posdef(cov):\n            errors.append(i)\n    if len(errors) > 0:\n        print(f\"at iterations {errors} not positive definite matrix. \\n{cov}\")"
  },
  {
    "objectID": "kalman/kalman_filter.html#kalmanfilter",
    "href": "kalman/kalman_filter.html#kalmanfilter",
    "title": "Kalman Filter",
    "section": "KalmanFilter",
    "text": "KalmanFilter\nThe Kalman Filter is an algorithm designed to estimate \\(P(x_t | y_{0:t})\\). As all state transitions and obss are linear with Gaussian distributed noise, these distributions can be represented exactly as Gaussian distributions with mean filt_state_means[t] and covs filt_state_covs[t]. Similarly, the Kalman Smoother is an algorithm designed to estimate \\(P(x_t | y_{0:T-1})\\)\n\nMain class\n\nsource\n\n\nKalmanFilter\n\n KalmanFilter (transition_matrices:torch.Tensor=None,\n               obs_matrices:torch.Tensor=None,\n               transition_cov:torch.Tensor=None,\n               obs_cov:torch.Tensor=None,\n               transition_offsets:torch.Tensor=None,\n               obs_offsets:torch.Tensor=None,\n               initial_state_mean:torch.Tensor=None,\n               initial_state_cov:torch.Tensor=None, n_dim_state:int=None,\n               n_dim_obs:int=None)\n\nKalman Filter and Smoother using PyTorch\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntransition_matrices\nTensor\nNone\n[n_timesteps-1, n_dim_state, n_dim_state] or [n_dim_state,n_dim_state]\n\n\nobs_matrices\nTensor\nNone\n[n_timesteps, n_dim_obs, n_dim_state] or [n_dim_obs, n_dim_state]\n\n\ntransition_cov\nTensor\nNone\n[n_dim_state, n_dim_state]\n\n\nobs_cov\nTensor\nNone\n[n_dim_obs, n_dim_obs]\n\n\ntransition_offsets\nTensor\nNone\n[n_timesteps-1, n_dim_state] or [n_dim_state]\n\n\nobs_offsets\nTensor\nNone\n[n_timesteps, n_dim_obs] or [n_dim_obs]\n\n\ninitial_state_mean\nTensor\nNone\n[n_dim_state]\n\n\ninitial_state_cov\nTensor\nNone\n[n_dim_state, n_dim_state]\n\n\nn_dim_state\nint\nNone\nNumber of dimensions for state - defaults to 1 if cannot be infered from parameters\n\n\nn_dim_obs\nint\nNone\nNumber of dimensions for observations - defaults to 1 if cannot be infered from parameters\n\n\n\n\nk = KalmanFilter()\n\n\nk.obs_cov\n\ntensor([[1.]], grad_fn=<MmBackward0>)\n\n\n\nk.initial_state_cov\n\ntensor([[1.]], grad_fn=<MmBackward0>)\n\n\n\nk.initial_state_cov = array2d(torch.tensor(3.))\n\n\nk.initial_state_cov\n\ntensor([[3.]], grad_fn=<MmBackward0>)\n\n\n\nk.initial_state_cov_raw\n\nParameter containing:\ntensor([[1.7320507765]], requires_grad=True)\n\n\n\nlist(k.named_parameters())\n\n[('transition_matrices',\n  Parameter containing:\n  tensor([[1.]], requires_grad=True)),\n ('transition_offsets',\n  Parameter containing:\n  tensor([0.], requires_grad=True)),\n ('transition_cov_raw',\n  Parameter containing:\n  tensor([[1.]], requires_grad=True)),\n ('obs_matrices',\n  Parameter containing:\n  tensor([[1.]], requires_grad=True)),\n ('obs_offsets',\n  Parameter containing:\n  tensor([0.], requires_grad=True)),\n ('obs_cov_raw',\n  Parameter containing:\n  tensor([[1.]], requires_grad=True)),\n ('initial_state_mean',\n  Parameter containing:\n  tensor([0.], requires_grad=True)),\n ('initial_state_cov_raw',\n  Parameter containing:\n  tensor([[1.7320507765]], requires_grad=True))]\n\n\n\nk = KalmanFilter()\n# pykalman reference implementation\npyk = pykalman.KalmanFilter()\n\n\nX = torch.tensor([1.,2,3])\nnX = X.numpy()\nX\n\ntensor([1., 2., 3.])\n\n\n2 dimensional dobs\n\nX2 = torch.tensor([[i, 10. * i] for i in range(1,5)])\nnX2 = X2.numpy()\n\n\nk2 = KalmanFilter(transition_matrices = torch.eye(2), obs_matrices=torch.eye(2))\n\npyk2 = pykalman.KalmanFilter(n_dim_obs = 2, n_dim_state=2)\n\n\nX2\n\ntensor([[ 1., 10.],\n        [ 2., 20.],\n        [ 3., 30.],\n        [ 4., 40.]])\n\n\n\nobs_mask = torch.ones_like(X2, dtype=torch.bool)\nobs_mask[0, 1] = False # only one value missing\nobs_mask[2, :] = False # whole row missing\n# Xm X missing\nX2m = X2.clone()\nX2m[~obs_mask] = torch.nan\n# nXm Numpy X missing\nnX2m = np.ma.masked_array(X2.numpy(), mask = ~obs_mask.numpy())\n\n\nobs_mask\n\ntensor([[ True, False],\n        [ True,  True],\n        [False, False],\n        [ True,  True]])\n\n\n\nX2m\n\ntensor([[ 1., nan],\n        [ 2., 20.],\n        [nan, nan],\n        [ 4., 40.]])\n\n\n\nnX2m\n\nmasked_array(\n  data=[[1.0, --],\n        [2.0, 20.0],\n        [--, --],\n        [4.0, 40.0]],\n  mask=[[False,  True],\n        [False, False],\n        [ True,  True],\n        [False, False]],\n  fill_value=1e+20,\n  dtype=float32)\n\n\n\n\nInitializer\nGiving all the parameters manually to the KalmanFilter init method is not convinient, hence we are having some methods that help initialize the class\n\n@patch(cls_method=True)\ndef init_random(cls: KalmanFilter, n_dim_obs, n_dim_state, dtype=torch.float32):\n    \n    params = {\n        'transition_matrices': torch.rand(n_dim_state, n_dim_state, dtype=dtype),\n        'transition_offsets':  torch.rand(n_dim_state, dtype=dtype),        \n        'transition_cov':      to_posdef(torch.rand(n_dim_state, n_dim_state, dtype=dtype)),        \n        'obs_matrices':        torch.rand(n_dim_obs, n_dim_state, dtype=dtype),\n        'obs_offsets':         torch.rand(n_dim_obs, dtype=dtype),          \n        'obs_cov':             to_posdef(torch.rand(n_dim_obs, n_dim_obs, dtype=dtype)),            \n        'initial_state_mean':  torch.rand(n_dim_state, dtype=dtype),        \n        'initial_state_cov':   to_posdef(torch.rand(n_dim_state, n_dim_state, dtype=dtype)),\n    } \n    return cls(**params)\n\n\nKalmanFilter.init_random(3,6, dtype=torch.float64)\n\nKalman Filter\n        N dim obs: 3, N dim state: 6\n\n\n\n\nTester\n\nsource\n\n\nKalmanFilterTester\n\n KalmanFilterTester (n_dim_state=3, n_dim_obs=3, n_obs=10, p_missing=0.3,\n                     seed=None, dtype=torch.float32, nan_mask=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntst = KalmanFilterTester()\n\n\ntst64 = KalmanFilterTester(dtype=torch.float64)\n\n\ntest_close(tst.params.values(), tst.params_pyk.values())\n\n\n\nFilter\n\nFilter predict\n\ndef _filter_predict2(transition_matrix, transition_covariance,\n                    transition_offset, current_state_mean,\n                    current_state_covariance):\n    predicted_state_mean = (\n        torch.matmul(transition_matrix, current_state_mean)\n        + transition_offset\n    )\n    predicted_state_covariance = (\n        torch.matmul(transition_matrix,\n               torch.matmul(current_state_covariance,\n                      transition_matrix.T))\n        + transition_covariance\n    )\n\n    return (predicted_state_mean, predicted_state_covariance)\n\n\n(\n    (tst.params['transition_matrices'] @ tst.params['initial_state_cov'] ) -\n    torch.matmul(tst.params['transition_matrices'], tst.params['initial_state_cov'])\n)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\n(\n    (tst.params['transition_matrices'] @ tst.params['initial_state_cov']  @ tst.params['transition_matrices'].T) -\n    torch.matmul(tst.params['transition_matrices'], torch.matmul(tst.params['initial_state_cov'], tst.params['transition_matrices'].T))\n)\n\ntensor([[ 1.1920928955e-07,  0.0000000000e+00,  1.1920928955e-07],\n        [ 0.0000000000e+00,  0.0000000000e+00,  1.1920928955e-07],\n        [ 0.0000000000e+00, -2.3841857910e-07, -1.1920928955e-07]])\n\n\n\n(\n    (tst.params['transition_matrices'] @ tst.params['initial_state_cov']  @ tst.params['transition_matrices'].T + tst.params['transition_cov']) -\n    (torch.matmul(tst.params['transition_matrices'], torch.matmul(tst.params['initial_state_cov'], tst.params['transition_matrices'].T)) + tst.params['transition_cov'])\n)\n\ntensor([[ 2.3841857910e-07,  0.0000000000e+00,  1.1920928955e-07],\n        [ 0.0000000000e+00,  0.0000000000e+00,  1.1920928955e-07],\n        [ 0.0000000000e+00, -2.3841857910e-07,  0.0000000000e+00]])\n\n\n\ntrans_m = np.eye(2)\ntrans_cov = np.eye(2)\ntrans_off = np.zeros((2,2))\ncurr_mean = np.ones((2,1))\ncurr_cov = np.zeros((2,2))\n\n\n_filter_predict(torch.tensor(trans_m) , torch.tensor(trans_cov), torch.tensor(trans_off), torch.tensor(curr_mean), torch.tensor(curr_cov))\n\n(tensor([[1., 1.],\n         [1., 1.]], dtype=torch.float64),\n tensor([[1., 0.],\n         [0., 1.]], dtype=torch.float64))\n\n\n\ntest_close(\n    pykalman.standard._filter_predict(trans_m , trans_cov, trans_off, curr_mean, curr_cov),\n    _filter_predict(torch.tensor(trans_m) , torch.tensor(trans_cov), torch.tensor(trans_off), torch.tensor(curr_mean), torch.tensor(curr_cov)))\n\n\npred_pyk = pykalman.standard._filter_predict(\n   tst.params_pyk['transition_matrices'],\n   tst.params_pyk['transition_covariance'],\n   tst.params_pyk['transition_offsets'],\n   tst.params_pyk['initial_state_mean'],\n   tst.params_pyk['initial_state_covariance'],\n)\n\n\npred_torch = _filter_predict(\n   tst.params['transition_matrices'],\n   tst.params['transition_cov'],\n   tst.params['transition_offsets'],\n   tst.params['initial_state_mean'],\n   tst.params['initial_state_cov'],\n\n)\n\n\npred_pyk[0] - pred_torch[0].numpy()\n\narray([0., 0., 0.], dtype=float32)\n\n\n\ntest_close(pred_pyk[0], pred_torch[0])\n\n\npred_pyk[1] - pred_torch[1].numpy()\n\narray([[-2.3841858e-07,  0.0000000e+00, -1.1920929e-07],\n       [ 0.0000000e+00,  0.0000000e+00, -1.1920929e-07],\n       [ 0.0000000e+00,  2.3841858e-07,  0.0000000e+00]], dtype=float32)\n\n\n\ntest_close(pred_pyk[1], pred_torch[1])\n\n\npykalman.standard._filter_predict??\n\n\nSignature:\npykalman.standard._filter_predict(\n    transition_matrix,\n    transition_covariance,\n    transition_offset,\n    current_state_mean,\n    current_state_covariance,\n)\nSource:   \ndef _filter_predict(transition_matrix, transition_covariance,\n                    transition_offset, current_state_mean,\n                    current_state_covariance):\n    r\"\"\"Calculate the mean and covariance of :math:`P(x_{t+1} | z_{0:t})`\n    Using the mean and covariance of :math:`P(x_t | z_{0:t})`, calculate the\n    mean and covariance of :math:`P(x_{t+1} | z_{0:t})`.\n    Parameters\n    ----------\n    transition_matrix : [n_dim_state, n_dim_state} array\n        state transition matrix from time t to t+1\n    transition_covariance : [n_dim_state, n_dim_state] array\n        covariance matrix for state transition from time t to t+1\n    transition_offset : [n_dim_state] array\n        offset for state transition from time t to t+1\n    current_state_mean: [n_dim_state] array\n        mean of state at time t given observations from times\n        [0...t]\n    current_state_covariance: [n_dim_state, n_dim_state] array\n        covariance of state at time t given observations from times\n        [0...t]\n    Returns\n    -------\n    predicted_state_mean : [n_dim_state] array\n        mean of state at time t+1 given observations from times [0...t]\n    predicted_state_covariance : [n_dim_state, n_dim_state] array\n        covariance of state at time t+1 given observations from times\n        [0...t]\n    \"\"\"\n    predicted_state_mean = (\n        np.dot(transition_matrix, current_state_mean)\n        + transition_offset\n    )\n    predicted_state_covariance = (\n        np.dot(transition_matrix,\n               np.dot(current_state_covariance,\n                      transition_matrix.T))\n        + transition_covariance\n    )\n    return (predicted_state_mean, predicted_state_covariance)\nFile:      ~/anaconda3/envs/data-science/lib/python3.10/site-packages/pykalman/standard.py\nType:      function\n\n\n\n\n\ntst.params_pyk['transition_matrices'].shape\n\n(3, 3)\n\n\n\ntest_close(\n   pykalman.standard._filter_predict(\n       tst.params_pyk['transition_matrices'],\n       tst.params_pyk['transition_covariance'],\n       tst.params_pyk['transition_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n    ),\n    \n    _filter_predict2(\n       tst.params['transition_matrices'],\n       tst.params['transition_cov'],\n       tst.params['transition_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n    \n    )\n)\n\n\ntest_close(\n    \n    _filter_predict2(\n       tst.params['transition_matrices'],\n       tst.params['transition_cov'],\n       tst.params['transition_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n    ),\n    _filter_predict(\n       tst.params['transition_matrices'],\n       tst.params['transition_cov'],\n       tst.params['transition_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n    )\n)\n\n\ntest_close(\n   pykalman.standard._filter_predict(\n       tst.params_pyk['transition_matrices'],\n       tst.params_pyk['transition_covariance'],\n       tst.params_pyk['transition_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n    ),\n    \n    _filter_predict(\n       tst.params['transition_matrices'],\n       tst.params['transition_cov'],\n       tst.params['transition_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n    \n    )\n)\n\nThe issues seems is was from check_posdef but torch.matmul and @ are correctly working as intended\n\n\nFilter correct\n\npykalman.standard._filter_correct??\n\n\nSignature:\npykalman.standard._filter_correct(\n    observation_matrix,\n    observation_covariance,\n    observation_offset,\n    predicted_state_mean,\n    predicted_state_covariance,\n    observation,\n)\nSource:   \ndef _filter_correct(observation_matrix, observation_covariance,\n                    observation_offset, predicted_state_mean,\n                    predicted_state_covariance, observation):\n    r\"\"\"Correct a predicted state with a Kalman Filter update\n    Incorporate observation `observation` from time `t` to turn\n    :math:`P(x_t | z_{0:t-1})` into :math:`P(x_t | z_{0:t})`\n    Parameters\n    ----------\n    observation_matrix : [n_dim_obs, n_dim_state] array\n        observation matrix for time t\n    observation_covariance : [n_dim_obs, n_dim_obs] array\n        covariance matrix for observation at time t\n    observation_offset : [n_dim_obs] array\n        offset for observation at time t\n    predicted_state_mean : [n_dim_state] array\n        mean of state at time t given observations from times\n        [0...t-1]\n    predicted_state_covariance : [n_dim_state, n_dim_state] array\n        covariance of state at time t given observations from times\n        [0...t-1]\n    observation : [n_dim_obs] array\n        observation at time t.  If `observation` is a masked array and any of\n        its values are masked, the observation will be ignored.\n    Returns\n    -------\n    kalman_gain : [n_dim_state, n_dim_obs] array\n        Kalman gain matrix for time t\n    corrected_state_mean : [n_dim_state] array\n        mean of state at time t given observations from times\n        [0...t]\n    corrected_state_covariance : [n_dim_state, n_dim_state] array\n        covariance of state at time t given observations from times\n        [0...t]\n    \"\"\"\n    if not np.any(np.ma.getmask(observation)):\n        predicted_observation_mean = (\n            np.dot(observation_matrix,\n                   predicted_state_mean)\n            + observation_offset\n        )\n        predicted_observation_covariance = (\n            np.dot(observation_matrix,\n                   np.dot(predicted_state_covariance,\n                          observation_matrix.T))\n            + observation_covariance\n        )\n        kalman_gain = (\n            np.dot(predicted_state_covariance,\n                   np.dot(observation_matrix.T,\n                          linalg.pinv(predicted_observation_covariance)))\n        )\n        corrected_state_mean = (\n            predicted_state_mean\n            + np.dot(kalman_gain, observation - predicted_observation_mean)\n        )\n        corrected_state_covariance = (\n            predicted_state_covariance\n            - np.dot(kalman_gain,\n                     np.dot(observation_matrix,\n                            predicted_state_covariance))\n        )\n    else:\n        n_dim_state = predicted_state_covariance.shape[0]\n        n_dim_obs = observation_matrix.shape[0]\n        kalman_gain = np.zeros((n_dim_state, n_dim_obs))\n        corrected_state_mean = predicted_state_mean\n        corrected_state_covariance = predicted_state_covariance\n    return (kalman_gain, corrected_state_mean,\n            corrected_state_covariance)\nFile:      ~/anaconda3/envs/data-science/lib/python3.10/site-packages/pykalman/standard.py\nType:      function\n\n\n\n\n\ntst = KalmanFilterTester(p_missing=0, nan_mask=False)\n\n\nobservation_matrix = tst.params_pyk['observation_matrices']\nobservation_covariance = tst.params_pyk['observation_covariance']\npredicted_state_mean = tst.params_pyk['initial_state_mean']\npredicted_state_covariance = tst.params_pyk['initial_state_covariance']\nobservation_offset = tst.params_pyk['observation_offsets']\nobservation = tst.data_pyk[0]\n\n\nobservation\n\nmasked_array(data=[0.6440156698226929, 0.7071067690849304,\n                   0.6581305861473083],\n             mask=[False, False, False],\n       fill_value=1e+20,\n            dtype=float32)\n\n\n\nobs_matrix = tst.params['obs_matrices']\nobs_cov = tst.params['obs_cov']\npred_state_mean = tst.params['initial_state_mean']\npred_state_cov = tst.params['initial_state_cov']\nobs_offset = tst.params['obs_offsets']\nobs = tst.data[0]\n\nPred obs mean\n\npredicted_observation_mean = (\n    np.dot(observation_matrix,\n           predicted_state_mean)\n    + observation_offset\n)\n\n\npred_obs_mean = obs_matrix @ pred_state_mean + obs_offset\n\n\npred_obs_mean\n\ntensor([2.2023458481, 0.6882513762, 1.5503764153])\n\n\n\ntest_close(pred_obs_mean, predicted_observation_mean)\n\nPred obs cov\n\npredicted_observation_covariance = (\n    np.dot(observation_matrix,\n           np.dot(predicted_state_covariance,\n                  observation_matrix.T))\n    + observation_covariance\n)\n\n\npred_obs_cov = obs_matrix @ pred_state_cov @ obs_matrix.T + obs_cov\n\n\npred_obs_cov\n\ntensor([[1.3011450768, 0.8057925701, 0.5043830872],\n        [0.8057925701, 1.3843567371, 0.9445988536],\n        [0.5043830872, 0.9445988536, 0.8853620291]])\n\n\n\ntest_close(predicted_observation_covariance, pred_obs_cov)\n\nKalman gain\n\nkalman_gain = (\n    np.dot(predicted_state_covariance,\n           np.dot(observation_matrix.T,\n                  np.linalg.pinv(predicted_observation_covariance)))\n)\n\n\nkalman_gain_torch = pred_state_cov @ obs_matrix.T @ torch.inverse(pred_obs_cov)\n\n\nkalman_gain_torch\n\ntensor([[ 0.1531807333, -0.2277897745,  0.3053238094],\n        [ 0.4359902442, -0.6390077472,  0.8577750921],\n        [ 0.7636307478,  0.0963483751, -0.3548507094]])\n\n\n\ntest_close(kalman_gain, kalman_gain_torch)\n\ncorr state mean\n\ncorrected_state_mean = (\n    predicted_state_mean\n    + np.dot(kalman_gain, observation - predicted_observation_mean))\n\n\ncorr_state_mean = pred_state_mean + kalman_gain_torch @ (obs - pred_obs_mean)\n\n\ncorr_state_mean\n\ntensor([ 0.1950036883, -0.5104005933, -0.0825284123])\n\n\n\ntest_close(corrected_state_mean, corr_state_mean)\n\ncorr state cov\n\ncorrected_state_covariance = (\n    predicted_state_covariance\n    - np.dot(kalman_gain,\n             np.dot(observation_matrix,\n                    predicted_state_covariance))\n)\n\n\ncorr_state_cov = pred_state_cov - kalman_gain_torch @ obs_matrix @ pred_state_cov\n\n\ncorr_state_cov\n\ntensor([[ 0.0347420089,  0.0862310678, -0.0056003332],\n        [ 0.0862310529,  0.2497220337, -0.0096721053],\n        [-0.0056003183, -0.0096720457,  0.2725932002]])\n\n\n\ntest_close(corrected_state_covariance, corr_state_cov)\n\n\ndef print_info(xs, name=''):\n    for x in listify(xs):\n        print(f\"{name} - shape: {x.shape}, type {x.dtype}, mean {x.mean()}\")\n\n\nnp.any(np.ma.getmask(observation))\n\nFalse\n\n\n\ndef _pyk_filter_correct(observation_matrix, observation_covariance,\n                    observation_offset, predicted_state_mean,\n                    predicted_state_covariance, observation):\n    if not np.any(np.ma.getmask(observation)):\n        predicted_observation_mean = (\n            np.dot(observation_matrix,\n                   predicted_state_mean)\n            + observation_offset\n        )\n        print_info(predicted_observation_mean, 'pred_obs_mean')\n        predicted_observation_covariance = (\n            np.dot(observation_matrix,\n                   np.dot(predicted_state_covariance,\n                          observation_matrix.T))\n            + observation_covariance\n        )\n        print_info(predicted_observation_covariance, 'pred_obs_cov')\n\n        kalman_gain = (\n            np.dot(predicted_state_covariance,\n                   np.dot(observation_matrix.T,\n                          np.linalg.pinv(predicted_observation_covariance)))\n        )\n        print_info(kalman_gain, 'kalman_gain')\n\n        corrected_state_mean = (\n            predicted_state_mean\n            + np.dot(kalman_gain, observation - predicted_observation_mean)\n        )\n        print_info(corrected_state_mean, 'corr_state_mean')\n        corrected_state_covariance = (\n            predicted_state_covariance\n            - np.dot(kalman_gain,\n                     np.dot(observation_matrix,\n                            predicted_state_covariance))\n        )\n        print_info(corrected_state_covariance, 'corr_state_cov')\n    else:\n        n_dim_state = predicted_state_covariance.shape[0]\n        n_dim_obs = observation_matrix.shape[0]\n        kalman_gain = np.zeros((n_dim_state, n_dim_obs))\n\n        corrected_state_mean = predicted_state_mean\n        corrected_state_covariance = predicted_state_covariance\n\n    return (kalman_gain, corrected_state_mean,\n            corrected_state_covariance)\n\n\ntst = KalmanFilterTester() # need nan\n\n\n_filter_correct(\n   tst.params['obs_matrices'],\n   tst.params['obs_cov'],\n   tst.params['obs_offsets'],\n   tst.params['initial_state_mean'],\n   tst.params['initial_state_cov'],\n   tst.data[0],\n   tst.mask[0]\n)\n\n(tensor([[ 0.1531807333, -0.2277897894,  0.3053237796],\n         [ 0.4359902143, -0.6390078068,  0.8577748537],\n         [ 0.7636308074,  0.0963483453, -0.3548508286]]),\n tensor([ 0.1950036883, -0.5104003549, -0.0825284123]),\n tensor([[ 0.0347420126,  0.0862310827, -0.0056003183],\n         [ 0.0862310976,  0.2497221231, -0.0096720159],\n         [-0.0056003109, -0.0096720159,  0.2725932002]]))\n\n\n\ntst.data\n\ntensor([[0.6440156698, 0.7071067691, 0.6581305861],\n        [         nan,          nan, 0.1447432041],\n        [0.5314818621, 0.1587299109, 0.6541759968],\n        [0.3278088570,          nan,          nan],\n        [0.9146959186, 0.2036490440, 0.2018010020],\n        [         nan,          nan, 0.6666255593],\n        [0.9811253548,          nan, 0.0040619373],\n        [         nan,          nan,          nan],\n        [         nan,          nan, 0.2417873144],\n        [0.1591441035, 0.7652890682,          nan]])\n\n\n\ntst.mask\n\ntensor([[ True,  True,  True],\n        [False, False,  True],\n        [ True,  True,  True],\n        [ True, False, False],\n        [ True,  True,  True],\n        [False, False,  True],\n        [ True, False,  True],\n        [False, False, False],\n        [False, False,  True],\n        [ True,  True, False]])\n\n\n\ntst.mask\n\ntensor([[ True,  True,  True],\n        [False, False,  True],\n        [ True,  True,  True],\n        [ True, False, False],\n        [ True,  True,  True],\n        [False, False,  True],\n        [ True, False,  True],\n        [False, False, False],\n        [False, False,  True],\n        [ True,  True, False]])\n\n\n\npykalman.standard._filter_correct(\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0]\n    )\n\n(array([[ 0.15318069, -0.22778963,  0.30532372],\n        [ 0.4359901 , -0.6390073 ,  0.8577748 ],\n        [ 0.76363057,  0.09634931, -0.35485128]], dtype=float32),\n masked_array(data=[0.19500380754470825, -0.510400116443634,\n                    -0.0825275182723999],\n              mask=[False, False, False],\n        fill_value=1e+20,\n             dtype=float32),\n array([[ 0.03474201,  0.08623108, -0.00560034],\n        [ 0.08623108,  0.24972206, -0.00967214],\n        [-0.00560029, -0.00967202,  0.27259302]], dtype=float32))\n\n\n\n_pyk_filter_correct(\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0]\n    )\n\npred_obs_mean - shape: (3,), type float32, mean 1.480324625968933\npred_obs_cov - shape: (3, 3), type float32, mean 0.8978236317634583\nkalman_gain - shape: (3, 3), type float32, mean 0.1545112431049347\ncorr_state_mean - shape: (3,), type float32, mean -0.13264185190200806\ncorr_state_cov - shape: (3, 3), type float32, mean 0.07766379415988922\n\n\n(array([[ 0.15318075, -0.22778977,  0.30532384],\n        [ 0.43599027, -0.63900775,  0.85777515],\n        [ 0.7636308 ,  0.09634857, -0.3548507 ]], dtype=float32),\n masked_array(data=[0.19500356912612915, -0.5104007124900818,\n                    -0.08252841234207153],\n              mask=[False, False, False],\n        fill_value=1e+20,\n             dtype=float32),\n array([[ 0.034742  ,  0.08623105, -0.00560036],\n        [ 0.08623105,  0.249722  , -0.00967214],\n        [-0.00560035, -0.00967214,  0.27259302]], dtype=float32))\n\n\n\npykalman.standard._filter_correct(\n   tst.params['obs_matrices'].numpy(),\n   tst.params['obs_cov'].numpy(),\n   tst.params['obs_offsets'].numpy(),\n   tst.params['initial_state_mean'].numpy(),\n   tst.params['initial_state_cov'].numpy(),\n   np.ma.masked_array(tst.data[0], mask=[True, True, True])\n)\n\n(array([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]),\n array([0.7104288, 0.9464111, 0.7890298], dtype=float32),\n array([[0.07919369, 0.21216275, 0.08634329],\n        [0.21216275, 0.6065132 , 0.25406086],\n        [0.08634329, 0.25406086, 0.9362984 ]], dtype=float32))\n\n\n\n_pyk_filter_correct(\n   tst.params['obs_matrices'].numpy(),\n   tst.params['obs_cov'].numpy(),\n   tst.params['obs_offsets'].numpy(),\n   tst.params['initial_state_mean'].numpy(),\n   tst.params['initial_state_cov'].numpy(),\n   np.ma.masked_array(tst.data[0], mask=[True, True, True])\n)\n\n(array([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]]),\n array([0.7104288, 0.9464111, 0.7890298], dtype=float32),\n array([[0.07919369, 0.21216275, 0.08634329],\n        [0.21216275, 0.6065132 , 0.25406086],\n        [0.08634329, 0.25406086, 0.9362984 ]], dtype=float32))\n\n\n\n(pykalman.standard._filter_correct(\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0]\n    )[0] -\n    \n    _filter_correct(\n       tst.params['obs_matrices'],\n       tst.params['obs_cov'],\n       tst.params['obs_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n       tst.data[0],\n       tst.mask[0]\n    )[0].numpy())\n\narray([[-4.4703484e-08,  1.6391277e-07, -5.9604645e-08],\n       [-1.1920929e-07,  5.3644180e-07, -5.9604645e-08],\n       [-2.3841858e-07,  9.6112490e-07, -4.4703484e-07]], dtype=float32)\n\n\n\nargs_np = {\n    'obs_m': np.eye(2),\n    'obs_cov': np.eye(2),\n    'obs_off': np.zeros((2,1)),\n    'pred_state_mean': np.ones(2),\n    'pred_state_cov': np.eye(2),\n    'obs': np.ones((2,1)),\n}\n\nargs_torch = {k: torch.tensor(v) for k,v in args_np.items()}\n\n\n_filter_correct(*args_torch.values(), mask=torch.ones_like(args_torch['obs'], dtype=torch.bool))\n\n(tensor([[0.5000000000, 0.0000000000],\n         [0.0000000000, 0.5000000000]], dtype=torch.float64),\n tensor([[1., 1.],\n         [1., 1.]], dtype=torch.float64),\n tensor([[0.5000000000, 0.0000000000],\n         [0.0000000000, 0.5000000000]], dtype=torch.float64))\n\n\n\ntest_close(\n    pykalman.standard._filter_correct(*args_np.values()),\n    _filter_correct(*args_torch.values(), mask=torch.ones_like(args_torch['obs'], dtype=torch.bool)))\n\n\ntest_close(\n   pykalman.standard._filter_correct(\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0]\n    ),\n    \n    _filter_correct(\n       tst.params['obs_matrices'],\n       tst.params['obs_cov'],\n       tst.params['obs_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n       tst.data[0],\n       tst.mask[0]\n    )\n)\n\ntrying multiple random values …\n\nfor _ in range(100):\n    tst = KalmanFilterTester(nan_mask=False)\n    test_close(\n       pykalman.standard._filter_correct(\n           tst.params_pyk['observation_matrices'],\n           tst.params_pyk['observation_covariance'],\n           tst.params_pyk['observation_offsets'],\n           tst.params_pyk['initial_state_mean'],\n           tst.params_pyk['initial_state_covariance'],\n           tst.data_pyk[0]\n        ),\n\n        _filter_correct(\n           tst.params['obs_matrices'],\n           tst.params['obs_cov'],\n           tst.params['obs_offsets'],\n           tst.params['initial_state_mean'],\n           tst.params['initial_state_cov'],\n           tst.data[0],\n           tst.mask[0]\n        )\n    )\n\n\n\nFilter\n\ndef test_filter():\n    pred_s, pred_s_cov, kal, filt_s, filt_s_cov =  pykalman.standard._filter(\n       tst.params_pyk['transition_matrices'],\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['transition_covariance'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['transition_offsets'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0:1]\n    )\n    \n    pyk = (pred_s, pred_s_cov, filt_s, filt_s_cov,) # results without kalman gain\n    \n    filter_torch = tuple(map(\n        torch.vstack, # need to convert lists to tensor\n          _filter(\n           tst.params['transition_matrices'],\n           tst.params['obs_matrices'],\n           tst.params['transition_cov'],\n           tst.params['obs_cov'],\n           tst.params['transition_offsets'],\n           tst.params['obs_offsets'],\n           tst.params['initial_state_mean'],\n           tst.params['initial_state_cov'],\n           tst.data[0:1],\n           tst.mask[0:1]\n    )))\n    \n    test_close(pyk, filter_torch)\n\n\ntest_filter()\n\n\nOld testing\n\nargs_filt_np = {\n    'trans_m': np.eye(2),\n    'obs_m': np.eye(2),\n    'trans_cov': np.eye(2),\n    'obs_cov': np.eye(2),\n    'trans_off': np.zeros((2,1)),\n    'obs_off': np.zeros((3,2)),\n    'init_state_mean': np.ones(2),\n    'init_state_cov': np.eye(2),\n    'obs': np.ones((3, 2)),\n}\n\nobs_mask = np.ones(3)\n\nargs_filt_torch = {k: torch.tensor(v, dtype = torch.float32) for k,v in args_filt_np.items()}\n\n\n_filter(*args_filt_torch.values(), obs_mask=obs_mask)\n\n([tensor([1., 1.]), tensor([1., 1.]), tensor([1., 1.])],\n [tensor([[1., 0.],\n          [0., 1.]]),\n  tensor([[1.5000000000, 0.0000000000],\n          [0.0000000000, 1.5000000000]]),\n  tensor([[1.5999999046, 0.0000000000],\n          [0.0000000000, 1.5999999046]])],\n [tensor([1., 1.]), tensor([1., 1.]), tensor([1., 1.])],\n [tensor([[0.5000000000, 0.0000000000],\n          [0.0000000000, 0.5000000000]]),\n  tensor([[0.5999999642, 0.0000000000],\n          [0.0000000000, 0.5999999642]]),\n  tensor([[0.6153845191, 0.0000000000],\n          [0.0000000000, 0.6153845191]])])\n\n\n\nfilt_pyk = list(pykalman.standard._filter(*args_filt_np.values()))\ndel filt_pyk[2] # remove kalman gain that is not returned py _filter\n\nfilt =  list(map(lambda x: torch.stack(x), _filter(*args_filt_torch.values(), obs_mask=obs_mask)))\n\ntest_close( filt_pyk, filt)\n\nmissing data\n\nobs_mask = np.array([True, False, True])\nargs_filt_np['obs'] = np.ma.masked_array(args_filt_np['obs'], mask = np.vstack([~obs_mask] * 2).T)\n\n\nargs_filt_np['obs']\n\nmasked_array(\n  data=[[1.0, 1.0],\n        [--, --],\n        [1.0, 1.0]],\n  mask=[[False, False],\n        [ True,  True],\n        [False, False]],\n  fill_value=1e+20)\n\n\n\nfilt_pyk = list(pykalman.standard._filter(*args_filt_np.values()))\ndel filt_pyk[2] # remove kalman gain that is not returned py _filter\n\nfilt =  list(map(lambda x: torch.stack(x), _filter(*args_filt_torch.values(), obs_mask=torch.tensor(obs_mask))))\n\ntest_close( filt_pyk, filt)\n\n\n\n\nKalmanFilter method\n\nsource\n\n\n\nKalmanFilter.filter\n\n KalmanFilter.filter (obs:torch.Tensor, mask=None, check_args=None)\n\nFilter observation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobs\nTensor\n\n[n_timesteps, n_dim_obs] obs for times [0…n_timesteps-1]\n\n\nmask\nNoneType\nNone\n\n\n\ncheck_args\nNoneType\nNone\n\n\n\nReturns\nListNormal\n\nFiltered state\n\n\n\n\nFinal Testing\ndue to numerical issue the error is bigger thatn 1e-5 which is default\n\n((tst.filter.filter(tst.data, tst.mask)[1] -\nnp.float32(tst.filter_pyk.filter(tst.data_pyk)[1]))).max()\n\ntensor(1.2040138245e-05)\n\n\n\ntst = KalmanFilterTester()\n\n\ntest_close(\n    tst.filter.filter(tst.data, tst.mask),\n    tst.filter_pyk.filter(tst.data_pyk), eps=4e-2) # need to increase the resolution\n\nusing float64 the error is smaller, which confirms that it is only a numerical issue\n\ntest_close(\n    tst64.filter.filter(tst.data, tst.mask),\n    tst64.filter_pyk.filter(tst.data_pyk))\n\n\n((tst64.filter.filter(tst64.data, tst64.mask)[1] -\ntst64.filter_pyk.filter(tst64.data_pyk)[1])).max()\n\ntensor(7.1054273576e-15, dtype=torch.float64)\n\n\n\nOld Testing\n\ntest_close(\n    pyk.filter(X.numpy()),\n    k.filter(X)\n)\n\n\ntest_close(\n    pyk2.filter(nX2),\n    k2.filter(X2)\n)\n\n\nk2.filter(X2m)\n\nListNormal(mean=tensor([[ 0.0000000000,  0.0000000000],\n        [ 1.3333332539, 13.3333320618],\n        [ 1.3333332539, 13.3333320618],\n        [ 3.2727270126, 32.7272720337]]), cov=tensor([[[1.0000000000, 0.0000000000],\n         [0.0000000000, 1.0000000000]],\n\n        [[0.6666667461, 0.0000000000],\n         [0.0000000000, 0.6666667461]],\n\n        [[1.6666667461, 0.0000000000],\n         [0.0000000000, 1.6666667461]],\n\n        [[0.7272728682, 0.0000000000],\n         [0.0000000000, 0.7272728682]]]))\n\n\n\ntest_close(\n    pyk.filter(X.numpy()),\n    k.filter(X)\n)\n\n\ntest_close(\n    pyk2.filter(nX2),\n    k2.filter(X2)\n)\n\n\ntest_close(\n    pyk2.filter(nX2m),\n    k2.filter(X2m)\n)\n\n\n\n\n\nSmooth\n\nSmooth step\n\nargs_np_sm = {\n    'pred_state_m': np.zeros(2),\n    'pred_state_cov': np.eye(2),\n    'filt_state_m': np.zeros(2),\n    'filt_state_cov': np.eye(2),\n    'next_state_m': np.zeros(2),\n    'next_state_cov': np.eye(2),\n    'trans_m': np.eye(2),\n}\n\nargs_torch_sm = {k: torch.tensor(v) for k,v in args_np_sm.items()}\n\n\npyk_mean, pyk_cov, _ = pykalman.standard._smooth_update(\n    args_np_sm['trans_m'],\n    args_np_sm['filt_state_m'],\n    args_np_sm['filt_state_cov'],\n    args_np_sm['pred_state_m'],\n    args_np_sm['pred_state_cov'],\n    args_np_sm['next_state_m'],\n    args_np_sm['next_state_cov'],\n)\n\n\ntorch_k = _smooth_update(\n    args_torch_sm['trans_m'],\n    Normal(args_torch_sm['filt_state_m'], args_torch_sm['filt_state_cov']),\n    Normal(args_torch_sm['pred_state_m'], args_torch_sm['pred_state_cov']),\n    Normal(args_torch_sm['next_state_m'], args_torch_sm['next_state_cov']),\n)\n\n\ntest_close((pyk_mean, pyk_cov, ), torch_k)\n\n\n(pred_state_means, pred_state_covs, filt_state_means, filt_state_covs ) = k2._filter_all(X2m)\n\n\ntorch_smooth = _smooth(k2.transition_matrices,  ListMNormal(filt_state_means, filt_state_covs), ListMNormal(pred_state_means, pred_state_covs))\n\n\npyk_sm_mean, pyk_sm_cov, _ = pykalman.standard._smooth(k2.transition_matrices.detach().numpy(),\n                          _stack_detach(filt_state_means).numpy(), _stack_detach(filt_state_covs).numpy(),\n                          _stack_detach(pred_state_means).numpy(), _stack_detach(pred_state_covs).numpy())\n\n\ntest_close((pyk_sm_mean, pyk_sm_cov,), torch_smooth)\n\n\ntorch_smooth\n\nListNormal(mean=tensor([[ 0.9090908170,  9.0909080505],\n        [ 1.8181817532, 18.1818180084],\n        [ 2.5454542637, 25.4545440674],\n        [ 3.2727270126, 32.7272720337]], grad_fn=<CopySlices>), cov=tensor([[[0.6363637447, 0.0000000000],\n         [0.0000000000, 0.6363637447]],\n\n        [[0.5454546213, 0.0000000000],\n         [0.0000000000, 0.5454546213]],\n\n        [[0.9090911746, 0.0000000000],\n         [0.0000000000, 0.9090911746]],\n\n        [[0.7272728682, 0.0000000000],\n         [0.0000000000, 0.7272728682]]], grad_fn=<CopySlices>))\n\n\n\n\nKalmanFilter method\n\nsource\n\n\n\nKalmanFilter.smooth\n\n KalmanFilter.smooth (obs:torch.Tensor, mask=None, check_args=None)\n\nKalman Filter Smoothing\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobs\nTensor\n\ndataset\n\n\nmask\nNoneType\nNone\n\n\n\ncheck_args\nNoneType\nNone\n\n\n\nReturns\nListNormal\n\n[n_timesteps, n_dim_state] smoothed hidden state distributions for times [0...n_timesteps-1]\n\n\n\n\nk.smooth(X)\n\nListNormal(mean=tensor([[0.9230768681],\n        [1.7692307234],\n        [2.3846154213]], grad_fn=<CopySlices>), cov=tensor([[[0.3846153915]],\n\n        [[0.4615384340]],\n\n        [[0.6153845191]]], grad_fn=<CopySlices>))\n\n\n\nFinal Testing\n\ntest_close(\n    tst.filter.smooth(tst.data, tst.mask),\n    tst.filter_pyk.smooth(tst.data_pyk), eps=4e-2) # need to increase the resolution\n\n\ntest_close(\n    tst64.filter.smooth(tst64.data, tst64.mask),\n    tst64.filter_pyk.smooth(tst64.data_pyk))\n\n\ntest_close(\n    pyk.smooth(nX),\n    k.smooth(X).detach()\n)\n\n\ntest_close(\n    pyk2.smooth(nX2),\n    k2.smooth(X2).detach()\n)\n\n\ntest_close(\n    pyk2.smooth(nX2m),\n    k2.smooth(X2m)\n)\n\n\n\n\nPredict\n\nfrom meteo_imp.gaussian import conditional_guassian\n\nIn order to have conditional predictions that make sense it’s not possible to return the full covariance matrix for the predictions but only the standard deviations\n\nobs = tst.data[1]\n\n\nmask = tst.mask[1]\n\n\nobs, mask\n\n(tensor([         nan,          nan, 0.1447432041]),\n tensor([False, False,  True]))\n\n\n\nconditional_guassian\n\n<function meteo_imp.gaussian.conditional_guassian(μ: torch.Tensor, Σ: torch.Tensor, obs: torch.Tensor, idx: torch.Tensor) -> meteo_imp.utils.ListNormal>\n\n\nThe conditional gaussian returns a distributions of 2 variables, which are the un-observed ones (you can see the nan in the observation vector)\n\nconditional_guassian(tst.params['initial_state_mean'], tst.params['initial_state_cov'], obs[mask], mask)\n\nListNormal(mean=tensor([0.6510141492, 0.7715864182]), cov=tensor([[0.0712313056, 0.1887338310],\n        [0.1887338310, 0.5375747681]]))\n\n\nwhich are correctly merged with the predictions\n\n_get_cond_pred(ListMNormal(tst.params['initial_state_mean'], tst.params['initial_state_cov']), obs, mask)\n\nListNormal(mean=tensor([0.6510141492, 0.7715864182, 0.7890297771]), cov=tensor([0.0712313056, 0.5375747681, 0.9362983704]))\n\n\n\nsource\n\n\nKalmanFilter.predict\n\n KalmanFilter.predict (obs, mask=None, smooth=True, check_args=None)\n\nPredicted observations at all times\n\nk.predict(obs=X)\n\nListNormal(mean=tensor([[0.9230768681],\n        [1.7692307234],\n        [2.3846154213]], grad_fn=<CopySlices>), cov=tensor([[1.3846154213],\n        [1.4615384340],\n        [1.6153845787]], grad_fn=<CopySlices>))\n\n\n\n@patch\ndef predict_times(self: KalmanFilter, times, obs, mask=None, smooth=True, check_args=None):\n    \"\"\"Predicted observations at specific times \"\"\"\n    state = self.smooth(obs, mask, check_args) if smooth else self.filter(obs, mask, check_args)\n    obs, mask = self._parse_obs(obs, mask)\n    times = array1d(times)\n    \n    n_timesteps = obs.shape[0]\n    n_features = obs.shape[1] if len(obs.shape) > 1 else 1\n    \n    if times.max() > n_timesteps or times.min() < 0:\n        raise ValueError(f\"provided times range from {times.min()} to {times.max()}, which is outside allowed range : 0 to {n_timesteps}\")\n\n    means = torch.empty((times.shape[0], n_features), dtype=obs.dtype, device=obs.device)\n    stds = torch.empty((times.shape[0], n_features), dtype=obs.dtype, device=obs.device) \n    for i, t in enumerate(times):\n        mean, std = self._obs_from_state(\n            state.mean[t],\n            state.cov[t],\n            {'t': t, **check_args} if check_args is not None else None\n        )\n        \n        means[i], stds[i] = _get_cond_pred(ListNormal(mean, std), obs[t], mask[t])\n    \n    return ListNormal(means, stds)\n\npykalman doesn’t support a predict method so cannot test it\n\ntst.filter.predict(obs = tst.data, mask = tst.mask)\n\nListNormal(mean=tensor([[-0.3030261397, -0.2419726700, -0.3667301536],\n        [-0.5841388106,  0.0571963340, -0.0602211729],\n        [-0.4957259297, -0.1866203099, -0.3876989782],\n        [-0.3683194518,  0.4738349319,  0.1002929062],\n        [-0.2453254759, -0.0729204565, -0.4634614289],\n        [ 0.7444251776,  0.9131455421, -0.2360927910],\n        [ 0.2539923191,  0.2802425325, -0.0793784633],\n        [ 1.0530376434,  0.5632419586,  0.3484775722],\n        [ 0.8671647310,  0.4598079920,  1.1614903212],\n        [ 6.1932973862,  2.9287741184,  0.3289620876]], grad_fn=<CopySlices>), cov=tensor([[5.1329714060e-01, 1.1908673048e+00, 7.2597950697e-01],\n        [4.8898196220e-01, 2.7054542303e-01, 7.3858928680e-01],\n        [5.2057659626e-01, 1.2010793686e+00, 7.2838747501e-01],\n        [6.7297101021e-01, 8.7734472752e-01, 6.3224875927e-01],\n        [6.1076289415e-01, 1.2208808661e+00, 7.5710517168e-01],\n        [1.2441244125e+00, 3.4210133553e-01, 1.3619506359e+00],\n        [1.1757915497e+01, 3.4084844589e-01, 3.3397865295e+00],\n        [4.8872512817e+01, 1.1506040573e+01, 1.2109507561e+01],\n        [2.7350158691e+00, 3.4780120850e-01, 4.8372123718e+01],\n        [8.5246258545e+02, 1.8208175659e+02, 2.4330139160e-01]],\n       grad_fn=<CopySlices>))\n\n\n\nmean, std = tst.filter.predict_times(obs = tst.data, times = torch.tensor([0,1]), mask = tst.mask)\n\n\nmean, std\n\n(tensor([[-0.3030261397, -0.2419726700, -0.3667301536],\n         [-0.5841388106,  0.0571963340, -0.0602211729]], grad_fn=<CopySlices>),\n tensor([[0.5132971406, 1.1908673048, 0.7259795070],\n         [0.4889819622, 0.2705454230, 0.7385892868]], grad_fn=<CopySlices>))\n\n\n\nprint_info((mean, std))\n\n - shape: torch.Size([2, 3]), type torch.float32, mean -0.2498154193162918\n - shape: torch.Size([2, 3]), type torch.float32, mean 0.6547101140022278\n\n\n\nmean, cov = tst64.filter.smooth(tst64.data, tst64.mask)\n\n\ntst64.filter._obs_from_state(mean[0], cov[0])\n\nListNormal(mean=tensor([0.3859194846, 0.7281861816, 0.1397059375], dtype=torch.float64,\n       grad_fn=<MvBackward0>), cov=tensor([[0.4320693928, 0.5322830879, 0.2673021478],\n        [0.5322830879, 1.1152050124, 0.5026130264],\n        [0.2673021478, 0.5026130264, 1.1322290900]], dtype=torch.float64,\n       grad_fn=<AddBackward0>))\n\n\n\n\nLog Likelihood\nThis code is old now as the log likelihood is not computed here\nTODO: open issue in pykalman for error in ll missing data\n\npykalman.standard.KalmanFilter.loglikelihood??\n\n\nSignature: pykalman.standard.KalmanFilter.loglikelihood(self, X)\nSource:   \n    def loglikelihood(self, X):\n        \"\"\"Calculate the log likelihood of all observations\n        Parameters\n        ----------\n        X : [n_timesteps, n_dim_obs] array\n            observations for time steps [0...n_timesteps-1]\n        Returns\n        -------\n        likelihood : float\n            likelihood of all observations\n        \"\"\"\n        Z = np.array(self._parse_observations(X))\n        # initialize parameters\n        (transition_matrices, transition_offsets,\n         transition_covariance, observation_matrices,\n         observation_offsets, observation_covariance,\n         initial_state_mean, initial_state_covariance) = (\n            self._initialize_parameters()\n        )\n        # apply the Kalman Filter\n        (predicted_state_means, predicted_state_covariances,\n         kalman_gains, filtered_state_means,\n         filtered_state_covariances) = (\n            _filter(\n                transition_matrices, observation_matrices,\n                transition_covariance, observation_covariance,\n                transition_offsets, observation_offsets,\n                initial_state_mean, initial_state_covariance,\n                Z\n            )\n        )\n        # get likelihoods for each time step\n        loglikelihoods = _loglikelihoods(\n          observation_matrices, observation_offsets, observation_covariance,\n          predicted_state_means, predicted_state_covariances, Z\n        )\n        return np.sum(loglikelihoods)\nFile:      ~/anaconda3/envs/data-science/lib/python3.10/site-packages/pykalman/standard.py\nType:      function\n\n\n\n\n\npykalman.standard._loglikelihoods??\n\n\nSignature:\npykalman.standard._loglikelihoods(\n    observation_matrices,\n    observation_offsets,\n    observation_covariance,\n    predicted_state_means,\n    predicted_state_covariances,\n    observations,\n)\nSource:   \ndef _loglikelihoods(observation_matrices, observation_offsets,\n                    observation_covariance, predicted_state_means,\n                    predicted_state_covariances, observations):\n    \"\"\"Calculate log likelihood of all observations\n    Parameters\n    ----------\n    observation_matrices : [n_timesteps, n_dim_obs, n_dim_obs] or [n_dim_obs,\n    n_dim_state] array\n        observation matrices for t in [0...n_timesteps-1]\n    observation_offsets : [n_timesteps, n_dim_obs] or [n_dim_obs] array\n        offsets for observations for t = [0...n_timesteps-1]\n    observation_covariance : [n_dim_obs, n_dim_obs] array\n        covariance matrix for all observations\n    predicted_state_means : [n_timesteps, n_dim_state] array\n        mean of state at time t given observations from times\n        [0...t-1] for t in [0...n_timesteps-1]\n    predicted_state_covariances : [n_timesteps, n_dim_state, n_dim_state] array\n        covariance of state at time t given observations from times\n        [0...t-1] for t in [0...n_timesteps-1]\n    observations : [n_dim_obs] array\n        All observations.  If `observations[t]` is a masked array and any of\n        its values are masked, the observation will be ignored.\n    Returns\n    -------\n    loglikelihoods: [n_timesteps] array\n        `loglikelihoods[t]` is the probability density of the observation\n        generated at time step t\n    \"\"\"\n    n_timesteps = observations.shape[0]\n    loglikelihoods = np.zeros(n_timesteps)\n    for t in range(n_timesteps):\n        observation = observations[t]\n        if not np.any(np.ma.getmask(observation)):\n            observation_matrix = _last_dims(observation_matrices, t)\n            observation_offset = _last_dims(observation_offsets, t, ndims=1)\n            predicted_state_mean = _last_dims(\n                predicted_state_means, t, ndims=1\n            )\n            predicted_state_covariance = _last_dims(\n                predicted_state_covariances, t\n            )\n            predicted_observation_mean = (\n                np.dot(observation_matrix,\n                       predicted_state_mean)\n                + observation_offset\n            )\n            predicted_observation_covariance = (\n                np.dot(observation_matrix,\n                       np.dot(predicted_state_covariance,\n                              observation_matrix.T))\n                + observation_covariance\n            )\n            loglikelihoods[t] = log_multivariate_normal_density(\n                observation[np.newaxis, :],\n                predicted_observation_mean[np.newaxis, :],\n                predicted_observation_covariance[np.newaxis, :, :]\n            )\n    return loglikelihoods\nFile:      ~/anaconda3/envs/data-science/lib/python3.10/site-packages/pykalman/standard.py\nType:      function\n\n\n\n\n\nsource\n\n\nKalmanFilter.filter_loglikelihood\n\n KalmanFilter.filter_loglikelihood (obs, mask=None)\n\nCompute log likelihood using only filter step\n\nk.filter_loglikelihood(X)\n\ntensor(-5.2315979004, grad_fn=<SumBackward0>)\n\n\n\ntest_close(k.filter_loglikelihood(X), pyk.loglikelihood(nX))\n\n\npred_state, pred_state_cov, _, _ = tuple(map(_stack_detach, tst.filter._filter_all(tst.data, tst.mask)))\n\n\ntst = KalmanFilterTester(nan_mask = False, p_missing=0)\n\n\npykalman.standard._loglikelihoods(\n    tst.params_pyk['observation_matrices'],\n    tst.params_pyk['observation_offsets'],\n    tst.params_pyk['observation_covariance'],\n    pred_state.numpy(),\n    pred_state_cov.numpy(),\n    np.array(tst.data_pyk)\n)\n\narray([-5.64984703, -8.634758  , -3.505826  , -3.89075327, -4.01303673,\n       -3.75743556, -4.12671423, -4.10942173, -6.93878555, -6.93154287])\n\n\n\ntest_close(\n    tst.filter.filter_loglikelihood(tst.data, tst.mask),\n    tst.filter_pyk.loglikelihood(tst.data_pyk)) # need to increase the resolution\n\nAssertionError: close:\n-35.311344146728516\n-42.055377029134384\n\n\n\ntst64 = KalmanFilterTester(p_missing=0, dtype=torch.float64)\n\n\n(tst64.filter.filter_loglikelihood(tst64.data, tst64.mask),\ntst64.filter_pyk.loglikelihood(tst64.data_pyk))\n\n(tensor(-30.2942523956, grad_fn=<SumBackward0>), -36.23070604799459)\n\n\n\nk2.filter_loglikelihood(X2)\n\ntensor(-181.6866760254, grad_fn=<SumBackward0>)\n\n\n\ntest_close(k2.filter_loglikelihood(X2), pyk2.loglikelihood(nX2), eps=1e-4)\n\nsince the goal is to fill gaps we want the log likelihood for the whole gap and only for it\n\nsource\n\n\nKalmanFilter.loglikelihood\n\n KalmanFilter.loglikelihood (obs_train:torch.Tensor, times:torch.Tensor,\n                             obs_test:torch.Tensor,\n                             mask:torch.Tensor=None)\n\nLog likelihood only for the obs_test at giben times\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobs_train\nTensor\n\n[n_timesteps, n_dim_obs] Observations use for the filter (can containt missing data)\n\n\ntimes\nTensor\n\n[n_pred_timesteps] time at which to calculate the log likelihood\n\n\nobs_test\nTensor\n\n[n_pred_timesteps, n_dim_obs] observed data to compute log likelihood\n\n\nmask\nTensor\nNone\n[n_timesteps, n_dim_obs]\n\n\nReturns\nTensor\n\nscalar that is sum of log likelihoods for all times\n\n\n\n\ntorch.diag(std[0]).dtype\n\ntorch.float32\n\n\n\ntst.filter.loglikelihood(tst.data, tst.mask, tst.data)\n\ntensor(-47.5645027161, grad_fn=<SumBackward0>)\n\n\n\nX2.dtype\n\ntorch.float32\n\n\n\nk.loglikelihood(X, [1,2], X[[1,2]])\n\ntensor(-3.1273550987, grad_fn=<SumBackward0>)\n\n\n\nk2.loglikelihood(X2, [1,2], X2[[1,2]])\n\ntensor(-84.5747680664, grad_fn=<SumBackward0>)\n\n\n\nk2.loglikelihood(X2m, [1,2], X2[[1,2]])\n\ntensor(-86.9698638916, grad_fn=<SumBackward0>)\n\n\n\n\nGet Info\n\nsource\n\n\nKalmanFilter.get_info\n\n KalmanFilter.get_info (var_names=None)\n\n\ndisplay_as_row(k.get_info())\n\n\n  A \n\n  \n    \n      latent\n      z_0\n    \n  \n  \n    \n      z_0\n      1.0000\n    \n  \n\n  H \n\n  \n    \n      z_0\n    \n  \n  \n    \n      1.0000\n    \n  \n\n  R \n\n  \n    \n      0\n    \n  \n  \n    \n      1.0000\n    \n  \n\n  Q \n\n  \n    \n      latent\n      z_0\n    \n  \n  \n    \n      z_0\n      1.0000\n    \n  \n\n \n\n\n\ndisplay_as_row(k2.get_info())\n\n\n  A \n\n  \n    \n      latent\n      z_0\n      z_1\n    \n  \n  \n    \n      z_0\n      1.0000\n      0.0000\n    \n    \n      z_1\n      0.0000\n      1.0000\n    \n  \n\n  H \n\n  \n    \n      z_0\n      z_1\n    \n  \n  \n    \n      1.0000\n      0.0000\n    \n    \n      0.0000\n      1.0000\n    \n  \n\n  R \n\n  \n    \n      0\n      1\n    \n  \n  \n    \n      1.0000\n      0.0000\n    \n    \n      0.0000\n      1.0000\n    \n  \n\n  Q \n\n  \n    \n      latent\n      z_0\n      z_1\n    \n  \n  \n    \n      z_0\n      1.0000\n      0.0000\n    \n    \n      z_1\n      0.0000\n      1.0000"
  },
  {
    "objectID": "kalman/kalman_filter.html#train-parameters",
    "href": "kalman/kalman_filter.html#train-parameters",
    "title": "Kalman Filter",
    "section": "Train Parameters",
    "text": "Train Parameters\nThis implementation of KalmanFilter allows to find the optimal parameters by maximising the log-likelihood using gradient descend\n\ntraining_iter = 200\nk = KalmanFilter()\nk.train()\n\noptimizer = torch.optim.Adam(k.parameters(), lr=0.005) \n\nlosses = []\nfor i in range(training_iter):\n    # Zero gradients from previous iteration\n    optimizer.zero_grad()\n    # Output from model\n    loss = - k.loglikelihood(X, range(len(X)), X)\n    losses.append(loss.item())\n    # backpropagate gradients\n    loss.backward()\n    optimizer.step()\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot(losses)\n\n\n\n\n\nlist(k.parameters())\n\n[Parameter containing:\n tensor([[1.3400094509]], requires_grad=True),\n Parameter containing:\n tensor([0.4287530482], requires_grad=True),\n Parameter containing:\n tensor([[0.1470584720]], requires_grad=True),\n Parameter containing:\n tensor([[0.1455851793]], requires_grad=True),\n Parameter containing:\n tensor([-0.4134024680], requires_grad=True),\n Parameter containing:\n tensor([[0.0268113427]], requires_grad=True),\n Parameter containing:\n tensor([0.4979022741], requires_grad=True),\n Parameter containing:\n tensor([[0.1377424598]], requires_grad=True)]"
  },
  {
    "objectID": "kalman/kalman_filter.html#other",
    "href": "kalman/kalman_filter.html#other",
    "title": "Kalman Filter",
    "section": "Other",
    "text": "Other\n\nTesting\n\n\nFuzzing smoother\ntrying to run the filter many times to see if some of the matrix are not symmetric\n\nfrom torch._C import _LinAlgError\n\n\ndef fuzz_symmetric(n_iter=10, n_obs=100, **kwargs):\n    tst = KalmanFilterTester(n_obs=n_obs, **kwargs)\n    _, sm_covs = tst.filter.smooth(tst.data, tst.mask)\n    i_posdef = []\n    for t, cov in enumerate(sm_covs):\n        i_posdef.append(check_posdef(cov))\n    return pd.concat(i_posdef)\n\n\ndef fuzz_smooth(n_obs=100, **kwargs):\n    tst = KalmanFilterTester(n_obs=n_obs, **kwargs)\n    _, sm_covs = tst.filter.smooth(tst.data, tst.mask, check_args= {})\n\n\ndef find_max_obs(start=100_000, end=100, steps=10, **kwargs):\n    for n in torch.logspace(torch.log10(start), torch.log10(end), 10):\n        try:\n\n            print(n, \"working\")\n            break\n        except _LinAlgError:\n            print(n, \"not working\")\n\n\nimport warnings\n\n\nposdef_log = pd.DataFrame()\n\n\ntotal_warn = []\nfor n in range(2, 200):\n    with warnings.catch_warnings(record=True) as w:\n        try:\n            fuzz_smooth(n_obs=n)\n        except _LinAlgError:\n            print(n)\n            break\n        finally:\n            total_warn.append((n, len(w)))\n\n27\n\n\n\nposdef_log\n\n\n\n\n\n  \n    \n      \n      is_pd_eigv\n      is_pd_chol\n      is_sym\n      sym_upto\n      eigv\n      matrix\n      name\n      t\n    \n  \n  \n    \n      0\n      True\n      True\n      True\n      -8\n      [0.0044204406, 0.51171917, 1.1058657]\n      [[0.07919369, 0.21216275, 0.08634329], [0.2121...\n      filter_correct\n      0\n    \n    \n      0\n      True\n      True\n      True\n      -8\n      [0.14680785, 0.6775383, 5.3274283]\n      [[2.041435, 1.3869133, 1.601786], [1.3869132, ...\n      filter_predict\n      1\n    \n    \n      0\n      True\n      True\n      True\n      -8\n      [0.14680785, 0.6775383, 5.3274283]\n      [[2.041435, 1.3869133, 1.601786], [1.3869132, ...\n      filter_correct\n      1\n    \n    \n      0\n      True\n      True\n      True\n      -8\n      [[0.0044204514, 0.5117193, 1.1058657], [0.0959...\n      [[[0.07919369, 0.21216275, 0.08634329], [0.212...\n      filter_correct\n      0\n    \n    \n      0\n      True\n      True\n      True\n      -8\n      [0.0044204406, 0.51171917, 1.1058657]\n      [[0.07919369, 0.21216275, 0.08634329], [0.2121...\n      filter_correct\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      0\n      True\n      True\n      True\n      -8\n      [0.3388391, 0.69478154, 18449.22]\n      [[7131.8594, 6422.1284, 6281.904], [6422.1284,...\n      filter_correct\n      9\n    \n    \n      0\n      True\n      True\n      True\n      -8\n      [0.33459213, 0.6852231, 77041.47]\n      [[29782.723, 26819.559, 26233.049], [26819.559...\n      filter_predict\n      10\n    \n    \n      0\n      True\n      True\n      True\n      -8\n      [0.33459213, 0.6852231, 77041.47]\n      [[29782.723, 26819.559, 26233.049], [26819.559...\n      filter_correct\n      10\n    \n    \n      0\n      False\n      False\n      False\n      2\n      [-797.5366, -56.76306, 29.798246]\n      [[-327.70764, -295.44025, -289.2465], [-348.85...\n      filter_predict\n      11\n    \n    \n      0\n      False\n      False\n      False\n      2\n      [-797.5366, -56.76306, 29.798246]\n      [[-327.70764, -295.44025, -289.2465], [-348.85...\n      filter_correct\n      11\n    \n  \n\n198 rows × 8 columns\n\n\n\n\nimport altair as alt\n\n\nalt.Chart(pd.DataFrame(total_warn, columns=[\"n_obs\", \"n_not_posdef\"])).mark_line().encode(alt.X(\"n_obs\"), alt.Y(\"n_not_posdef\"))\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nalt.data_transformers.enable('data_server')\n\nDataTransformerRegistry.enable('data_server')\n\n\n\nalt.Chart(posdef_log).mark_line().encode(alt.X(\"t\"), alt.Y(\"average(sym_upto)\"))\n\n\n\n\n\n\n\nalt.Chart(posdef_log).mark_point().encode(alt.X(\"t\"), alt.Y(\"count(is_sym)\"))\n\n\n\n\n\n\n\nposdef_log[[\"t\", \"name\"]]\n\n\n\n\n\n  \n    \n      \n      t\n      name\n    \n  \n  \n    \n      0\n      0\n      filter_correct\n    \n    \n      0\n      1\n      filter_predict\n    \n    \n      0\n      1\n      filter_correct\n    \n    \n      0\n      0\n      filter_correct\n    \n    \n      0\n      0\n      filter_correct\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      0\n      9\n      filter_correct\n    \n    \n      0\n      10\n      filter_predict\n    \n    \n      0\n      10\n      filter_correct\n    \n    \n      0\n      11\n      filter_predict\n    \n    \n      0\n      11\n      filter_correct\n    \n  \n\n198 rows × 2 columns\n\n\n\n\nplt.scatter(posdef_log.reset_index().index, posdef_log.sym_upto)\n\n<matplotlib.collections.PathCollection>\n\n\n\n\n\n\nfor i in range(3):\n\nCPU times: user 2 µs, sys: 0 ns, total: 2 µs\nWall time: 3.58 µs\nCPU times: user 2 µs, sys: 0 ns, total: 2 µs\nWall time: 2.86 µs\nCPU times: user 2 µs, sys: 0 ns, total: 2 µs\nWall time: 2.86 µs\n\n\n\n# find_max_obs(dtype=torch.float64)\n\nThe function takes 5 min to run so this is the output saved\nwith float64 there is no problem with positive definite matrices even with 100k observations CPU times: user 30min 43s, sys: 28.6 s, total: 31min 11s Wall time: 5min 26s tensor(100000.) working\n\ntst = KalmanFilterTester(n_obs=100)\n\n\nis_posdef(tst.params['obs_cov'])\n\nTrue\n\n\n\nis_posdef(tst.params['transition_cov'])\n\nTrue\n\n\n\nis_posdef(tst.params['initial_state_cov'])\n\nTrue\n\n\n\ntst.filter.smooth(tst.data, tst.mask);\n\n_LinAlgError: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite).\n\n\n\nRandom Testing\nThe goal is to generate random set of data and parameters and check that meteo_imp implementation is the same of pykalman implementation\n\nn_dim_state = 3 \nn_dim_obs = 3\nn_obs = 10\np_missing = .3\n\n\nto_posdef = PosDef().transform\n\n\ndata = torch.rand(n_obs, n_dim_obs)\nmask = torch.rand(n_obs, n_dim_obs) > p_missing\nmask = mask.all(1)\n\n\nmask[:10]\n\n\nmask[:10]\n\n\nparams = {\n    'transition_matrices': torch.rand(n_dim_state, n_dim_state),\n    'transition_offsets':  torch.rand(n_dim_state),        \n    'transition_cov':      to_posdef(torch.rand(n_dim_state, n_dim_state)),        \n    'obs_matrices':        torch.rand(n_dim_obs, n_dim_state),\n    'obs_offsets':         torch.rand(n_dim_obs),          \n    'obs_cov':             to_posdef(torch.rand(n_dim_obs, n_dim_obs)),            \n    'initial_state_mean':  torch.rand(n_dim_state),        \n    'initial_state_cov':   to_posdef(torch.rand(n_dim_state, n_dim_state)),\n}\n\n\nparams2pyk = {\n    'transition_matrices': 'transition_matrices',\n    'transition_offsets':  'transition_offsets',        \n    'transition_cov':      'transition_covariance',        \n    'obs_matrices':        'observation_matrices',\n    'obs_offsets':         'observation_offsets',          \n    'obs_cov':             'observation_covariance',            \n    'initial_state_mean':  'initial_state_mean',        \n    'initial_state_cov':   'initial_state_covariance',\n}\n\n\nparams\n\n\nk = KalmanFilter(**params)\n\n\npred = k.smooth(data, mask)\n\nmake a pykalman model using the same parameters\n\ndata_pyk = np.ma.masked_array(data.numpy(), mask = mask.numpy())\n\n\npyk_k = pykalman.standard.KalmanFilter(\n\n    transition_matrices=k.transition_matrices.detach().numpy(),\n    transition_offsets=k.transition_offsets.detach().numpy(),\n    transition_covariance=k.transition_cov.detach().numpy(),\n    observation_matrices=k.obs_matrices.detach().numpy(),\n    observation_offsets=k.obs_offsets.detach().numpy(),\n    observation_covariance=k.obs_cov.detach().numpy(),\n    initial_state_mean=k.initial_state_mean.detach().numpy(),\n    initial_state_covariance=k.initial_state_cov.detach().numpy()\n)\n\n\npred_pyk = pyk_k.smooth(data_pyk)\n\n\nfor p in params.keys():\n    print(p, getattr(k, p))\n\n\nfor p in params.keys():\n    print(p, getattr(pyk_k, params2pyk[p]))\n\n\n\n\nCompare Statsmodels\n\nimport statsmodels.api as sm\nimport statsmodels\n\n\n# sm_kf = statsmodels.tsa.statespace.kalman_filter.KalmanFilter(\n#     k_endog = 3,\n#     k_states = 3,\n#     initialization = 'known',\n#     initial_state = pyk_ncov.initial_state_mean,\n#     initial_state_cov = pyk_ncov.initial_state_covariance,\n#     design = pyk_ncov.observation_matrices,\n#     obs_cov = pyk_ncov.observation_covariance,\n#     transition = pyk_ncov.transition_matrices,\n#     state_cov = pyk_ncov.transition_covariance)\n\n\n# sm_kf.bind(X_ncov.detach().numpy())\n\n\n# sm_pred = sm_kf.filter()\n\n\n# sm_pred.predicted_state.shape\n\n\n# sm_pred.predicted_state_cov.shape\n\n\n# mean = MultivariateNormal(torch.tensor(sm_pred.predicted_state[:, 0]), torch.tensor(sm_pred.predicted_state_cov[:, :, 0]))\n\n\n# sm_kf.loglikeobs()"
  },
  {
    "objectID": "kalman/kalman_filter.html#export",
    "href": "kalman/kalman_filter.html#export",
    "title": "Kalman Filter",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/old statsmodels - statespace.html",
    "href": "kalman/old statsmodels - statespace.html",
    "title": "[Not Working] StatsModels Kalman filter",
    "section": "",
    "text": "import pandas as pd\nfrom fastcore.basics import store_attr\nimport statsmodels.api as sm\nimport numpy as np\nfrom meteo_imp.results import ImputationResult\nto work properly with the statsmodel the dataframe can have missing data, but the data should be have the index properly set\nFor testing purposes use models from statsmodel"
  },
  {
    "objectID": "kalman/old statsmodels - statespace.html#introduction",
    "href": "kalman/old statsmodels - statespace.html#introduction",
    "title": "[Not Working] StatsModels Kalman filter",
    "section": "Introduction",
    "text": "Introduction\nThe model uses a Kalman filter with the simplest possible equations\nThe notation used is from @durbin_time_2012 since the one used in the statsmodel library that is used for this implementation.\nThe state of the model changes only due to the process noise (the design matrix is one) The transition matrix is also one\nThe general notation is:\n\\[\\begin{split}y_t & = Z_t \\alpha_t + d_t + \\varepsilon_t \\\\\n\\alpha_{t+1} & = T_t \\alpha_t + c_t + R_t \\eta_t \\\\\\end{split}\\]\nwhere \\[\\begin{split}\\varepsilon_t \\sim N(0, H_t) \\\\\n\\eta_t \\sim N(0, Q_t) \\\\\\end{split}\\]\n\nZ : design \\((k\\_endog \\times k\\_states \\times nobs)\\)\nd : obs_intercept \\(k\\_endog \\times nobs)\\)\nH : obs_cov \\(k\\_endog \\times k\\_endog \\times nobs)\\)\nT : transition \\(k\\_states \\times k\\_states \\times nobs)\\)\nc : state_intercept \\(k\\_states \\times nobs)\\)\nR : selection \\(k\\_states \\times k\\_posdef \\times nobs)\\)\nQ : state_cov \\(k\\_posdef \\times k\\_posdef \\times nobs)\\)\n\n\nLocal Level\nThe random walker makes the following assumptions:\n\nthe design matrix is one, which means that the change over time of the state is only due to the random process noise\nthe transition matrix is one, which means that the observations are equal to the state plus a random measurement noise\nthe process and measurement noise don’t change over time\n\nHence the equations of the model are:\n\\[\\begin{split}\ny_t & = \\alpha_t +  \\varepsilon \\\\\n\\alpha_{t+1} & = \\alpha_t + \\eta\n\\end{split}\\]\nThe parameters of the models are \\(\\varepsilon\\) and \\(\\eta\\) which are estimated by maximising the log likelihood\nthis first implementation considers only a univariate scenario"
  },
  {
    "objectID": "kalman/old statsmodels - statespace.html#implementation",
    "href": "kalman/old statsmodels - statespace.html#implementation",
    "title": "[Not Working] StatsModels Kalman filter",
    "section": "Implementation",
    "text": "Implementation\n\nimport numpy as np\nimport statsmodels.api as sm\nfrom fastcore.basics import patch\nimport pandas as pd\n\n\n\"\"\"\nUnivariate Random Walker or local level model\n\"\"\"\nclass UnivarLocalLevel(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog, **kwargs):\n        # Model order\n        k_states = k_posdef = 1\n\n        # Initialize the statespace\n        super(UnivarLocalLevel, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states, **kwargs\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1])\n        self.ssm['transition'] = np.array([1])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['epsilon', 'eta']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*2\n    \n    # This is to guarantee that the parameters are positive\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(UnivarLocalLevel, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm['state_cov', 0, 0] = params[1]\n\n\nY = np.arange(0,10)\n\n\nrw = UnivarLocalLevel(Y)\n\n\nres = rw.fit()\n\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  1.87798D+00    |proj g|=  2.84223D-01\n\nAt iterate    5    f=  1.27704D+00    |proj g|=  2.20201D-04\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      7     13      1     0     0   1.860D-06   1.277D+00\n  F =   1.2770446799271464     \n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n\n\n This problem is unconstrained.\n\n\n\nres.summary()\n\n\n\nStatespace Model Results\n\n  Dep. Variable:            y           No. Observations:     10   \n\n\n  Model:           UnivarRandomWalker   Log Likelihood      -12.770\n\n\n  Date:             Sat, 12 Nov 2022    AIC                 29.541 \n\n\n  Time:                 19:06:40        BIC                 29.935 \n\n\n  Sample:                   0           HQIC                28.690 \n\n\n                           - 10                                    \n\n\n  Covariance Type:         opg                                     \n\n\n\n\n             coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  epsilon  1.454e-11     1.061  1.37e-11  1.000    -2.079     2.079\n\n\n  eta         1.0000  1.66e+05  6.03e-06  1.000 -3.25e+05  3.25e+05\n\n\n\n\n  Ljung-Box (L1) (Q):     0.38   Jarque-Bera (JB):   2.70 \n\n\n  Prob(Q):                0.54   Prob(JB):           0.26 \n\n\n  Heteroskedasticity (H): 1.00   Skew:               -1.34\n\n\n  Prob(H) (two-sided):    1.00   Kurtosis:           2.79 \n\nWarnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\nres.params\n\narray([1.45422542e-11, 9.99987933e-01])\n\n\n\npredict = res.get_prediction()\n\n\npredict.se_mean\n\narray([1.00000000e+03, 9.99993967e-01, 9.99993967e-01, 9.99993967e-01,\n       9.99993967e-01, 9.99993967e-01, 9.99993967e-01, 9.99993967e-01,\n       9.99993967e-01, 9.99993967e-01])\n\n\n\nres.get_prediction(1, 10)\n\n<statsmodels.tsa.statespace.mlemodel.PredictionResultsWrapper>"
  },
  {
    "objectID": "kalman/old_02_pykalman_imputation.html",
    "href": "kalman/old_02_pykalman_imputation.html",
    "title": "[OLD] Imputation PyKalman Model",
    "section": "",
    "text": "@patch\ndef inverse_transform_std(self: sklearn.preprocessing.StandardScaler, \n                         x_std # standard deviations\n                        ):\n    return x_std * self.scale_"
  },
  {
    "objectID": "kalman/old_02_pykalman_imputation.html#imputation",
    "href": "kalman/old_02_pykalman_imputation.html#imputation",
    "title": "[OLD] Imputation PyKalman Model",
    "section": "Imputation",
    "text": "Imputation\n\nx = np.stack([np.eye(3)*i for i in  range(1,4)])\n\n\nx\n\narray([[[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]],\n\n       [[2., 0., 0.],\n        [0., 2., 0.],\n        [0., 0., 2.]],\n\n       [[3., 0., 0.],\n        [0., 3., 0.],\n        [0., 0., 3.]]])\n\n\n\nnp.diagonal(x, axis1=1, axis2=2)\n\narray([[1., 1., 1.],\n       [2., 2., 2.],\n       [3., 3., 3.]])\n\n\n\nsource\n\nKalmanImputation\n\n KalmanImputation (data:pandas.core.frame.DataFrame,\n                   model:meteo_imp.old.kalman.model.KalmanModel,\n                   pred_all:bool=False)\n\nImputation using a kalman model\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\n\n\n\nmodel\nKalmanModel\n\na subclass of MLEModel tto be used as model\n\n\npred_all\nbool\nFalse\nIf the dataset should be replaced by the model predictions\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\n\n\nreset_seed(1)\ndata = MeteoDataTest.generate_gpfa(2, 5).add_random_missing()\n\n\ndata.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      0.023263\n      NaN\n    \n    \n      1\n      0.219627\n      0.268028\n    \n    \n      2\n      -0.039892\n      0.063075\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      -0.645490\n      -0.144866\n    \n  \n\n\n\n\n\nk_imp = KalmanImputation(data.data, LocalLevelModel)\n\n\nk_imp.fit()\n\n<__main__.KalmanImputation>\n\n\n\nk_imp.impute()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0\n      x0\n      -0.117147\n      0.394932\n    \n    \n      1\n      1\n      x0\n      0.219627\n      0.000000\n    \n    \n      2\n      2\n      x0\n      -0.039892\n      0.000000\n    \n    \n      3\n      3\n      x0\n      -0.170874\n      0.395551\n    \n    \n      4\n      4\n      x0\n      -0.645490\n      0.000000\n    \n    \n      5\n      0\n      x1\n      0.062976\n      0.206893\n    \n    \n      6\n      1\n      x1\n      0.268028\n      0.000000\n    \n    \n      7\n      2\n      x1\n      0.063075\n      0.000000\n    \n    \n      8\n      3\n      x1\n      0.062976\n      0.206893\n    \n    \n      9\n      4\n      x1\n      -0.144866\n      0.000000\n    \n  \n\n\n\n\n\nk_imp.impute(pred_all=True)\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0\n      x0\n      -0.117147\n      0.394932\n    \n    \n      1\n      1\n      x0\n      -0.118482\n      0.383000\n    \n    \n      2\n      2\n      x0\n      -0.100529\n      0.384614\n    \n    \n      3\n      3\n      x0\n      -0.170874\n      0.395551\n    \n    \n      4\n      4\n      x0\n      -0.241218\n      0.389215\n    \n    \n      5\n      0\n      x1\n      0.062976\n      0.206893\n    \n    \n      6\n      1\n      x1\n      0.062976\n      0.206893\n    \n    \n      7\n      2\n      x1\n      0.062976\n      0.206893\n    \n    \n      8\n      3\n      x1\n      0.062976\n      0.206893\n    \n    \n      9\n      4\n      x1\n      0.062976\n      0.206893\n    \n  \n\n\n\n\n\nResult\n\nsource\n\n\n\nKalmanImputation.to_result\n\n KalmanImputation.to_result (data_compl, var_names=None, units=None,\n                             pred_all=False)\n\n\nX = np.hstack([np.arange(0,3.), np.arange(3., 0, -1)]).reshape(6, 1)\n\n\nres = k_imp.to_result(data.data_compl_tidy)\n\n\nres.display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/sklearn/metrics/_regression.py:796: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      x0\n      0.6300\n    \n    \n      x1\n      0.6336\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n    \n  \n  \n    \n      x0\n      0.1959\n    \n    \n      x1\n      0.0975\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      x0\n      nan\n    \n    \n      x1\n      -0.1820\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n    \n  \n  \n    \n      x0\n      0.4150\n    \n    \n      x1\n      0.1541\n    \n  \n\n \n\n\n\nModel Info  A \n\n  \n    \n      latent\n      z_0\n      z_1\n    \n  \n  \n    \n      z_0\n      1.0000\n      0.0000\n    \n    \n      z_1\n      0.0000\n      1.0000\n    \n  \n\n  H \n\n  \n    \n      z_0\n      z_1\n    \n  \n  \n    \n      1.0000\n      0.0000\n    \n    \n      0.0000\n      1.0000\n    \n  \n\n  R \n\n  \n    \n      0\n      1\n    \n  \n  \n    \n      1.1428\n      1.1225\n    \n    \n      1.1225\n      1.2565\n    \n  \n\n  Q \n\n  \n    \n      latent\n      z_0\n      z_1\n    \n  \n  \n    \n      z_0\n      0.2046\n      -0.0000\n    \n    \n      z_1\n      -0.0000\n      -0.0000"
  },
  {
    "objectID": "kalman/old_02_pykalman_imputation.html#export",
    "href": "kalman/old_02_pykalman_imputation.html#export",
    "title": "[OLD] Imputation PyKalman Model",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html",
    "href": "kalman/old_01_pykalman_models.html",
    "title": "[OLD] PyKalman Filter Models",
    "section": "",
    "text": "[TODO] add proper introduction here\nThe models uses a latent state variable \\(x\\) that is modelled over time, to impute gaps in \\(y\\)\n\n\nThe equations of the model are:\n\\[\\begin{align} p(x_t | x_{t-1}) & = \\mathcal{N}(x_t, Ax_{t-1}, Q) \\\\\np(y_t | x_t) & = \\mathcal{N}(Hx_t, T) \\end{align}\\]\nThe Kalman filter has 3 steps:\n\nfilter (updating the state at time t with observations till time t-1)\nupdate (update the state at time t using the observation at time t)\nsmooth (update the state using the observations at time t+1)\n\nIn case of missing data the update step is skipped.\nAfter smoothing the whole dataset the missing data (\\(y_t\\)) can be imputed from the state (\\(x_t\\)) using this formula: \\[p(y_t) = \\mathcal{N}(Hx_x, R + HP^s_tH)\\]"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html#kalman-filter-model",
    "href": "kalman/old_01_pykalman_models.html#kalman-filter-model",
    "title": "[OLD] PyKalman Filter Models",
    "section": "Kalman Filter Model",
    "text": "Kalman Filter Model\ngeneral kalman filter model implemented used pykalman\n\nPyKalman\nexamples from pykalman lib\n\nkf = pykalman.KalmanFilter(transition_matrices = [[1, 1], [0, 1]], observation_matrices = [[0.1, 0.5], [-0.3, 0.0]])\nmeasurements = np.asarray([[1,0], [0,0], [0,1]])\n(smoothed_state_means, smoothed_state_covariances) = kf.smooth(measurements)\n\n\nkf.em(measurements)\n\n<pykalman.standard.KalmanFilter>\n\n\n\nsmoothed_state_means\n\narray([[-0.10923868,  0.0935127 ],\n       [-0.23121289, -0.07957144],\n       [-0.5533711 , -0.0415223 ]])\n\n\n\nsmoothed_state_covariances\n\narray([[[ 0.83148067, -0.12300405],\n        [-0.12300405,  0.53081415]],\n\n       [[ 1.60960449,  0.01009906],\n        [ 0.01009906,  0.80412661]],\n\n       [[ 2.87663094,  0.45474213],\n        [ 0.45474213,  1.27365905]]])\n\n\n\nsmoothed_state_covariances.shape\n\n(3, 2, 2)\n\n\nsupport is not limited for models matrices that don’t change over time\n\nsource\n\n\nKalmanModel\n\n KalmanModel (data:numpy.ma.core.MaskedArray, transition_matrices=None,\n              observation_matrices=None, transition_covariance=None,\n              observation_covariance=None, transition_offsets=None,\n              observation_offsets=None, initial_state_mean=None,\n              initial_state_covariance=None, random_state=None,\n              em_vars=['transition_covariance', 'observation_covariance',\n              'initial_state_mean', 'initial_state_covariance'],\n              n_dim_state=None, n_dim_obs=None)\n\nBase Model for Kalman filter that wraps pykalman.KalmanFilter. Doesn’t support parameters that change over time\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nMaskedArray\n\nnumpy array of observations\n\n\ntransition_matrices\nNoneType\nNone\n\n\n\nobservation_matrices\nNoneType\nNone\n\n\n\ntransition_covariance\nNoneType\nNone\n\n\n\nobservation_covariance\nNoneType\nNone\n\n\n\ntransition_offsets\nNoneType\nNone\n\n\n\nobservation_offsets\nNoneType\nNone\n\n\n\ninitial_state_mean\nNoneType\nNone\n\n\n\ninitial_state_covariance\nNoneType\nNone\n\n\n\nrandom_state\nNoneType\nNone\n\n\n\nem_vars\nlist\n[‘transition_covariance’, ‘observation_covariance’, ‘initial_state_mean’, ‘initial_state_covariance’]\n\n\n\nn_dim_state\nNoneType\nNone\n\n\n\nn_dim_obs\nNoneType\nNone\n\n\n\n\n\nX = np.hstack([np.arange(0,3.), np.arange(3., 0, -1)]).reshape(6, 1)\n\n\nX\n\narray([[0.],\n       [1.],\n       [2.],\n       [3.],\n       [2.],\n       [1.]])\n\n\n\nX.shape\n\n(6, 1)\n\n\n\nk = KalmanModel(X)\n\n\nk.fit(10)\n\n<__main__.KalmanModel>\n\n\n\nT = np.arange(0,X.shape[0])\n\n\nT\n\narray([0, 1, 2, 3, 4, 5])\n\n\n\nk.predict(T)\n\nListNormal(mean=array([[0.21296793],\n       [1.01950281],\n       [1.98157689],\n       [2.79672264],\n       [1.99068721],\n       [1.1103803 ]]), cov=array([[[0.12474143]],\n\n       [[0.19659773]],\n\n       [[0.19733477]],\n\n       [[0.19734326]],\n\n       [[0.19743451]],\n\n       [[0.20632342]]]))\n\n\n\nGet Info\n\nsource\n\n\n\nKalmanModel.get_info\n\n KalmanModel.get_info (var_names=None)\n\n\ndisplay_as_row(k.get_info())\n\n\n  A \n\n  \n    \n      latent\n      z_0\n    \n  \n  \n    \n      z_0\n      1.0000\n    \n  \n\n  H \n\n  \n    \n      z_0\n    \n  \n  \n    \n      1.0000\n    \n  \n\n  R \n\n  \n    \n      0\n    \n  \n  \n    \n      0.1087\n    \n  \n\n  Q \n\n  \n    \n      latent\n      z_0\n    \n  \n  \n    \n      z_0\n      0.8666\n    \n  \n\n \n\n\nshit numpy arrays…\n\n_shift_1d(np.stack([np.vstack([np.arange(1, 4.)*i]*2) for i in range(4)]),2)\n\narray([[[nan, nan, nan],\n        [nan, nan, nan]],\n\n       [[nan, nan, nan],\n        [nan, nan, nan]],\n\n       [[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]],\n\n       [[ 1.,  2.,  3.],\n        [ 1.,  2.,  3.]]])\n\n\n\n_shift_1d(np.vstack([np.arange(1, 4.)*i for i in range(4)]),-1)\n\narray([[ 1.,  2.,  3.],\n       [ 2.,  4.,  6.],\n       [ 3.,  6.,  9.],\n       [nan, nan, nan]])"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html#local-level-model",
    "href": "kalman/old_01_pykalman_models.html#local-level-model",
    "title": "[OLD] PyKalman Filter Models",
    "section": "Local Level Model",
    "text": "Local Level Model\nLocal level models is a model that uses Kalman filter, where the design matrix (A) and the Transition matrix (H) are identity matrix. This means that the state of model is equal to the observations and the changes in the state are only from the process noise.\n\nsource\n\nLocalLevelModel\n\n LocalLevelModel (data)\n\nLocal level model using a kalman filter\n\nll = LocalLevelModel(X)\n\n\nll.fit()\n\n<__main__.LocalLevelModel>\n\n\n\nll.predict(np.array([5, 3]))\n\nListNormal(mean=array([[1.22847659],\n       [2.59752825]]), cov=array([[[0.45973462]],\n\n       [[0.42588627]]]))\n\n\n\nll2 = LocalLevelModel(np.hstack([X, X*2]))\n\n\nll2.fit()\n\n<__main__.LocalLevelModel>\n\n\n\nll2.predict(np.array([5, 3]))\n\nListNormal(mean=array([[1.10135575, 2.18600305],\n       [2.81259028, 5.65372346]]), cov=array([[[0.25086027, 0.35069288],\n        [0.35069288, 0.77948375]],\n\n       [[0.23700609, 0.33877014],\n        [0.33877014, 0.74905336]]]))\n\n\n\nll2.get_info()\n\n{'A':   latent  z_0  z_1\n 0    z_0  1.0  0.0\n 1    z_1  0.0  1.0,\n 'H':    z_0  z_1\n 0  1.0  0.0\n 1  0.0  1.0,\n 'R':           0         1\n 0  0.136841  0.181315\n 1  0.181315  0.408279,\n 'Q':   latent       z_0       z_1\n 0    z_0  0.960236  1.852132\n 1    z_1  1.852132  3.847645}\n\n\nWarning this implementation of the EM algorithm may actually result in matrices that aren’t correct for multivariate variables"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html#local-slope-model",
    "href": "kalman/old_01_pykalman_models.html#local-slope-model",
    "title": "[OLD] PyKalman Filter Models",
    "section": "Local slope Model",
    "text": "Local slope Model\nLocal slope models are an extentions of local level model that in the state variable keep track of also the slope\nThe transition matrix (A) is:\n\\[A = \\left[\\begin{array}{cc}1 & 1 \\\\ 0 & 1\\end{array}\\right]\\]\nthe state \\(x \\in \\mathbb{R}^(2N) \\times 1\\) where the upper half keep track of the level and the lower half of the slope. \\(A \\in \\mathbb{R}^2N \\times 2N\\)\nhence the observation matrix (H) is:\n\\[A = \\left[\\begin{array}{cc}1 & 0 end{array}\\right]\\]\nFor the multivariate case the 1 are replaced with an identiy matrix\n\nn_features = 2\nZero = np.zeros((n_features, n_features), dtype=np.float32)\nI = np.eye(n_features)\n\nA = np.vstack([np.hstack([I   , I]),\n               np.hstack([Zero, I])])\nH = np.hstack([I, Zero])\n\n\nA.shape\n\n(4, 4)\n\n\n\nI.shape\n\n(2, 2)\n\n\n\nZero.shape\n\n(2, 2)\n\n\n\nH.shape\n\n(2, 4)\n\n\n\nA, H = _init_local_slope(2)\n\n\nsource\n\nLocalSlopeModel\n\n LocalSlopeModel (data)\n\nLocal slope model using a kalman filter\n\nls = LocalSlopeModel(X)\n\n\nls.model\n\n<pykalman.standard.KalmanFilter>\n\n\n\nls.fit()\n\n<__main__.LocalSlopeModel>\n\n\n\nls.predict(np.array([3, 2]))\n\nListNormal(mean=array([[2.716141  ],\n       [2.01802129]]), cov=array([[[0.32816569]],\n\n       [[0.32739003]]]))\n\n\n\nX = np.array([[1, 2],\n              [2, 3],\n              [4, 5],\n              [6, 7]])\n\n\nX.shape\n\n(4, 2)\n\n\n\n(A @ X).shape\n\n(4, 2)\n\n\n\nH.shape\n\n(2, 4)\n\n\n\nH @ X\n\narray([[1., 2.],\n       [2., 3.]])\n\n\n\nls2 = LocalSlopeModel(np.hstack([X, X*2]))\n\n\nls2.fit()\n\n<__main__.LocalSlopeModel>\n\n\n\nls2.predict(np.array([1, 2]))\n\nListNormal(mean=array([[1.76475393, 2.53536399, 3.52950785, 5.07072797],\n       [3.88624563, 4.80683778, 7.77249127, 9.61367556]]), cov=array([[[0.62753041, 0.7826417 , 0.83204436, 1.56528339],\n        [0.7826417 , 1.707989  , 1.56528339, 2.99296154],\n        [0.83204436, 1.56528339, 1.87559695, 3.13056678],\n        [1.56528339, 2.99296154, 3.13056678, 6.19743131]],\n\n       [[0.66321396, 0.84217581, 0.90182172, 1.68435163],\n        [0.84217581, 1.81191923, 1.68435163, 3.19923227],\n        [0.90182172, 1.68435163, 2.01594655, 3.36870325],\n        [1.68435163, 3.19923227, 3.36870325, 6.61076763]]]))\n\n\n\ndisplay_as_row(ls2.get_info())\n\n\n  A \n\n  \n    \n      latent\n      level_x_0\n      level_x_1\n      level_x_2\n      level_x_3\n      slope_x_0\n      slope_x_1\n      slope_x_2\n      slope_x_3\n    \n  \n  \n    \n      level_x_0\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      level_x_1\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n    \n    \n      level_x_2\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n    \n    \n      level_x_3\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n    \n    \n      slope_x_0\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      slope_x_1\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n    \n    \n      slope_x_2\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n    \n    \n      slope_x_3\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n    \n  \n\n  H \n\n  \n    \n      level_x_0\n      level_x_1\n      level_x_2\n      level_x_3\n      slope_x_0\n      slope_x_1\n      slope_x_2\n      slope_x_3\n    \n  \n  \n    \n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n    \n  \n\n  R \n\n  \n    \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0.4218\n      0.5610\n      0.5865\n      1.1220\n    \n    \n      0.5610\n      1.2150\n      1.1220\n      2.1728\n    \n    \n      0.5865\n      1.1220\n      1.3015\n      2.2439\n    \n    \n      1.1220\n      2.1728\n      2.2439\n      4.4742\n    \n  \n\n  Q \n\n  \n    \n      latent\n      level_x_0\n      level_x_1\n      level_x_2\n      level_x_3\n      slope_x_0\n      slope_x_1\n      slope_x_2\n      slope_x_3\n    \n  \n  \n    \n      level_x_0\n      1.4877\n      1.6293\n      2.4502\n      3.2585\n      1.3881\n      1.2723\n      2.6069\n      2.5446\n    \n    \n      level_x_1\n      1.6293\n      2.4422\n      3.2585\n      4.3593\n      1.7377\n      1.7777\n      3.4754\n      3.3860\n    \n    \n      level_x_2\n      2.4502\n      3.2585\n      5.1629\n      6.5170\n      2.6069\n      2.5446\n      5.2985\n      5.0892\n    \n    \n      level_x_3\n      3.2585\n      4.3593\n      6.5170\n      8.9812\n      3.4754\n      3.3860\n      6.9508\n      6.8566\n    \n    \n      slope_x_0\n      1.3881\n      1.7377\n      2.6069\n      3.4754\n      1.8250\n      1.4320\n      2.9129\n      2.8640\n    \n    \n      slope_x_1\n      1.2723\n      1.7777\n      2.5446\n      3.3860\n      1.4320\n      1.7896\n      2.8640\n      2.8421\n    \n    \n      slope_x_2\n      2.6069\n      3.4754\n      5.2985\n      6.9508\n      2.9129\n      2.8640\n      6.1944\n      5.7279\n    \n    \n      slope_x_3\n      2.5446\n      3.3860\n      5.0892\n      6.8566\n      2.8640\n      2.8421\n      5.7279\n      6.0527\n    \n  \n\n \n\n\n\n\nState\n\nsource\n\n\nKalmanModel.plot_state\n\n KalmanModel.plot_state (n_cols=2, bind_interaction=True, properties={})\n\n\nls.plot_state()\n\n/tmp/ipykernel_84377/4282844104.py:4: RuntimeWarning: invalid value encountered in sqrt\n  return np.diagonal(np.sqrt(x), axis1=1, axis2=2)\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html#export",
    "href": "kalman/old_01_pykalman_models.html#export",
    "title": "[OLD] PyKalman Filter Models",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/kalman_learner.html",
    "href": "kalman/kalman_learner.html",
    "title": "Kalman Filter Models",
    "section": "",
    "text": "[TODO] add proper introduction here\nThe models uses a latent state variable \\(x\\) that is modelled over time, to impute gaps in \\(y\\)\n\n\nThe equations of the model are:\n\\[\\begin{align} p(x_t | x_{t-1}) & = \\mathcal{N}(x_t, Ax_{t-1}, Q) \\\\\np(y_t | x_t) & = \\mathcal{N}(Hx_t, T) \\end{align}\\]\nThe Kalman filter has 3 steps:\n\nfilter (updating the state at time t with observations till time t-1)\nupdate (update the state at time t using the observation at time t)\nsmooth (update the state using the observations at time t+1)\n\nIn case of missing data the update step is skipped.\nAfter smoothing the whole dataset the missing data (\\(y_t\\)) can be imputed from the state (\\(x_t\\)) using this formula: \\[p(y_t) = \\mathcal{N}(Hx_x, R + HP^s_tH)\\]"
  },
  {
    "objectID": "kalman/kalman_learner.html#kalman-filter-model",
    "href": "kalman/kalman_learner.html#kalman-filter-model",
    "title": "Kalman Filter Models",
    "section": "Kalman Filter Model",
    "text": "Kalman Filter Model\n\nsource\n\nKalmanModel\n\n KalmanModel (data:torch.Tensor, **kwargs)\n\nKalman Model wtih max likelihood and gradient descend to optimize paramters and support for missing observations\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nTensor\narray of observations containg NaN for missing obs\n\n\nkwargs\n\n\n\n\n\n\nX = torch.arange(6).unsqueeze(0).T\n\n\nX\n\ntensor([[0],\n        [1],\n        [2],\n        [3],\n        [4],\n        [5]])\n\n\n\nX.shape\n\ntorch.Size([6, 1])\n\n\n\nk = KalmanModel(X)\n\n\nk.train(None, None, n_iter = 10)\n\n\n\n\n<__main__.KalmanModel>\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot(k.losses)\n\n\n\n\n\nT = torch.arange(1,X.shape[0])\n\n\nT\n\ntensor([1, 2, 3, 4, 5])\n\n\n\nk.predict(T)\n\nListNormal(mean=tensor([[1.0892],\n        [1.9796],\n        [2.9386],\n        [4.0071],\n        [5.2515]], grad_fn=<CopySlices>), cov=tensor([[[0.5605]],\n\n        [[0.5763]],\n\n        [[0.5945]],\n\n        [[0.6328]],\n\n        [[0.7286]]], grad_fn=<CopySlices>))\n\n\n\n\nState\n\nsource\n\n\nKalmanModel.plot_state\n\n KalmanModel.plot_state (n_cols=2, bind_interaction=True, properties={})\n\n\n# k.plot_state()\n\n\n\nPlot Loss\n\nsource\n\n\nKalmanModel.plot_loss\n\n KalmanModel.plot_loss (size={'width': 250, 'height': 120})\n\n\nk.plot_loss()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():"
  },
  {
    "objectID": "kalman/kalman_learner.html#export",
    "href": "kalman/kalman_learner.html#export",
    "title": "Kalman Filter Models",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/kalman_fastai.html",
    "href": "kalman/kalman_fastai.html",
    "title": "Implement Kalman model using FastAI",
    "section": "",
    "text": "The aim of the data preparation pipeline is to: - take the original time series and split it into time blocks - for each block generate a random gap (need to figure out the properties of the gap) - split some time blocks for testing\nthe input of the pipeline is: - a dataframe containing all observations\nthe input of the model is: - observed data (potentially containing NaN where data is missing) - missing data mask (which is telling where the data is missing) - the data needs to be standardized\n\nfrom meteo_imp.utils import *\n\n\nreset_seed()\n\n\nimport torch\n\n\nfrom fastai.tabular.core import *\nfrom fastai.data.core import *\n\n\n@cache_disk(\"full_hai\")\ndef load_data():\n    return read_fluxnet_csv(hai_path, None)\n\nhai = load_data()\n\n\n\nthe first step is to transfrom the original dataframe into blocks of a specified block_len\ntwo different strategies are possible:\n\ncontigous blocks\nrandom block in the dataframe\n\n\nsource\n\n\n\n\n BlockDfTransform (df, block_len=200)\n\ndivide timeseries DataFrame into blocks\n\nm = BlockDfTransform(hai[:100], 10)\n\n\nm\n\nBlockDfTransform:\nencodes: (int,object) -> encodes\ndecodes: \n\n\n\nm(1)\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 05:30:00\n      -0.23\n      0.00\n      0.138\n    \n    \n      2000-01-01 06:00:00\n      -0.23\n      0.00\n      0.122\n    \n    \n      2000-01-01 06:30:00\n      -0.22\n      0.00\n      0.098\n    \n    \n      2000-01-01 07:00:00\n      -0.24\n      0.00\n      0.066\n    \n    \n      2000-01-01 07:30:00\n      -0.23\n      0.00\n      0.044\n    \n    \n      2000-01-01 08:00:00\n      -0.22\n      0.00\n      0.026\n    \n    \n      2000-01-01 08:30:00\n      -0.19\n      0.45\n      0.016\n    \n    \n      2000-01-01 09:00:00\n      -0.14\n      3.70\n      0.010\n    \n    \n      2000-01-01 09:30:00\n      -0.03\n      7.26\n      0.006\n    \n    \n      2000-01-01 10:00:00\n      0.04\n      12.24\n      0.006\n    \n  \n\n\n\n\n\nm(9)\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2000-01-02 21:30:00\n      0.97\n      0.0\n      0.192\n    \n    \n      2000-01-02 22:00:00\n      0.85\n      0.0\n      0.149\n    \n    \n      2000-01-02 22:30:00\n      0.77\n      0.0\n      0.112\n    \n    \n      2000-01-02 23:00:00\n      0.63\n      0.0\n      0.075\n    \n    \n      2000-01-02 23:30:00\n      0.52\n      0.0\n      0.038\n    \n    \n      2000-01-03 00:00:00\n      0.48\n      0.0\n      0.021\n    \n    \n      2000-01-03 00:30:00\n      0.41\n      0.0\n      0.013\n    \n    \n      2000-01-03 01:00:00\n      0.29\n      0.0\n      0.004\n    \n    \n      2000-01-03 01:30:00\n      0.31\n      0.0\n      0.000\n    \n    \n      2000-01-03 02:00:00\n      0.42\n      0.0\n      0.000\n    \n  \n\n\n\n\n\n\n\nadds a mask which includes a random gap\n\ndef _make_random_gap(\n    gap_length: int, # The length of the gap\n    total_length: int, # The total number of observations\n    gap_start: int = None # Optional start of gap\n): # (total_length) array of bools to indicicate if the data is missing or not\n    \"Add a continous gap of ginve length at random position\"\n    if(gap_length >= total_length):\n        return np.repeat(True, total_length)\n    gap_start = np.random.randint(total_length - gap_length) if gap_start is None else gap_start\n    return np.hstack([\n        np.repeat(False, gap_start),\n        np.repeat(True, gap_length),\n        np.repeat(False, total_length - (gap_length + gap_start))\n    ])\n\n\nsource\n\n\n\n\n AddGapTransform (variables, gap_length)\n\nAdds a random gap to a TimeSTensor\n\na_gap = AddGapTransform(['TA', 'VPD'], 5)\na_gap\n\nAddGapTransform:\nencodes: (DataFrame,object) -> encodes\ndecodes: \n\n\n\na_gap(m(1))\n\nMaskedDf(data=                       TA  SW_IN    VPD\ntime                                   \n2000-01-01 05:30:00 -0.23   0.00  0.138\n2000-01-01 06:00:00 -0.23   0.00  0.122\n2000-01-01 06:30:00 -0.22   0.00  0.098\n2000-01-01 07:00:00 -0.24   0.00  0.066\n2000-01-01 07:30:00 -0.23   0.00  0.044\n2000-01-01 08:00:00 -0.22   0.00  0.026\n2000-01-01 08:30:00 -0.19   0.45  0.016\n2000-01-01 09:00:00 -0.14   3.70  0.010\n2000-01-01 09:30:00 -0.03   7.26  0.006\n2000-01-01 10:00:00  0.04  12.24  0.006, mask=                        TA  SW_IN    VPD\ntime                                    \n2000-01-01 05:30:00   True   True   True\n2000-01-01 06:00:00   True   True   True\n2000-01-01 06:30:00   True   True   True\n2000-01-01 07:00:00  False   True  False\n2000-01-01 07:30:00  False   True  False\n2000-01-01 08:00:00  False   True  False\n2000-01-01 08:30:00  False   True  False\n2000-01-01 09:00:00  False   True  False\n2000-01-01 09:30:00   True   True   True\n2000-01-01 10:00:00   True   True   True)\n\n\n\nm_df = a_gap(m(3))\n\n\nm_df.data\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 15:30:00\n      0.52\n      8.09\n      0.000\n    \n    \n      2000-01-01 16:00:00\n      0.57\n      6.37\n      0.000\n    \n    \n      2000-01-01 16:30:00\n      0.73\n      1.72\n      0.000\n    \n    \n      2000-01-01 17:00:00\n      0.77\n      0.06\n      0.000\n    \n    \n      2000-01-01 17:30:00\n      0.84\n      0.00\n      0.000\n    \n    \n      2000-01-01 18:00:00\n      0.99\n      0.00\n      0.000\n    \n    \n      2000-01-01 18:30:00\n      1.35\n      0.00\n      0.000\n    \n    \n      2000-01-01 19:00:00\n      1.86\n      0.00\n      0.002\n    \n    \n      2000-01-01 19:30:00\n      2.01\n      0.00\n      0.009\n    \n    \n      2000-01-01 20:00:00\n      2.07\n      0.00\n      0.014\n    \n  \n\n\n\n\n\nm_df.mask\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 15:30:00\n      False\n      True\n      False\n    \n    \n      2000-01-01 16:00:00\n      False\n      True\n      False\n    \n    \n      2000-01-01 16:30:00\n      False\n      True\n      False\n    \n    \n      2000-01-01 17:00:00\n      False\n      True\n      False\n    \n    \n      2000-01-01 17:30:00\n      False\n      True\n      False\n    \n    \n      2000-01-01 18:00:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 18:30:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 19:00:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 19:30:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 20:00:00\n      True\n      True\n      True\n    \n  \n\n\n\n\n\nsource\n\n\n\n\n MaskedDf.tidy ()\n\n\nm_df.tidy()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_present\n    \n  \n  \n    \n      0\n      2000-01-01 15:30:00\n      TA\n      0.520\n      False\n    \n    \n      1\n      2000-01-01 16:00:00\n      TA\n      0.570\n      False\n    \n    \n      2\n      2000-01-01 16:30:00\n      TA\n      0.730\n      False\n    \n    \n      3\n      2000-01-01 17:00:00\n      TA\n      0.770\n      False\n    \n    \n      4\n      2000-01-01 17:30:00\n      TA\n      0.840\n      False\n    \n    \n      5\n      2000-01-01 18:00:00\n      TA\n      0.990\n      True\n    \n    \n      6\n      2000-01-01 18:30:00\n      TA\n      1.350\n      True\n    \n    \n      7\n      2000-01-01 19:00:00\n      TA\n      1.860\n      True\n    \n    \n      8\n      2000-01-01 19:30:00\n      TA\n      2.010\n      True\n    \n    \n      9\n      2000-01-01 20:00:00\n      TA\n      2.070\n      True\n    \n    \n      10\n      2000-01-01 15:30:00\n      SW_IN\n      8.090\n      True\n    \n    \n      11\n      2000-01-01 16:00:00\n      SW_IN\n      6.370\n      True\n    \n    \n      12\n      2000-01-01 16:30:00\n      SW_IN\n      1.720\n      True\n    \n    \n      13\n      2000-01-01 17:00:00\n      SW_IN\n      0.060\n      True\n    \n    \n      14\n      2000-01-01 17:30:00\n      SW_IN\n      0.000\n      True\n    \n    \n      15\n      2000-01-01 18:00:00\n      SW_IN\n      0.000\n      True\n    \n    \n      16\n      2000-01-01 18:30:00\n      SW_IN\n      0.000\n      True\n    \n    \n      17\n      2000-01-01 19:00:00\n      SW_IN\n      0.000\n      True\n    \n    \n      18\n      2000-01-01 19:30:00\n      SW_IN\n      0.000\n      True\n    \n    \n      19\n      2000-01-01 20:00:00\n      SW_IN\n      0.000\n      True\n    \n    \n      20\n      2000-01-01 15:30:00\n      VPD\n      0.000\n      False\n    \n    \n      21\n      2000-01-01 16:00:00\n      VPD\n      0.000\n      False\n    \n    \n      22\n      2000-01-01 16:30:00\n      VPD\n      0.000\n      False\n    \n    \n      23\n      2000-01-01 17:00:00\n      VPD\n      0.000\n      False\n    \n    \n      24\n      2000-01-01 17:30:00\n      VPD\n      0.000\n      False\n    \n    \n      25\n      2000-01-01 18:00:00\n      VPD\n      0.000\n      True\n    \n    \n      26\n      2000-01-01 18:30:00\n      VPD\n      0.000\n      True\n    \n    \n      27\n      2000-01-01 19:00:00\n      VPD\n      0.002\n      True\n    \n    \n      28\n      2000-01-01 19:30:00\n      VPD\n      0.009\n      True\n    \n    \n      29\n      2000-01-01 20:00:00\n      VPD\n      0.014\n      True\n    \n  \n\n\n\n\n\nplot_rug(m_df.tidy())\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\ndef plot_points(df, y_label = \"\", sel = def_selection(), props = {}):\n    return alt.Chart(df).mark_point(\n            color='black',\n            strokeWidth = 1,\n            fillOpacity = 1\n        ).encode(\n            x = alt.X(\"time\", axis=alt.Axis(domain=False, labels = False, ticks=False, title=None)),\n            y = alt.Y(\"value\", title = y_label, scale=alt.Scale(zero=False)),\n            fill= alt.Fill(\"is_present\", scale = alt.Scale(range=[\"black\", \"#ffffff00\"]),\n                           legend = alt.Legend(title =[\"Observed data\"])),\n            shape = \"is_present\",\n        )\n\n\nplot_points(m_df.tidy())\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nplot_line(m_df.tidy())\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nplot_variable(m_df.tidy(), \"TA\", title=\"title TA\")\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n MaskedDf.show (ax=None, ctx=None, n_cols:int=3,\n                bind_interaction:bool=True, props:dict=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nax\nNoneType\nNone\n\n\n\nctx\nNoneType\nNone\n\n\n\nn_cols\nint\n3\n\n\n\nbind_interaction\nbool\nTrue\nWhether the sub-plots for each variable should be connected for zooming/panning\n\n\nprops\ndict\nNone\nadditional properties (eg. size) for altair plot\n\n\nReturns\nChart\n\n\n\n\n\n\nm_df.show()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\na_gap(m(3)).show()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\na_gap(m(4)).show()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nidx = L(*m(1).columns).argwhere(lambda x: x in ['TA','SW_IN'])\n\n\nmask = np.ones_like(m(1), dtype=bool)\n\n\nmask\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\n\ngap = _make_random_gap(2, 10, 2)\n\n\ngap\n\narray([False, False,  True,  True, False, False, False, False, False,\n       False])\n\n\n\nnp.argwhere(gap)\n\narray([[2],\n       [3]])\n\n\n\nmask[np.argwhere(gap), idx] = False\n\n\nmask\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [False, False,  True],\n       [False, False,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\n\nmask[gap]\n\narray([[False, False,  True],\n       [False, False,  True]])\n\n\n\n\n\n\nsource\n\n\n\n\n MaskedTensor (x=None, *rest)\n\nA tuple with elementwise ops and more friendly init behavior\n\nsource\n\n\n\n\n MaskedDf2Tensor (enc=None, dec=None, split_idx=None, order=None)\n\nA transform that always take tuples as items\n\ntfms = TfmdLists([1,2,3], [BlockDfTransform(hai, 10), AddGapTransform(['TA','SW_IN'], 2), MaskedDf2Tensor ])\n\n\ntfms[1]\n\n(tensor([[1.0000e-01, 1.6510e+01, 6.0000e-03],\n         [1.8000e-01, 2.4730e+01, 1.1000e-02],\n         [2.1000e-01, 4.7420e+01, 1.9000e-02],\n         [2.3000e-01, 2.2050e+01, 1.4000e-02],\n         [3.3000e-01, 1.8860e+01, 8.0000e-03],\n         [4.1000e-01, 2.1100e+01, 6.0000e-03],\n         [4.4000e-01, 2.8870e+01, 0.0000e+00],\n         [4.8000e-01, 2.4220e+01, 0.0000e+00],\n         [4.9000e-01, 2.4350e+01, 0.0000e+00],\n         [5.1000e-01, 1.5680e+01, 0.0000e+00]]),\n tensor([[False, False,  True],\n         [False, False,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True]]))\n\n\n\ntype(tfms[0])\n\n__main__.MaskedTensor\n\n\n\n\n\n\nsource\n\n\n\n\n get_stats (df)\n\n\nsource\n\n\n\n\n NormalizeMasked (mean=None, std=None, axes=(0,))\n\nNormalize/denorm MaskedTensor column-wise\n\nb = TfmdLists([0,1], [BlockDfTransform(hai, 10), AddGapTransform(['TA','SW_IN'], 2), MaskedDf2Tensor, NormalizeMasked(*get_stats(hai))]).dataloaders(bs=2).one_batch()[0]\n\n\nb[0].mean(0)\n\ntensor([-1.1083, -0.5929, -0.7457], device='cuda:0')\n\n\n\nb[1].mean(0)\n\ntensor([-1.0730, -0.5813, -0.7617], device='cuda:0')\n\n\n\nb.std(axis=(0,1))\n\ntensor([0.0232, 0.0154, 0.0128], device='cuda:0')\n\n\n\n\n\n\nblock_len = 10\nblock_ids = list(range(0, (len(hai) // block_len) - 1))[:10]\ngap_len = 2\n\n\nsource\n\n\n\n\n imp_pipeline (df, block_len, gap_len)\n\n\npipeline, block_ids = imp_pipeline(hai, block_len, gap_len)\n\n\npipeline\n\n[BlockDfTransform:\n encodes: (int,object) -> encodes\n decodes: ,\n AddGapTransform:\n encodes: (DataFrame,object) -> encodes\n decodes: ,\n __main__.MaskedDf2Tensor,\n NormalizeMasked:\n encodes: (MaskedTensor,object) -> encodes\n (object,object) -> encodes\n decodes: (ListNormal,object) -> decodes]\n\n\n\n\n\nrandom splitter for validation/training set\n\nreset_seed()\n\n\nsplits = RandomSplitter()(block_ids)\n\nRepeat twice the pipeline since is the same pipeline both for training data and for labels\n\nds = Datasets(block_ids, [pipeline, pipeline], splits=splits)\n\n\ndls = ds.dataloaders(bs=1)\n\n\ndls.device\n\ndevice(type='cuda', index=0)\n\n\n\ndls.one_batch()\n\n((tensor([[[ 0.1989,  0.0843,  0.1587],\n           [ 0.1837, -0.2318,  0.0536],\n           [ 0.1497, -0.4158, -0.0613],\n           [ 0.1017, -0.5118, -0.1707],\n           [ 0.0487, -0.5848, -0.2554],\n           [-0.0106, -0.5929, -0.3399],\n           [-0.0510, -0.5929, -0.3753],\n           [-0.0838, -0.5929, -0.4113],\n           [-0.1166, -0.5929, -0.4420],\n           [-0.1746, -0.5929, -0.4591]]], device='cuda:0'),\n  tensor([[[ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [False, False,  True],\n           [False, False,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True]]], device='cuda:0')),\n (tensor([[[ 0.1989,  0.0843,  0.1587],\n           [ 0.1837, -0.2318,  0.0536],\n           [ 0.1497, -0.4158, -0.0613],\n           [ 0.1017, -0.5118, -0.1707],\n           [ 0.0487, -0.5848, -0.2554],\n           [-0.0106, -0.5929, -0.3399],\n           [-0.0510, -0.5929, -0.3753],\n           [-0.0838, -0.5929, -0.4113],\n           [-0.1166, -0.5929, -0.4420],\n           [-0.1746, -0.5929, -0.4591]]], device='cuda:0'),\n  tensor([[[False, False,  True],\n           [False, False,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True]]], device='cuda:0')))\n\n\n\n@typedispatch\ndef show_batch(x: MaskedTensor, y, samples, ctxs=None, max_n=6):\n    print(x)\n\n\ndls.show_batch()\n\n(tensor([[[-0.8939, -0.5929, -0.5592],\n         [-0.9482, -0.5929, -0.6807],\n         [-0.9242, -0.5929, -0.7212],\n         [-0.9065, -0.5929, -0.7434],\n         [-0.8523, -0.5929, -0.7389],\n         [-0.8195, -0.5929, -0.7302],\n         [-0.8056, -0.5929, -0.7476],\n         [-0.7904, -0.5929, -0.7560],\n         [-0.7576, -0.5929, -0.7590],\n         [-0.7387, -0.5929, -0.7606]]]), tensor([[[ True,  True,  True],\n         [False, False,  True],\n         [False, False,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True]]]))\n\n\n\ndls._types\n\n{tuple: [{__main__.MaskedTensor: [torch.Tensor, torch.Tensor]},\n  {__main__.MaskedTensor: [torch.Tensor, torch.Tensor]}]}\n\n\n\ndls.show_batch()\n\n(tensor([[[-0.0913,  0.0613, -0.4278],\n         [-0.0447,  0.4846, -0.4010],\n         [ 0.0109,  0.6791, -0.3737],\n         [ 0.1080,  0.7323, -0.2334],\n         [ 0.1850,  0.7927, -0.1441],\n         [ 0.2342,  0.4668, -0.0913],\n         [ 0.2304,  0.5042, -0.1041],\n         [ 0.2128,  0.2841, -0.1384],\n         [ 0.1749, -0.0679, -0.2201],\n         [ 0.1598, -0.1345, -0.2510]]]), tensor([[[ True,  True,  True],\n         [False, False,  True],\n         [False, False,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True]]]))\n\n\n\nDatasets\n\nfastai.data.core.Datasets\n\n\n\nsource\n\n\n\n\n make_dataloader (df, block_len, gap_len, bs=10)\n\n\ndls = make_dataloader(hai, 200, 10)\n\n\ndls.one_batch()[0][0].shape\n\ntorch.Size([10, 200, 3])\n\n\n\ndls = dls.cpu()"
  },
  {
    "objectID": "kalman/kalman_fastai.html#model",
    "href": "kalman/kalman_fastai.html#model",
    "title": "Implement Kalman model using FastAI",
    "section": "Model",
    "text": "Model\n\nForward Function\nin order to the a pytorch module we need a forward method to the kalman filter\n\nsource\n\n\nListNormal\n\n ListNormal (x=None, *rest)\n\nA tuple with elementwise ops and more friendly init behavior\n\nsource\n\n\nKalmanFilter.forward\n\n KalmanFilter.forward (masked_data:__main__.MaskedTensor)\n\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\n.. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class:Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\n\ninput = dls.one_batch()[0]\ntarget = dls.one_batch()[1]\n\n\nmodel = KalmanFilter(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1])\n\n\nmodel.state_dict()\n\nOrderedDict([('transition_matrices',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]])),\n             ('transition_offsets', tensor([0., 0., 0.])),\n             ('transition_cov_raw',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]])),\n             ('obs_matrices',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]])),\n             ('obs_offsets', tensor([0., 0., 0.])),\n             ('obs_cov_raw',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]])),\n             ('initial_state_mean', tensor([0., 0., 0.])),\n             ('initial_state_cov_raw',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]]))])\n\n\n\ndata = input[0][0]\ndata.shape\n\ntorch.Size([200, 3])\n\n\n\nmask = input[1][0]\n\n\nmask.shape\n\ntorch.Size([200, 3])\n\n\n\ndata.device\n\ndevice(type='cpu')\n\n\n\ntorch.device\n\ntorch.device\n\n\n\ndata.shape, mask.shape\n\n(torch.Size([200, 3]), torch.Size([200, 3]))\n\n\n\nmodel.predict(data, mask);\n\n\nmodel.use_smooth = True\n\n\npred = model(input)\n\n\npred[0].shape\n\ntorch.Size([10, 200, 3])\n\n\n\npred[1].shape\n\ntorch.Size([10, 200, 3])\n\n\n\nmodel.use_smooth = False\n\n\npred_filt = model(input)\n\n\npred_filt[0].shape\n\ntorch.Size([10, 200, 3])\n\n\n\npred_filt[1].shape\n\ntorch.Size([10, 200, 3])\n\n\n\ntest_ne(pred, pred_filt)\n\n\n\nLoss Function\nlog the limit at which the covatiances matrices are not symmetric anymore\n\npred[0][0].flatten().shapea\n\ntorch.Size([600])\n\n\n\ntorch.diag(pred[1][0].flatten()).shape\n\ntorch.Size([600, 600])\n\n\n\nMultivariateNormal(pred[0][0].flatten(), torch.diag(pred[1][0].flatten())).log_prob(target[0][0].flatten()).sum()\n\ntensor(-821.3179, grad_fn=<SumBackward0>)\n\n\n\nimp_ll_loss(pred, target).backward()\n\nadd support for complete loss (also outside gap) and for filter loss (don’t run the smooher)\nTODO document: - only_gap - only_std\nPlay around with flatting + diagonal\n\na = torch.diag(torch.tensor([1,2,3]))\nd = torch.stack([a, a*10])\nm = torch.stack([a.diag(), a.diag()*10])\nd\n\ntensor([[[ 1,  0,  0],\n         [ 0,  2,  0],\n         [ 0,  0,  3]],\n\n        [[10,  0,  0],\n         [ 0, 20,  0],\n         [ 0,  0, 30]]])\n\n\n\nm.flatten()\n\ntensor([ 1,  2,  3, 10, 20, 30])\n\n\n\nd\n\ntensor([[[ 1,  0,  0],\n         [ 0,  2,  0],\n         [ 0,  0,  3]],\n\n        [[10,  0,  0],\n         [ 0, 20,  0],\n         [ 0,  0, 30]]])\n\n\n\ntorch.diagonal(d, dim1=1, dim2=2).flatten()\n\ntensor([ 1,  2,  3, 10, 20, 30])\n\n\n\nmeans, stds = pred\ndata, mask = target\n\n\n# make a big matrix with all variables and observations and compute ll\nmask = mask.flatten() \nobs = data.flatten()[mask]\nmeans = data.flatten()[mask]\nstds = stds.flatten()[mask] # need to support batches\n\nMultivariateNormal(means, torch.diag(stds)).log_prob(obs)\n\ntensor(-6531.1284, grad_fn=<SubBackward0>)\n\n\n\ncovs.shape\n\ntorch.Size([10, 200, 3])\n\n\n\nsource\n\n\nKalmanLoss\n\n KalmanLoss (only_gap:bool=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nonly_gap\nbool\nTrue\nloss for all predictions or only gap?\n\n\n\n\nKalmanLoss()(pred, target)\n\ntensor(-6531.1284, grad_fn=<SubBackward0>)\n\n\n\nKalmanLoss(only_gap=False)(pred, target)\n\ntensor(-6751.8809, grad_fn=<SubBackward0>)\n\n\n\n\nMetrics\nWrapper around fastai metrics to support masked tensors and normal distributions\n\nsource\n\n\nto_meteo_imp_metric\n\n to_meteo_imp_metric (metric)\n\n\n\nCallback\nsave the model state\n\nsource\n\n\nSaveParams\n\n SaveParams (param_name)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nProgressCallback().name\n\n'progress'\n\n\n\nrange_of?\n\n\nSignature: range_of(a, b=None, step=None)\nDocstring: All indices of collection `a`, if `a` is a collection, otherwise `range`\nFile:      ~/.local/lib/python3.10/site-packages/fastcore/basics.py\nType:      function\n\n\n\n\n\nsource\n\n\nSaveParams\n\n SaveParams (param_name)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\n\nLearner\n\nobs_cov_history = SaveParams('obs_cov')\n\n\nall_data = CollectDataCallback()\n\n\nmodel = KalmanFilter(n_dim_state = hai.shape[1], n_dim_obs=hai.shape[1])\n\n\n# model._set_constraint('obs_cov', model.obs_cov, train=False)\n\n\ndls = make_dataloader(hai[:2000], 200, 10, bs=1).cpu()\n\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(), cbs = [obs_cov_history, all_data] )\n\n\nlearn.fit(1, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      -676.813843\n      -706.480713\n      00:10\n    \n  \n\n\n\n\n\n\n\nlearn.model.state_dict()\n\n\nfrom pyprojroot import here\n\n\n# torch.save(learn.model, here('trained_models'))\n\n\nlearn.model.obs_cov\n\n\nlearn.model.transition_cov_raw\n\n\nimport meteo_imp\n\n\nmeteo_imp.kalman.filter.posdef_log\n\n\nlen(obs_cov_history.params)\n\n\nobs_cov_history.params[-1]\n\n\ntorch.tensor(list(map(symmetric_upto, obs_cov_history.params)))\n\n\ntorch.linalg.eigvalsh(obs_cov_history.params[-1])\n\n\ntorch.linalg.cholesky_ex(obs_cov_history.params[-1])\n\n\ntt = torch.tensor([[ 1.0696,  0.5199, -0.5249],\n        [ 0.5484,  1.1091,  0.5322],\n        [-0.5279,  0.5506,  1.0742]])\n\n\ntorch.linalg.eigvalsh(tt)\n\n\nsymmetric_upto(tt)\n\n\n# torch.save(learn.model.state_dict(), \"partial_traning_15_dec_not_pos_def_error\")\n\n\nlearn.recorder.plot_loss()\n\n\n# learn.lr_find()\n\n\ndisplay_as_row(learn.model.get_info())\n\n\nlearn.show_results()"
  },
  {
    "objectID": "kalman/kalman_fastai.html#double-precisions",
    "href": "kalman/kalman_fastai.html#double-precisions",
    "title": "Implement Kalman model using FastAI",
    "section": "Double Precisions",
    "text": "Double Precisions\n\nhai = read_fluxnet_csv(hai_path, 2000, num_dtype=np.float64)\n\n\nmodel = KalmanFilter(**KalmanFilterTester(dtype=torch.float64).params)\n\n\ndls = make_dataloader(hai[:2000], 200, 10, bs=1)\n\n\ninput = dls.one_batch()[0]\ntarget = dls.one_batch()[1]\n\n\ndata, mask = input\n\n\ndata.dtype\n\n\nmodel.predict(data.squeeze(), torch.tensor([0]))\n\n\npred = model(input)\n\n\nimp_ll_loss(pred, target)\n\n\nsource\n\nFloat64Callback\n\n Float64Callback (after_create=None, before_fit=None, before_epoch=None,\n                  before_train=None, before_batch=None, after_pred=None,\n                  after_loss=None, before_backward=None,\n                  after_cancel_backward=None, after_backward=None,\n                  before_step=None, after_cancel_step=None,\n                  after_step=None, after_cancel_batch=None,\n                  after_batch=None, after_cancel_train=None,\n                  after_train=None, before_validate=None,\n                  after_cancel_validate=None, after_validate=None,\n                  after_cancel_epoch=None, after_epoch=None,\n                  after_cancel_fit=None, after_fit=None)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nlearn = Learner(dls, model, loss_func=imp_ll_loss, cbs = [ShowGraphCallback, Float64Callback] )\n\n\nlearn.fit(10, 1e-3)"
  },
  {
    "objectID": "data_preparation.html#data-generator",
    "href": "data_preparation.html#data-generator",
    "title": "Data Preparation",
    "section": "Data generator",
    "text": "Data generator\ngenerate some fake data in order to test the imputation\nWhat is does is: - take a function to generate the “true” latent - use some random coefficient to generate all the N features - add some random noise\n\nclass GPFADataGenerator:\n    def __init__(self,\n                    n_features: int,\n                    n_obs: int,\n                    latent_func = lambda x: torch.sin(3*x), # Functions used to generate the true latent\n                    noise_std = .2,\n                    Lambda = None\n                ):\n        \n        self.n_features, self.n_obs = n_features, n_obs\n        self.time = torch.arange(0, self.n_obs, dtype=torch.float)\n        \n        self.latent = latent_func(self.time)\n        \n        self.Lambda = torch.tensor(Lambda).reshape(n_features, 1) if Lambda is not None else torch.rand(n_features, 1)\n        \n        self.exact_X = (self.Lambda * self.latent).T\n        \n        self.X =  self.exact_X + torch.normal(0., noise_std, size = (n_obs, n_features)) \n        \n        self.data = pd.DataFrame(self.X.numpy(), columns = [f\"x{i}\" for i in range(self.n_features)])\n\n\nsource\n\nGPFADataGenerator\n\n GPFADataGenerator (n_features:int, n_obs:int, latent_func=<function\n                    <lambda>>, noise_std=0.2, Lambda=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nint\n\n\n\n\nn_obs\nint\n\n\n\n\nlatent_func\nfunction\n\nFunctions used to generate the true latent\n\n\nnoise_std\nfloat\n0.2\n\n\n\nLambda\nNoneType\nNone\n\n\n\n\n\nfdg = GPFADataGenerator(3, 4)\n\n\nfd_plot = pd.DataFrame(fdg.X.numpy(), columns = [\"x1\", \"x2\", \"x3\"])\nfd_plot[\"latent\"] = pd.Series(fdg.latent.numpy())\n\n\nfd_plot.plot()\n\n<AxesSubplot: >"
  },
  {
    "objectID": "data_preparation.html#missing-data",
    "href": "data_preparation.html#missing-data",
    "title": "Data Preparation",
    "section": "Missing Data",
    "text": "Missing Data\n\nclass MeteoDataTest:\n    \"Utility class to keep track of dataset, missing data and export to right format\"\n    def __init__(self, data: pd.DataFrame):\n        \" Init with provided dataset\"\n        self.data = data.copy()\n        self.data_complete = self.data.copy()\n        self.n_features, self.n_obs = data.shape[1], data.shape[0]\n        self.time = torch.arange(0, self.n_obs, dtype=torch.float)\n    @classmethod\n    def generate_gpfa(cls, *args, **kwargs):\n        generator = GPFADataGenerator(*args, **kwargs)\n        self = MeteoDataTest(generator.data)\n        self.generator = generator\n        return self\n\n\nsource\n\nMeteoDataTest\n\n MeteoDataTest (data:pandas.core.frame.DataFrame)\n\nUtility class to keep track of dataset, missing data and export to right format\n\n\nMissing Data\ngenerate artificial gaps in the data\n\nMissing at Random\n\n@patch()\ndef add_random_missing(self: MeteoDataTest,\n                       prob_miss_row: float = .2,  #  Probability an entire row is missing\n                       prob_miss_value: float = .1  # Probability a single observation is missing\n                       ):\n    \"\"\"Make some row and same values randomly missing \"\"\"\n    # keep the original data\n        \n    self.is_miss_row = torch.rand(self.n_obs) <= prob_miss_row\n    \n    self.data[self.is_miss_row.numpy()] = np.nan\n    \n    self.is_miss_value = (torch.rand(self.n_obs * self.n_features) <= prob_miss_value).reshape(-1, self.n_features)\n    \n    self.data[self.is_miss_value.numpy()] = np.nan\n    \n    return self\n\n\nsource\n\n\n\nMeteoDataTest.add_random_missing\n\n MeteoDataTest.add_random_missing (prob_miss_row:float=0.2,\n                                   prob_miss_value:float=0.1)\n\nMake some row and same values randomly missing\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob_miss_row\nfloat\n0.2\nProbability an entire row is missing\n\n\nprob_miss_value\nfloat\n0.1\nProbability a single observation is missing\n\n\n\n\nfd = MeteoDataTest.generate_gpfa(3, 4)\n\n\nfd.add_random_missing().data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x2\n    \n  \n  \n    \n      0\n      -0.001938\n      -0.127557\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      0.137062\n      0.151774\n      -0.092625\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nMeteoDataTest.generate_gpfa(2, 10).add_random_missing(prob_miss_value = .7, prob_miss_row=.0).data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      -0.203655\n      -0.117674\n    \n    \n      1\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      -0.111792\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n    \n    \n      5\n      NaN\n      0.112875\n    \n    \n      6\n      NaN\n      NaN\n    \n    \n      7\n      NaN\n      NaN\n    \n    \n      8\n      NaN\n      NaN\n    \n    \n      9\n      NaN\n      0.409263\n    \n  \n\n\n\n\n\nContinous gap\nThe missing data is all clustered around a gap\nparameteres: - gap length - variable for gap\n\ndef _make_random_gap(\n    gap_length: int, # The length of the gap\n    total_length: int, # The total number of observations\n    gap_start: int = None # Optional start of gap\n): # (total_length) array of bools to indicicate if the data is missing or not\n    \"Add a continous gap of ginve length at random position\"\n    if(gap_length >= total_length):\n        return np.repeat(True, total_length)\n    gap_start = np.random.randint(total_length - gap_length) if gap_start is None else gap_start\n    return np.hstack([\n        np.repeat(False, gap_start),\n        np.repeat(True, gap_length),\n        np.repeat(False, total_length - (gap_length + gap_start))\n    ])\n\n\n_make_random_gap(3, 10)\n\narray([False, False, False, False, False,  True,  True,  True, False,\n       False])\n\n\n\ntest_eq(_make_random_gap(3, 10).sum(), 3) # correct gap length\n\n\n@patch\ndef add_gap(self: MeteoDataTest,\n            gap_length:int,  # length of gap\n            variables: Collection[str],  # variables that should be affected by the gap\n            gap_start: int = None  # Optional start of the gap\n            ):\n    \n    \n    self.is_gap = _make_random_gap(gap_length, self.data.shape[0], gap_start)\n    self.data.loc[self.is_gap, variables] = np.nan\n    return self\n\n\nsource\n\n\n\nMeteoDataTest.add_gap\n\n MeteoDataTest.add_gap (gap_length:int, variables:Collection[str],\n                        gap_start:int=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngap_length\nint\n\nlength of gap\n\n\nvariables\nCollection\n\nvariables that should be affected by the gap\n\n\ngap_start\nint\nNone\nOptional start of the gap\n\n\n\n\nMeteoDataTest.generate_gpfa(5, 10).add_gap(4, [\"x1\", \"x2\"]).data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x2\n      x3\n      x4\n    \n  \n  \n    \n      0\n      0.147414\n      -0.162167\n      0.534020\n      -0.314157\n      -0.145421\n    \n    \n      1\n      0.227970\n      -0.250645\n      -0.280364\n      0.298646\n      0.023241\n    \n    \n      2\n      -0.277233\n      NaN\n      NaN\n      0.011927\n      0.350378\n    \n    \n      3\n      0.081553\n      NaN\n      NaN\n      0.091917\n      0.162622\n    \n    \n      4\n      -0.488591\n      NaN\n      NaN\n      -0.383358\n      -0.142712\n    \n    \n      5\n      0.850323\n      NaN\n      NaN\n      0.378259\n      -0.198052\n    \n    \n      6\n      -0.554473\n      -0.438560\n      -0.345700\n      -0.368945\n      -0.081258\n    \n    \n      7\n      0.904893\n      0.390038\n      0.473491\n      0.916413\n      -0.053904\n    \n    \n      8\n      -0.463149\n      -0.567804\n      -0.318116\n      -0.687993\n      0.118367\n    \n    \n      9\n      0.931138\n      0.398865\n      0.561607\n      0.701557\n      0.026598\n    \n  \n\n\n\n\n\n\nSave as DataFrame\n\n@patch\ndef tidy_df(self: MeteoDataTest,\n            complete = False,  # full dataset (False) or the one with missing data (True)\n            is_missing = False  # add flag whether value is missing\n            ):\n    \n    df = self.data if not complete else self.data_complete # no need to copy here because next lines does a copy anyway\n    df = df.assign(time = self.time.numpy())\n        \n    df = df.melt(\"time\")\n    \n    if is_missing: df = df.assign(is_missing = self.data.melt().value.isna()) #missing data is not from complete data\n        \n    return df\n\n\nsource\n\n\nMeteoDataTest.tidy_df\n\n MeteoDataTest.tidy_df (complete=False, is_missing=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncomplete\nbool\nFalse\nfull dataset (False) or the one with missing data (True)\n\n\nis_missing\nbool\nFalse\nadd flag whether value is missing\n\n\n\n\nfd.tidy_df()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n    \n    \n      1\n      1.0\n      x0\n      NaN\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n    \n    \n      3\n      3.0\n      x0\n      NaN\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n    \n    \n      5\n      1.0\n      x1\n      NaN\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n    \n    \n      7\n      3.0\n      x1\n      NaN\n    \n    \n      8\n      0.0\n      x2\n      NaN\n    \n    \n      9\n      1.0\n      x2\n      NaN\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n    \n    \n      11\n      3.0\n      x2\n      NaN\n    \n  \n\n\n\n\n\nfd.tidy_df(complete=True)\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n    \n    \n      1\n      1.0\n      x0\n      0.021966\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n    \n    \n      3\n      3.0\n      x0\n      0.031551\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n    \n    \n      5\n      1.0\n      x1\n      0.295225\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n    \n    \n      7\n      3.0\n      x1\n      0.116207\n    \n    \n      8\n      0.0\n      x2\n      0.139568\n    \n    \n      9\n      1.0\n      x2\n      0.029323\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n    \n    \n      11\n      3.0\n      x2\n      0.160619\n    \n  \n\n\n\n\n\nfd.tidy_df(complete=False, is_missing=True)\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_missing\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n      False\n    \n    \n      1\n      1.0\n      x0\n      NaN\n      True\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n      False\n    \n    \n      3\n      3.0\n      x0\n      NaN\n      True\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n      False\n    \n    \n      5\n      1.0\n      x1\n      NaN\n      True\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n      False\n    \n    \n      7\n      3.0\n      x1\n      NaN\n      True\n    \n    \n      8\n      0.0\n      x2\n      NaN\n      True\n    \n    \n      9\n      1.0\n      x2\n      NaN\n      True\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n      False\n    \n    \n      11\n      3.0\n      x2\n      NaN\n      True\n    \n  \n\n\n\n\nThe export as a dataframe is working correctly with the missing data\n\nfd_df = fd.tidy_df()\n\n\nalt.Chart(fd_df).mark_line(point=True).encode(\n    x = \"time\",\n    y = \"value\",\n    color = \"variable\"\n)\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nfd.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x2\n    \n  \n  \n    \n      0\n      -0.001938\n      -0.127557\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      0.137062\n      0.151774\n      -0.092625\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nsource\n\n\nMeteoDataTest.data_compl_tidy\n\n MeteoDataTest.data_compl_tidy ()\n\n\nfd.data_compl_tidy\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_missing\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n      False\n    \n    \n      1\n      1.0\n      x0\n      0.021966\n      True\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n      False\n    \n    \n      3\n      3.0\n      x0\n      0.031551\n      True\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n      False\n    \n    \n      5\n      1.0\n      x1\n      0.295225\n      True\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n      False\n    \n    \n      7\n      3.0\n      x1\n      0.116207\n      True\n    \n    \n      8\n      0.0\n      x2\n      0.139568\n      True\n    \n    \n      9\n      1.0\n      x2\n      0.029323\n      True\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n      False\n    \n    \n      11\n      3.0\n      x2\n      0.160619\n      True"
  },
  {
    "objectID": "data_preparation.html#standard-scaler",
    "href": "data_preparation.html#standard-scaler",
    "title": "Data Preparation",
    "section": "Standard Scaler",
    "text": "Standard Scaler\nThe different variables in the can have pretty different values so we standardize so they are more comparable. Have numbers between 0 and 1 should also help with the computation accuracy.\nOne additional complexity is the need to backtransform not only the mean but also the standard deviation.\nSo we need a but of math\n\\[x_{norm} = \\frac{x - \\mu_x}{\\sigma_x}\\] then \\[x = x_{norm}\\sigma_x + \\mu_x \\]\nusing properties of Guassian distributions 1\n\\[p(x_{norm}) = \\mathcal{N}(\\mu_{norm}, \\sigma^2_{norm})\\]\n\\[p(x) = \\mathcal{N}(\\sigma_x\\mu_{norm} + \\mu_x, \\sigma^2_x \\sigma^2_{norm})\\]\n\nsource\n\nStandardScaler\n\n StandardScaler (x:torch.Tensor)\n\nInit normalizer by storing mean and std dev\n\n\n\n\nType\nDetails\n\n\n\n\nx\nTensor\nup to 2D Tensor\n\n\n\n\nx = torch.randn(20).reshape(-1,2)\nnorm = StandardScaler(x)\ntest_close(x, norm.inverse_transform(norm.transform(x)))\ntest_close(x.std(axis=0), norm.inverse_transform_std(norm.transform(x).std(axis=0)))\n\n\nnx = x.clone()\nnx[3] = torch.nan\nStandardScaler(nx).transform(nx)\n\ntensor([[ 1.0434, -0.7018],\n        [-0.3413, -0.2264],\n        [ 0.7216, -0.9768],\n        [    nan,     nan],\n        [ 0.3866,  0.1091],\n        [-1.5779,  0.1705],\n        [ 0.1036, -1.2577],\n        [-1.6543,  1.3312],\n        [ 0.7548,  1.7514],\n        [ 0.5635, -0.1993]])"
  },
  {
    "objectID": "data_preparation.html#log-transform",
    "href": "data_preparation.html#log-transform",
    "title": "Data Preparation",
    "section": "Log transform",
    "text": "Log transform\n\nsource\n\nMeteoDataTest.log_transform\n\n MeteoDataTest.log_transform (vars:Union[str,Collection[str]])\n\nTranform the given var with log(x+1)\n\n\n\n\nType\nDetails\n\n\n\n\nvars\nUnion\nlist of variables names to log-transform\n\n\nReturns\nMeteoDataTest\n\n\n\n\n\nMeteoDataTest.generate_gpfa(3, 4).log_transform(['x1']).data\n\n\n\n\n\n  \n    \n      \n      x0\n      x2\n      log_x1\n    \n  \n  \n    \n      0\n      -0.209523\n      0.137226\n      0.221309\n    \n    \n      1\n      0.308402\n      0.304317\n      0.229732\n    \n    \n      2\n      -0.079178\n      -0.082795\n      -0.302681\n    \n    \n      3\n      0.218236\n      0.400733\n      0.326986\n    \n  \n\n\n\n\n\norig_data = MeteoDataTest.generate_gpfa(1, 200)\n\norig_data.data = np.abs(orig_data.data)\n\ndata = orig_data.log_transform(['x0'])\n\n\ndata.data.hist()\n\narray([[<AxesSubplot: title={'center': 'log_x0'}>]], dtype=object)"
  },
  {
    "objectID": "data_preparation.html#export",
    "href": "data_preparation.html#export",
    "title": "Data Preparation",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "GPFA/gpfa.html",
    "href": "GPFA/gpfa.html",
    "title": "Gaussian Processes Factor Analysis",
    "section": "",
    "text": "Derivation of the equations to solve the Gaussian Processes Factor Analysis as described in: Yu, B.M., Cunningham, J.P., Santhanam, G., Ryu, S., Shenoy, K.V., Sahani, M., 2008. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity, in: Advances in Neural Information Processing Systems. Curran Associates, Inc."
  },
  {
    "objectID": "GPFA/gpfa.html#notation",
    "href": "GPFA/gpfa.html#notation",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Notation",
    "text": "Notation\n\n\\(T\\) Number of time steps\n\\(N\\) Number of variables observed\n\\(K\\) Number of dimensions of latent variable\n\\(x_{:,t}\\) vector of all the \\(N\\) variables at time \\(t\\), \\(\\in \\mathbb{R}^N\\)\n\\(x_{n,:}\\) vector of the \\(n\\)th variable at for time steps in \\(T\\), \\(\\in \\mathbb{R}^T\\)\n\\(x_{n,t}\\) \\(n\\)th variable at time \\(t\\), \\(\\in \\mathbb{R}\\)\n\\(X_M = [x_{:,1}, ... x_{:, T}]\\) Matrix with all the \\(N\\) variables at all time steps \\(T\\), \\(\\in \\mathbb{R}^{N \\times T}\\)\n\\(X\\) is a vector obtained by “flattening” \\(X_M\\), by putting next to each other all variable at time \\(t\\), \\(\\in \\mathbb{R}^{(N \\cdot T)}\\)\n\\(t\\) time step\n\\(z_{i, t}\\) \\(i\\)th latent variable at time \\(t\\), \\(\\in \\mathbb{R}\\)\n\\(Z = [z_1 , ... z_t]\\) Vector with \\(z\\) at all time steps in \\(T\\), \\(\\in \\mathbb{R}^{K \\times T}\\)"
  },
  {
    "objectID": "GPFA/gpfa.html#gaussian-processes-factor-analysis-model",
    "href": "GPFA/gpfa.html#gaussian-processes-factor-analysis-model",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Gaussian Processes Factor Analysis model",
    "text": "Gaussian Processes Factor Analysis model\nWe model the variables in this way \\[x_{:,t} = \\Lambda z_{:,t} + \\epsilon \\] where:\n\n\\(\\Lambda\\) is a Factor loading matrix that transforms \\(z_{:,t}\\) into \\(x_{:,t}\\), \\(\\in \\mathbb{R}^{N \\times K}\\)\n\\(\\epsilon\\) Random noise. The random noise is independent between the different time steps, \\(\\in \\mathbb{R}^N\\):\n\n\\(p(\\epsilon) = \\mathcal{N}(0, \\psi)\\) distribution of noise\n\\(\\psi\\), covariance matrix of noise, it is a diagional matrix, \\(\\in \\mathbb{R}^{N \\times N}\\)\n\n\nThe model consider \\(\\langle X \\rangle = 0\\) (if \\(X\\) doesn’t have a 0 mean it can be easily transformed by substracting the mean)\nThe latent variable \\(z\\) is modelled over time using a Gaussian Process, one process for each dimension \\(k\\) for simplicity we assumed that \\(z\\) has only one dimension (\\(k = 1\\))\n\\[p(Z) = \\mathcal{GP}(0, k(t, t \\prime))\\]"
  },
  {
    "objectID": "GPFA/gpfa.html#derivation-of-px",
    "href": "GPFA/gpfa.html#derivation-of-px",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Derivation of \\(p(X)\\)",
    "text": "Derivation of \\(p(X)\\)\n\\(p(x_{:,t}|z_{:,t}) = \\mathcal{N}(\\Lambda z_{:,t}, \\psi)\\) is easy to derive and then \\(p(x_{:,t})\\) and \\(p(z_{:,t}|x_{:,t})\\) can be obtained using the rules of Gaussian inference.\nHowever, what is interesting is to have the analytical form of \\(p(X)\\), which models both the relations between \\(z\\) and \\(x\\) and the \\(z\\) and \\(t\\). The likelihood of \\(p(X)\\) can then be maximized to obtain the parameters of the latent transformation and the kernel hyperparameter.\n\\(p(X)\\) is a Guassian distribution with \\(T\\cdot N\\) dimensions.\n\\(p(X) = \\mathcal{N}(\\langle X \\rangle, \\langle X X^T \\rangle)\\)\n\nDiagonal of the covariance matrix\nLet’s start with the diagonal of the covariance matrix (\\(t = t \\prime\\))\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t} + \\epsilon_{t})^T \\rangle\\)\nby multipling the two vectors together we obtain\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t}^T + \\epsilon_t \\Lambda^T z_{:,t}^T + \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThe using the linearity of the expectation we can:\n\ntransform the expecations of a sum into a sum of expecations\nmove the \\(\\Lambda\\) out of the expecation, as it doesn’t depend on \\(z\\)\n\\(\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle\\) because \\(z_{:,t}\\) and \\(\\epsilon_t\\) are independent random variables\n\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThen considering that \\(\\langle z_{:,t} \\rangle = 0\\) and that \\(\\langle \\epsilon_t \\rangle = 0\\) the expression can be simplified as:\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThen substituting:\n\n\\(\\langle z_{:,t} z_{:,t}^T\\rangle = k(t, t)\\) as that is the covariance matrix of the Gaussian process\n\\(\\langle \\epsilon_t \\epsilon_t^T \\rangle= \\psi\\)\n\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda k(t,t) \\Lambda^T + \\psi\\)\n\n\nOff-diagonal\nsimilar to the steps of above\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t \\prime} + \\epsilon_{t \\prime})^T \\rangle\\)\nby multipling the two vectors together we obtain\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t \\prime}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t \\prime}^T + \\epsilon_t \\Lambda^T z_{:,t \\prime}^T + \\epsilon_t \\epsilon_{t \\prime}^T \\rangle\\)\nThen using the linearity of the expectation we can:\n\ntransform the expecations of a sum into a sum of expecations\nmove the \\(\\Lambda\\) out of the expecatios, as it doesn’t depend on t\n\\(\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle\\) because \\(z_{:,t}\\) and \\(\\epsilon_t\\) are independent random variables\n\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThen considering that \\(\\langle z_{:,t} \\rangle = 0\\) and that \\(\\langle \\epsilon_t \\rangle = 0\\) the expression can be simplified as:\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t \\prime}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle\\)\nThen substituting: 1) \\(\\langle z_{:,t} z_{:,t \\prime}^T\\rangle = k(t,t \\prime)\\) as that is the covariance matrix of the Gaussian process 2) \\(\\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle= 0\\) as \\(\\epsilon_t\\) and \\(\\epsilon_{t \\prime}\\) are independent and \\(\\langle \\epsilon_t \\rangle = 0\\)\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T\\)\n\n\nResult\nThe equation for the diagonal and off-diagonal element can be summarized as:\n\\[\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi\\]\nwhere \\(\\delta(x) = \\begin{cases}  1 & if\\ x=0 \\\\  0 & if\\ x \\ne 0 \\\\  \\end{cases}\\)\nTherefore \\(p(X)\\) can be modelled as:\n\\[p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{cccc}\n    \\Lambda k(t_1,t_1) \\Lambda^T + \\delta(1-1)\\psi & \\Lambda k(t_{1},t_{2}) \\Lambda^T + \\delta(1-2)\\psi& \\cdots & \\Lambda k(t_1 ,t_t) \\Lambda^T + \\delta(1-t)\\psi\\\\\n    \\Lambda k(t_{2},t_{1}) \\Lambda^T+ \\delta(2-1)\\psi &  \\Lambda k(t_{2},t_{2}) \\Lambda^T + \\delta(2-2)\\psi & \\cdots & \\Lambda k(t_{2},t_{t}) \\Lambda^T + \\delta(2-t)\\psi\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    \\Lambda k(t_{t}, t_{1}) \\Lambda^T+ \\delta(t-1)\\psi & \\Lambda k(t_{t},t_{2}) \\Lambda^T + \\delta(t-2)\\psi& \\cdots & \\Lambda k(t_{t},t_{t}) \\Lambda^T + \\delta(t-t)\\psi\\\\\n    \\end{array} } \\right )\\]\nand this is also Gaussian Process with a “special” kernel. Multiplying kernel with a constant (\\(\\Lambda\\)) or adding a kernel (\\(\\delta\\)) yields another valid kernel\nIf we define a new kernel as \\[K(t,t \\prime) = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi\\]\nThen\n\\[ p(X) = \\mathcal{GP}(0, K(t, t\\prime))\\]\n\n\nLatent variable with more than one dimension\nIn order to have a latent variable with more than one-dimesions, we need to make small changes to the formula\nThe starting point is the covariance matrix of the latent at time \\(t\\), which is:\n\\[\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} \\langle z_{1:,t} z_{1:,t}^T\\rangle & \\cdots & \\langle z_{1:,t} z_{k:,t}^T\\rangle\\\\ \\vdots & \\ddots & \\vdots \\\\ \\langle z_{k:,t} z_{1:,t}^T \\rangle & \\cdots & \\langle z_{k:,t} z_{k:,t}^T\\rangle\\end{array}\\right)\\]\nsince each dimension in \\(z\\) is indipendent:\n\\(\\langle z_{k,t} z_{k\\prime,t} \\rangle = 0\\)\neach dimension in \\(z\\) is modelled using a different kernel (\\(k_{z_k}(t,t\\prime)\\)), hence:\n\\[\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} k_{z_1}(t, t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t)\\end{array}\\right)\\]\nso the GPFA Kernel is:\n\\[K(t,t \\prime) = \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t, t \\prime) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t \\prime)\\end{array}\\right) \\Lambda^T + \\delta(t - t \\prime)\\psi\\]\nso p(X) is:\n\\[p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{ccc}\n    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_t)\\psi\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_t)\\psi\\\\\n    \\end{array} } \\right )\\]"
  },
  {
    "objectID": "GPFA/gpfa.html#next-steps",
    "href": "GPFA/gpfa.html#next-steps",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Next steps",
    "text": "Next steps\n\nThe parameters of the final GP (\\(\\Lambda, \\psi\\) and the kernel hyperparameters) can be fitted by maximizing the likelihood of \\(p(X)\\) using gradient descent"
  },
  {
    "objectID": "GPFA/gpfa.html#kernel",
    "href": "GPFA/gpfa.html#kernel",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Kernel",
    "text": "Kernel\n\nclass GPFAKernel(gpytorch.kernels.Kernel):\n    \"\"\"\n    Kernel to implement Gaussian Processes Factor Analysis\n    \"\"\"\n    def __init__(self,\n                 n_features: int, # number of variables at each time step\n                 latent_kernel: gpytorch.kernels.Kernel, # func that returns any valid GPyTorch Kernel used to model the relationship over time of the latent\n                 latent_dims:int = 1,  # Number of latent dims\n                 Lambda: torch.tensor = None, #(n_features * latent_dims) initial value for factor loading matrix. If None init to one\n                 psi: torch.tensor = None, #(n_features) initial value for random noise covariance. Note this is only the diagonal matrix\n                 **kwargs):\n        super(GPFAKernel, self).__init__(**kwargs)\n        \n        # Number of features in the X for each time step\n        self.n_features = n_features\n\n        self.latent_dims = latent_dims\n        \n        # see GPyTorch Kernels\n        self.register_parameter(\n            name = \"Lambda\",\n            parameter = torch.nn.Parameter(torch.rand(self.n_features, self.latent_dims)))\n        \n        # each dim has it's own latent kernel\n        self.latent_kernels = torch.nn.ModuleList([latent_kernel() for _ in range(self.latent_dims)])\n        \n        self.register_parameter(\n            name = \"raw_psi_diag\",\n            parameter = torch.nn.Parameter(torch.zeros(self.n_features))) \n        self.register_constraint(\"raw_psi_diag\", gpytorch.constraints.Positive())\n        if psi is not None: self.psi = psi\n    \n    # Convenient getter and setter for psi, since there is the Positive() constraint\n    @property\n    def psi(self):\n        # when accessing the parameter, apply the constraint transform\n        return self.raw_psi_diag_constraint.transform(self.raw_psi_diag)\n\n    @psi.setter\n    def psi(self, value):\n        return self._set_psi(value)\n\n    def _set_psi(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_psi_diag)\n        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n        self.initialize(raw_psi_diag=self.raw_psi_diag_constraint.inverse_transform(value))\n    \n\n        \n    def forward(self, t1, t2, diag = False, last_dim_is_batch=False, **params):\n\n        # not implemented yet\n        assert diag is False\n        assert last_dim_is_batch is False\n\n        # take the number of observations from the input\n        n_obs = t1.shape[0]\n\n        # compute the latent kernel\n        kT = torch.stack([ kernel(t1, t2, diag, last_dim_is_batch, **params).evaluate() # this may make the whole thing slow as it breaks lazy evaluations\n                         for kernel in self.latent_kernels], dim=2)\n        return compute_gpfa_covariance(self.Lambda, kT, self.psi, self.n_features, n_obs)\n    \n    def num_outputs_per_input(self, x1,x2):\n        return self.n_features\n\n# this is a separate function, because torch script cannot take self as a parameter\n@torch.jit.script\ndef compute_gpfa_covariance(Lambda, kT, psi, n_features, n_obs):\n    # pre allocate covariance matrix\n    X_cov = torch.empty(n_features * n_obs, n_features * n_obs, device=Lambda.device)\n    for i in torch.arange(n_obs):\n        for j in torch.arange(n_obs):\n            # i:i+1 is required to keep the number of dimensions\n            cov =  Lambda @ torch.diag(kT[i,j,:]) @ Lambda.T\n            # only diagonals add the noise\n            if i == j: cov += torch.diag(psi)\n            # add a block of size n_features*n_features to the covariance matrix\n            X_cov[i*n_features:(i*n_features + n_features),j*n_features:(j*n_features+n_features)] = cov\n    return X_cov\n\n\n\nScriptFunction object at 0x7fb22faceca0>\n\nsource\n\n\nGPFAKernel\n\n GPFAKernel (n_features:int, latent_kernel:gpytorch.kernels.kernel.Kernel,\n             latent_dims:int=1, Lambda:<built-\n             inmethodtensoroftypeobjectat0x7fb2a04dd460>=None, psi:<built-\n             inmethodtensoroftypeobjectat0x7fb2a04dd460>=None, **kwargs)\n\nKernel to implement Gaussian Processes Factor Analysis\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nint\n\nnumber of variables at each time step\n\n\nlatent_kernel\nKernel\n\nfunc that returns any valid GPyTorch Kernel used to model the relationship over time of the latent\n\n\nlatent_dims\nint\n1\nNumber of latent dims\n\n\nLambda\ntensor\nNone\n(n_features * latent_dims) initial value for factor loading matrix. If None init to one\n\n\npsi\ntensor\nNone\n(n_features) initial value for random noise covariance. Note this is only the diagonal matrix\n\n\nkwargs\n\n\n\n\n\n\n\ngpfa_k = GPFAKernel(n_features=2, latent_kernel=gpytorch.kernels.RBFKernel)\n\nThe parameters are correctly registered\n\nlist(gpfa_k.named_parameters())\n\n[('Lambda',\n  Parameter containing:\n  tensor([[0.5982],\n          [0.9041]], requires_grad=True)),\n ('raw_psi_diag',\n  Parameter containing:\n  tensor([0., 0.], requires_grad=True)),\n ('latent_kernels.0.raw_lengthscale',\n  Parameter containing:\n  tensor([[0.]], requires_grad=True))]\n\n\nCheck that the Kernel is running\n\ngpfa_k(torch.tensor((1, 2, 3))).evaluate()\n\ntensor([[1.0510, 0.5409, 0.1264, 0.1910, 0.0056, 0.0084],\n        [0.5409, 1.5106, 0.1910, 0.2887, 0.0084, 0.0127],\n        [0.1264, 0.1910, 1.0510, 0.5409, 0.1264, 0.1910],\n        [0.1910, 0.2887, 0.5409, 1.5106, 0.1910, 0.2887],\n        [0.0056, 0.0084, 0.1264, 0.1910, 1.0510, 0.5409],\n        [0.0084, 0.0127, 0.1910, 0.2887, 0.5409, 1.5106]],\n       grad_fn=<CopySlices>)"
  },
  {
    "objectID": "GPFA/gpfa.html#gpfa",
    "href": "GPFA/gpfa.html#gpfa",
    "title": "Gaussian Processes Factor Analysis",
    "section": "GPFA",
    "text": "GPFA\n\nsource\n\nGPFAZeroMean\n\n GPFAZeroMean (n_features, device)\n\nZero Mean function to be used in GPFA, as it takes into account the number of features\nto change the latent_kernel you should subcall GPFA in this way the get_info function for the kernel can be changed to include the latent kernel details\n\nsource\n\n\nGPFA\n\n GPFA (train_x, train_y, likelihood, n_features, latent_dims=1)\n\nThe base class for any Gaussian process latent function to be used in conjunction with exact inference.\n:param torch.Tensor train_inputs: (size n x d) The training features :math:\\mathbf X. :param torch.Tensor train_targets: (size n) The training targets :math:\\mathbf y. :param ~gpytorch.likelihoods.GaussianLikelihood likelihood: The Gaussian likelihood that defines the observational distribution. Since we’re using exact inference, the likelihood must be Gaussian.\nThe :meth:forward function should describe how to compute the prior latent distribution on a given input. Typically, this will involve a mean and kernel function. The result must be a :obj:~gpytorch.distributions.MultivariateNormal.\nCalling this model will return the posterior of the latent Gaussian process when conditioned on the training data. The output will be a :obj:~gpytorch.distributions.MultivariateNormal.\nExample: >>> class MyGP(gpytorch.models.ExactGP): >>> def init(self, train_x, train_y, likelihood): >>> super().__init__(train_x, train_y, likelihood) >>> self.mean_module = gpytorch.means.ZeroMean() >>> self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) >>> >>> def forward(self, x): >>> mean = self.mean_module(x) >>> covar = self.covar_module(x) >>> return gpytorch.distributions.MultivariateNormal(mean, covar) >>> >>> # train_x = …; train_y = … >>> likelihood = gpytorch.likelihoods.GaussianLikelihood() >>> model = MyGP(train_x, train_y, likelihood) >>> >>> # test_x = …; >>> model(test_x) # Returns the GP latent function at test_x >>> likelihood(model(test_x)) # Returns the (approximate) predictive posterior distribution at test_x\nmake some very simple test data, to check that the model is working and can learn the parameters\n\nT = torch.arange(1,5)\n\n\nX = torch.hstack([torch.arange(0,3) + 2* i for i in T])\n\n\nX\n\ntensor([ 2,  3,  4,  4,  5,  6,  6,  7,  8,  8,  9, 10])\n\n\n\nX.shape\n\ntorch.Size([12])\n\n\n\nT\n\ntensor([1, 2, 3, 4])\n\n\n\n# initialize likelihood and model\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = GPFA(T, X, likelihood, n_features = 3)\n\n\nmodel\n\nGPFA(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\nGetting the prior from the GP\n\nmodel(T)\n\nMultivariateNormal(loc: torch.Size([12]))\n\n\nFitting the parameters using gradient descend\n\n# this is for running the notebook in our testing framework\ntraining_iter = 10\n\n# Find optimal model hyperparameters\nmodel.train()\nlikelihood.train()\n\n# Use the adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n\n# \"Loss\" for GPs - the marginal log likelihood\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\nlosses = []\nfor i in range(training_iter):\n    # Zero gradients from previous iteration\n    optimizer.zero_grad()\n    # Output from model\n    output = model(T)\n    # Calc loss and backprop gradients\n    loss = -mll(output, X)\n    losses.append(loss.item())\n    loss.backward()\n    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f, Lambda: %.3f   noise: %.3f' % (\n        i + 1, training_iter, loss.item(),\n        model.covar_module.latent_kernels[0].lengthscale.item(),\n        model.covar_module.Lambda.mean().item(),\n        model.likelihood.noise.item()\n    ))\n    optimizer.step()\n\nIter 1/10 - Loss: 10.056   lengthscale: 0.693, Lambda: 0.591   noise: 0.693\nIter 2/10 - Loss: 8.484   lengthscale: 0.744, Lambda: 0.691   noise: 0.744\nIter 3/10 - Loss: 7.256   lengthscale: 0.798, Lambda: 0.789   noise: 0.797\nIter 4/10 - Loss: 6.304   lengthscale: 0.853, Lambda: 0.884   noise: 0.850\nIter 5/10 - Loss: 5.564   lengthscale: 0.910, Lambda: 0.975   noise: 0.904\nIter 6/10 - Loss: 4.984   lengthscale: 0.968, Lambda: 1.061   noise: 0.956\nIter 7/10 - Loss: 4.526   lengthscale: 1.027, Lambda: 1.141   noise: 1.008\nIter 8/10 - Loss: 4.159   lengthscale: 1.086, Lambda: 1.216   noise: 1.058\nIter 9/10 - Loss: 3.863   lengthscale: 1.145, Lambda: 1.286   noise: 1.106\nIter 10/10 - Loss: 3.622   lengthscale: 1.203, Lambda: 1.350   noise: 1.152\n\n\nThe model is training!"
  },
  {
    "objectID": "GPFA/gpfa.html#multi-dimensional-latent-variable",
    "href": "GPFA/gpfa.html#multi-dimensional-latent-variable",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Multi-dimensional latent variable",
    "text": "Multi-dimensional latent variable\n\n2 dimensions\n\n# initialize likelihood and model\nlikelihood_m = gpytorch.likelihoods.GaussianLikelihood()\nmodel_m = GPFA(T, X, likelihood, n_features = 3, latent_dims=2)\n\n\nmodel_m\n\nGPFA(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (1): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\ncheck GP is running\n\nmodel_m.covar_module.latent_kernels\n\nModuleList(\n  (0): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (1): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.8592, 0.4173],\n        [0.0988, 0.0617],\n        [0.4201, 0.6319]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))\n\n\n\n\n5 dimensions\n\n# initialize likelihood and model\nlikelihood_m = gpytorch.likelihoods.GaussianLikelihood()\nmodel_m = GPFA(T, X, likelihood, n_features = 3, latent_dims=5)\n\n\nmodel_m\n\nGPFA(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (1): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (2): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (3): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (4): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\ncheck GP is running\n\nmodel_m.covar_module.latent_kernels\n\nModuleList(\n  (0): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (1): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (2): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (3): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (4): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.2383, 0.0719, 0.9205, 0.6215, 0.9508],\n        [0.5192, 0.2102, 0.9280, 0.0948, 0.4027],\n        [0.4778, 0.7141, 0.3596, 0.9803, 0.4140]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))"
  },
  {
    "objectID": "GPFA/gpfa.html#get-info",
    "href": "GPFA/gpfa.html#get-info",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Get info",
    "text": "Get info\nthis is to return the kernel info for printing, both during training and for the final results\nThe general api is:\nget_info(var_names=None) -> dict[str, pd.DataFrame] where the string the title of a kernel parameter\n\n@patch\ndef get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -> dict[str, pd.DataFrame]:\n    \"Model info for a GPFA with a RBFKernel\"\n    out = {}\n    \n    latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n    \n    out[\"Lambda\"] = pd.concat([\n        None if var_names is None else pd.Series(var_names, name='variable'),\n        pd.DataFrame(\n            self.covar_module.Lambda.detach().cpu().numpy(),\n            columns=latent_names)],\n        axis=1)\n    \n    ls = [self.covar_module.latent_kernels[i].lengthscale.detach().item() for i in range(self.covar_module.latent_dims)]\n    out[\"lengthscale\"] = pd.DataFrame({\n        'latent': latent_names,\n        'lengthscale': ls\n    })\n    \n    psi = self.covar_module.psi.detach().cpu().numpy()\n    out[\"psi\"] = pd.DataFrame({\n        'variable': var_names,\n        'psi': psi \n    })\n    \n    out[\"likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.detach().item()]})\n    \n    return out\n\n\nsource\n\nGPFA.get_info\n\n GPFA.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nmodel_m.get_info()\n\n{'Lambda':          z0        z1        z2        z3        z4\n 0  0.238300  0.071901  0.920541  0.621538  0.950799\n 1  0.519228  0.210204  0.928017  0.094791  0.402726\n 2  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0     None  0.693147\n 1     None  0.693147\n 2     None  0.693147,\n 'likelihood':       noise\n 0  1.195812}\n\n\n\nmodel_m.get_info([\"a\", \"b\", \"c\"])\n\n{'Lambda':   variable        z0        z1        z2        z3        z4\n 0        a  0.238300  0.071901  0.920541  0.621538  0.950799\n 1        b  0.519228  0.210204  0.928017  0.094791  0.402726\n 2        c  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0        a  0.693147\n 1        b  0.693147\n 2        c  0.693147,\n 'likelihood':       noise\n 0  1.195812}"
  },
  {
    "objectID": "GPFA/gpfa.html#export",
    "href": "GPFA/gpfa.html#export",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Export",
    "text": "Export\n\nmodel_m.covar_module.latent_kernels\n\nModuleList(\n  (0): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (1): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (2): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (3): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (4): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.2383, 0.0719, 0.9205, 0.6215, 0.9508],\n        [0.5192, 0.2102, 0.9280, 0.0948, 0.4027],\n        [0.4778, 0.7141, 0.3596, 0.9803, 0.4140]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))"
  },
  {
    "objectID": "GPFA/gpfa.html#get-info-1",
    "href": "GPFA/gpfa.html#get-info-1",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Get info",
    "text": "Get info\nthis is to return the kernel info for printing, both during training and for the final results\nThe general api is:\nget_info(var_names=None) -> dict[str, pd.DataFrame] where the string the title of a kernel parameter\n\n@patch\ndef get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -> dict[str, pd.DataFrame]:\n    \"Model info for a GPFA with a RBFKernel\"\n    out = {}\n    \n    latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n    \n    out[\"Lambda\"] = pd.concat([\n        None if var_names is None else pd.Series(var_names, name='variable'),\n        pd.DataFrame(\n            self.covar_module.Lambda.detach().cpu().numpy(),\n            columns=latent_names)],\n        axis=1)\n    \n    ls = [self.covar_module.latent_kernels[i].lengthscale.detach().item() for i in range(self.covar_module.latent_dims)]\n    out[\"lengthscale\"] = pd.DataFrame({\n        'latent': latent_names,\n        'lengthscale': ls\n    })\n    \n    psi = self.covar_module.psi.detach().cpu().numpy()\n    out[\"psi\"] = pd.DataFrame({\n        'variable': var_names,\n        'psi': psi \n    })\n    \n    out[\"likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.detach().item()]})\n    \n    return out\n\n\nsource\n\nGPFA.get_info\n\n GPFA.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nmodel_m.get_info()\n\n{'Lambda':          z0        z1        z2        z3        z4\n 0  0.238300  0.071901  0.920541  0.621538  0.950799\n 1  0.519228  0.210204  0.928017  0.094791  0.402726\n 2  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0     None  0.693147\n 1     None  0.693147\n 2     None  0.693147,\n 'likelihood':       noise\n 0  1.195812}\n\n\n\nmodel_m.get_info([\"a\", \"b\", \"c\"])\n\n{'Lambda':   variable        z0        z1        z2        z3        z4\n 0        a  0.238300  0.071901  0.920541  0.621538  0.950799\n 1        b  0.519228  0.210204  0.928017  0.094791  0.402726\n 2        c  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0        a  0.693147\n 1        b  0.693147\n 2        c  0.693147,\n 'likelihood':       noise\n 0  1.195812}\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.2383, 0.0719, 0.9205, 0.6215, 0.9508],\n        [0.5192, 0.2102, 0.9280, 0.0948, 0.4027],\n        [0.4778, 0.7141, 0.3596, 0.9803, 0.4140]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))"
  },
  {
    "objectID": "GPFA/gpfa.html#get-info-2",
    "href": "GPFA/gpfa.html#get-info-2",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Get info",
    "text": "Get info\nthis is to return the kernel info for printing, both during training and for the final results\nThe general api is:\nget_info(var_names=None) -> dict[str, pd.DataFrame] where the string the title of a kernel parameter\n\n@patch\ndef get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -> dict[str, pd.DataFrame]:\n    \"Model info for a GPFA with a RBFKernel\"\n    out = {}\n    \n    latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n    \n    out[\"Lambda\"] = pd.concat([\n        None if var_names is None else pd.Series(var_names, name='variable'),\n        pd.DataFrame(\n            self.covar_module.Lambda.detach().cpu().numpy(),\n            columns=latent_names)],\n        axis=1)\n    \n    ls = [self.covar_module.latent_kernels[i].lengthscale.detach().item() for i in range(self.covar_module.latent_dims)]\n    out[\"lengthscale\"] = pd.DataFrame({\n        'latent': latent_names,\n        'lengthscale': ls\n    })\n    \n    psi = self.covar_module.psi.detach().cpu().numpy()\n    out[\"psi\"] = pd.DataFrame({\n        'variable': var_names,\n        'psi': psi \n    })\n    \n    out[\"likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.detach().item()]})\n    \n    return out\n\n\nsource\n\nGPFA.get_info\n\n GPFA.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nmodel_m.get_info()\n\n{'Lambda':          z0        z1        z2        z3        z4\n 0  0.238300  0.071901  0.920541  0.621538  0.950799\n 1  0.519228  0.210204  0.928017  0.094791  0.402726\n 2  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0     None  0.693147\n 1     None  0.693147\n 2     None  0.693147,\n 'likelihood':       noise\n 0  1.195812}\n\n\n\nmodel_m.get_info([\"a\", \"b\", \"c\"])\n\n{'Lambda':   variable        z0        z1        z2        z3        z4\n 0        a  0.238300  0.071901  0.920541  0.621538  0.950799\n 1        b  0.519228  0.210204  0.928017  0.094791  0.402726\n 2        c  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0        a  0.693147\n 1        b  0.693147\n 2        c  0.693147,\n 'likelihood':       noise\n 0  1.195812}"
  },
  {
    "objectID": "GPFA/gpfa.html#export-1",
    "href": "GPFA/gpfa.html#export-1",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "GPFA/learner.html",
    "href": "GPFA/learner.html",
    "title": "GPFA Learner",
    "section": "",
    "text": "from fastcore.test import *\nThe first thing that we need is a Learner object to keep track of:\nand that has methods to help with:\nThe first thing we need is a training loop, just wrap in a function the example one from GPyTorch"
  },
  {
    "objectID": "GPFA/learner.html#learner",
    "href": "GPFA/learner.html#learner",
    "title": "GPFA Learner",
    "section": "Learner",
    "text": "Learner\n\nsource\n\nGPFALearner\n\n GPFALearner (X:torch.Tensor, T:torch.Tensor=None, latent_dims:int=1,\n              model=<class 'meteo_imp.gpfa.gpfa.GPFA'>, var_names=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nTensor\n\n(n_features * n_obs) Multivariate time series\n\n\nT\nTensor\nNone\n(n_obs) Vector of time of observations.\n\n\nlatent_dims\nint\n1\nNumber of latent variables in GPFA\n\n\nmodel\ntype\nGPFA\nsub-class of GPFA\n\n\nvar_names\nNoneType\nNone\nfor model info\n\n\n\n\n# test data\nT = torch.arange(0,6)\n\nX = torch.vstack([(torch.arange(0,3, dtype=torch.float32) + 2 + i) * i for i in T])\n\n\nX\n\ntensor([[ 0.,  0.,  0.],\n        [ 3.,  4.,  5.],\n        [ 8., 10., 12.],\n        [15., 18., 21.],\n        [24., 28., 32.],\n        [35., 40., 45.]])\n\n\n\n# l for learner\nl = GPFALearner(X)\n\n\ntest_eq(T, l.T)\n\n\n# with explicit time\ntest_eq(T, GPFALearner(X, T).T)\n\n\ntest_eq(l.n_features, 3)\n\n\nl.X\n\ntensor([-1.0590, -1.0955, -1.1236, -0.8347, -0.8326, -0.8305, -0.4610, -0.4382,\n        -0.4201,  0.0623,  0.0876,  0.1075,  0.7350,  0.7449,  0.7523,  1.5573,\n         1.5337,  1.5145])\n\n\n\nl.train()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\ncheck that can train for more than once\n\nl.train()\n\n\n\n\n\nl.losses\n\ntensor([ 1.3691,  1.3265,  1.2867,  1.2495,  1.2107,  1.1702,  1.1304,  1.0929,\n         1.0566,  1.0200,  0.9822,  0.9434,  0.9039,  0.8642,  0.8246,  0.7845,\n         0.7437,  0.7019,  0.6595,  0.6170,  0.5749,  0.5328,  0.4905,  0.4481,\n         0.4058,  0.3636,  0.3214,  0.2793,  0.2370,  0.1943,  0.1520,  0.1097,\n         0.0667,  0.0243, -0.0181, -0.0608, -0.1031, -0.1452, -0.1874, -0.2293,\n        -0.2710, -0.3128, -0.3539, -0.3953, -0.4360, -0.4769, -0.5173, -0.5577,\n        -0.5974, -0.6373, -0.6766, -0.7156, -0.7544, -0.7927, -0.8304, -0.8673,\n        -0.9023, -0.9291, -0.9688, -1.0017, -1.0382, -1.0711, -1.1033, -1.1396,\n        -1.1642, -1.2035, -1.2218, -1.2612, -1.2789, -1.3111, -1.3358, -1.3537,\n        -1.3841, -1.3909, -1.3967, -1.4299, -1.4527, -1.4624, -1.4806, -1.4976,\n        -1.5096, -1.5176, -1.5356, -1.5441, -1.5443, -1.5641, -1.5724, -1.5653,\n        -1.5789, -1.5920, -1.5886, -1.5960, -1.6030, -1.5963, -1.6003, -1.6098,\n        -1.6074, -1.6175, -1.6281, -1.6264, -1.6340,  0.7754, -1.4940, -1.1752,\n        -0.3677, -0.6071, -1.2489, -1.6029, -1.5123, -1.2290, -1.0733, -1.1438,\n        -1.3421, -1.5151, -1.5719, -1.5156, -1.4143, -1.3454, -1.3485, -1.4113,\n        -1.4900, -1.5405, -1.5443, -1.5109, -1.4704, -1.4521, -1.4677, -1.5063,\n        -1.5450, -1.5630, -1.5564, -1.5371, -1.5254, -1.5339, -1.5587, -1.5839,\n        -1.5945, -1.5893, -1.5795, -1.5800, -1.5942, -1.6127, -1.6220, -1.6192,\n        -1.6136, -1.6172, -1.6295, -1.6398, -1.6410, -1.6373, -1.6382, -1.6460,\n        -1.6539, -1.6547, -1.6524, -1.6549, -1.6613, -1.6652, -1.6646, -1.6646,\n        -1.6685, -1.6730, -1.6738, -1.6737, -1.6759, -1.6800, -1.6818, -1.6816,\n        -1.6829, -1.6861, -1.6876, -1.6880, -1.6895, -1.6909, -1.6925, -1.6931,\n        -1.6938, -1.6957, -1.6964, -1.6974, -1.6982, -1.6985, -1.6993, -1.7003,\n        -1.7012, -1.7019, -1.7021, -1.7030, -1.7040, -1.7040, -1.7047, -1.7052,\n        -1.7061, -1.7067, -1.7071, -1.7073, -1.7083, -1.7082, -1.7087, -1.7090])"
  },
  {
    "objectID": "GPFA/learner.html#predictions",
    "href": "GPFA/learner.html#predictions",
    "title": "GPFA Learner",
    "section": "Predictions",
    "text": "Predictions\nadd a function to get predictions from the model\n\nsource\n\nGPFALearner.predict_raw\n\n GPFALearner.predict_raw (T)\n\n\nraw_out = l.predict_raw(T)\nraw_out\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n  warnings.warn(\n\n\nMultivariateNormal(loc: torch.Size([18]))\n\n\nthe model prediction is a distribution with len(T)*n_features dimensions\nwhich is in the in the wrong shape and need to be rescaled after the normalization\nAlso we don’t need th full distribution but only the mean and stddev for each variable at every time step\nAnd we can “fix” the shape by transforming back to a matrix\n\nraw_stddev = raw_out.stddev.reshape(-1, l.n_features)\nraw_mean = raw_out.mean.reshape(-1, l.n_features)\n\n\nraw_stddev\n\ntensor([[0.0284, 0.0148, 0.0193],\n        [0.0279, 0.0138, 0.0186],\n        [0.0277, 0.0134, 0.0183],\n        [0.0277, 0.0134, 0.0183],\n        [0.0279, 0.0138, 0.0186],\n        [0.0284, 0.0147, 0.0193]], grad_fn=<ReshapeAliasBackward0>)\n\n\nThis function transforms the raw output of the Gaussian Process (p(X)) into a prediction that can be used. need to do two things:\n\ntake mean and stddev (this is the diagonal of the covariance matrix) for each variable\nreshape so that each row is a time step and each column a variable\nreverse the normalization\n\nthe mean and the std are passed individually because in the conditional predictions is not possible to have the whole covariance matrix (and thus a MultiNormal) but the mean and stddev are enough.\n\nsource\n\n\nGPFALearner.prediction_from_raw\n\n GPFALearner.prediction_from_raw (raw_mean, raw_std)\n\nTakes a raw prediction and produces and final prediction, by reshaping and reversing normalization\n\n# TODO document this function better\n\n\n@patch\ndef predict(self: GPFALearner, T):\n    pred_raw = self.predict_raw(T)\n    return self.prediction_from_raw(pred_raw.mean, pred_raw.stddev)\n\n\nl.predict(T)\n\nNormalParameters(mean=tensor([[-0.5417, -0.0635,  0.4033],\n        [ 3.0714,  4.0467,  5.0139],\n        [ 8.2941,  9.9889, 11.6774],\n        [15.3474, 18.0108, 20.6732],\n        [24.2270, 28.1071, 31.9975],\n        [34.6030, 39.9107, 45.2332]]), std=tensor([[0.3796, 0.2245, 0.3295],\n        [0.3730, 0.2095, 0.3169],\n        [0.3705, 0.2033, 0.3116],\n        [0.3703, 0.2033, 0.3113],\n        [0.3729, 0.2092, 0.3166],\n        [0.3797, 0.2243, 0.3295]]))\n\n\n\npred = l.predict(T)\n\n\npred.mean.shape\n\ntorch.Size([6, 3])\n\n\n\npred.std.shape\n\ntorch.Size([6, 3])\n\n\n\n\nCheck learning is working\nThe idea is to use the current model to generate a dataset, that can be for sure modelled using a GPFA (because is the output of GPFA) and then train another model and see if the parameters converge\n\n# create a dummy GPFA with 3 features\nLt = GPFALearner(X)\n\n\ntest_params = {\n   \"Lambda\": torch.tensor([-1, 0.3, .8]).reshape(Lt.n_features, -1),\n   \"psi\": torch.tensor([1e-5, 5e-5, 2e-5]),\n}\n\n\nLt.model.covar_module.initialize(**test_params)\n\nGPFAKernel(\n  (latent_kernels): ModuleList(\n    (0): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n  )\n  (raw_psi_diag_constraint): Positive()\n)\n\n\n\nLt.model.covar_module.latent_kernels[0].initialize(lengthscale = torch.tensor(5))\n\nRBFKernel(\n  (raw_lengthscale_constraint): Positive()\n)\n\n\n\ntarget_X = Lt.predict(T).mean\n\n\nl2 = GPFALearner(target_X)\n\n\nl2.train()\n\n\n\n\n\nl2.predict(T).mean - target_X\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n  warnings.warn(\n\n\ntensor([[ 0.0058,  0.0006,  0.0034],\n        [ 0.0033,  0.0005,  0.0025],\n        [ 0.0011,  0.0002,  0.0008],\n        [-0.0011, -0.0002, -0.0010],\n        [-0.0034, -0.0005, -0.0025],\n        [-0.0056, -0.0006, -0.0033]])\n\n\nthey seems pretty small numbers, so the model is working!\n\nprint(\"Lambda:\\n\", l2.model.covar_module.Lambda.detach())\n\nprint(\"psi: \", l2.model.covar_module.psi.detach())\n\nprint(\"lengthscale:\", l2.model.covar_module.latent_kernels[0].lengthscale.item())\n\nLambda:\n tensor([[-1.6212],\n        [ 1.6023],\n        [ 1.5982]])\npsi:  tensor([4.9890e-05, 3.7080e-05, 3.7402e-05])\nlengthscale: 5.521887302398682\n\n\n\n\nConditional Predictions\nThis add the supports for conditional predictions, which means that at the time (t) when we are making the predictions some of the variables have been actually observed. Since the model prediction is a normal distribution we can condition on the observed values and thus improve the predictions.\nTherefore we need to compute the conditional distribution of a normal 1\n\\[ X = \\left[\\begin{array}{c} x \\\\ o \\end{array} \\right] \\]\n\\[ p(X) = N\\left(\\left[ \\begin{array}{c} \\mu_x \\\\ \\mu_o \\end{array} \\right], \\left[\\begin{array}{cc} \\Sigma_{xx} & \\Sigma_{xo} \\\\ \\Sigma_{ox} & \\Sigma_{oo} \\end{array} \\right]\\right)\\]\nwhere \\(X\\) is a vector of variable that need to predicted and \\(o\\) is a vector of the variables that have been observed\nThe mean is in “flat format”, where all the features from one time step are next to each other followed by the features of the next time step.\nthen\n\\[p(x|o) = N(\\mu_x + \\Sigma_{xo}\\Sigma_{oo}^{-1}(o - \\mu_o), \\Sigma_{xx} - \\Sigma_{xo}\\Sigma_{oo}^{-1}\\Sigma_{ox})\\]\n\n# example distribution with only 2 variables\nμ = torch.tensor([.5, 1.])\nΣ = torch.tensor([[1., .5], [.5 ,1.]])\n\ngauss = MultivariateNormal(μ, Σ)\n\nidx = torch.tensor([True, False]) # second variable is the observed one\n\nobs = torch.tensor(5.) # value of second variable\n\ngauss_cond = conditional_guassian(gauss, obs, idx)\n\n# hardcoded values to test that the code is working, see also for alternative implementation https://python.quantecon.org/multivariate_normal.html\ntest_close(3.25, gauss_cond.mean.item())\ntest_close(.75, gauss_cond.covariance_matrix.item())\n\nTest with multiple variables?\noverwrite the predict method to add support for conditional predictions\nNeed to have the mean and std for both the conditional predictions and the observations, with the same shape and order of the complete prediction.\n\ngauss_cond.covariance_matrix\n\ntensor([[0.7500]])\n\n\n\nmerge_pred = _merge_raw_cond_pred(gauss, gauss_cond, obs, idx)\nmerge_pred\n\nNormalParameters(mean=tensor([5.0000, 3.2500]), std=tensor([0.0000, 0.8660]))\n\n\n\n# manually calculated\ntest_close(merge_pred.mean, torch.tensor([5., 3.25]))\ntest_close(merge_pred.std, torch.tensor([0., math.sqrt(.75)]))\n\nThe problem is that the mean and the std for normalization are different for each feature, so in order to have the normalization working it is necessary to give the observations as a 2D array and not like a 1D array (like required by the model)\n\nT_pred = torch.tensor([6, 7])\n\n\nl.predict(T_pred)\n\nNormalParameters(mean=tensor([[45.8037, 52.6492, 59.5189],\n        [56.8583, 65.2222, 73.6200]]), std=tensor([[0.6374, 0.6243, 0.7313],\n        [1.5561, 1.7313, 1.9531]]))\n\n\n\nidx = torch.zeros(T_pred.shape[0] * X.shape[1], dtype=torch.bool)\n# simulate an observation using sensible numbers from the prediction\nidx[[0,2]] = torch.tensor([True, True])\nobs = torch.tensor([42., 61.])\n\n\nl._standard_obs(obs, idx)\n\ntensor([2.0806, 2.4525])\n\n\n\nsource\n\n\nGPFALearner.predict\n\n GPFALearner.predict (T:torch.Tensor, obs:torch.Tensor=None,\n                      idx:torch.Tensor=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nTensor\n\n(n_pred) time where prediction is needed\n\n\nobs\nTensor\nNone\n(n_obs_pred) Optional - if at the times of the prediction there are some observationsarray with the values of observations to condition distribution\n\n\nidx\nTensor\nNone\n((n_pred*n_features)) Optional - necessary if obs are presentBoolean array that is True where an observation is present and False where a prediction is neededThis is a 1D array with the length equal to n_pred (number time steps to predict) times n_features\n\n\n\n\nl._standard_obs(obs, idx)\n\ntensor([2.0806, 2.4525])\n\n\n\nobs\n\ntensor([42., 61.])\n\n\n\nl.predict_raw(T_pred).mean\n\ntensor([2.3649, 2.3651, 2.3656, 3.1912, 3.1915, 3.1923],\n       grad_fn=<ViewBackward0>)\n\n\n\nl.predict(T_pred, obs, idx)\n\nNormalParameters(mean=tensor([[42.0000, 52.3809, 61.0000],\n        [56.2001, 64.4710, 72.7773]]), std=tensor([[0.0000, 0.2729, 0.0000],\n        [0.7405, 0.7571, 0.8751]]))\n\n\n\nl.predict(T_pred)\n\nNormalParameters(mean=tensor([[45.8037, 52.6492, 59.5189],\n        [56.8583, 65.2222, 73.6200]]), std=tensor([[0.6374, 0.6243, 0.7313],\n        [1.5561, 1.7313, 1.9531]]))\n\n\nThere is a small change in the predicted values after conditioning as you would expect"
  },
  {
    "objectID": "GPFA/learner.html#gpu-support",
    "href": "GPFA/learner.html#gpu-support",
    "title": "GPFA Learner",
    "section": "GPU Support",
    "text": "GPU Support\nadd support for CUDA to model\n\n# l for learner\nl_cuda = GPFALearner(X.cuda())\n\nAttrs of interest are:\n\nT\nX\nlikelihood\nmodel\nnorm\n\ncuda() modifies in place the tensors and the modules!\n\nl_cuda.T.cuda()\n\ntensor([0, 1, 2, 3, 4, 5], device='cuda:0')\n\n\n\nl_cuda.T.device\n\ndevice(type='cuda', index=0)\n\n\n\n@patch\ndef cuda(self: GPFALearner):\n    \"\"\"Moves all learner to gpu\"\"\"\n    for par in ['T', 'X', 'model', 'likelihood']:\n        self.__getattribute__(par).cuda()\n    self.norm.x_mean.cuda()\n    self.norm.x_std.cuda()\n\n\nsource\n\nGPFALearner.cuda\n\n GPFALearner.cuda ()\n\nMoves all learner to gpu\n\nl_cuda.cuda()\n\nparameters are on the gpu!\n\nnext(l_cuda.likelihood.parameters()).device\n\ndevice(type='cuda', index=0)\n\n\n\ngpytorch.distributions.MultivariateNormal(torch.zeros(1).cuda(), torch.ones(1, 1).cuda())\n\nMultivariateNormal(loc: tensor([0.], device='cuda:0'), covariance_matrix: tensor([[1.]], device='cuda:0'))\n\n\n\nl_cuda.model.covar_module.latent_kernels[0].lengthscale.device\n\ndevice(type='cuda', index=0)\n\n\n\nl_cuda.train()"
  },
  {
    "objectID": "GPFA/learner.html#multi-dimensional-latent",
    "href": "GPFA/learner.html#multi-dimensional-latent",
    "title": "GPFA Learner",
    "section": "Multi-dimensional latent",
    "text": "Multi-dimensional latent\n\nl2 = GPFALearner(X, T, latent_dims=2)\n\n\nl2.train()\n\n\n\n\n\nplt.plot(l2.losses)\n\n\n\n\n\nl2.model.covar_module.Lambda\n\nParameter containing:\ntensor([[1.6626, 1.0702],\n        [1.5197, 1.1574],\n        [1.3999, 1.2026]], requires_grad=True)"
  },
  {
    "objectID": "GPFA/learner.html#saving",
    "href": "GPFA/learner.html#saving",
    "title": "GPFA Learner",
    "section": "Saving",
    "text": "Saving\nload and save models parameters\n\nsource\n\nGPFALearner.load\n\n GPFALearner.load (path:pathlib.Path|str)\n\n\nsource\n\n\nGPFALearner.save\n\n GPFALearner.save (path:pathlib.Path|str)\n\n\nfrom tempfile import tempdir\n\n\np = Path(tempdir) / \"model_test.pickle\"\n\n\nl.save(p)\n\n\nnl = GPFALearner(X)\nnl.model.state_dict()\n\nOrderedDict([('likelihood.noise_covar.raw_noise', tensor([0.])),\n             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n              tensor(1.0000e-04)),\n             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.Lambda',\n              tensor([[0.6782],\n                      [0.2307],\n                      [0.7072]])),\n             ('covar_module.raw_psi_diag', tensor([0., 0., 0.])),\n             ('covar_module.latent_kernels.0.raw_lengthscale', tensor([[0.]])),\n             ('covar_module.latent_kernels.0.raw_lengthscale_constraint.lower_bound',\n              tensor(0.)),\n             ('covar_module.latent_kernels.0.raw_lengthscale_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.raw_psi_diag_constraint.lower_bound', tensor(0.)),\n             ('covar_module.raw_psi_diag_constraint.upper_bound',\n              tensor(inf))])\n\n\n\nnl.load(p)\nnl.model.state_dict()\n\nOrderedDict([('likelihood.noise_covar.raw_noise', tensor([-10.4628])),\n             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n              tensor(1.0000e-04)),\n             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.Lambda',\n              tensor([[2.2274],\n                      [2.2276],\n                      [2.2281]])),\n             ('covar_module.raw_psi_diag',\n              tensor([ -7.4228, -11.5954,  -8.7099])),\n             ('covar_module.latent_kernels.0.raw_lengthscale',\n              tensor([[5.5376]])),\n             ('covar_module.latent_kernels.0.raw_lengthscale_constraint.lower_bound',\n              tensor(0.)),\n             ('covar_module.latent_kernels.0.raw_lengthscale_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.raw_psi_diag_constraint.lower_bound', tensor(0.)),\n             ('covar_module.raw_psi_diag_constraint.upper_bound',\n              tensor(inf))])"
  },
  {
    "objectID": "GPFA/learner.html#plot-learning-progress",
    "href": "GPFA/learner.html#plot-learning-progress",
    "title": "GPFA Learner",
    "section": "Plot learning progress",
    "text": "Plot learning progress\n\n@patch\ndef plot_progress(self: GPFALearner, size={'width': 250, 'height': 120}):\n    \n    sel = alt.selection_interval(bind=\"scales\", encodings=['x'])\n    \n    plt_losses = alt.Chart(\n        pd.DataFrame({'loss': self.losses, 'n_iter': range(self.losses.shape[0])})\n    ).mark_line().encode(\n        x = 'n_iter',\n        y = 'loss'\n    ).properties(title=\"loss\", **size).add_selection(sel)\n    \n    out_plot = [plt_losses]\n    for info_name in self.model_infos[0].keys():\n        \n        values = pd.concat([info[info_name].assign(n_iter=i) for i, info in enumerate(self.model_infos)])\n        \n        if values.shape[1] == 2:\n            # only one column so add fake facet\n            values.insert(0, 'info', info_name)\n        \n        facet = values.columns[0] # first column is either latent or variable\n        \n        values = values.melt([facet, 'n_iter'], var_name='prop')\n        \n        plt = alt.Chart(values).mark_line().encode(\n            x = 'n_iter',\n            y = 'value',\n            color = 'prop',\n            facet = facet\n        ).properties(title=info_name, **size).add_selection(sel)\n        \n        out_plot.append(plt)\n    \n    return alt.VConcatChart(vconcat=out_plot).resolve_scale(\n        color='independent'\n    )\n\n\nsource\n\nGPFALearner.plot_progress\n\n GPFALearner.plot_progress (size={'width': 250, 'height': 120})\n\n\nl.plot_progress()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nself = l\n\nv =[]\nfor info_name in self.model_infos[0].keys(): \n    values = pd.concat([info[info_name].assign(n_iter=i) for i, info in enumerate(self.model_infos)])\n    v.append(values)\n\n\nv\n\n[          z0  n_iter\n 0   0.746957       0\n 1   0.404298       0\n 2   0.050215       0\n 0   0.646957       1\n 1   0.504298       1\n ..       ...     ...\n 1   2.226480     198\n 2   2.226368     198\n 0   2.226924     199\n 1   2.226828     199\n 2   2.227455     199\n \n [600 rows x 2 columns],\n    latent  lengthscale  n_iter\n 0      z0     0.693147       0\n 0      z0     0.744397       1\n 0      z0     0.798158       2\n 0      z0     0.854517       3\n 0      z0     0.913540       4\n ..    ...          ...     ...\n 0      z0     5.539773     195\n 0      z0     5.541732     196\n 0      z0     5.540282     197\n 0      z0     5.537724     198\n 0      z0     5.537501     199\n \n [200 rows x 3 columns],\n    variable       psi  n_iter\n 0      None  0.693147       0\n 1      None  0.693147       0\n 2      None  0.693147       0\n 0      None  0.644397       1\n 1      None  0.644397       1\n ..      ...       ...     ...\n 1      None  0.000010     198\n 2      None  0.000164     198\n 0      None  0.000600     199\n 1      None  0.000009     199\n 2      None  0.000164     199\n \n [600 rows x 3 columns],\n        noise  n_iter\n 0   0.693247       0\n 0   0.644497       1\n 0   0.598178       2\n 0   0.554247       3\n 0   0.512663       4\n ..       ...     ...\n 0   0.000131     195\n 0   0.000131     196\n 0   0.000130     197\n 0   0.000130     198\n 0   0.000129     199\n \n [200 rows x 2 columns]]"
  },
  {
    "objectID": "GPFA/learner.html#export",
    "href": "GPFA/learner.html#export",
    "title": "GPFA Learner",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "GPFA/learner.html#other",
    "href": "GPFA/learner.html#other",
    "title": "GPFA Learner",
    "section": "Other",
    "text": "Other\n\n# #| exporti\n# def get_parameter_value(name, param, constraint):\n#     if constraint is not None:\n#         value = constraint.transform(param.data.detach())\n#         name = name.replace(\"raw_\", \"\") # parameter is not raw anymore\n#     else:\n#         value = param.data.detach()\n#     return (name, value)\n\n# name = \"covar_module.psi\"\n# test_eq(l.model.covar_module.psi.detach(), get_parameter_value(name, l.model.covar_module.raw_psi_diag, l.model.covar_module.raw_psi_diag_constraint)[1])\n\n# #| exporti\n# def tensor_to_first_item(tensor):\n#     if tensor.dim() > 0:\n#         return tensor_to_first_item(tensor[0])\n#     return tensor.item()\n\n\n# def format_parameter(name, value):\n#     value = tensor_to_first_item(value)\n#     name = name.split(\".\")[-1] # get only last part of name\n#     return f\"{name}: {value:.3f}\"\n\n# #| export\n# @patch\n# def get_formatted_params(self: GPFALearner):\n#     return \", \".join([\n#         format_parameter(*get_parameter_value(name, value, constraint))\n#         for name, value, constraint in\n#         self.model.named_parameters_and_constraints()\n#     ])\n\n# l.get_formatted_params()\n\n# # this is not really working at the moment, but it's not important\n# @patch\n# def plot_loss_printer(self: GPFALearner, i_iter):\n#     if i_iter ==0: return\n#     x = torch.arange(0, i_iter)\n#     y = self.losses[:i_iter]\n#     plot_data = [[x, y]]\n#     self.pb.update_graph(plot_data)\n    \n#     x_bounds = [x.min(), x.max()+1]\n#     y_bounds = [y.min(), y.max()]\n#     self.pb.names = [\"Training loss\"]\n\n# #|export\n# @patch\n# def printer(self: GPFALearner, i_iter):\n\n#     if i_iter%10 == 0:\n#         update_str = f\"loss: {self.losses[i_iter].item():.3f}, \" + self.get_formatted_params()\n#         #self.plot_loss(i_iter)\n    \n#     #self.pb.write(update_str)\n\n# l.train(lr = 0.01)\n\n# import matplotlib.pyplot as plt\n\n# plt.plot(l.losses)"
  },
  {
    "objectID": "GPFA/imputation.html",
    "href": "GPFA/imputation.html",
    "title": "Imputation time series",
    "section": "",
    "text": "the goal of this notebook is to be able to:"
  },
  {
    "objectID": "GPFA/imputation.html#gpfa-imputation",
    "href": "GPFA/imputation.html#gpfa-imputation",
    "title": "Imputation time series",
    "section": "GPFA Imputation",
    "text": "GPFA Imputation\nThis is the core class that does the imputation using a GPFA\nThe inputs is:\n\na dataframe containing the observed data, where the row with missing data have been removed\na vector of times where the data is missing\n\nit returns:\n\na complete dataframe with the prediction of the model\n\nThe goal is that GPFAImputation takes as imput a dataframe containing missing values and then it imputes them using GPFALearner. Therefore it needs to divide the dataframe in 3 sections:\n\ntraining data (rows with no NAs)\ntimes to be imputed (rows with some NAs)\nobservations (variables in the pred rows that are not missing) for conditional predictions\n\n\nt_df = pd.DataFrame([\n    [1., 3., 4.],\n    [2., 6., np.nan],\n    [np.nan, np.nan, np.nan],\n    [np.nan, 8., np.nan],\n    [3., 4., 5.]\n]\n)\n\n\ntrain_idx = ~t_df.isna().any(1)\n\n/tmp/ipykernel_6506/3664883938.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n  train_idx = ~t_df.isna().any(1)\n\n\n\nt_df[train_idx]\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      1.0\n      3.0\n      4.0\n    \n    \n      4\n      3.0\n      4.0\n      5.0\n    \n  \n\n\n\n\n\npred_data = t_df[~train_idx].to_numpy()\n\n\npred_data\n\narray([[ 2.,  6., nan],\n       [nan, nan, nan],\n       [nan,  8., nan]])\n\n\n\nidx_cond = ~t_df[~train_idx].isna()\n\n\nidx_cond\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      1\n      True\n      True\n      False\n    \n    \n      2\n      False\n      False\n      False\n    \n    \n      3\n      False\n      True\n      False\n    \n  \n\n\n\n\n\nidx_cond = idx_cond.to_numpy().flatten() # learner needs a 1D index\n\npred_data.flatten()[idx_cond]\n\ntrain_idx = t_df.isna().any(axis=1)\n\ntorch.tensor(~t_df[~train_idx].isna().to_numpy().flatten())\n\ntensor([True, True, True, True, True, True])\n\n\nImplement this into a function\n\nclass GPFAImputation:\n    def __init__(\n        self,\n        data: pd.DataFrame , #observed data with missing data as NA\n        latent_dims = 1,\n        cuda = False, # Use GPU?\n        model = GPFA # sub-class of `GPFA` \n    ):\n        self.data = data.copy()\n        self.latent_dims = latent_dims\n        \n        device = 'cuda' if cuda else 'cpu'\n        \n        self.T = torch.arange(0, len(data), dtype=torch.float32, device=device) # time is encoded with a increase of 1\n        \n        # Training data\n        self.train_idx = ~self.data.isna().any(axis=1)\n        self.train_data = torch.tensor(self.data[self.train_idx].to_numpy().astype(np.float32), device=device)\n        self.train_T = self.T[self.train_idx]\n        \n        self.learner = GPFALearner(X = self.train_data, T = self.train_T, latent_dims=latent_dims, model=model, var_names= self.data.columns)\n        \n\n        # Prediction data\n        self.pred_T = self.T[~self.train_idx]\n        self.cond_idx = torch.tensor(~self.data[~self.train_idx].isna().to_numpy().flatten(), device=device) # conditional obsevations\n        self.cond_obs = torch.tensor(self.data[~self.train_idx].to_numpy().astype(np.float32).flatten()[self.cond_idx.cpu()], device=device)\n        \n        if cuda: self.learner.cuda()\n        \n                                   \n    def fit(self, n_iter=100):\n        \"Fit learner to training data\"\n        self.learner.train(n_iter = n_iter)\n        return self\n\n    def impute(self,\n               add_time = True, # add column with time?\n               tidy = True, # tidy data?\n               ):\n        \n        self.pred = self.learner.predict(self.pred_T, obs = self.cond_obs, idx = self.cond_idx)\n        if not hasattr(self, \"pred\"):\n            self.fit()\n\n        \n        if tidy: return self._impute_tidy(add_time)\n        else: return self._impute_wide(add_time)\n        \n        \n    def _impute_wide(self, add_time):\n        \"\"\" Impute in wide format\"\"\"\n        \n        imp_data = self.data.copy()\n        for col_idx, col_name in enumerate(imp_data.columns):\n            imp_data.loc[~self.train_idx, col_name] = self.pred.mean[:, col_idx].cpu().numpy()\n            imp_data.loc[~self.train_idx, col_name + \"_std\"] = self.pred.std[:, col_idx].cpu().numpy()\n        \n        if add_time:\n            imp_data[\"time\"] = self.T.cpu()\n        \n        return imp_data \n    \n    def _impute_tidy(self, add_time):\n        \"\"\" transform the pred output into a tidy dataframe suitable for plotting\"\"\"\n        feature_names = self.data.columns\n\n        pred_mean = pd.DataFrame(self.pred.mean.cpu(), columns = feature_names).assign(time = self.pred_T.cpu()).melt(\"time\", value_name=\"mean\")\n        pred_std = pd.DataFrame(self.pred.std.cpu(), columns = feature_names).assign(time = self.pred_T.cpu()).melt(\"time\", value_name=\"std\")\n        \n        pred = pd.merge(pred_mean, pred_std, on=['time', 'variable'])  \n        \n        train_data = self.data[self.train_idx].assign(time = self.train_T.cpu()).melt(\"time\", value_name = \"mean\")\n               \n        imp_data = pd.concat((train_data, pred))\n        \n        self.pred_wide = imp_data\n        \n        return imp_data\n\n\nsource\n\nGPFAImputation\n\n GPFAImputation (data:pandas.core.frame.DataFrame, latent_dims=1,\n                 cuda=False, model=<class 'meteo_imp.gpfa.gpfa.GPFA'>)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nobserved data with missing data as NA\n\n\nlatent_dims\nint\n1\n\n\n\ncuda\nbool\nFalse\nUse GPU?\n\n\nmodel\ntype\nGPFA\nsub-class of GPFA\n\n\n\n\nfd = MeteoDataTest.generate_gpfa(2, 10, Lambda=[1,2.]).add_random_missing()\n\n\nfd.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      -0.024085\n      0.089268\n    \n    \n      1\n      -0.133942\n      0.258532\n    \n    \n      2\n      -0.604650\n      -0.603501\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      -0.478994\n      -1.161096\n    \n    \n      5\n      NaN\n      NaN\n    \n    \n      6\n      -0.594717\n      -1.284512\n    \n    \n      7\n      NaN\n      1.251743\n    \n    \n      8\n      NaN\n      -2.001107\n    \n    \n      9\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nimp = GPFAImputation(fd.data)\n\n\nimp\n\n<__main__.GPFAImputation>\n\n\nTidy\n\nimp.impute()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.024085\n      NaN\n    \n    \n      1\n      1.0\n      x0\n      -0.133942\n      NaN\n    \n    \n      2\n      2.0\n      x0\n      -0.604650\n      NaN\n    \n    \n      3\n      4.0\n      x0\n      -0.478994\n      NaN\n    \n    \n      4\n      6.0\n      x0\n      -0.594717\n      NaN\n    \n    \n      5\n      0.0\n      x1\n      0.089268\n      NaN\n    \n    \n      6\n      1.0\n      x1\n      0.258532\n      NaN\n    \n    \n      7\n      2.0\n      x1\n      -0.603501\n      NaN\n    \n    \n      8\n      4.0\n      x1\n      -1.161096\n      NaN\n    \n    \n      9\n      6.0\n      x1\n      -1.284512\n      NaN\n    \n    \n      0\n      3.0\n      x0\n      -0.416516\n      0.386124\n    \n    \n      1\n      5.0\n      x0\n      -0.419649\n      0.386142\n    \n    \n      2\n      7.0\n      x0\n      -0.358267\n      0.387923\n    \n    \n      3\n      8.0\n      x0\n      -0.397247\n      0.390719\n    \n    \n      4\n      9.0\n      x0\n      -0.384120\n      0.391765\n    \n    \n      5\n      3.0\n      x1\n      -0.562824\n      0.833950\n    \n    \n      6\n      5.0\n      x1\n      -0.564259\n      0.833952\n    \n    \n      7\n      7.0\n      x1\n      1.251743\n      0.000000\n    \n    \n      8\n      8.0\n      x1\n      -2.001107\n      0.000000\n    \n    \n      9\n      9.0\n      x1\n      -0.547979\n      0.834502\n    \n  \n\n\n\n\nwide\n\nimp.impute(tidy=False)\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x0_std\n      x1_std\n      time\n    \n  \n  \n    \n      0\n      -0.024085\n      0.089268\n      NaN\n      NaN\n      0.0\n    \n    \n      1\n      -0.133942\n      0.258532\n      NaN\n      NaN\n      1.0\n    \n    \n      2\n      -0.604650\n      -0.603501\n      NaN\n      NaN\n      2.0\n    \n    \n      3\n      -0.416516\n      -0.562824\n      0.386124\n      0.833950\n      3.0\n    \n    \n      4\n      -0.478994\n      -1.161096\n      NaN\n      NaN\n      4.0\n    \n    \n      5\n      -0.419649\n      -0.564259\n      0.386142\n      0.833952\n      5.0\n    \n    \n      6\n      -0.594717\n      -1.284512\n      NaN\n      NaN\n      6.0\n    \n    \n      7\n      -0.358267\n      1.251743\n      0.387923\n      0.000000\n      7.0\n    \n    \n      8\n      -0.397247\n      -2.001107\n      0.390719\n      0.000000\n      8.0\n    \n    \n      9\n      -0.384120\n      -0.547979\n      0.391765\n      0.834502\n      9.0\n    \n  \n\n\n\n\n\n\nGPU\ncheck that the GPU support is working\n\nimp_gpu = GPFAImputation(fd.data, cuda=True)\n\n\nimp_gpu.impute()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.024085\n      NaN\n    \n    \n      1\n      1.0\n      x0\n      -0.133942\n      NaN\n    \n    \n      2\n      2.0\n      x0\n      -0.604650\n      NaN\n    \n    \n      3\n      4.0\n      x0\n      -0.478994\n      NaN\n    \n    \n      4\n      6.0\n      x0\n      -0.594717\n      NaN\n    \n    \n      5\n      0.0\n      x1\n      0.089268\n      NaN\n    \n    \n      6\n      1.0\n      x1\n      0.258532\n      NaN\n    \n    \n      7\n      2.0\n      x1\n      -0.603501\n      NaN\n    \n    \n      8\n      4.0\n      x1\n      -1.161096\n      NaN\n    \n    \n      9\n      6.0\n      x1\n      -1.284512\n      NaN\n    \n    \n      0\n      3.0\n      x0\n      -0.370183\n      0.319018\n    \n    \n      1\n      5.0\n      x0\n      -0.372927\n      0.319018\n    \n    \n      2\n      7.0\n      x0\n      -0.355584\n      0.318874\n    \n    \n      3\n      8.0\n      x0\n      -0.377191\n      0.318883\n    \n    \n      4\n      9.0\n      x0\n      -0.373072\n      0.319039\n    \n    \n      5\n      3.0\n      x1\n      -0.634743\n      0.998508\n    \n    \n      6\n      5.0\n      x1\n      -0.723967\n      0.998508\n    \n    \n      7\n      7.0\n      x1\n      1.251743\n      0.000000\n    \n    \n      8\n      8.0\n      x1\n      -2.001107\n      0.000000\n    \n    \n      9\n      9.0\n      x1\n      -0.728671\n      1.005502\n    \n  \n\n\n\n\nthe gpu and cpu version return similar results!\n\nimp.impute()[[\"mean\", \"std\"]].to_numpy() - imp_gpu.impute()[[\"mean\", \"std\"]].to_numpy()\n\narray([[ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [-4.6333283e-02,  6.7106664e-02],\n       [-4.6722233e-02,  6.7123741e-02],\n       [-2.6827753e-03,  6.9049209e-02],\n       [-2.0056546e-02,  7.1836144e-02],\n       [-1.1047989e-02,  7.2726548e-02],\n       [ 7.1919739e-02, -1.6455758e-01],\n       [ 1.5970773e-01, -1.6455603e-01],\n       [ 0.0000000e+00,  0.0000000e+00],\n       [ 2.3841858e-07,  0.0000000e+00],\n       [ 1.8069166e-01, -1.7099941e-01]], dtype=float32)\n\n\n\n\nRepr\nadd __repr__ and __str__ to imputation objects\n\n@patch\ndef __repr__(self: GPFAImputation):\n    return f\"\"\"GPFA Imputation:\n    N obs: {self.data.shape[0]}\n    N features {self.data.shape[1]} ({', '.join(self.data.columns)})\n    N missing observations {(~self.cond_idx).sum()}\n    N latent: {self.learner.latent_dims}\"\"\"\n\n@patch\ndef __str__(self: GPFAImputation):\n    return self.__repr__()\n\n\nimp\n\nGPFA Imputation:\n    N obs: 10\n    N features 2 (x0, x1)\n    N missing observations 8\n    N latent: 1\n\n\n\nstr(imp)\n\n'GPFA Imputation:\\n    N obs: 10\\n    N features 2 (x0, x1)\\n    N missing observations 8\\n    N latent: 1'"
  },
  {
    "objectID": "GPFA/imputation.html#gpfa-imputation-explorer",
    "href": "GPFA/imputation.html#gpfa-imputation-explorer",
    "title": "Imputation time series",
    "section": "GPFA Imputation Explorer",
    "text": "GPFA Imputation Explorer\nThis is a class that is used for exploring the results for a GPFAImputation, the main difference is that it always return the model predictions and not only the training data\n\nclass GPFAImputationExplorer(GPFAImputation):\n    \"GPFAImputation where predictions are for all times not only missing data\"\n    \n    def predict(self):\n        \"Predict for all times, also when there is an observation, supporting cond obs, with valid std\"\n        imp_mean = pd.DataFrame({'time': self.T.cpu()})\n        imp_std = pd.DataFrame({'time': self.T.cpu()})\n        \n        # Fill using general predictions\n        \n        all_pred = self.learner.predict(self.T)\n        \n        for col_idx, col_name in enumerate(self.data.columns):\n            imp_mean.loc[:, col_name] = all_pred.mean[:, col_idx].cpu().numpy()\n            imp_std.loc[:, col_name] = all_pred.std[:, col_idx].cpu().numpy()\n        \n        # Fine tune with cond predictions\n        \n        pred_cond = self.learner.predict(self.pred_T, obs = self.cond_obs, idx = self.cond_idx)\n        obs_mask = self.cond_idx.reshape(-1, self.data.shape[1]).cpu().numpy()\n\n        for col_idx, col_name in enumerate(self.data.columns):\n            mean= pred_cond.mean[:, col_idx].cpu().numpy()\n            std = pred_cond.std[:, col_idx].cpu().numpy()\n\n            # when there is a cond obs the std is nan, which is replaced with the std without the conditional prediction\n            mask_data = ~self.train_idx.to_numpy()\n            mask_data[mask_data] = ~obs_mask[:, col_idx]\n\n            imp_mean.loc[mask_data, col_name] = mean[~obs_mask[:, col_idx]]\n            imp_std.loc[mask_data, col_name] = std[~obs_mask[:, col_idx]]\n        \n        # make tidy\n        \n        return pd.merge(\n            imp_mean.melt('time', value_name = \"mean\"),\n            imp_std.melt('time', value_name = \"std\"),\n            on = ['time', 'variable']\n        )\n\n\nsource\n\nGPFAImputationExplorer\n\n GPFAImputationExplorer (data:pandas.core.frame.DataFrame, latent_dims=1,\n                         cuda=False, model=<class\n                         'meteo_imp.gpfa.gpfa.GPFA'>)\n\nGPFAImputation where predictions are for all times not only missing data\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nobserved data with missing data as NA\n\n\nlatent_dims\nint\n1\n\n\n\ncuda\nbool\nFalse\nUse GPU?\n\n\nmodel\ntype\nGPFA\nsub-class of GPFA\n\n\n\n\nGPFAImputationExplorer(fd.data).fit().predict()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.138583\n      0.123488\n    \n    \n      1\n      1.0\n      x0\n      -0.115288\n      0.123250\n    \n    \n      2\n      2.0\n      x0\n      -0.398590\n      0.123467\n    \n    \n      3\n      3.0\n      x0\n      -0.472666\n      0.149725\n    \n    \n      4\n      4.0\n      x0\n      -0.566287\n      0.123645\n    \n    \n      5\n      5.0\n      x0\n      -0.994120\n      0.149824\n    \n    \n      6\n      6.0\n      x0\n      -0.611462\n      0.123656\n    \n    \n      7\n      7.0\n      x0\n      0.127221\n      0.123713\n    \n    \n      8\n      8.0\n      x0\n      -0.786960\n      0.123999\n    \n    \n      9\n      9.0\n      x0\n      -1.270282\n      0.183502\n    \n    \n      10\n      0.0\n      x1\n      0.130877\n      0.154138\n    \n    \n      11\n      1.0\n      x1\n      0.199238\n      0.152487\n    \n    \n      12\n      2.0\n      x1\n      -0.632152\n      0.153993\n    \n    \n      13\n      3.0\n      x1\n      -0.849540\n      0.292393\n    \n    \n      14\n      4.0\n      x1\n      -1.124286\n      0.155218\n    \n    \n      15\n      5.0\n      x1\n      -2.379823\n      0.292828\n    \n    \n      16\n      6.0\n      x1\n      -1.256858\n      0.155297\n    \n    \n      17\n      7.0\n      x1\n      -0.990569\n      0.469497\n    \n    \n      18\n      8.0\n      x1\n      -0.675226\n      0.612087\n    \n    \n      19\n      9.0\n      x1\n      -3.190263\n      0.427115\n    \n  \n\n\n\n\n\n\nRepr\nadd __repr__ and __str__ to imputation objects\n\n@patch\ndef __repr__(self: GPFAImputationExplorer):\n    return f\"\"\"GPFA Imputation Explorer:\n    N obs: {self.data.shape[0]}\n    N features {self.data.shape[1]} ({', '.join(self.data.columns)})\n    N missing observations {self.data.isna().to_numpy().flatten().sum()}\n    N latent: {self.learner.latent_dims}\"\"\"\n\n@patch\ndef __str__(self: GPFAImputationExplorer):\n    return self.__repr__()\n\n\nimp_exp = GPFAImputationExplorer(fd.data)\n\n\nimp_exp\n\nGPFA Imputation Explorer:\n    N obs: 10\n    N features 2 (x0, x1)\n    N missing observations 8\n    N latent: 1\n\n\n\nstr(imp)\n\n'GPFA Imputation:\\n    N obs: 10\\n    N features 2 (x0, x1)\\n    N missing observations 8\\n    N latent: 1'\n\n\n\nimp_exp.predict()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.264771\n      0.343444\n    \n    \n      1\n      1.0\n      x0\n      -0.286058\n      0.342875\n    \n    \n      2\n      2.0\n      x0\n      -0.390137\n      0.343443\n    \n    \n      3\n      3.0\n      x0\n      -0.399087\n      0.349827\n    \n    \n      4\n      4.0\n      x0\n      -0.413978\n      0.344008\n    \n    \n      5\n      5.0\n      x0\n      -0.409062\n      0.349828\n    \n    \n      6\n      6.0\n      x0\n      -0.437668\n      0.344009\n    \n    \n      7\n      7.0\n      x0\n      -0.324878\n      0.347765\n    \n    \n      8\n      8.0\n      x0\n      -0.412507\n      0.348563\n    \n    \n      9\n      9.0\n      x0\n      -0.393487\n      0.351376\n    \n    \n      10\n      0.0\n      x1\n      -0.360775\n      0.857899\n    \n    \n      11\n      1.0\n      x1\n      -0.398049\n      0.857201\n    \n    \n      12\n      2.0\n      x1\n      -0.580288\n      0.857897\n    \n    \n      13\n      3.0\n      x1\n      -0.595959\n      0.865770\n    \n    \n      14\n      4.0\n      x1\n      -0.622034\n      0.858591\n    \n    \n      15\n      5.0\n      x1\n      -0.613425\n      0.865772\n    \n    \n      16\n      6.0\n      x1\n      -0.663515\n      0.858592\n    \n    \n      17\n      7.0\n      x1\n      -0.583371\n      0.866950\n    \n    \n      18\n      8.0\n      x1\n      -0.542161\n      0.868132\n    \n    \n      19\n      9.0\n      x1\n      -0.586155\n      0.867691"
  },
  {
    "objectID": "GPFA/imputation.html#export",
    "href": "GPFA/imputation.html#export",
    "title": "Imputation time series",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "results.html#export",
    "href": "results.html#export",
    "title": "Results",
    "section": "Export",
    "text": "Export"
  }
]