[
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n cache_disk (base_file)\n\nDecorator to cache function output to disk\n\nimport time\nfrom tempfile import tempdir\n\n\n@cache_disk(Path(tempdir) / \"test_cache\")\ndef slow_add(a,b):\n    time.sleep(1)\n    return a + b\n\nthis time is the first time so not from the cache\n\n\n\nCPU times: user 0 ns, sys: 1.43 ms, total: 1.43 ms\nWall time: 1 s\n\n\n3\n\n\nnow is much faster beacuse of the cache\n\n\n\nCPU times: user 2 µs, sys: 1 µs, total: 3 µs\nWall time: 5.48 µs\n\n\n3\n\n\nadding comments doesn’t change the hash, so the function is still cached\n\n@cache_disk(\"test_cache\")\ndef slow_add(a,b):\n    time.sleep(1)\n    # this is a comment\n    return a + b\n\n\n\n\nCPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\nWall time: 5.25 µs\n\n\n3\n\n\nwhile changing the code changes the hash, so there is no cache\n\n@cache_disk(\"test_cache\")\ndef slow_add(a,b):\n    time.sleep(1)\n    return b + a\n\n\n\n\nCPU times: user 1.13 ms, sys: 56 µs, total: 1.19 ms\nWall time: 1 s\n\n\n3"
  },
  {
    "objectID": "utils.html#random-seed",
    "href": "utils.html#random-seed",
    "title": "Utils",
    "section": "Random Seed",
    "text": "Random Seed\nreset both pytorch and numpy random seeds\n\nsource\n\nreset_seed\n\n reset_seed (seed=27)"
  },
  {
    "objectID": "gap_finder.html",
    "href": "gap_finder.html",
    "title": "Find Gaps in Fluxnet data",
    "section": "",
    "text": "test_file_zip = Path(\"../../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip\")\ntest_file = Path(\"../../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4/FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\ntmp_dir = Path(\"/tmp\")\nout_dir = Path(\"../../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\nunzip the file and load it lazily with polars\ncolumns selection, interested only in QC columns to find gaps\nThe goal is to find where the data is missing in the dataset (which means that it has been gap-filled) and find:\nwe filter out the rows where there is no gap (QC =0)\nthen find the start and end of gap by comparing with the original row number of the previous entry\nit works!\nsource"
  },
  {
    "objectID": "gap_finder.html#find-gaps",
    "href": "gap_finder.html#find-gaps",
    "title": "Find Gaps in Fluxnet data",
    "section": "Find gaps",
    "text": "Find gaps\n\ndef _find_gap_df(df, col_name):\n    \"Find gaps with a df with a single QC column\"\n    return df.filter(\n        pl.col(col_name) != 0\n    ).with_columns([\n        (pl.col(\"row_num\") - pl.col(\"row_num\").shift() ).alias(\"before\"),\n        (pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\")).alias(\"after\"),\n    ]).filter(\n        (pl.col(\"before\") != 1) | (pl.col(\"after\") != 1)\n    ).with_column(\n        (pl.when((pl.col(\"before\") != 1) & (pl.col(\"after\") != 1))\n        .then(pl.col(\"start\"))\n        .otherwise(pl.col(\"start\").shift(-1))\n        .alias(\"gap_end\"))\n    ).filter(\n        pl.col(\"before\") != 1\n    ).select(\n        [pl.col(\"start\").alias(\"gap_start\"), \"gap_end\"]\n    )\n\n\ndef find_gap_variable(df, col_name):\n    \n    # row numembering has to happen before filtering\n    df = df.with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    )\n    \n    # start with null values\n    dff = df.filter(\n            pl.col(col_name).is_null()\n        )\n    gaps = [\n        _find_gap_df(dff, col_name).with_columns(\n            pl.lit(None).alias(\"gap_value\")\n        )]\n    \n    \n    # all other values\n    # here the QC flags are merged together as we we not interested in the QC alg only if there is a gap\n    dff = df.filter(\n            ~pl.col(col_name).is_null()\n        )\n    gaps.append(\n        _find_gap_df(dff, col_name).with_columns(\n            pl.lit(1).alias(\"gap_value\")\n        ))\n    \n    return pl.concat(gaps)\n\n\nfind_gap_variable(df, \"SW_IN_F_QC\").collect().head()\n\nNotFoundError: start\n\n\n\ndef find_all_gaps(df):\n    return pl.concat(\n        [find_gap(df, col_name) for col_name in df.select(pl.col(\"^.*_QC$\")).columns]\n    )\n\n\nsource\n\nfind_all_gaps\n\n find_all_gaps (df)\n\n\ngaps_all = find_all_gaps(df).collect()\n\n\ngaps_all.groupby(\"variable\").agg(pl.col(\"gap_len\").sum() / df.collect().shape[0])\n\n\n\n\nshape: (45, 2)\n\n\n\n\nvariable\n\n\ngap_len\n\n\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"NEE_VUT_84_QC\"\n\n\n0.722569\n\n\n\n\n\"NEE_VUT_16_QC\"\n\n\n0.725254\n\n\n\n\n\"NEE_CUT_16_QC\"\n\n\n0.727048\n\n\n\n\n\"NEE_VUT_50_QC\"\n\n\n0.6978\n\n\n\n\n\"NEE_VUT_95_QC\"\n\n\n0.742042\n\n\n\n\n\"TA_F_MDS_QC\"\n\n\n0.012959\n\n\n\n\n\"H_F_MDS_QC\"\n\n\n0.198801\n\n\n\n\n\"LW_IN_F_QC\"\n\n\n0.243714\n\n\n\n\n\"TS_F_MDS_2_QC\"\n\n\n0.007857\n\n\n\n\n\"SWC_F_MDS_2_QC...\n\n\n0.242968\n\n\n\n\n\"CO2_F_MDS_QC\"\n\n\n0.097889\n\n\n\n\n\"NEE_CUT_75_QC\"\n\n\n0.709597\n\n\n\n\n...\n\n\n...\n\n\n\n\n\"SW_IN_F_MDS_QC...\n\n\n0.015802\n\n\n\n\n\"LW_IN_F_MDS_QC...\n\n\n0.243714\n\n\n\n\n\"SW_IN_F_QC\"\n\n\n0.015802\n\n\n\n\n\"NEE_CUT_95_QC\"\n\n\n0.745916\n\n\n\n\n\"LW_IN_JSB_QC\"\n\n\n0.022492\n\n\n\n\n\"PA_F_QC\"\n\n\n0.016109\n\n\n\n\n\"NEE_VUT_MEAN_Q...\n\n\n0.96898\n\n\n\n\n\"SWC_F_MDS_1_QC...\n\n\n0.015516\n\n\n\n\n\"TS_F_MDS_4_QC\"\n\n\n0.850741\n\n\n\n\n\"NEE_VUT_USTAR5...\n\n\n0.703832\n\n\n\n\n\"NEE_VUT_75_QC\"\n\n\n0.70968\n\n\n\n\n\"TS_F_MDS_1_QC\"\n\n\n0.006352\n\n\n\n\n\n\n\n\ndef download_and_find_gaps(urls, download_dir, out_dir, tmp_dir):\n    site_infos = []\n    for url in tqdm(urls):\n        file_zip = download_fluxnet(url, download_dir)\n        file, site_info = find_gaps_fluxnet_archive(file_zip, out_dir, tmp_dir)\n        site_infos.append(site_info)\n        print(file)\n        \n    return pl.concat(site_infos)\n\n\nsource\n\n\ndownload_and_find_gaps\n\n download_and_find_gaps (urls, download_dir, out_dir, tmp_dir)\n\n\nurls = [\"https://ftp.fluxdata.org/.fluxnet_downloads_86523/FLUXNET2015/FLX_AR-SLu_FLUXNET2015_FULLSET_2009-2011_1-4.zip?=mone27\",\n\"https://ftp.fluxdata.org/.fluxnet_downloads_86523/FLUXNET2015/FLX_AR-Vir_FLUXNET2015_FULLSET_2009-2012_1-4.zip?=mone27\"]\n\n\ndownload_and_find_gaps(urls, download_dir, out_dir, tmp_dir)\n\n\n\n\nFLX_AR-SLu_FLUXNET2015_FULLSET_HH_2009-2011_1-4\nFLX_AR-Vir_FLUXNET2015_FULLSET_HH_2009-2012_1-4\n\n\n\n\n\nshape: (2, 3)\n\n\n\n\nstart\n\n\nend\n\n\nsite\n\n\n\n\ni64\n\n\ni64\n\n\nstr\n\n\n\n\n\n\n200901010030\n\n\n201201010000\n\n\n\"AR-SLu\"\n\n\n\n\n200901010030\n\n\n201301010000\n\n\n\"AR-Vir\""
  },
  {
    "objectID": "gpfa.html",
    "href": "gpfa.html",
    "title": "Gaussian Processes Factor Analysis",
    "section": "",
    "text": "Derivation of the equations to solve the Gaussian Processes Factor Analysis as described in: Yu, B.M., Cunningham, J.P., Santhanam, G., Ryu, S., Shenoy, K.V., Sahani, M., 2008. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity, in: Advances in Neural Information Processing Systems. Curran Associates, Inc."
  },
  {
    "objectID": "gpfa.html#notation",
    "href": "gpfa.html#notation",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Notation",
    "text": "Notation\n\n\\(T\\) Number of time steps\n\\(N\\) Number of variables observed\n\\(K\\) Number of dimensions of latent variable\n\\(x_{:,t}\\) vector of all the \\(N\\) variables at time \\(t\\), \\(\\in \\mathbb{R}^N\\)\n\\(x_{n,:}\\) vector of the \\(n\\)th variable at for time steps in \\(T\\), \\(\\in \\mathbb{R}^T\\)\n\\(x_{n,t}\\) \\(n\\)th variable at time \\(t\\), \\(\\in \\mathbb{R}\\)\n\\(X_M = [x_{:,1}, ... x_{:, T}]\\) Matrix with all the \\(N\\) variables at all time steps \\(T\\), \\(\\in \\mathbb{R}^{N \\times T}\\)\n\\(X\\) is a vector obtained by “flattening” \\(X_M\\), by putting next to each other all variable at time \\(t\\), \\(\\in \\mathbb{R}^{(N \\cdot T)}\\)\n\\(t\\) time step\n\\(z_{i, t}\\) \\(i\\)th latent variable at time \\(t\\), \\(\\in \\mathbb{R}\\)\n\\(Z = [z_1 , ... z_t]\\) Vector with \\(z\\) at all time steps in \\(T\\), \\(\\in \\mathbb{R}^{K \\times T}\\)"
  },
  {
    "objectID": "gpfa.html#gaussian-processes-factor-analysis-model",
    "href": "gpfa.html#gaussian-processes-factor-analysis-model",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Gaussian Processes Factor Analysis model",
    "text": "Gaussian Processes Factor Analysis model\nWe model the variables in this way \\[x_{:,t} = \\Lambda z_{:,t} + \\epsilon \\] where:\n\n\\(\\Lambda\\) is a Factor loading matrix that transforms \\(z_{:,t}\\) into \\(x_{:,t}\\), \\(\\in \\mathbb{R}^{N \\times K}\\)\n\\(\\epsilon\\) Random noise. The random noise is independent between the different time steps, \\(\\in \\mathbb{R}^N\\):\n\n\\(p(\\epsilon) = \\mathcal{N}(0, \\psi)\\) distribution of noise\n\\(\\psi\\), covariance matrix of noise, it is a diagional matrix, \\(\\in \\mathbb{R}^{N \\times N}\\)\n\n\nThe model consider \\(\\langle X \\rangle = 0\\) (if \\(X\\) doesn’t have a 0 mean it can be easily transformed by substracting the mean)\nThe latent variable \\(z\\) is modelled over time using a Gaussian Process, one process for each dimension \\(k\\) for simplicity we assumed that \\(z\\) has only one dimension (\\(k = 1\\))\n\\[p(Z) = \\mathcal{GP}(0, k(t, t \\prime))\\]"
  },
  {
    "objectID": "gpfa.html#derivation-of-px",
    "href": "gpfa.html#derivation-of-px",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Derivation of \\(p(X)\\)",
    "text": "Derivation of \\(p(X)\\)\n\\(p(x_{:,t}|z_{:,t}) = \\mathcal{N}(\\Lambda z_{:,t}, \\psi)\\) is easy to derive and then \\(p(x_{:,t})\\) and \\(p(z_{:,t}|x_{:,t})\\) can be obtained using the rules of Gaussian inference.\nHowever, what is interesting is to have the analytical form of \\(p(X)\\), which models both the relations between \\(z\\) and \\(x\\) and the \\(z\\) and \\(t\\). The likelihood of \\(p(X)\\) can then be maximized to obtain the parameters of the latent transformation and the kernel hyperparameter.\n\\(p(X)\\) is a Guassian distribution with \\(T\\cdot N\\) dimensions.\n\\(p(X) = \\mathcal{N}(\\langle X \\rangle, \\langle X X^T \\rangle)\\)\n\nDiagonal of the covariance matrix\nLet’s start with the diagonal of the covariance matrix (\\(t = t \\prime\\))\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t} + \\epsilon_{t})^T \\rangle\\)\nby multipling the two vectors together we obtain\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t}^T + \\epsilon_t \\Lambda^T z_{:,t}^T + \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThe using the linearity of the expectation we can:\n\ntransform the expecations of a sum into a sum of expecations\nmove the \\(\\Lambda\\) out of the expecation, as it doesn’t depend on \\(z\\)\n\\(\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle\\) because \\(z_{:,t}\\) and \\(\\epsilon_t\\) are independent random variables\n\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThen considering that \\(\\langle z_{:,t} \\rangle = 0\\) and that \\(\\langle \\epsilon_t \\rangle = 0\\) the expression can be simplified as:\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThen substituting:\n\n\\(\\langle z_{:,t} z_{:,t}^T\\rangle = k(t, t)\\) as that is the covariance matrix of the Gaussian process\n\\(\\langle \\epsilon_t \\epsilon_t^T \\rangle= \\psi\\)\n\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda k(t,t) \\Lambda^T + \\psi\\)\n\n\nOff-diagonal\nsimilar to the steps of above\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t \\prime} + \\epsilon_{t \\prime})^T \\rangle\\)\nby multipling the two vectors together we obtain\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t \\prime}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t \\prime}^T + \\epsilon_t \\Lambda^T z_{:,t \\prime}^T + \\epsilon_t \\epsilon_{t \\prime}^T \\rangle\\)\nThen using the linearity of the expectation we can:\n\ntransform the expecations of a sum into a sum of expecations\nmove the \\(\\Lambda\\) out of the expecatios, as it doesn’t depend on t\n\\(\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle\\) because \\(z_{:,t}\\) and \\(\\epsilon_t\\) are independent random variables\n\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThen considering that \\(\\langle z_{:,t} \\rangle = 0\\) and that \\(\\langle \\epsilon_t \\rangle = 0\\) the expression can be simplified as:\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t \\prime}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle\\)\nThen substituting: 1) \\(\\langle z_{:,t} z_{:,t \\prime}^T\\rangle = k(t,t \\prime)\\) as that is the covariance matrix of the Gaussian process 2) \\(\\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle= 0\\) as \\(\\epsilon_t\\) and \\(\\epsilon_{t \\prime}\\) are independent and \\(\\langle \\epsilon_t \\rangle = 0\\)\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T\\)\n\n\nResult\nThe equation for the diagonal and off-diagonal element can be summarized as:\n\\[\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi\\]\nwhere \\(\\delta(x) = \\begin{cases}  1 & if\\ x=0 \\\\  0 & if\\ x \\ne 0 \\\\  \\end{cases}\\)\nTherefore \\(p(X)\\) can be modelled as:\n\\[p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{cccc}\n    \\Lambda k(t_1,t_1) \\Lambda^T + \\delta(1-1)\\psi & \\Lambda k(t_{1},t_{2}) \\Lambda^T + \\delta(1-2)\\psi& \\cdots & \\Lambda k(t_1 ,t_t) \\Lambda^T + \\delta(1-t)\\psi\\\\\n    \\Lambda k(t_{2},t_{1}) \\Lambda^T+ \\delta(2-1)\\psi &  \\Lambda k(t_{2},t_{2}) \\Lambda^T + \\delta(2-2)\\psi & \\cdots & \\Lambda k(t_{2},t_{t}) \\Lambda^T + \\delta(2-t)\\psi\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    \\Lambda k(t_{t}, t_{1}) \\Lambda^T+ \\delta(t-1)\\psi & \\Lambda k(t_{t},t_{2}) \\Lambda^T + \\delta(t-2)\\psi& \\cdots & \\Lambda k(t_{t},t_{t}) \\Lambda^T + \\delta(t-t)\\psi\\\\\n    \\end{array} } \\right )\\]\nand this is also Gaussian Process with a “special” kernel. Multiplying kernel with a constant (\\(\\Lambda\\)) or adding a kernel (\\(\\delta\\)) yields another valid kernel\nIf we define a new kernel as \\[K(t,t \\prime) = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi\\]\nThen\n\\[ p(X) = \\mathcal{GP}(0, K(t, t\\prime))\\]\n\n\nLatent variable with more than one dimension\nIn order to have a latent variable with more than one-dimesions, we need to make small changes to the formula\nThe starting point is the covariance matrix of the latent at time \\(t\\), which is:\n\\[\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} \\langle z_{1:,t} z_{1:,t}^T\\rangle & \\cdots & \\langle z_{1:,t} z_{k:,t}^T\\rangle\\\\ \\vdots & \\ddots & \\vdots \\\\ \\langle z_{k:,t} z_{1:,t}^T \\rangle & \\cdots & \\langle z_{k:,t} z_{k:,t}^T\\rangle\\end{array}\\right)\\]\nsince each dimension in \\(z\\) is indipendent:\n\\(\\langle z_{k,t} z_{k\\prime,t} \\rangle = 0\\)\neach dimension in \\(z\\) is modelled using a different kernel (\\(k_{z_k}(t,t\\prime)\\)), hence:\n\\[\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} k_{z_1}(t, t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t)\\end{array}\\right)\\]\nso the GPFA Kernel is:\n\\[K(t,t \\prime) = \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t, t \\prime) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t \\prime)\\end{array}\\right) \\Lambda^T + \\delta(t - t \\prime)\\psi\\]\nso p(X) is:\n\\[p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{ccc}\n    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_t)\\psi\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_t)\\psi\\\\\n    \\end{array} } \\right )\\]"
  },
  {
    "objectID": "gpfa.html#next-steps",
    "href": "gpfa.html#next-steps",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Next steps",
    "text": "Next steps\n\nThe parameters of the final GP (\\(\\Lambda, \\psi\\) and the kernel hyperparameters) can be fitted by maximizing the likelihood of \\(p(X)\\) using gradient descent"
  },
  {
    "objectID": "gpfa.html#kernel",
    "href": "gpfa.html#kernel",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Kernel",
    "text": "Kernel\n\nclass GPFAKernel(gpytorch.kernels.Kernel):\n    \"\"\"\n    Kernel to implement Gaussian Processes Factor Analysis\n    \"\"\"\n    def __init__(self,\n                 n_features: int, # number of variables at each time step\n                 latent_kernel: gpytorch.kernels.Kernel, # func that returns any valid GPyTorch Kernel used to model the relationship over time of the latent\n                 latent_dims:int = 1,  # Number of latent dims\n                 Lambda: torch.tensor = None, #(n_features * latent_dims) initial value for factor loading matrix. If None init to one\n                 psi: torch.tensor = None, #(n_features) initial value for random noise covariance. Note this is only the diagonal matrix\n                 **kwargs):\n        super(GPFAKernel, self).__init__(**kwargs)\n        \n        # Number of features in the X for each time step\n        self.n_features = n_features\n\n        self.latent_dims = latent_dims\n        \n        # see GPyTorch Kernels\n        self.register_parameter(\n            name = \"Lambda\",\n            parameter = torch.nn.Parameter(torch.rand(self.n_features, self.latent_dims)))\n        \n        # each dim has it's own latent kernel\n        self.latent_kernels = torch.nn.ModuleList([latent_kernel() for _ in range(self.latent_dims)])\n        \n        self.register_parameter(\n            name = \"raw_psi_diag\",\n            parameter = torch.nn.Parameter(torch.zeros(self.n_features))) \n        self.register_constraint(\"raw_psi_diag\", gpytorch.constraints.Positive())\n        if psi is not None: self.psi = psi\n    \n    # Convenient getter and setter for psi, since there is the Positive() constraint\n    @property\n    def psi(self):\n        # when accessing the parameter, apply the constraint transform\n        return self.raw_psi_diag_constraint.transform(self.raw_psi_diag)\n\n    @psi.setter\n    def psi(self, value):\n        return self._set_psi(value)\n\n    def _set_psi(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_psi_diag)\n        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n        self.initialize(raw_psi_diag=self.raw_psi_diag_constraint.inverse_transform(value))\n    \n\n        \n    def forward(self, t1, t2, diag = False, last_dim_is_batch=False, **params):\n\n        # not implemented yet\n        assert diag is False\n        assert last_dim_is_batch is False\n\n        # take the number of observations from the input\n        n_obs = t1.shape[0]\n\n        # compute the latent kernel\n        kT = torch.stack([ kernel(t1, t2, diag, last_dim_is_batch, **params).evaluate() # this may make the whole thing slow as it breaks lazy evaluations\n                         for kernel in self.latent_kernels], dim=2)\n        return compute_gpfa_covariance(self.Lambda, kT, self.psi, self.n_features, n_obs)\n    \n    def num_outputs_per_input(self, x1,x2):\n        return self.n_features\n\n# this is a separate function, because torch script cannot take self as a parameter\n@torch.jit.script\ndef compute_gpfa_covariance(Lambda, kT, psi, n_features, n_obs):\n    # pre allocate covariance matrix\n    X_cov = torch.empty(n_features * n_obs, n_features * n_obs, device=Lambda.device)\n    for i in torch.arange(n_obs):\n        for j in torch.arange(n_obs):\n            # i:i+1 is required to keep the number of dimensions\n            cov =  Lambda @ torch.diag(kT[i,j,:]) @ Lambda.T\n            # only diagonals add the noise\n            if i == j: cov += torch.diag(psi)\n            # add a block of size n_features*n_features to the covariance matrix\n            X_cov[i*n_features:(i*n_features + n_features),j*n_features:(j*n_features+n_features)] = cov\n    return X_cov\n\n\n\nScriptFunction object at 0x7fc6a4fc73d0>\n\nsource\n\n\nGPFAKernel\n\n GPFAKernel (n_features:int, latent_kernel:gpytorch.kernels.kernel.Kernel,\n             latent_dims:int=1, Lambda:<built-\n             inmethodtensoroftypeobjectat0x7fc700e3b9c0>=None, psi:<built-\n             inmethodtensoroftypeobjectat0x7fc700e3b9c0>=None, **kwargs)\n\nKernel to implement Gaussian Processes Factor Analysis\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nint\n\nnumber of variables at each time step\n\n\nlatent_kernel\nKernel\n\nfunc that returns any valid GPyTorch Kernel used to model the relationship over time of the latent\n\n\nlatent_dims\nint\n1\nNumber of latent dims\n\n\nLambda\ntensor\nNone\n(n_features * latent_dims) initial value for factor loading matrix. If None init to one\n\n\npsi\ntensor\nNone\n(n_features) initial value for random noise covariance. Note this is only the diagonal matrix\n\n\nkwargs\n\n\n\n\n\n\n\ngpfa_k = GPFAKernel(n_features=2, latent_kernel=gpytorch.kernels.RBFKernel)\n\nThe parameters are correctly registered\n\nlist(gpfa_k.named_parameters())\n\n[('Lambda',\n  Parameter containing:\n  tensor([[0.4277],\n          [0.9292]], requires_grad=True)),\n ('raw_psi_diag',\n  Parameter containing:\n  tensor([0., 0.], requires_grad=True)),\n ('latent_kernels.0.raw_lengthscale',\n  Parameter containing:\n  tensor([[0.]], requires_grad=True))]\n\n\nCheck that the Kernel is running\n\ngpfa_k(torch.tensor((1, 2, 3))).evaluate()\n\ntensor([[0.8760, 0.3974, 0.0646, 0.1404, 0.0028, 0.0062],\n        [0.3974, 1.5567, 0.1404, 0.3050, 0.0062, 0.0134],\n        [0.0646, 0.1404, 0.8760, 0.3974, 0.0646, 0.1404],\n        [0.1404, 0.3050, 0.3974, 1.5567, 0.1404, 0.3050],\n        [0.0028, 0.0062, 0.0646, 0.1404, 0.8760, 0.3974],\n        [0.0062, 0.0134, 0.1404, 0.3050, 0.3974, 1.5567]],\n       grad_fn=<CopySlices>)"
  },
  {
    "objectID": "gpfa.html#gpfa",
    "href": "gpfa.html#gpfa",
    "title": "Gaussian Processes Factor Analysis",
    "section": "GPFA",
    "text": "GPFA\n\nsource\n\nGPFAZeroMean\n\n GPFAZeroMean (n_features, device)\n\nZero Mean function to be used in GPFA, as it takes into account the number of features\nto change the latent_kernel you should subcall GPFA in this way the get_info function for the kernel can be changed to include the latent kernel details\n\nsource\n\n\nGPFA\n\n GPFA (train_x, train_y, likelihood, n_features, latent_dims=1)\n\nThe base class for any Gaussian process latent function to be used in conjunction with exact inference.\n:param torch.Tensor train_inputs: (size n x d) The training features :math:\\mathbf X. :param torch.Tensor train_targets: (size n) The training targets :math:\\mathbf y. :param ~gpytorch.likelihoods.GaussianLikelihood likelihood: The Gaussian likelihood that defines the observational distribution. Since we’re using exact inference, the likelihood must be Gaussian.\nThe :meth:forward function should describe how to compute the prior latent distribution on a given input. Typically, this will involve a mean and kernel function. The result must be a :obj:~gpytorch.distributions.MultivariateNormal.\nCalling this model will return the posterior of the latent Gaussian process when conditioned on the training data. The output will be a :obj:~gpytorch.distributions.MultivariateNormal.\nExample: >>> class MyGP(gpytorch.models.ExactGP): >>> def init(self, train_x, train_y, likelihood): >>> super().__init__(train_x, train_y, likelihood) >>> self.mean_module = gpytorch.means.ZeroMean() >>> self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) >>> >>> def forward(self, x): >>> mean = self.mean_module(x) >>> covar = self.covar_module(x) >>> return gpytorch.distributions.MultivariateNormal(mean, covar) >>> >>> # train_x = …; train_y = … >>> likelihood = gpytorch.likelihoods.GaussianLikelihood() >>> model = MyGP(train_x, train_y, likelihood) >>> >>> # test_x = …; >>> model(test_x) # Returns the GP latent function at test_x >>> likelihood(model(test_x)) # Returns the (approximate) predictive posterior distribution at test_x\nmake some very simple test data, to check that the model is working and can learn the parameters\n\nT = torch.arange(1,5)\n\n\nX = torch.hstack([torch.arange(0,3) + 2* i for i in T])\n\n\nX\n\ntensor([ 2,  3,  4,  4,  5,  6,  6,  7,  8,  8,  9, 10])\n\n\n\nX.shape\n\ntorch.Size([12])\n\n\n\nT\n\ntensor([1, 2, 3, 4])\n\n\n\n# initialize likelihood and model\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = GPFA(T, X, likelihood, n_features = 3)\n\n\nmodel\n\nGPFA(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\nGetting the prior from the GP\n\nmodel(T)\n\nMultivariateNormal(loc: torch.Size([12]))\n\n\nFitting the parameters using gradient descend\n\n# this is for running the notebook in our testing framework\ntraining_iter = 10\n\n# Find optimal model hyperparameters\nmodel.train()\nlikelihood.train()\n\n# Use the adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n\n# \"Loss\" for GPs - the marginal log likelihood\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\nlosses = []\nfor i in range(training_iter):\n    # Zero gradients from previous iteration\n    optimizer.zero_grad()\n    # Output from model\n    output = model(T)\n    # Calc loss and backprop gradients\n    loss = -mll(output, X)\n    losses.append(loss.item())\n    loss.backward()\n    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f, Lambda: %.3f   noise: %.3f' % (\n        i + 1, training_iter, loss.item(),\n        model.covar_module.latent_kernels[0].lengthscale.item(),\n        model.covar_module.Lambda.mean().item(),\n        model.likelihood.noise.item()\n    ))\n    optimizer.step()\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n\n\nIter 1/10 - Loss: 10.368   lengthscale: 0.693, Lambda: 0.468   noise: 0.693\nIter 2/10 - Loss: 8.550   lengthscale: 0.744, Lambda: 0.568   noise: 0.744\nIter 3/10 - Loss: 7.164   lengthscale: 0.798, Lambda: 0.666   noise: 0.797\nIter 4/10 - Loss: 6.122   lengthscale: 0.854, Lambda: 0.762   noise: 0.848\nIter 5/10 - Loss: 5.339   lengthscale: 0.911, Lambda: 0.854   noise: 0.899\nIter 6/10 - Loss: 4.746   lengthscale: 0.968, Lambda: 0.942   noise: 0.948\nIter 7/10 - Loss: 4.292   lengthscale: 1.027, Lambda: 1.026   noise: 0.995\nIter 8/10 - Loss: 3.939   lengthscale: 1.085, Lambda: 1.105   noise: 1.039\nIter 9/10 - Loss: 3.661   lengthscale: 1.143, Lambda: 1.179   noise: 1.080\nIter 10/10 - Loss: 3.440   lengthscale: 1.200, Lambda: 1.249   noise: 1.119\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nThe model is training!"
  },
  {
    "objectID": "gpfa.html#multi-dimensional-latent-variable",
    "href": "gpfa.html#multi-dimensional-latent-variable",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Multi-dimensional latent variable",
    "text": "Multi-dimensional latent variable\n\n2 dimensions\n\n# initialize likelihood and model\nlikelihood_m = gpytorch.likelihoods.GaussianLikelihood()\nmodel_m = GPFA(T, X, likelihood, n_features = 3, latent_dims=2)\n\n\nmodel_m\n\nGPFA(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (1): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\ncheck GP is running\n\nmodel_m.covar_module.latent_kernels\n\nModuleList(\n  (0): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (1): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.6841, 0.8607],\n        [0.6201, 0.5034],\n        [0.3585, 0.7198]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))\n\n\n\n\n5 dimensions\n\n# initialize likelihood and model\nlikelihood_m = gpytorch.likelihoods.GaussianLikelihood()\nmodel_m = GPFA(T, X, likelihood, n_features = 3, latent_dims=5)\n\n\nmodel_m\n\nGPFA(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (1): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (2): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (3): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (4): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\ncheck GP is running\n\nmodel_m.covar_module.latent_kernels\n\nModuleList(\n  (0): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (1): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (2): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (3): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (4): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.1433, 0.6319, 0.2940, 0.8762, 0.6009],\n        [0.7275, 0.7272, 0.2959, 0.3548, 0.8001],\n        [0.9632, 0.0958, 0.9202, 0.0039, 0.7309]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))"
  },
  {
    "objectID": "gpfa.html#get-info",
    "href": "gpfa.html#get-info",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Get info",
    "text": "Get info\nthis is to return the kernel info for printing, both during training and for the final results\nThe general api is:\nget_info(var_names=None) -> dict[str, pd.DataFrame] where the string the title of a kernel parameter\n\n@patch\ndef get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -> dict[str, pd.DataFrame]:\n    \"Model info for a GPFA with a RBFKernel\"\n    out = {}\n    \n    latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n    \n    out[\"Lambda\"] = pd.concat([\n        None if var_names is None else pd.Series(var_names),\n        pd.DataFrame(\n            self.covar_module.Lambda.detach().cpu().numpy(),\n            columns=latent_names)],\n        axis=1)\n    \n    ls = [self.covar_module.latent_kernels[i].lengthscale.detach().item() for i in range(self.covar_module.latent_dims)]\n    out[\"lengthscale\"] = pd.DataFrame({\n        'latent': latent_names,\n        'lengthscale': ls\n    })\n    \n    psi = self.covar_module.psi.detach().cpu().numpy()\n    out[\"psi\"] = pd.DataFrame({\n        'variable': var_names,\n        'psi': psi \n    })\n    \n    out[\"likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.item()]})\n    \n    return out\n\n\nsource\n\nGPFA.get_info\n\n GPFA.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nmodel_m.get_info()\n\n{'Lambda':          z0        z1        z2        z3        z4\n 0  0.143287  0.631874  0.294008  0.876215  0.600861\n 1  0.727490  0.727183  0.295870  0.354760  0.800073\n 2  0.963249  0.095809  0.920186  0.003902  0.730877,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0     None  0.693147\n 1     None  0.693147\n 2     None  0.693147,\n 'likelihood':       noise\n 0  1.155176}\n\n\n\nmodel_m.get_info([\"a\", \"b\", \"c\"])\n\n{'Lambda':    0        z0        z1        z2        z3        z4\n 0  a  0.143287  0.631874  0.294008  0.876215  0.600861\n 1  b  0.727490  0.727183  0.295870  0.354760  0.800073\n 2  c  0.963249  0.095809  0.920186  0.003902  0.730877,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0        a  0.693147\n 1        b  0.693147\n 2        c  0.693147,\n 'likelihood':       noise\n 0  1.155176}"
  },
  {
    "objectID": "gpfa.html#export",
    "href": "gpfa.html#export",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "imputation.html",
    "href": "imputation.html",
    "title": "Imputation time series",
    "section": "",
    "text": "the goal of this notebook is to be able to:"
  },
  {
    "objectID": "imputation.html#gpfa-imputation",
    "href": "imputation.html#gpfa-imputation",
    "title": "Imputation time series",
    "section": "GPFA Imputation",
    "text": "GPFA Imputation\nThis is the core class that does the imputation using a GPFA\nThe inputs is:\n\na dataframe containing the observed data, where the row with missing data have been removed\na vector of times where the data is missing\n\nit returns:\n\na complete dataframe with the prediction of the model\n\nThe goal is that GPFAImputation takes as imput a dataframe containing missing values and then it imputes them using GPFALearner. Therefore it needs to divide the dataframe in 3 sections:\n\ntraining data (rows with no NAs)\ntimes to be imputed (rows with some NAs)\nobservations (variables in the pred rows that are not missing) for conditional predictions\n\n\nt_df = pd.DataFrame([\n    [1., 3., 4.],\n    [2., 6., np.nan],\n    [np.nan, np.nan, np.nan],\n    [np.nan, 8., np.nan],\n    [3., 4., 5.]\n]\n)\n\n\ntrain_idx = ~t_df.isna().any(1)\n\n/tmp/ipykernel_40166/3664883938.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n  train_idx = ~t_df.isna().any(1)\n\n\n\nt_df[train_idx]\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      1.0\n      3.0\n      4.0\n    \n    \n      4\n      3.0\n      4.0\n      5.0\n    \n  \n\n\n\n\n\npred_data = t_df[~train_idx].to_numpy()\n\n\npred_data\n\narray([[ 2.,  6., nan],\n       [nan, nan, nan],\n       [nan,  8., nan]])\n\n\n\nidx_cond = ~t_df[~train_idx].isna()\n\n\nidx_cond\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      1\n      True\n      True\n      False\n    \n    \n      2\n      False\n      False\n      False\n    \n    \n      3\n      False\n      True\n      False\n    \n  \n\n\n\n\n\nidx_cond = idx_cond.to_numpy().flatten() # learner needs a 1D index\n\npred_data.flatten()[idx_cond]\n\ntrain_idx = t_df.isna().any(axis=1)\n\ntorch.tensor(~t_df[~train_idx].isna().to_numpy().flatten())\n\ntensor([True, True, True, True, True, True])\n\n\nImplement this into a function\n\nclass GPFAImputation:\n    def __init__(\n        self,\n        data: pd.DataFrame , #observed data with missing data as NA\n        latent_dims = 1,\n        cuda = False, # Use GPU?\n        units = None, # Dict of unit for each column. Used for plotting\n        model = GPFA # sub-class of `GPFA` \n    ):\n        self.data = data.copy()\n        self.units=units\n        self.latent_dims = latent_dims\n        \n        \n        device = 'cuda' if cuda else 'cpu'\n        \n        self.T = torch.arange(0, len(data), dtype=torch.float32, device=device) # time is encoded with a increase of 1\n        \n        # Training data\n        self.train_idx = ~self.data.isna().any(axis=1)\n        self.train_data = torch.tensor(self.data[self.train_idx].to_numpy().astype(np.float32), device=device)\n        self.train_T = self.T[self.train_idx]\n        \n        self.learner = GPFALearner(X = self.train_data, T = self.train_T, latent_dims=latent_dims, model=model)\n        \n\n        # Prediction data\n        self.pred_T = self.T[~self.train_idx]\n        self.cond_idx = torch.tensor(~self.data[~self.train_idx].isna().to_numpy().flatten(), device=device) # conditional obsevations\n        self.cond_obs = torch.tensor(self.data[~self.train_idx].to_numpy().astype(np.float32).flatten()[self.cond_idx.cpu()], device=device)\n        \n        if cuda: self.learner.cuda()\n        \n    def fit(self):\n        \"Fit learner to training data\"\n        self.learner.train()\n        return self\n\n    def impute(self,\n               add_time = True, # add column with time?\n               tidy = True, # tidy data?\n               ):\n        \n        self.pred = self.learner.predict(self.pred_T, obs = self.cond_obs, idx = self.cond_idx)\n        if not hasattr(self, \"pred\"):\n            self.fit()\n\n        \n        if tidy: return self._impute_tidy(add_time)\n        else: return self._impute_wide(add_time)\n        \n        \n    def _impute_wide(self, add_time):\n        \"\"\" Impute in wide format\"\"\"\n        \n        imp_data = self.data.copy()\n        for col_idx, col_name in enumerate(imp_data.columns):\n            imp_data.loc[~self.train_idx, col_name] = self.pred.mean[:, col_idx].cpu().numpy()\n            imp_data.loc[~self.train_idx, col_name + \"_std\"] = self.pred.std[:, col_idx].cpu().numpy()\n        \n        if add_time:\n            imp_data[\"time\"] = self.T.cpu()\n        \n        return imp_data \n    \n    def _impute_tidy(self, add_time):\n        \"\"\" transform the pred output into a tidy dataframe suitable for plotting\"\"\"\n        feature_names = self.data.columns\n\n        pred_mean = pd.DataFrame(self.pred.mean.cpu(), columns = feature_names).assign(time = self.pred_T.cpu()).melt(\"time\", value_name=\"mean\")\n        pred_std = pd.DataFrame(self.pred.std.cpu(), columns = feature_names).assign(time = self.pred_T.cpu()).melt(\"time\", value_name=\"std\")\n        \n        pred = pd.merge(pred_mean, pred_std, on=['time', 'variable'])  \n        \n        train_data = self.data[self.train_idx].assign(time = self.train_T.cpu()).melt(\"time\", value_name = \"mean\")\n               \n        imp_data = pd.concat((train_data, pred))\n        \n        self.pred_wide = imp_data\n        \n        return imp_data\n\n\nsource\n\nGPFAImputation\n\n GPFAImputation (data:pandas.core.frame.DataFrame, latent_dims=1,\n                 cuda=False, units=None, model=<class\n                 'gpfa_imputation.gpfa.GPFA'>)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nobserved data with missing data as NA\n\n\nlatent_dims\nint\n1\n\n\n\ncuda\nbool\nFalse\nUse GPU?\n\n\nunits\nNoneType\nNone\nDict of unit for each column. Used for plotting\n\n\nmodel\ntype\nGPFA\nsub-class of GPFA\n\n\n\n\nfd = GPFADataTest.generate(2, 10, Lambda=[1,2.]).add_random_missing()\n\n\nfd.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      -0.024085\n      0.089268\n    \n    \n      1\n      -0.133942\n      0.258532\n    \n    \n      2\n      -0.604650\n      -0.603501\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      -0.478994\n      -1.161096\n    \n    \n      5\n      NaN\n      NaN\n    \n    \n      6\n      -0.594717\n      -1.284512\n    \n    \n      7\n      NaN\n      1.251743\n    \n    \n      8\n      NaN\n      -2.001107\n    \n    \n      9\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nimp = GPFAImputation(fd.data)\n\n\nimp\n\n<__main__.GPFAImputation>\n\n\nTidy\n\nimp.impute()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.024085\n      NaN\n    \n    \n      1\n      1.0\n      x0\n      -0.133942\n      NaN\n    \n    \n      2\n      2.0\n      x0\n      -0.604650\n      NaN\n    \n    \n      3\n      4.0\n      x0\n      -0.478994\n      NaN\n    \n    \n      4\n      6.0\n      x0\n      -0.594717\n      NaN\n    \n    \n      5\n      0.0\n      x1\n      0.089268\n      NaN\n    \n    \n      6\n      1.0\n      x1\n      0.258532\n      NaN\n    \n    \n      7\n      2.0\n      x1\n      -0.603501\n      NaN\n    \n    \n      8\n      4.0\n      x1\n      -1.161096\n      NaN\n    \n    \n      9\n      6.0\n      x1\n      -1.284512\n      NaN\n    \n    \n      0\n      3.0\n      x0\n      -0.416516\n      0.386124\n    \n    \n      1\n      5.0\n      x0\n      -0.419649\n      0.386142\n    \n    \n      2\n      7.0\n      x0\n      -0.358267\n      0.387923\n    \n    \n      3\n      8.0\n      x0\n      -0.397247\n      0.390719\n    \n    \n      4\n      9.0\n      x0\n      -0.384120\n      0.391765\n    \n    \n      5\n      3.0\n      x1\n      -0.562824\n      0.833950\n    \n    \n      6\n      5.0\n      x1\n      -0.564259\n      0.833952\n    \n    \n      7\n      7.0\n      x1\n      1.251743\n      0.000000\n    \n    \n      8\n      8.0\n      x1\n      -2.001107\n      0.000000\n    \n    \n      9\n      9.0\n      x1\n      -0.547979\n      0.834502\n    \n  \n\n\n\n\nwide\n\nimp.impute(tidy=False)\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x0_std\n      x1_std\n      time\n    \n  \n  \n    \n      0\n      -0.024085\n      0.089268\n      NaN\n      NaN\n      0.0\n    \n    \n      1\n      -0.133942\n      0.258532\n      NaN\n      NaN\n      1.0\n    \n    \n      2\n      -0.604650\n      -0.603501\n      NaN\n      NaN\n      2.0\n    \n    \n      3\n      -0.416516\n      -0.562824\n      0.386124\n      0.833950\n      3.0\n    \n    \n      4\n      -0.478994\n      -1.161096\n      NaN\n      NaN\n      4.0\n    \n    \n      5\n      -0.419649\n      -0.564259\n      0.386142\n      0.833952\n      5.0\n    \n    \n      6\n      -0.594717\n      -1.284512\n      NaN\n      NaN\n      6.0\n    \n    \n      7\n      -0.358267\n      1.251743\n      0.387923\n      0.000000\n      7.0\n    \n    \n      8\n      -0.397247\n      -2.001107\n      0.390719\n      0.000000\n      8.0\n    \n    \n      9\n      -0.384120\n      -0.547979\n      0.391765\n      0.834502\n      9.0\n    \n  \n\n\n\n\n\n\nGPU\ncheck that the GPU support is working\n\nimp_gpu = GPFAImputation(fd.data, cuda=True)\n\n\nimp_gpu.impute()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.024085\n      NaN\n    \n    \n      1\n      1.0\n      x0\n      -0.133942\n      NaN\n    \n    \n      2\n      2.0\n      x0\n      -0.604650\n      NaN\n    \n    \n      3\n      4.0\n      x0\n      -0.478994\n      NaN\n    \n    \n      4\n      6.0\n      x0\n      -0.594717\n      NaN\n    \n    \n      5\n      0.0\n      x1\n      0.089268\n      NaN\n    \n    \n      6\n      1.0\n      x1\n      0.258532\n      NaN\n    \n    \n      7\n      2.0\n      x1\n      -0.603501\n      NaN\n    \n    \n      8\n      4.0\n      x1\n      -1.161096\n      NaN\n    \n    \n      9\n      6.0\n      x1\n      -1.284512\n      NaN\n    \n    \n      0\n      3.0\n      x0\n      -0.370183\n      0.319018\n    \n    \n      1\n      5.0\n      x0\n      -0.372927\n      0.319018\n    \n    \n      2\n      7.0\n      x0\n      -0.355584\n      0.318874\n    \n    \n      3\n      8.0\n      x0\n      -0.377191\n      0.318883\n    \n    \n      4\n      9.0\n      x0\n      -0.373072\n      0.319039\n    \n    \n      5\n      3.0\n      x1\n      -0.634743\n      0.998508\n    \n    \n      6\n      5.0\n      x1\n      -0.723967\n      0.998508\n    \n    \n      7\n      7.0\n      x1\n      1.251743\n      0.000000\n    \n    \n      8\n      8.0\n      x1\n      -2.001107\n      0.000000\n    \n    \n      9\n      9.0\n      x1\n      -0.728671\n      1.005502\n    \n  \n\n\n\n\nthe gpu and cpu version return similar results!\n\nimp.impute()[[\"mean\", \"std\"]].to_numpy() - imp_gpu.impute()[[\"mean\", \"std\"]].to_numpy()\n\narray([[ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [-4.6333283e-02,  6.7106664e-02],\n       [-4.6722233e-02,  6.7123741e-02],\n       [-2.6827753e-03,  6.9049209e-02],\n       [-2.0056546e-02,  7.1836144e-02],\n       [-1.1047989e-02,  7.2726548e-02],\n       [ 7.1919739e-02, -1.6455758e-01],\n       [ 1.5970773e-01, -1.6455603e-01],\n       [ 0.0000000e+00,  0.0000000e+00],\n       [ 2.3841858e-07,  0.0000000e+00],\n       [ 1.8069166e-01, -1.7099941e-01]], dtype=float32)\n\n\n\n\nRepr\nadd __repr__ and __str__ to imputation objects\n\n@patch\ndef __repr__(self: GPFAImputation):\n    return f\"\"\"GPFA Imputation:\n    N obs: {self.data.shape[0]}\n    N features {self.data.shape[1]} ({', '.join(self.data.columns)})\n    N missing observations {(~self.cond_idx).sum()}\n    N latent: {self.learner.latent_dims}\"\"\"\n\n@patch\ndef __str__(self: GPFAImputation):\n    return self.__repr__()\n\n\nimp\n\nGPFA Imputation:\n    N obs: 10\n    N features 2 (x0, x1)\n    N missing observations 8\n    N latent: 1\n\n\n\nstr(imp)\n\n'GPFA Imputation:\\n    N obs: 10\\n    N features 2 (x0, x1)\\n    N missing observations 8\\n    N latent: 1'"
  },
  {
    "objectID": "imputation.html#gpfa-imputation-explorer",
    "href": "imputation.html#gpfa-imputation-explorer",
    "title": "Imputation time series",
    "section": "GPFA Imputation Explorer",
    "text": "GPFA Imputation Explorer\nThis is a class that is used for exploring the results for a GPFAImputation, the main difference is that it always return the model predictions and not only the training data\n\nclass GPFAImputationExplorer:\n    def __init__(\n        self,\n        data: pd.DataFrame , #observed data with missing data as NA\n        latent_dims = 1,\n        cuda = False, # Use GPU?\n        model = GPFA # sub-class of `GPFA` \n    ):\n        self.data = data\n        self.latent_dims = latent_dims\n        \n        device = 'cuda' if cuda else 'cpu'\n        \n        self.T = torch.arange(0, len(data), dtype=torch.float32, device=device) # time is encoded with a increase of 1\n        \n        # Training data\n        self.train_idx = ~self.data.isna().any(axis=1)\n        self.train_data = torch.tensor(self.data[self.train_idx].to_numpy().astype(np.float32), device=device)\n        self.train_T = self.T[self.train_idx]\n        \n        self.learner = GPFALearner(X = self.train_data, T = self.train_T, latent_dims=latent_dims, model=model)\n        \n        \n        # There is no conditional observation here since it probably doesn't make much sense here\n               \n        if cuda: self.learner.cuda()\n        \n    def fit(self):\n        \"Fit learner to training data\"\n        self.learner.train()\n        return self\n\n    def predict(self):\n        \n        # return always tidy df\n        \n        self.pred = self.learner.predict(self.T)\n        \n        feature_names = self.data.columns\n        pred_mean = pd.DataFrame(self.pred.mean.cpu(), columns = feature_names).assign(time = self.T.cpu()).melt(\"time\", value_name=\"mean\")\n        pred_std = pd.DataFrame(self.pred.std.cpu(), columns = feature_names).assign(time = self.T.cpu()).melt(\"time\", value_name=\"std\")\n        \n        return pd.merge(pred_mean, pred_std, on=['time', 'variable'])\n    \n    def fit_predict(self):\n        self.fit()\n        return self.predict()\n\n\nsource\n\nGPFAImputationExplorer\n\n GPFAImputationExplorer (data:pandas.core.frame.DataFrame, latent_dims=1,\n                         cuda=False, model=<class\n                         'gpfa_imputation.gpfa.GPFA'>)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nobserved data with missing data as NA\n\n\nlatent_dims\nint\n1\n\n\n\ncuda\nbool\nFalse\nUse GPU?\n\n\nmodel\ntype\nGPFA\nsub-class of GPFA\n\n\n\n\n\nRepr\nadd __repr__ and __str__ to imputation objects\n\n@patch\ndef __repr__(self: GPFAImputationExplorer):\n    return f\"\"\"GPFA Imputation Explorer:\n    N obs: {self.data.shape[0]}\n    N features {self.data.shape[1]} ({', '.join(self.data.columns)})\n    N missing observations {self.data.isna().to_numpy().flatten().sum()}\n    N latent: {self.learner.latent_dims}\"\"\"\n\n@patch\ndef __str__(self: GPFAImputationExplorer):\n    return self.__repr__()\n\n\nimp_exp = GPFAImputationExplorer(fd.data)\n\n\nimp_exp\n\nGPFA Imputation Explorer:\n    N obs: 10\n    N features 2 (x0, x1)\n    N missing observations 8\n    N latent: 1\n\n\n\nstr(imp)\n\n'GPFA Imputation:\\n    N obs: 10\\n    N features 2 (x0, x1)\\n    N missing observations 8\\n    N latent: 1'\n\n\n\nimp_exp.predict()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.297978\n      0.341699\n    \n    \n      1\n      1.0\n      x0\n      -0.320394\n      0.341324\n    \n    \n      2\n      2.0\n      x0\n      -0.391404\n      0.341698\n    \n    \n      3\n      3.0\n      x0\n      -0.388192\n      0.345208\n    \n    \n      4\n      4.0\n      x0\n      -0.390354\n      0.342071\n    \n    \n      5\n      5.0\n      x0\n      -0.389361\n      0.345208\n    \n    \n      6\n      6.0\n      x0\n      -0.408291\n      0.342072\n    \n    \n      7\n      7.0\n      x0\n      -0.381646\n      0.345724\n    \n    \n      8\n      8.0\n      x0\n      -0.367911\n      0.346240\n    \n    \n      9\n      9.0\n      x0\n      -0.367281\n      0.346241\n    \n    \n      10\n      0.0\n      x1\n      -0.515997\n      0.829065\n    \n    \n      11\n      1.0\n      x1\n      -0.523846\n      0.829046\n    \n    \n      12\n      2.0\n      x1\n      -0.548710\n      0.829065\n    \n    \n      13\n      3.0\n      x1\n      -0.547585\n      0.829244\n    \n    \n      14\n      4.0\n      x1\n      -0.548342\n      0.829084\n    \n    \n      15\n      5.0\n      x1\n      -0.547994\n      0.829244\n    \n    \n      16\n      6.0\n      x1\n      -0.554623\n      0.829084\n    \n    \n      17\n      7.0\n      x1\n      -0.545293\n      0.829270\n    \n    \n      18\n      8.0\n      x1\n      -0.540484\n      0.829296\n    \n    \n      19\n      9.0\n      x1\n      -0.540263\n      0.829296"
  },
  {
    "objectID": "imputation.html#export",
    "href": "imputation.html#export",
    "title": "Imputation time series",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "learner.html",
    "href": "learner.html",
    "title": "GPFA Learner",
    "section": "",
    "text": "from fastcore.test import *\nThe first thing that we need is a Learner object to keep track of:\nand that has methods to help with:\nThe first thing we need is a training loop, just wrap in a function the example one from GPyTorch"
  },
  {
    "objectID": "learner.html#learner",
    "href": "learner.html#learner",
    "title": "GPFA Learner",
    "section": "Learner",
    "text": "Learner\n\nsource\n\nGPFALearner\n\n GPFALearner (X:torch.Tensor, T:torch.Tensor=None, latent_dims:int=1,\n              model=<class 'gpfa_imputation.gpfa.GPFA'>)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nTensor\n\n(n_features * n_obs) Multivariate time series\n\n\nT\nTensor\nNone\n(n_obs) Vector of time of observations.\n\n\nlatent_dims\nint\n1\nNumber of latent variables in GPFA\n\n\nmodel\ntype\nGPFA\nsub-class of GPFA\n\n\n\n\n# test data\nT = torch.arange(0,6)\n\nX = torch.vstack([(torch.arange(0,3, dtype=torch.float32) + 2 + i) * i for i in T])\n\n\nX\n\ntensor([[ 0.,  0.,  0.],\n        [ 3.,  4.,  5.],\n        [ 8., 10., 12.],\n        [15., 18., 21.],\n        [24., 28., 32.],\n        [35., 40., 45.]])\n\n\n\n# l for learner\nl = GPFALearner(X)\n\n\ntest_eq(T, l.T)\n\n\n# with explicit time\ntest_eq(T, GPFALearner(X, T).T)\n\n\ntest_eq(l.n_features, 3)\n\n\nl.X\n\ntensor([-1.0590, -1.0955, -1.1236, -0.8347, -0.8326, -0.8305, -0.4610, -0.4382,\n        -0.4201,  0.0623,  0.0876,  0.1075,  0.7350,  0.7449,  0.7523,  1.5573,\n         1.5337,  1.5145])\n\n\n\nl.train()\n\n\n\n\ncheck that can train for more than once\n\nl.train()\n\n\n\n\n\nl.losses\n\ntensor([ 1.3875,  1.3412,  1.2952,  1.2515,  1.2120,  1.1761,  1.1403,  1.1030,\n         1.0644,  1.0249,  0.9848,  0.9445,  0.9043,  0.8644,  0.8248,  0.7852,\n         0.7454,  0.7046,  0.6627,  0.6198,  0.5764,  0.5333,  0.4910,  0.4490,\n         0.4067,  0.3636,  0.3202,  0.2773,  0.2355,  0.1936,  0.1508,  0.1076,\n         0.0655,  0.0238, -0.0190, -0.0618, -0.1035, -0.1455, -0.1883, -0.2299,\n        -0.2714, -0.3136, -0.3546, -0.3959, -0.4372, -0.4775, -0.5185, -0.5583,\n        -0.5986, -0.6380, -0.6776, -0.7162, -0.7546, -0.7900, -0.8300, -0.8682,\n        -0.9026, -0.9405, -0.9765, -1.0097, -1.0448, -1.0804, -1.1123, -1.1417,\n        -1.1708, -1.1997, -1.2224, -1.2525, -1.2880, -1.3045, -1.3355, -1.3613,\n        -1.3731, -1.4074, -1.4237, -1.4331, -1.4655, -1.4761, -1.4839, -1.5108,\n        -1.5190, -1.5217, -1.5401, -1.5276, -1.4662, -1.5441, -1.5399, -1.5408,\n        -1.5678, -1.5540, -1.5865, -1.5616, -1.6051, -1.5710, -1.6148, -1.5850,\n        -1.6196, -1.5999, -1.6246, -1.6134])"
  },
  {
    "objectID": "learner.html#predictions",
    "href": "learner.html#predictions",
    "title": "GPFA Learner",
    "section": "Predictions",
    "text": "Predictions\nadd a function to get predictions from the model\n\nsource\n\nGPFALearner.predict_raw\n\n GPFALearner.predict_raw (T)\n\n\nraw_out = l.predict_raw(T)\nraw_out\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n  warnings.warn(\n\n\nMultivariateNormal(loc: torch.Size([18]))\n\n\nthe model prediction is a distribution with len(T)*n_features dimensions\nwhich is in the in the wrong shape and need to be rescaled after the normalization\nAlso we don’t need th full distribution but only the mean and stddev for each variable at every time step\nAnd we can “fix” the shape by transforming back to a matrix\n\nraw_stddev = raw_out.stddev.reshape(-1, l.n_features)\nraw_mean = raw_out.mean.reshape(-1, l.n_features)\n\n\nraw_stddev\n\ntensor([[0.0251, 0.0198, 0.0227],\n        [0.0242, 0.0187, 0.0217],\n        [0.0239, 0.0183, 0.0214],\n        [0.0239, 0.0183, 0.0214],\n        [0.0242, 0.0187, 0.0218],\n        [0.0251, 0.0197, 0.0227]], grad_fn=<ReshapeAliasBackward0>)\n\n\nThis function transforms the raw output of the Gaussian Process (p(X)) into a prediction that can be used. need to do two things:\n\ntake mean and stddev (this is the diagonal of the covariance matrix) for each variable\nreshape so that each row is a time step and each column a variable\nreverse the normalization\n\nthe mean and the std are passed individually because in the conditional predictions is not possible to have the whole covariance matrix (and thus a MultiNormal) but the mean and stddev are enough.\n\nsource\n\n\nGPFALearner.prediction_from_raw\n\n GPFALearner.prediction_from_raw (raw_mean, raw_std)\n\nTakes a raw prediction and produces and final prediction, by reshaping and reversing normalization\n\n# TODO document this function better\n\n\n@patch\ndef predict(self: GPFALearner, T):\n    pred_raw = self.predict_raw(T)\n    return self.prediction_from_raw(pred_raw.mean, pred_raw.stddev)\n\n\nl.predict(T)\n\nNormalParameters(mean=tensor([[-5.1709e-01, -2.8349e-02,  5.4034e-01],\n        [ 3.0475e+00,  4.0246e+00,  5.0628e+00],\n        [ 8.2432e+00,  9.9321e+00,  1.1652e+01],\n        [ 1.5310e+01,  1.7969e+01,  2.0619e+01],\n        [ 2.4245e+01,  2.8124e+01,  3.1951e+01],\n        [ 3.4665e+01,  3.9972e+01,  4.5168e+01]]), std=tensor([[0.3357, 0.3006, 0.3877],\n        [0.3243, 0.2841, 0.3709],\n        [0.3201, 0.2782, 0.3654],\n        [0.3202, 0.2780, 0.3656],\n        [0.3243, 0.2841, 0.3713],\n        [0.3358, 0.3004, 0.3874]]))\n\n\n\npred = l.predict(T)\n\n\npred.mean.shape\n\ntorch.Size([6, 3])\n\n\n\npred.std.shape\n\ntorch.Size([6, 3])\n\n\n\n\nCheck learning is working\nThe idea is to use the current model to generate a dataset, that can be for sure modelled using a GPFA (because is the output of GPFA) and then train another model and see if the parameters converge\n\n# create a dummy GPFA with 3 features\nLt = GPFALearner(X)\n\n\ntest_params = {\n   \"Lambda\": torch.tensor([-1, 0.3, .8]).reshape(Lt.n_features, -1),\n   \"psi\": torch.tensor([1e-5, 5e-5, 2e-5]),\n}\n\n\nLt.model.covar_module.initialize(**test_params)\n\nGPFAKernel(\n  (latent_kernels): ModuleList(\n    (0): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n  )\n  (raw_psi_diag_constraint): Positive()\n)\n\n\n\nLt.model.covar_module.latent_kernels[0].initialize(lengthscale = torch.tensor(5))\n\nRBFKernel(\n  (raw_lengthscale_constraint): Positive()\n)\n\n\n\ntarget_X = Lt.predict(T).mean\n\n\nl2 = GPFALearner(target_X)\n\n\nl2.train()\n\n\n\n\n\nl2.predict(T).mean - target_X\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n  warnings.warn(\n\n\ntensor([[ 0.0060,  0.0016,  0.0008],\n        [ 0.0034,  0.0011,  0.0009],\n        [ 0.0011,  0.0004,  0.0003],\n        [-0.0012, -0.0004, -0.0004],\n        [-0.0036, -0.0011, -0.0008],\n        [-0.0058, -0.0015, -0.0008]])\n\n\nthey seems pretty small numbers, so the model is working!\n\nprint(\"Lambda:\\n\", l2.model.covar_module.Lambda.detach())\n\nprint(\"psi: \", l2.model.covar_module.psi.detach())\n\nprint(\"lengthscale:\", l2.model.covar_module.latent_kernels[0].lengthscale.item())\n\nLambda:\n tensor([[-1.6031],\n        [ 1.5766],\n        [ 1.5863]])\npsi:  tensor([4.8988e-05, 4.2124e-05, 4.1885e-05])\nlengthscale: 5.493414402008057\n\n\n\n\nConditional Predictions\nThis add the supports for conditional predictions, which means that at the time (t) when we are making the predictions some of the variables have been actually observed. Since the model prediction is a normal distribution we can condition on the observed values and thus improve the predictions.\nTherefore we need to compute the conditional distribution of a normal 1\n\\[ X = \\left[\\begin{array}{c} x \\\\ o \\end{array} \\right] \\]\n\\[ p(X) = N\\left(\\left[ \\begin{array}{c} \\mu_x \\\\ \\mu_o \\end{array} \\right], \\left[\\begin{array}{cc} \\Sigma_{xx} & \\Sigma_{xo} \\\\ \\Sigma_{ox} & \\Sigma_{oo} \\end{array} \\right]\\right)\\]\nwhere \\(X\\) is a vector of variable that need to predicted and \\(o\\) is a vector of the variables that have been observed\nThe mean is in “flat format”, where all the features from one time step are next to each other followed by the features of the next time step.\nthen\n\\[p(x|o) = N(\\mu_x + \\Sigma_{xo}\\Sigma_{oo}^{-1}(o - \\mu_o), \\Sigma_{xx} - \\Sigma_{xo}\\Sigma_{oo}^{-1}\\Sigma_{ox})\\]\n\n# example distribution with only 2 variables\nμ = torch.tensor([.5, 1.])\nΣ = torch.tensor([[1., .5], [.5 ,1.]])\n\ngauss = MultivariateNormal(μ, Σ)\n\nidx = torch.tensor([True, False]) # second variable is the observed one\n\nobs = torch.tensor(5.) # value of second variable\n\ngauss_cond = conditional_guassian(gauss, obs, idx)\n\n# hardcoded values to test that the code is working, see also for alternative implementation https://python.quantecon.org/multivariate_normal.html\ntest_close(3.25, gauss_cond.mean.item())\ntest_close(.75, gauss_cond.covariance_matrix.item())\n\nTest with multiple variables?\noverwrite the predict method to add support for conditional predictions\nNeed to have the mean and std for both the conditional predictions and the observations, with the same shape and order of the complete prediction.\n\ngauss_cond.covariance_matrix\n\ntensor([[0.7500]])\n\n\n\nmerge_pred = _merge_raw_cond_pred(gauss, gauss_cond, obs, idx)\nmerge_pred\n\nNormalParameters(mean=tensor([5.0000, 3.2500]), std=tensor([0.0000, 0.8660]))\n\n\n\n# manually calculated\ntest_close(merge_pred.mean, torch.tensor([5., 3.25]))\ntest_close(merge_pred.std, torch.tensor([0., math.sqrt(.75)]))\n\nThe problem is that the mean and the std for normalization are different for each feature, so in order to have the normalization working it is necessary to give the observations as a 2D array and not like a 1D array (like required by the model)\n\nT_pred = torch.tensor([6, 7])\n\n\nl.predict(T_pred)\n\nNormalParameters(mean=tensor([[45.7915, 52.6208, 59.2834],\n        [56.5321, 64.8324, 72.9081]]), std=tensor([[0.6919, 0.7510, 0.8597],\n        [1.7853, 2.0160, 2.2576]]))\n\n\n\nidx = torch.zeros(T_pred.shape[0] * X.shape[1], dtype=torch.bool)\n# simulate an observation using sensible numbers from the prediction\nidx[[0,2]] = torch.tensor([True, True])\nobs = torch.tensor([42., 61.])\n\n\nl._normalize_obs(obs, idx)\n\ntensor([2.0806, 2.4525])\n\n\n\nsource\n\n\nGPFALearner.predict\n\n GPFALearner.predict (T:torch.Tensor, obs:torch.Tensor=None,\n                      idx:torch.Tensor=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nTensor\n\n(n_pred) time where prediction is needed\n\n\nobs\nTensor\nNone\n(n_obs_pred) Optional - if at the times of the prediction there are some observationsarray with the values of observations to condition distribution\n\n\nidx\nTensor\nNone\n((n_pred*n_features)) Optional - necessary if obs are presentBoolean array that is True where an observation is present and False where a prediction is neededThis is a 1D array with the length equal to n_pred (number time steps to predict) times n_features\n\n\n\n\nl._normalize_obs(obs, idx)\n\ntensor([2.0806, 2.4525])\n\n\n\nobs\n\ntensor([42., 61.])\n\n\n\nl.predict_raw(T_pred).mean\n\ntensor([2.3640, 2.3632, 2.3518, 3.1668, 3.1659, 3.1506],\n       grad_fn=<ViewBackward0>)\n\n\n\nl.predict(T_pred, obs, idx)\n\nNormalParameters(mean=tensor([[42.0000, 51.6759, 61.0000],\n        [54.2909, 62.2784, 70.0641]]), std=tensor([[0.0000, 0.3322, 0.0000],\n        [0.7696, 0.8428, 0.9601]]))\n\n\n\nl.predict(T_pred)\n\nNormalParameters(mean=tensor([[45.7915, 52.6208, 59.2834],\n        [56.5321, 64.8324, 72.9081]]), std=tensor([[0.6919, 0.7510, 0.8597],\n        [1.7853, 2.0160, 2.2576]]))\n\n\nThere is a small change in the predicted values after conditioning as you would expect"
  },
  {
    "objectID": "learner.html#gpu-support",
    "href": "learner.html#gpu-support",
    "title": "GPFA Learner",
    "section": "GPU Support",
    "text": "GPU Support\nadd support for CUDA to model\n\n# l for learner\nl_cuda = GPFALearner(X.cuda())\n\nAttrs of interest are:\n\nT\nX\nlikelihood\nmodel\nnorm\n\ncuda() modifies in place the tensors and the modules!\n\nl_cuda.T.cuda()\n\ntensor([0, 1, 2, 3, 4, 5], device='cuda:0')\n\n\n\nl_cuda.T.device\n\ndevice(type='cuda', index=0)\n\n\n\n@patch\ndef cuda(self: GPFALearner):\n    \"\"\"Moves all learner to gpu\"\"\"\n    for par in ['T', 'X', 'model', 'likelihood']:\n        self.__getattribute__(par).cuda()\n    self.norm.x_mean.cuda()\n    self.norm.x_std.cuda()\n\n\nsource\n\nGPFALearner.cuda\n\n GPFALearner.cuda ()\n\nMoves all learner to gpu\n\nl_cuda.cuda()\n\nparameters are on the gpu!\n\nnext(l_cuda.likelihood.parameters()).device\n\ndevice(type='cuda', index=0)\n\n\n\ngpytorch.distributions.MultivariateNormal(torch.zeros(1).cuda(), torch.ones(1, 1).cuda())\n\nMultivariateNormal(loc: tensor([0.], device='cuda:0'), covariance_matrix: tensor([[1.]], device='cuda:0'))\n\n\n\nl_cuda.model.covar_module.latent_kernels[0].lengthscale.device\n\ndevice(type='cuda', index=0)\n\n\n\nl_cuda.train()"
  },
  {
    "objectID": "learner.html#multi-dimensional-latent",
    "href": "learner.html#multi-dimensional-latent",
    "title": "GPFA Learner",
    "section": "Multi-dimensional latent",
    "text": "Multi-dimensional latent\n\nl2 = GPFALearner(X, T, latent_dims=2)\n\n\nl2.train()\n\n\n\n\n\nplt.plot(l2.losses)\n\n\n\n\n\nl2.model.covar_module.Lambda\n\nParameter containing:\ntensor([[0.9218, 1.7595],\n        [0.9006, 1.7484],\n        [0.9687, 1.6968]], requires_grad=True)"
  },
  {
    "objectID": "learner.html#export",
    "href": "learner.html#export",
    "title": "GPFA Learner",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "learner.html#printing",
    "href": "learner.html#printing",
    "title": "GPFA Learner",
    "section": "Printing",
    "text": "Printing\n\nOther\n\n# #| exporti\n# def get_parameter_value(name, param, constraint):\n#     if constraint is not None:\n#         value = constraint.transform(param.data.detach())\n#         name = name.replace(\"raw_\", \"\") # parameter is not raw anymore\n#     else:\n#         value = param.data.detach()\n#     return (name, value)\n\n# name = \"covar_module.psi\"\n# test_eq(l.model.covar_module.psi.detach(), get_parameter_value(name, l.model.covar_module.raw_psi_diag, l.model.covar_module.raw_psi_diag_constraint)[1])\n\n# #| exporti\n# def tensor_to_first_item(tensor):\n#     if tensor.dim() > 0:\n#         return tensor_to_first_item(tensor[0])\n#     return tensor.item()\n\n\n# def format_parameter(name, value):\n#     value = tensor_to_first_item(value)\n#     name = name.split(\".\")[-1] # get only last part of name\n#     return f\"{name}: {value:.3f}\"\n\n# #| export\n# @patch\n# def get_formatted_params(self: GPFALearner):\n#     return \", \".join([\n#         format_parameter(*get_parameter_value(name, value, constraint))\n#         for name, value, constraint in\n#         self.model.named_parameters_and_constraints()\n#     ])\n\n# l.get_formatted_params()\n\n# # this is not really working at the moment, but it's not important\n# @patch\n# def plot_loss_printer(self: GPFALearner, i_iter):\n#     if i_iter ==0: return\n#     x = torch.arange(0, i_iter)\n#     y = self.losses[:i_iter]\n#     plot_data = [[x, y]]\n#     self.pb.update_graph(plot_data)\n    \n#     x_bounds = [x.min(), x.max()+1]\n#     y_bounds = [y.min(), y.max()]\n#     self.pb.names = [\"Training loss\"]\n\n# #|export\n# @patch\n# def printer(self: GPFALearner, i_iter):\n\n#     if i_iter%10 == 0:\n#         update_str = f\"loss: {self.losses[i_iter].item():.3f}, \" + self.get_formatted_params()\n#         #self.plot_loss(i_iter)\n    \n#     #self.pb.write(update_str)\n\n# l.train(lr = 0.01)\n\n# import matplotlib.pyplot as plt\n\n# plt.plot(l.losses)"
  },
  {
    "objectID": "simplegp_imputation.html",
    "href": "simplegp_imputation.html",
    "title": "Simple GP Imputation",
    "section": "",
    "text": "source\n\n\n\n SimpleGP (train_x, train_y, likelihood)\n\nExact GP implemnetation using GPyTorch\n\nk = SimpleGP(torch.tensor([1,2,3]), torch.tensor([1,2,3]), gpytorch.likelihoods.GaussianLikelihood())\n\n\nk\n\nSimpleGP(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): ConstantMean()\n  (covar_module): ScaleKernel(\n    (base_kernel): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n    (raw_outputscale_constraint): Positive()\n  )\n)\n\n\n\nk.covar_module.base_kernel.lengthscale.item()\n\n0.6931471824645996\n\n\n\nk.covar_module.outputscale.item()\n\n0.6931471824645996\n\n\n\nsource\n\n\n\n\n SimpleGP.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nk.get_info()\n\n{'lengthscale':    lengthscale\n 0     0.693147,\n 'outputscale':    outputscale\n 0     0.693147,\n 'likelihood':       noise\n 0  0.693247}"
  },
  {
    "objectID": "simplegp_imputation.html#learner",
    "href": "simplegp_imputation.html#learner",
    "title": "Simple GP Imputation",
    "section": "Learner",
    "text": "Learner\n\nsource\n\nSimpleGPLearner\n\n SimpleGPLearner (X:torch.Tensor, T:torch.Tensor=None)\n\nLearner for a simple GP process. It handles only 1 dimensional time series\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nTensor\n\n(n_obs) Univariate time series\n\n\nT\nTensor\nNone\n(n_obs) Vector of time of observations.\n\n\n\n\nX = torch.tensor([1.,2,3,4])\n\n\nl = SimpleGPLearner(X)\n\n\nl.train()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n\n\n\nl.predict(torch.tensor([5, 7]))\n\nNormalParameters(mean=tensor([5.5955, 6.4173]), std=tensor([0.1980, 0.6468]))\n\n\nImputation\nThe imputation using simple GPs make a separate GP process for each variable, which are completely independent\n\nsource\n\n\nSimpleGPImputationExplorer\n\n SimpleGPImputationExplorer (data:pandas.core.frame.DataFrame, cuda=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nobserved data with missing data as NA\n\n\ncuda\nbool\nFalse\nUse GPU?\n\n\n\n\nfd = GPFADataTest.generate(2, 7).add_gap(3, [\"x1\"]).add_gap(2, [\"x0\"])\n\n\ngp_imp = SimpleGPImputationExplorer(fd.data)\n\n\ngp_imp\n\nSimple GP Imputation Explorer:\n    N obs: 7\n    N features 2 (x0, x1)\n    N missing observations 5\n\n\n\ngp_imp.fit()\n\n\n\n\n\n\n\nSimple GP Imputation Explorer:\n    N obs: 7\n    N features 2 (x0, x1)\n    N missing observations 5\n\n\n\ngp_imp.learners[0].predict(torch.tensor([3.]))\n\nNormalParameters(mean=tensor([-0.1003]), std=tensor([0.2664]))\n\n\n\ngp_imp.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      0.322117\n      0.294301\n    \n    \n      1\n      -0.119194\n      0.224219\n    \n    \n      2\n      -0.193881\n      -0.017484\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n    \n    \n      5\n      0.540541\n      NaN\n    \n    \n      6\n      -0.410130\n      -0.271418\n    \n  \n\n\n\n\n\ngp_imp.train_data\n\ntensor([[ 0.3221,  0.2943],\n        [-0.1192,  0.2242],\n        [-0.1939, -0.0175],\n        [-0.4101, -0.2714]])\n\n\n\ngp_imp.predict()\n\n\n\n\n\n  \n    \n      \n      mean\n      time\n      std\n      variable\n    \n  \n  \n    \n      0\n      0.105723\n      0.0\n      0.232544\n      x0\n    \n    \n      1\n      -0.109509\n      1.0\n      0.232544\n      x0\n    \n    \n      2\n      -0.145935\n      2.0\n      0.232544\n      x0\n    \n    \n      3\n      -0.100289\n      3.0\n      0.266372\n      x0\n    \n    \n      4\n      -0.100289\n      4.0\n      0.266372\n      x0\n    \n    \n      5\n      -0.100289\n      5.0\n      0.266372\n      x0\n    \n    \n      6\n      -0.251402\n      6.0\n      0.232544\n      x0\n    \n    \n      0\n      0.294510\n      0.0\n      0.023458\n      x1\n    \n    \n      1\n      0.220237\n      1.0\n      0.023208\n      x1\n    \n    \n      2\n      -0.014538\n      2.0\n      0.023457\n      x1\n    \n    \n      3\n      -0.220033\n      3.0\n      0.083291\n      x1\n    \n    \n      4\n      -0.293843\n      4.0\n      0.150042\n      x1\n    \n    \n      5\n      -0.293429\n      5.0\n      0.122185\n      x1\n    \n    \n      6\n      -0.270590\n      6.0\n      0.023595\n      x1"
  },
  {
    "objectID": "simplegp_imputation.html#results",
    "href": "simplegp_imputation.html#results",
    "title": "Simple GP Imputation",
    "section": "results",
    "text": "results\n\nself = gp_imp\ninfos = [learner.model.get_info() for learner in self.learners]\n\n\ninfos\n\n[{'lengthscale':    lengthscale\n  0     0.192713,\n  'outputscale':    outputscale\n  0     0.366132,\n  'likelihood':       noise\n  0  0.384582},\n {'lengthscale':    lengthscale\n  0     1.679326,\n  'outputscale':    outputscale\n  0     0.910373,\n  'likelihood':       noise\n  0  0.004235}]\n\n\n\nsource\n\nSimpleGPImputationExplorer.model_info\n\n SimpleGPImputationExplorer.model_info ()\n\nCombine parameters of different kernels into one output\n\ngp_imp.model_info()\n\n{'lengthscale':   variable  lengthscale\n 0       x0     0.192713\n 0       x1     1.679326,\n 'outputscale':   variable  outputscale\n 0       x0     0.366132\n 0       x1     0.910373,\n 'likelihood':   variable     noise\n 0       x0  0.384582\n 0       x1  0.004235}\n\n\n\nsource\n\n\nSimpleGPImputationExplorer.to_result\n\n SimpleGPImputationExplorer.to_result (data_complete, units=None)\n\n\nGPFAResult??\n\n\nInit signature:\nGPFAResult(\n    data_imputed,\n    data_complete,\n    model_info,\n    units=None,\n    metrics_all_data=True,\n)\nDocstring:      <no docstring>\nSource:        \nclass GPFAResult:\n    def __init__(self,\n                 data_imputed, #imputed data in tidy format\n                 data_complete, # complete data in tidy format\n                 model_info, # learner for parameters display\n                 units = None, # units for plots\n                 metrics_all_data = True # Compute metrics only for gap or for all data?\n                ):\n        store_attr()\nFile:           ~/Documents/uni/Thesis/GPFA_imputation/gpfa_imputation/results.py\nType:           type\nSubclasses:     \n\n\n\n\n\ngp_imp.to_result(fd.data_compl_tidy).display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      x0\n      0.2123\n    \n    \n      x1\n      0.7596\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n    \n  \n  \n    \n      x0\n      0.2954\n    \n    \n      x1\n      0.1396\n    \n  \n\n \n\n\n\nModel Info  lengthscale \n\n  \n    \n      variable\n      lengthscale\n    \n  \n  \n    \n      x0\n      0.1927\n    \n    \n      x1\n      1.6793\n    \n  \n\n  outputscale \n\n  \n    \n      variable\n      outputscale\n    \n  \n  \n    \n      x0\n      0.3661\n    \n    \n      x1\n      0.9104\n    \n  \n\n  likelihood \n\n  \n    \n      variable\n      noise\n    \n  \n  \n    \n      x0\n      0.3846\n    \n    \n      x1\n      0.0042"
  },
  {
    "objectID": "data_preparation.html#data-generator",
    "href": "data_preparation.html#data-generator",
    "title": "Data Preparation",
    "section": "Data generator",
    "text": "Data generator\ngenerate some fake data in order to test the imputation\nWhat is does is: - take a function to generate the “true” latent - use some random coefficient to generate all the N features - add some random noise\n\nclass GPFADataGenerator:\n    def __init__(self,\n                    n_features: int,\n                    n_obs: int,\n                    latent_func = lambda x: torch.sin(3*x), # Functions used to generate the true latent\n                    noise_std = .2,\n                    Lambda = None\n                ):\n        \n        self.n_features, self.n_obs = n_features, n_obs\n        self.time = torch.arange(0, self.n_obs, dtype=torch.float)\n        \n        self.latent = latent_func(self.time)\n        \n        self.Lambda = torch.tensor(Lambda).reshape(n_features, 1) if Lambda is not None else torch.rand(n_features, 1)\n        \n        self.exact_X = (self.Lambda * self.latent).T\n        \n        self.X =  self.exact_X + torch.normal(0., noise_std, size = (n_obs, n_features)) \n        \n        self.data = pd.DataFrame(self.X.numpy(), columns = [f\"x{i}\" for i in range(self.n_features)])\n\n\nsource\n\nGPFADataGenerator\n\n GPFADataGenerator (n_features:int, n_obs:int, latent_func=<function\n                    <lambda>>, noise_std=0.2, Lambda=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nint\n\n\n\n\nn_obs\nint\n\n\n\n\nlatent_func\nfunction\n\nFunctions used to generate the true latent\n\n\nnoise_std\nfloat\n0.2\n\n\n\nLambda\nNoneType\nNone\n\n\n\n\n\nfdg = GPFADataGenerator(3, 4)\n\n\nfd_plot = pd.DataFrame(fdg.X.numpy(), columns = [\"x1\", \"x2\", \"x3\"])\nfd_plot[\"latent\"] = pd.Series(fdg.latent.numpy())\n\n\nfd_plot.plot()\n\n<AxesSubplot: >"
  },
  {
    "objectID": "data_preparation.html#missing-data",
    "href": "data_preparation.html#missing-data",
    "title": "Data Preparation",
    "section": "Missing Data",
    "text": "Missing Data\n\nclass GPFADataTest:\n    \"Utility class to keep track of dataset, missing data and export to right format\"\n    def __init__(self, data: pd.DataFrame):\n        \" Init with provided dataset\"\n        self.data = data.copy()\n        self.data_complete = self.data.copy()\n        self.n_features, self.n_obs = data.shape[1], data.shape[0]\n        self.time = torch.arange(0, self.n_obs, dtype=torch.float)\n    @classmethod\n    def generate(cls, *args, **kwargs):\n        generator = GPFADataGenerator(*args, **kwargs)\n        self = GPFADataTest(generator.data)\n        self.generator = generator\n        return self\n\n\nsource\n\nGPFADataTest\n\n GPFADataTest (data:pandas.core.frame.DataFrame)\n\nUtility class to keep track of dataset, missing data and export to right format\n\n\nMissing Data\ngenerate artificial gaps in the data\n\nMissing at Random\n\n@patch()\ndef add_random_missing(self: GPFADataTest,\n                prob_miss_row: float = .2,#  Probability an entire row is missing\n                prob_miss_value: float = .1 # Probability a single observation is missing       \n               ):\n    \"\"\"Make some row and same values randomly missing \"\"\"\n    # keep the original data\n        \n    self.is_miss_row = torch.rand(self.n_obs) <= prob_miss_row\n    \n    self.data[self.is_miss_row.numpy()] = np.nan\n    \n    self.is_miss_value = (torch.rand(self.n_obs * self.n_features) <= prob_miss_value).reshape(-1, self.n_features)\n    \n    self.data[self.is_miss_value.numpy()] = np.nan\n    \n    return self\n\n\nsource\n\n\n\nGPFADataTest.add_random_missing\n\n GPFADataTest.add_random_missing (prob_miss_row:float=0.2,\n                                  prob_miss_value:float=0.1)\n\nMake some row and same values randomly missing\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob_miss_row\nfloat\n0.2\nProbability an entire row is missing\n\n\nprob_miss_value\nfloat\n0.1\nProbability a single observation is missing\n\n\n\n\nfd = GPFADataTest.generate(3, 4)\n\n\nfd.add_random_missing().data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x2\n    \n  \n  \n    \n      0\n      -0.001938\n      -0.127557\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      0.137062\n      0.151774\n      -0.092625\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nGPFADataTest.generate(2, 10).add_random_missing(prob_miss_value = .7, prob_miss_row=.0).data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      -0.203655\n      -0.117674\n    \n    \n      1\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      -0.111792\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n    \n    \n      5\n      NaN\n      0.112875\n    \n    \n      6\n      NaN\n      NaN\n    \n    \n      7\n      NaN\n      NaN\n    \n    \n      8\n      NaN\n      NaN\n    \n    \n      9\n      NaN\n      0.409263\n    \n  \n\n\n\n\n\nContinous gap\nThe missing data is all clustered around a gap\nparameteres: - gap length - variable for gap\n\ndef _make_random_gap(\n    gap_length: int, # The length of the gap\n    total_length: int # The total number of observations\n): # (total_length) array of bools to indicicate if the data is missing or not\n    \"Add a continous gap of ginve length at random position\"\n    if(gap_length >= total_length):\n        return np.repeat(True, total_length)\n    gap_start = np.random.randint(total_length - gap_length)\n    return np.hstack([\n        np.repeat(False, gap_start),\n        np.repeat(True, gap_length),\n        np.repeat(False, total_length - (gap_length + gap_start))\n    ])\n\n\n_make_random_gap(3, 10)\n\narray([False, False, False, False,  True,  True,  True, False, False,\n       False])\n\n\n\ntest_eq(_make_random_gap(3, 10).sum(), 3) # correct gap length\n\n\n@patch\ndef add_gap(self: GPFADataTest,\n            gap_length:int, # length of gap\n            variables: list # variables that should be affected by the gap\n           ):    \n    \n    \n    self.is_gap = _make_random_gap(gap_length, self.data.shape[0])\n    self.data.loc[self.is_gap, variables] = np.nan\n    return self\n\n\nsource\n\n\n\nGPFADataTest.add_gap\n\n GPFADataTest.add_gap (gap_length:int, variables:list)\n\n\n\n\n\nType\nDetails\n\n\n\n\ngap_length\nint\nlength of gap\n\n\nvariables\nlist\nvariables that should be affected by the gap\n\n\n\n\nGPFADataTest.generate(5, 10).add_gap(4, [\"x1\", \"x2\"]).data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x2\n      x3\n      x4\n    \n  \n  \n    \n      0\n      0.147414\n      -0.162167\n      0.534020\n      -0.314157\n      -0.145421\n    \n    \n      1\n      0.227970\n      NaN\n      NaN\n      0.298646\n      0.023241\n    \n    \n      2\n      -0.277233\n      NaN\n      NaN\n      0.011927\n      0.350378\n    \n    \n      3\n      0.081553\n      NaN\n      NaN\n      0.091917\n      0.162622\n    \n    \n      4\n      -0.488591\n      NaN\n      NaN\n      -0.383358\n      -0.142712\n    \n    \n      5\n      0.850323\n      0.871718\n      0.314558\n      0.378259\n      -0.198052\n    \n    \n      6\n      -0.554473\n      -0.438560\n      -0.345700\n      -0.368945\n      -0.081258\n    \n    \n      7\n      0.904893\n      0.390038\n      0.473491\n      0.916413\n      -0.053904\n    \n    \n      8\n      -0.463149\n      -0.567804\n      -0.318116\n      -0.687993\n      0.118367\n    \n    \n      9\n      0.931138\n      0.398865\n      0.561607\n      0.701557\n      0.026598\n    \n  \n\n\n\n\n\n\nSave as DataFrame\n\n@patch\ndef tidy_df(self: GPFADataTest,\n          complete = False, # full dataset (False) or the one with missing data (True)\n          is_missing = False # add flag whether value is missing\n         ):\n    \n    df = self.data if not complete else self.data_complete # no need to copy here because next lines does a copy anyway\n    df = df.assign(time = self.time.numpy())\n        \n    df = df.melt(\"time\")\n    \n    if is_missing: df = df.assign(is_missing = self.data.melt().value.isna()) #missing data is not from complete data\n        \n    return df\n\n\nsource\n\n\nGPFADataTest.tidy_df\n\n GPFADataTest.tidy_df (complete=False, is_missing=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncomplete\nbool\nFalse\nfull dataset (False) or the one with missing data (True)\n\n\nis_missing\nbool\nFalse\nadd flag whether value is missing\n\n\n\n\nfd.tidy_df()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n    \n    \n      1\n      1.0\n      x0\n      NaN\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n    \n    \n      3\n      3.0\n      x0\n      NaN\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n    \n    \n      5\n      1.0\n      x1\n      NaN\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n    \n    \n      7\n      3.0\n      x1\n      NaN\n    \n    \n      8\n      0.0\n      x2\n      NaN\n    \n    \n      9\n      1.0\n      x2\n      NaN\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n    \n    \n      11\n      3.0\n      x2\n      NaN\n    \n  \n\n\n\n\n\nfd.tidy_df(complete=True)\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n    \n    \n      1\n      1.0\n      x0\n      0.021966\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n    \n    \n      3\n      3.0\n      x0\n      0.031551\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n    \n    \n      5\n      1.0\n      x1\n      0.295225\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n    \n    \n      7\n      3.0\n      x1\n      0.116207\n    \n    \n      8\n      0.0\n      x2\n      0.139568\n    \n    \n      9\n      1.0\n      x2\n      0.029323\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n    \n    \n      11\n      3.0\n      x2\n      0.160619\n    \n  \n\n\n\n\n\nfd.tidy_df(complete=False, is_missing=True)\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_missing\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n      False\n    \n    \n      1\n      1.0\n      x0\n      NaN\n      True\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n      False\n    \n    \n      3\n      3.0\n      x0\n      NaN\n      True\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n      False\n    \n    \n      5\n      1.0\n      x1\n      NaN\n      True\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n      False\n    \n    \n      7\n      3.0\n      x1\n      NaN\n      True\n    \n    \n      8\n      0.0\n      x2\n      NaN\n      True\n    \n    \n      9\n      1.0\n      x2\n      NaN\n      True\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n      False\n    \n    \n      11\n      3.0\n      x2\n      NaN\n      True\n    \n  \n\n\n\n\nThe export as a dataframe is working correctly with the missing data\n\nfd_df = fd.tidy_df()\n\n\nalt.Chart(fd_df).mark_line(point=True).encode(\n    x = \"time\",\n    y = \"value\",\n    color = \"variable\"\n)\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nfd.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x2\n    \n  \n  \n    \n      0\n      -0.001938\n      -0.127557\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      0.137062\n      0.151774\n      -0.092625\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nsource\n\n\nGPFADataTest.data_compl_tidy\n\n GPFADataTest.data_compl_tidy ()\n\n\nfd.data_compl_tidy\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_missing\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n      False\n    \n    \n      1\n      1.0\n      x0\n      0.021966\n      True\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n      False\n    \n    \n      3\n      3.0\n      x0\n      0.031551\n      True\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n      False\n    \n    \n      5\n      1.0\n      x1\n      0.295225\n      True\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n      False\n    \n    \n      7\n      3.0\n      x1\n      0.116207\n      True\n    \n    \n      8\n      0.0\n      x2\n      0.139568\n      True\n    \n    \n      9\n      1.0\n      x2\n      0.029323\n      True\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n      False\n    \n    \n      11\n      3.0\n      x2\n      0.160619\n      True\n    \n  \n\n\n\n\n\n\nNormalization\nThe different variables in the can have pretty different values so we normalize so they are more comparable. Have numbers between 0 and 1 should also help with the computation accuracy.\nOne additional complexity is the need to backtransform not only the mean but also the standard deviation.\nSo we need a but of math\n\\[x_{norm} = \\frac{x - \\mu_x}{\\sigma_x}\\] then \\[x = x_{norm}\\sigma_x + \\mu_x \\]\nusing properties of Guassian distributions 1\n\\[p(x_{norm}) = \\mathcal{N}(\\mu_{norm}, \\sigma^2_{norm})\\]\n\\[p(x) = \\mathcal{N}(\\sigma_x\\mu_{norm} + \\mu_x, \\sigma^2_x \\sigma^2_{norm})\\]\n\nsource\n\n\nNormalizer\n\n Normalizer (x:torch.Tensor)\n\nInit normalizer by storing mean and std dev\n\n\n\n\nType\nDetails\n\n\n\n\nx\nTensor\nup to 2D Tensor\n\n\n\n\nx = torch.randn(20).reshape(-1,2)\nnorm = Normalizer(x)\ntest_close(x, norm.reverse_normalize(norm.normalize(x)))\ntest_close(x.std(axis=0), norm.reverse_normalize_std(norm.normalize(x).std(axis=0)))"
  },
  {
    "objectID": "data_preparation.html#log-transform",
    "href": "data_preparation.html#log-transform",
    "title": "Data Preparation",
    "section": "Log transform",
    "text": "Log transform\n\nsource\n\nGPFADataTest.log_transform\n\n GPFADataTest.log_transform (vars:Union[str,Collection[str]])\n\nTranform the given var with log(x+1)\n\n\n\n\nType\nDetails\n\n\n\n\nvars\nUnion\nlist of variables names to log-transform\n\n\nReturns\nGPFADataTest\n\n\n\n\n\nGPFADataTest.generate(3, 4).log_transform(['x1']).data\n\n\n\n\n\n  \n    \n      \n      x0\n      x2\n      log_x1\n    \n  \n  \n    \n      0\n      -0.037518\n      -0.187460\n      0.284498\n    \n    \n      1\n      0.103395\n      0.440612\n      0.223854\n    \n    \n      2\n      -0.655654\n      -0.328445\n      -0.278969\n    \n    \n      3\n      0.422344\n      0.429424\n      0.268449\n    \n  \n\n\n\n\n\norig_data = GPFADataTest.generate(1, 200)\n\norig_data.data = np.abs(orig_data.data)\n\ndata = orig_data.log_transform(['x0'])\n\n\ndata.data.hist()\n\narray([[<AxesSubplot: title={'center': 'log_x0'}>]], dtype=object)\n\n\n\n\n\n\nNormalizer(data.data).normalize(data.data).hist()\n\narray([[<AxesSubplot: title={'center': 'log_x0'}>]], dtype=object)"
  },
  {
    "objectID": "data_preparation.html#export",
    "href": "data_preparation.html#export",
    "title": "Data Preparation",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPFA Imputation",
    "section": "",
    "text": "This is developed by me, Simone Massaro, as part of my master thesis"
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "results.html#export",
    "href": "results.html#export",
    "title": "Results",
    "section": "Export",
    "text": "Export"
  }
]