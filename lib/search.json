[
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Implement Kalman model using FastAI",
    "section": "",
    "text": "reset_seed()\nhai_path\n\nPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/data/FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4_float32.parquet')\n# hai = pd.read_parquet(hai_path)\nhai = pd.read_parquet(hai_path64)\nhai_era = pd.read_parquet(hai_era_path64)\n# hai_era64 = pd.read_parquet(hai_era_path64)"
  },
  {
    "objectID": "training.html#data-preparation",
    "href": "training.html#data-preparation",
    "title": "Implement Kalman model using FastAI",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe aim of the data preparation pipeline is to: - take the original time series and split it into time blocks - for each block generate a random gap (need to figure out the properties of the gap) - split some time blocks for testing\nthe input of the pipeline is: - a dataframe containing all observations\nthe input of the model is: - observed data (potentially containing NaN where data is missing) - missing data mask (which is telling where the data is missing) - the data needs to be standardized\n\nUtils\n\n\nItem\n\nsource\n\n\nMeteoImpItem\n\n MeteoImpItem (i:int, shift:int, var_sel:list[str], gap_len:int)\n\n\nitem = MeteoImpItem(2, 3, 'TA', 10)\nitem\n\nMeteoImpItem(i=2, shift=3, var_sel=['TA'], gap_len=10)\n\n\n\n\n1) Block Index\nthe first step is to transfrom the original dataframe into blocks of a specified block_len\ntwo different strategies are possible:\n\ncontigous blocks\nrandom block in the dataframe\n\nfor now contigous blocks are used\n\nsource\n\n\nMeteoImpIndex\n\n MeteoImpIndex (index:pandas.core.indexes.datetimes.DatetimeIndex,\n                var_sel:list[str], gap_len:int)\n\n\nsource\n\n\nBlockIndexTransform\n\n BlockIndexTransform (idx:pandas.core.indexes.datetimes.DatetimeIndex,\n                      block_len:int=200, offset=1)\n\ndivide timeseries DataFrame index into blocks\n\nblk = BlockIndexTransform(hai.index, 10)\n\n\nblk\n\n\n  \n    BlockIndexTransform\n  \n  (MeteoImpItem,object) -&gt; encodes\n\n  \n\n\n\n\nblk(item)\n\nMeteoImpIndex(index=DatetimeIndex(['2000-01-01 12:30:00', '2000-01-01 13:00:00',\n               '2000-01-01 13:30:00', '2000-01-01 14:00:00',\n               '2000-01-01 14:30:00', '2000-01-01 15:00:00',\n               '2000-01-01 15:30:00', '2000-01-01 16:00:00',\n               '2000-01-01 16:30:00', '2000-01-01 17:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None), var_sel=['TA'], gap_len=10)\n\n\n\n\n2) Meteo Imp Block DataFrames\nGet a chunck out of dataframes given an index\n\nsource\n\n\nDataControl\n\n DataControl (data:pandas.core.frame.DataFrame,\n              control:pandas.core.frame.DataFrame, var_sel:list[str],\n              gap_len:int)\n\n\ndf = hai.loc[blk(item).index]\n\n\nAdd lag\n\ndf.rename(columns=_rename_lag(1))\n\n\n\n\n\n\n\n\nTA_lag_1\nSW_IN_lag_1\nVPD_lag_1\n\n\ntime\n\n\n\n\n\n\n\n2000-01-01 12:30:00\n0.33\n18.86\n0.008\n\n\n2000-01-01 13:00:00\n0.41\n21.10\n0.006\n\n\n2000-01-01 13:30:00\n0.44\n28.87\n0.000\n\n\n2000-01-01 14:00:00\n0.48\n24.22\n0.000\n\n\n2000-01-01 14:30:00\n0.49\n24.35\n0.000\n\n\n2000-01-01 15:00:00\n0.51\n15.68\n0.000\n\n\n2000-01-01 15:30:00\n0.52\n8.09\n0.000\n\n\n2000-01-01 16:00:00\n0.57\n6.37\n0.000\n\n\n2000-01-01 16:30:00\n0.73\n1.72\n0.000\n\n\n2000-01-01 17:00:00\n0.77\n0.06\n0.000\n\n\n\n\n\n\n\n\n_lag_df(df, 1)\n\n\n\n\n\n\n\n\nTA_lag_1\nSW_IN_lag_1\nVPD_lag_1\n\n\ntime\n\n\n\n\n\n\n\n2000-01-01 12:30:00\nNaN\nNaN\nNaN\n\n\n2000-01-01 13:00:00\n0.33\n18.86\n0.008\n\n\n2000-01-01 13:30:00\n0.41\n21.10\n0.006\n\n\n2000-01-01 14:00:00\n0.44\n28.87\n0.000\n\n\n2000-01-01 14:30:00\n0.48\n24.22\n0.000\n\n\n2000-01-01 15:00:00\n0.49\n24.35\n0.000\n\n\n2000-01-01 15:30:00\n0.51\n15.68\n0.000\n\n\n2000-01-01 16:00:00\n0.52\n8.09\n0.000\n\n\n2000-01-01 16:30:00\n0.57\n6.37\n0.000\n\n\n2000-01-01 17:00:00\n0.73\n1.72\n0.000\n\n\n\n\n\n\n\n\n_add_lags_df(df, [1,2])\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\nTA_lag_1\nSW_IN_lag_1\nVPD_lag_1\nTA_lag_2\nSW_IN_lag_2\nVPD_lag_2\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000-01-01 12:30:00\n0.33\n18.86\n0.008\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2000-01-01 13:00:00\n0.41\n21.10\n0.006\n0.33\n18.86\n0.008\nNaN\nNaN\nNaN\n\n\n2000-01-01 13:30:00\n0.44\n28.87\n0.000\n0.41\n21.10\n0.006\n0.33\n18.86\n0.008\n\n\n2000-01-01 14:00:00\n0.48\n24.22\n0.000\n0.44\n28.87\n0.000\n0.41\n21.10\n0.006\n\n\n2000-01-01 14:30:00\n0.49\n24.35\n0.000\n0.48\n24.22\n0.000\n0.44\n28.87\n0.000\n\n\n2000-01-01 15:00:00\n0.51\n15.68\n0.000\n0.49\n24.35\n0.000\n0.48\n24.22\n0.000\n\n\n2000-01-01 15:30:00\n0.52\n8.09\n0.000\n0.51\n15.68\n0.000\n0.49\n24.35\n0.000\n\n\n2000-01-01 16:00:00\n0.57\n6.37\n0.000\n0.52\n8.09\n0.000\n0.51\n15.68\n0.000\n\n\n2000-01-01 16:30:00\n0.73\n1.72\n0.000\n0.57\n6.37\n0.000\n0.52\n8.09\n0.000\n\n\n2000-01-01 17:00:00\n0.77\n0.06\n0.000\n0.73\n1.72\n0.000\n0.57\n6.37\n0.000\n\n\n\n\n\n\n\n\nsource\n\n\n\nBlockDfTransform\n\n BlockDfTransform (data:pandas.core.frame.DataFrame,\n                   control:pandas.core.frame.DataFrame,\n                   control_lags:Union[int,Iterable[int]])\n\ndivide timeseries DataFrame index into blocks\n\nblkdf = BlockDfTransform(hai, hai_era, 1)\n\n\nblkdf(blk(item))\n\n\nData Control (['TA'], 10)  data \n\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-01-01 12:30:00\n0.3300\n18.8600\n0.0080\n\n\n2000-01-01 13:00:00\n0.4100\n21.1000\n0.0060\n\n\n2000-01-01 13:30:00\n0.4400\n28.8700\n0.0000\n\n\n2000-01-01 14:00:00\n0.4800\n24.2200\n0.0000\n\n\n2000-01-01 14:30:00\n0.4900\n24.3500\n0.0000\n\n\n2000-01-01 15:00:00\n0.5100\n15.6800\n0.0000\n\n\n2000-01-01 15:30:00\n0.5200\n8.0900\n0.0000\n\n\n2000-01-01 16:00:00\n0.5700\n6.3700\n0.0000\n\n\n2000-01-01 16:30:00\n0.7300\n1.7200\n0.0000\n\n\n2000-01-01 17:00:00\n0.7700\n0.0600\n0.0000\n\n\n\n\ncontrol\n\n\n\n \nTA_ERA\nSW_IN_ERA\nVPD_ERA\nTA_ERA_lag_1\nSW_IN_ERA_lag_1\nVPD_ERA_lag_1\n\n\ntime\n \n \n \n \n \n \n\n\n\n\n2000-01-01 12:30:00\n1.1160\n26.1870\n0.5940\n0.9950\n25.5130\n0.5920\n\n\n2000-01-01 13:00:00\n1.2370\n25.9150\n0.5960\n1.1160\n26.1870\n0.5940\n\n\n2000-01-01 13:30:00\n1.3580\n15.7740\n0.5970\n1.2370\n25.9150\n0.5960\n\n\n2000-01-01 14:00:00\n1.4090\n14.4120\n0.5830\n1.3580\n15.7740\n0.5970\n\n\n2000-01-01 14:30:00\n1.4590\n12.4860\n0.5690\n1.4090\n14.4120\n0.5830\n\n\n2000-01-01 15:00:00\n1.5100\n10.0280\n0.5550\n1.4590\n12.4860\n0.5690\n\n\n2000-01-01 15:30:00\n1.5610\n7.0820\n0.5410\n1.5100\n10.0280\n0.5550\n\n\n2000-01-01 16:00:00\n1.6110\n3.6960\n0.5270\n1.5610\n7.0820\n0.5410\n\n\n2000-01-01 16:30:00\n1.6620\n0.0000\n0.5130\n1.6110\n3.6960\n0.5270\n\n\n2000-01-01 17:00:00\n1.8450\n0.0000\n0.5130\n1.6620\n0.0000\n0.5130\n\n\n\n\n\n \n\n\ntaking a day in the summer so there is an higher values for the variables\n\nblkdf(blk(MeteoImpItem(800,0, 'TA', 10))).data\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\n\n\ntime\n\n\n\n\n\n\n\n2000-06-15 17:00:00\n14.22\n224.80\n5.799\n\n\n2000-06-15 17:30:00\n14.11\n195.28\n6.577\n\n\n2000-06-15 18:00:00\n14.23\n244.17\n6.931\n\n\n2000-06-15 18:30:00\n14.40\n253.92\n7.286\n\n\n2000-06-15 19:00:00\n14.09\n177.31\n7.251\n\n\n2000-06-15 19:30:00\n13.71\n97.07\n6.683\n\n\n2000-06-15 20:00:00\n13.08\n39.71\n5.851\n\n\n2000-06-15 20:30:00\n12.41\n10.65\n5.254\n\n\n2000-06-15 21:00:00\n12.27\n0.32\n5.164\n\n\n2000-06-15 21:30:00\n12.20\n0.00\n5.037\n\n\n\n\n\n\n\n\ntfms1 = TfmdLists([MeteoImpItem(800+i,0, 'TA', 10) for i in range(3)], [BlockIndexTransform(hai.index, 10), BlockDfTransform(hai, hai_era, control_lags=1)])\n\n\ntfms1[0]\n\n\nData Control (['TA'], 10)  data \n\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-06-15 17:00:00\n14.2200\n224.8000\n5.7990\n\n\n2000-06-15 17:30:00\n14.1100\n195.2800\n6.5770\n\n\n2000-06-15 18:00:00\n14.2300\n244.1700\n6.9310\n\n\n2000-06-15 18:30:00\n14.4000\n253.9200\n7.2860\n\n\n2000-06-15 19:00:00\n14.0900\n177.3100\n7.2510\n\n\n2000-06-15 19:30:00\n13.7100\n97.0700\n6.6830\n\n\n2000-06-15 20:00:00\n13.0800\n39.7100\n5.8510\n\n\n2000-06-15 20:30:00\n12.4100\n10.6500\n5.2540\n\n\n2000-06-15 21:00:00\n12.2700\n0.3200\n5.1640\n\n\n2000-06-15 21:30:00\n12.2000\n0.0000\n5.0370\n\n\n\n\ncontrol\n\n\n\n \nTA_ERA\nSW_IN_ERA\nVPD_ERA\nTA_ERA_lag_1\nSW_IN_ERA_lag_1\nVPD_ERA_lag_1\n\n\ntime\n \n \n \n \n \n \n\n\n\n\n2000-06-15 17:00:00\n15.0500\n255.1930\n5.1020\n15.1390\n287.1000\n4.9000\n\n\n2000-06-15 17:30:00\n14.9610\n221.4270\n5.3050\n15.0500\n255.1930\n5.1020\n\n\n2000-06-15 18:00:00\n14.8720\n186.3800\n5.5070\n14.9610\n221.4270\n5.3050\n\n\n2000-06-15 18:30:00\n14.7830\n150.6500\n5.7100\n14.8720\n186.3800\n5.5070\n\n\n2000-06-15 19:00:00\n14.6940\n114.8490\n5.9120\n14.7830\n150.6500\n5.7100\n\n\n2000-06-15 19:30:00\n14.6060\n34.7280\n6.1140\n14.6940\n114.8490\n5.9120\n\n\n2000-06-15 20:00:00\n14.3800\n19.8430\n6.0740\n14.6060\n34.7280\n6.1140\n\n\n2000-06-15 20:30:00\n14.1550\n5.7120\n6.0340\n14.3800\n19.8430\n6.0740\n\n\n2000-06-15 21:00:00\n13.9290\n0.0000\n5.9940\n14.1550\n5.7120\n6.0340\n\n\n2000-06-15 21:30:00\n13.7040\n0.0000\n5.9540\n13.9290\n0.0000\n5.9940\n\n\n\n\n\n \n\n\n\n\n3) Gaps\nadds a mask which includes a random gap\n\nMake random Gap\n\nidx = L(*tfms1[0].data.columns).argwhere(lambda x: x in ['TA','SW_IN'])\n\n\nmask = np.ones_like(tfms1[0].data, dtype=bool)\n\n\nmask\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\n\ndef _make_random_gap(\n    gap_length: int, # The length of the gap\n    total_length: int, # The total number of observations\n    gap_start: int # Optional start of gap\n)-&gt; np.ndarray: # [total_length] array of bools to indicicate if the data is missing or not\n    \"Add a continous gap of given length at random position\"\n    if(gap_length &gt;= total_length):\n        return np.repeat(True, total_length)\n    return np.hstack([\n        np.repeat(False, gap_start),\n        np.repeat(True, gap_length),\n        np.repeat(False, total_length - (gap_length + gap_start))\n    ])\n\n\ngap = _make_random_gap(2, 10, 2)\n\n\ngap\n\narray([False, False,  True,  True, False, False, False, False, False,\n       False])\n\n\n\nnp.argwhere(gap)\n\narray([[2],\n       [3]])\n\n\n\nmask[np.argwhere(gap), idx] = False\n\n\nmask\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [False, False,  True],\n       [False, False,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\n\nmask[gap]\n\narray([[False, False,  True],\n       [False, False,  True]])\n\n\n\n\nAdd Gap Transform\n\nsource\n\n\n\nMeteoImpDf\n\n MeteoImpDf (data:pandas.core.frame.DataFrame,\n             mask:pandas.core.frame.DataFrame,\n             control:pandas.core.frame.DataFrame)\n\n\nsource\n\n\nAddGapTransform\n\n AddGapTransform ()\n\nAdds a random gap to a dataframe\n\na_gap = AddGapTransform(1)\na_gap\n\n\n  \n    AddGapTransform\n  \n  (DataControl,object) -&gt; encodes\n\n  \n\n\n\n\na_gap(tfms1[0])\n\n\nMeteo Imp Df  data \n\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-06-15 17:00:00\n14.2200\n224.8000\n5.7990\n\n\n2000-06-15 17:30:00\n14.1100\n195.2800\n6.5770\n\n\n2000-06-15 18:00:00\n14.2300\n244.1700\n6.9310\n\n\n2000-06-15 18:30:00\n14.4000\n253.9200\n7.2860\n\n\n2000-06-15 19:00:00\n14.0900\n177.3100\n7.2510\n\n\n2000-06-15 19:30:00\n13.7100\n97.0700\n6.6830\n\n\n2000-06-15 20:00:00\n13.0800\n39.7100\n5.8510\n\n\n2000-06-15 20:30:00\n12.4100\n10.6500\n5.2540\n\n\n2000-06-15 21:00:00\n12.2700\n0.3200\n5.1640\n\n\n2000-06-15 21:30:00\n12.2000\n0.0000\n5.0370\n\n\n\n\nmask\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-06-15 17:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 17:30:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 18:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 18:30:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 19:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 19:30:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 20:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 20:30:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 21:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 21:30:00\nFalse\nTrue\nTrue\n\n\n\n\n\ncontrol\n\n\n\n \nTA_ERA\nSW_IN_ERA\nVPD_ERA\nTA_ERA_lag_1\nSW_IN_ERA_lag_1\nVPD_ERA_lag_1\n\n\ntime\n \n \n \n \n \n \n\n\n\n\n2000-06-15 17:00:00\n15.0500\n255.1930\n5.1020\n15.1390\n287.1000\n4.9000\n\n\n2000-06-15 17:30:00\n14.9610\n221.4270\n5.3050\n15.0500\n255.1930\n5.1020\n\n\n2000-06-15 18:00:00\n14.8720\n186.3800\n5.5070\n14.9610\n221.4270\n5.3050\n\n\n2000-06-15 18:30:00\n14.7830\n150.6500\n5.7100\n14.8720\n186.3800\n5.5070\n\n\n2000-06-15 19:00:00\n14.6940\n114.8490\n5.9120\n14.7830\n150.6500\n5.7100\n\n\n2000-06-15 19:30:00\n14.6060\n34.7280\n6.1140\n14.6940\n114.8490\n5.9120\n\n\n2000-06-15 20:00:00\n14.3800\n19.8430\n6.0740\n14.6060\n34.7280\n6.1140\n\n\n2000-06-15 20:30:00\n14.1550\n5.7120\n6.0340\n14.3800\n19.8430\n6.0740\n\n\n2000-06-15 21:00:00\n13.9290\n0.0000\n5.9940\n14.1550\n5.7120\n6.0340\n\n\n2000-06-15 21:30:00\n13.7040\n0.0000\n5.9540\n13.9290\n0.0000\n5.9940\n\n\n\n\n\n \n\n\n\ntfms2 = TfmdLists(tfms1.items, [*tfms1.fs, AddGapTransform(5)])\n\n\nTidy\n\nm_df = a_gap(tfms1[0])\n\n\nsource\n\n\n\nMeteoImpDf.tidy\n\n MeteoImpDf.tidy (control_map:Optional[dict[str,str]]=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontrol_map\nOptional\nNone\nmapping from control var names to obs names\n\n\n\n\nm_df.tidy()\n\n\n\n\n\n\n\n\ntime\nvariable\nvalue\nis_present\n\n\n\n\n0\n2000-06-15 17:00:00\nTA\n14.220\nFalse\n\n\n1\n2000-06-15 17:30:00\nTA\n14.110\nFalse\n\n\n2\n2000-06-15 18:00:00\nTA\n14.230\nFalse\n\n\n3\n2000-06-15 18:30:00\nTA\n14.400\nFalse\n\n\n4\n2000-06-15 19:00:00\nTA\n14.090\nFalse\n\n\n5\n2000-06-15 19:30:00\nTA\n13.710\nFalse\n\n\n6\n2000-06-15 20:00:00\nTA\n13.080\nFalse\n\n\n7\n2000-06-15 20:30:00\nTA\n12.410\nFalse\n\n\n8\n2000-06-15 21:00:00\nTA\n12.270\nFalse\n\n\n9\n2000-06-15 21:30:00\nTA\n12.200\nFalse\n\n\n10\n2000-06-15 17:00:00\nSW_IN\n224.800\nTrue\n\n\n11\n2000-06-15 17:30:00\nSW_IN\n195.280\nTrue\n\n\n12\n2000-06-15 18:00:00\nSW_IN\n244.170\nTrue\n\n\n13\n2000-06-15 18:30:00\nSW_IN\n253.920\nTrue\n\n\n14\n2000-06-15 19:00:00\nSW_IN\n177.310\nTrue\n\n\n15\n2000-06-15 19:30:00\nSW_IN\n97.070\nTrue\n\n\n16\n2000-06-15 20:00:00\nSW_IN\n39.710\nTrue\n\n\n17\n2000-06-15 20:30:00\nSW_IN\n10.650\nTrue\n\n\n18\n2000-06-15 21:00:00\nSW_IN\n0.320\nTrue\n\n\n19\n2000-06-15 21:30:00\nSW_IN\n0.000\nTrue\n\n\n20\n2000-06-15 17:00:00\nVPD\n5.799\nTrue\n\n\n21\n2000-06-15 17:30:00\nVPD\n6.577\nTrue\n\n\n22\n2000-06-15 18:00:00\nVPD\n6.931\nTrue\n\n\n23\n2000-06-15 18:30:00\nVPD\n7.286\nTrue\n\n\n24\n2000-06-15 19:00:00\nVPD\n7.251\nTrue\n\n\n25\n2000-06-15 19:30:00\nVPD\n6.683\nTrue\n\n\n26\n2000-06-15 20:00:00\nVPD\n5.851\nTrue\n\n\n27\n2000-06-15 20:30:00\nVPD\n5.254\nTrue\n\n\n28\n2000-06-15 21:00:00\nVPD\n5.164\nTrue\n\n\n29\n2000-06-15 21:30:00\nVPD\n5.037\nTrue\n\n\n\n\n\n\n\n\nm_df.tidy(control_map={'TA_ERA': 'TA'})\n\n\n\n\n\n\n\n\ntime\nvariable\nvalue\ncontrol\nis_present\n\n\n\n\n0\n2000-06-15 17:00:00\nTA\n14.220\n15.050\nFalse\n\n\n1\n2000-06-15 17:30:00\nTA\n14.110\n14.961\nFalse\n\n\n2\n2000-06-15 18:00:00\nTA\n14.230\n14.872\nFalse\n\n\n3\n2000-06-15 18:30:00\nTA\n14.400\n14.783\nFalse\n\n\n4\n2000-06-15 19:00:00\nTA\n14.090\n14.694\nFalse\n\n\n5\n2000-06-15 19:30:00\nTA\n13.710\n14.606\nFalse\n\n\n6\n2000-06-15 20:00:00\nTA\n13.080\n14.380\nFalse\n\n\n7\n2000-06-15 20:30:00\nTA\n12.410\n14.155\nFalse\n\n\n8\n2000-06-15 21:00:00\nTA\n12.270\n13.929\nFalse\n\n\n9\n2000-06-15 21:30:00\nTA\n12.200\n13.704\nFalse\n\n\n10\n2000-06-15 17:00:00\nSW_IN\n224.800\nNaN\nTrue\n\n\n11\n2000-06-15 17:30:00\nSW_IN\n195.280\nNaN\nTrue\n\n\n12\n2000-06-15 18:00:00\nSW_IN\n244.170\nNaN\nTrue\n\n\n13\n2000-06-15 18:30:00\nSW_IN\n253.920\nNaN\nTrue\n\n\n14\n2000-06-15 19:00:00\nSW_IN\n177.310\nNaN\nTrue\n\n\n15\n2000-06-15 19:30:00\nSW_IN\n97.070\nNaN\nTrue\n\n\n16\n2000-06-15 20:00:00\nSW_IN\n39.710\nNaN\nTrue\n\n\n17\n2000-06-15 20:30:00\nSW_IN\n10.650\nNaN\nTrue\n\n\n18\n2000-06-15 21:00:00\nSW_IN\n0.320\nNaN\nTrue\n\n\n19\n2000-06-15 21:30:00\nSW_IN\n0.000\nNaN\nTrue\n\n\n20\n2000-06-15 17:00:00\nVPD\n5.799\nNaN\nTrue\n\n\n21\n2000-06-15 17:30:00\nVPD\n6.577\nNaN\nTrue\n\n\n22\n2000-06-15 18:00:00\nVPD\n6.931\nNaN\nTrue\n\n\n23\n2000-06-15 18:30:00\nVPD\n7.286\nNaN\nTrue\n\n\n24\n2000-06-15 19:00:00\nVPD\n7.251\nNaN\nTrue\n\n\n25\n2000-06-15 19:30:00\nVPD\n6.683\nNaN\nTrue\n\n\n26\n2000-06-15 20:00:00\nVPD\n5.851\nNaN\nTrue\n\n\n27\n2000-06-15 20:30:00\nVPD\n5.254\nNaN\nTrue\n\n\n28\n2000-06-15 21:00:00\nVPD\n5.164\nNaN\nTrue\n\n\n29\n2000-06-15 21:30:00\nVPD\n5.037\nNaN\nTrue\n\n\n\n\n\n\n\n\nm_df.tidy(control_map=hai_control).head()\n\n\n\n\n\n\n\n\ntime\nvariable\nvalue\ncontrol\nis_present\n\n\n\n\n0\n2000-06-15 17:00:00\nTA\n14.22\n15.050\nFalse\n\n\n1\n2000-06-15 17:30:00\nTA\n14.11\n14.961\nFalse\n\n\n2\n2000-06-15 18:00:00\nTA\n14.23\n14.872\nFalse\n\n\n3\n2000-06-15 18:30:00\nTA\n14.40\n14.783\nFalse\n\n\n4\n2000-06-15 19:00:00\nTA\n14.09\n14.694\nFalse\n\n\n\n\n\n\n\n\nPlotting\n\nRug\n\nplot_rug(m_df.tidy())\n\n\n\n\n\n\n\ndf = m_df.tidy()\n\n\ndf = df[df.variable==\"TA\"].copy()\n\n\ndf['row_number'] = df.reset_index().index\n\n\ndf\n\n\n\n\n\n\n\n\ntime\nvariable\nvalue\nis_present\nrow_number\n\n\n\n\n0\n2000-06-15 17:00:00\nTA\n14.22\nFalse\n0\n\n\n1\n2000-06-15 17:30:00\nTA\n14.11\nFalse\n1\n\n\n2\n2000-06-15 18:00:00\nTA\n14.23\nFalse\n2\n\n\n3\n2000-06-15 18:30:00\nTA\n14.40\nFalse\n3\n\n\n4\n2000-06-15 19:00:00\nTA\n14.09\nFalse\n4\n\n\n5\n2000-06-15 19:30:00\nTA\n13.71\nFalse\n5\n\n\n6\n2000-06-15 20:00:00\nTA\n13.08\nFalse\n6\n\n\n7\n2000-06-15 20:30:00\nTA\n12.41\nFalse\n7\n\n\n8\n2000-06-15 21:00:00\nTA\n12.27\nFalse\n8\n\n\n9\n2000-06-15 21:30:00\nTA\n12.20\nFalse\n9\n\n\n\n\n\n\n\n\ndf.iloc[1]\n\ntime          2000-06-15 17:30:00\nvariable                       TA\nvalue                       14.11\nis_present                  False\nrow_number                      1\nName: 1, dtype: object\n\n\n\ndf.loc[2, \"is_present\"] = True\n\n\ndf\n\n\n\n\n\n\n\n\ntime\nvariable\nvalue\nis_present\nrow_number\n\n\n\n\n0\n2000-06-15 17:00:00\nTA\n14.22\nFalse\n0\n\n\n1\n2000-06-15 17:30:00\nTA\n14.11\nFalse\n1\n\n\n2\n2000-06-15 18:00:00\nTA\n14.23\nTrue\n2\n\n\n3\n2000-06-15 18:30:00\nTA\n14.40\nFalse\n3\n\n\n4\n2000-06-15 19:00:00\nTA\n14.09\nFalse\n4\n\n\n5\n2000-06-15 19:30:00\nTA\n13.71\nFalse\n5\n\n\n6\n2000-06-15 20:00:00\nTA\n13.08\nFalse\n6\n\n\n7\n2000-06-15 20:30:00\nTA\n12.41\nFalse\n7\n\n\n8\n2000-06-15 21:00:00\nTA\n12.27\nFalse\n8\n\n\n9\n2000-06-15 21:30:00\nTA\n12.20\nFalse\n9\n\n\n\n\n\n\n\n\ni = 1\nprev, curr, _next= df.iloc[i-1], df.iloc[i], df.iloc[i+1]\n\n\nprev, curr, _next\n\n(time          2000-06-15 17:00:00\n variable                       TA\n value                       14.22\n is_present                  False\n row_number                      0\n Name: 0, dtype: object,\n time          2000-06-15 17:30:00\n variable                       TA\n value                       14.11\n is_present                  False\n row_number                      1\n Name: 1, dtype: object,\n time          2000-06-15 18:00:00\n variable                       TA\n value                       14.23\n is_present                   True\n row_number                      2\n Name: 2, dtype: object)\n\n\n\ndf\n\n\n\n\n\n\n\n\ntime\nvariable\nvalue\nis_present\nrow_number\n\n\n\n\n0\n2000-06-15 17:00:00\nTA\n14.22\nFalse\n0\n\n\n1\n2000-06-15 17:30:00\nTA\n14.11\nFalse\n1\n\n\n2\n2000-06-15 18:00:00\nTA\n14.23\nTrue\n2\n\n\n3\n2000-06-15 18:30:00\nTA\n14.40\nFalse\n3\n\n\n4\n2000-06-15 19:00:00\nTA\n14.09\nFalse\n4\n\n\n5\n2000-06-15 19:30:00\nTA\n13.71\nFalse\n5\n\n\n6\n2000-06-15 20:00:00\nTA\n13.08\nFalse\n6\n\n\n7\n2000-06-15 20:30:00\nTA\n12.41\nFalse\n7\n\n\n8\n2000-06-15 21:00:00\nTA\n12.27\nFalse\n8\n\n\n9\n2000-06-15 21:30:00\nTA\n12.20\nFalse\n9\n\n\n\n\n\n\n\n\n\nMissing Area\n\nfor i in range(len(df)):\n    # handle boundaries\n    prev = df.iloc[i-1].is_present if i&gt;0 else True \n    _next = df.iloc[i+1].is_present if i&lt;(len(df)-1) else True \n    curr = df.iloc[i]\n    if not curr.is_present and prev:\n        print(\"gap start\", curr.time)\n    if not curr.is_present and _next:\n        print(\"gap end\", curr.time)\n\ngap start 2000-06-15 17:00:00\ngap end 2000-06-15 17:30:00\ngap start 2000-06-15 18:30:00\ngap end 2000-06-15 21:30:00\n\n\n\nsource\n\n\n\n\nfind_gap_limits\n\n find_gap_limits (df)\n\n\nfind_gap_limits(df)\n\n\n\n\n\n\n\n\ngap_start\ngap_end\n\n\n\n\n0\n2000-06-15 17:00:00\n2000-06-15 17:30:00\n\n\n1\n2000-06-15 18:30:00\n2000-06-15 21:30:00\n\n\n\n\n\n\n\n\nsource\n\n\nplot_missing_area\n\n plot_missing_area (df, sel=Parameter('param_2', SelectionParameter({\n                    bind: 'scales',   name: 'param_2',   select:\n                    IntervalSelectionConfig({     type: 'interval'   })\n                    })), props={})\n\n\nplot_missing_area(df)\n\n\n\n\n\n\n\nPoints\n\nsource\n\n\n\nplot_points\n\n plot_points (df, y='value', y_label='', sel=Parameter('param_3',\n              SelectionParameter({   bind: 'scales',   name: 'param_3',\n              select: IntervalSelectionConfig({     type: 'interval'   })\n              })), props={})\n\n\nplot_points(m_df.tidy())\n\n\n\n\n\n\n\nLine\n\nplot_line(m_df.tidy())\n\n\n\n\n\n\n\n\nControl\n\nplot_control(m_df.tidy(), y=\"value\")\n\n\n\n\n\n\n\n\nErrorband\n\nplot_error(m_df.tidy().assign(std=5))\n\n\n\n\n\n\n\n\nVariable\n\nsource\n\n\n\nplot_variable\n\n plot_variable (df, variable, ys=['value', 'value', 'control'], title='',\n                y_label='', sel=None, error=False, point=True,\n                gap_area=True, control=False, props={})\n\n\nplot_variable(m_df.tidy(), \"TA\", title=\"title TA\", gap_area=False)\n\n\n\n\n\n\n\nplot_variable(m_df.tidy(control_map=hai_control), \"TA\", title=\"title TA\", control=True)\n\n\n\n\n\n\n\nplot_variable(m_df.tidy().assign(std=.5), \"TA\", title=\"title TA\", error=True)\n\n\n\n\n\n\n\nplot_variable(m_df.tidy().assign(std=.5), \"TA\", title=\"title TA\", error=True, point=False, gap_area=False)\n\n\n\n\n\n\n\nFacet\n\nsource\n\n\n\nfacet_variable\n\n facet_variable (df, n_cols:int=3, bind_interaction:bool=True, units=None,\n                 ys=['value', 'value', 'control'], title='', sel=None,\n                 error=False, point=True, gap_area=True, control=False,\n                 props={})\n\nPlot all values of the column variable in different subplots\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ntidy dataframe\n\n\nn_cols\nint\n3\n\n\n\nbind_interaction\nbool\nTrue\nWhether the sub-plots for each variable should be connected for zooming/panning\n\n\nunits\nNoneType\nNone\n\n\n\nys\nlist\n[‘value’, ‘value’, ‘control’]\n\n\n\ntitle\nstr\n\n\n\n\nsel\nNoneType\nNone\n\n\n\nerror\nbool\nFalse\n\n\n\npoint\nbool\nTrue\n\n\n\ngap_area\nbool\nTrue\n\n\n\ncontrol\nbool\nFalse\n\n\n\nprops\ndict\n{}\n\n\n\nReturns\nChart\n\n\n\n\n\n\nShow\n\nsource\n\n\n\nMeteoImpDf.show\n\n MeteoImpDf.show (ax=None, ctx=None, n_cols:int=3,\n                  bind_interaction:bool=True, props:dict=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nax\nNoneType\nNone\n\n\n\nctx\nNoneType\nNone\n\n\n\nn_cols\nint\n3\n\n\n\nbind_interaction\nbool\nTrue\nWhether the sub-plots for each variable should be connected for zooming/panning\n\n\nprops\ndict\nNone\nadditional properties (eg. size) for altair plot\n\n\nReturns\nChart\n\n\n\n\n\n\nm_df.show(bind_interaction = False)\n\n\n\n\n\n\n\ntfms2[0].show()\n\n\n\n\n\n\n\ntfms2[2].show()\n\n\n\n\n\n\n\n\n4) To Tensor\nthis needs to handle both the init with a list of items and when the first item is a sequence of list of items\n\nsource\n\n\nMeteoImpTensor\n\n MeteoImpTensor (*args)\n\nAll the operations on a read-only sequence.\nConcrete subclasses must override new or init, getitem, and len.\n\nsource\n\n\nMeteoImpDf2Tensor\n\n MeteoImpDf2Tensor (enc=None, dec=None, split_idx=None, order=None)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nto_t = MeteoImpDf2Tensor()\n\n\nto_t.setup(tfms2)\n\n\nto_t(tfms2[0])\n\n\n data tensor([[ 14.2200, 224.8000,   5.7990],\n        [ 14.1100, 195.2800,   6.5770],\n        [ 14.2300, 244.1700,   6.9310],\n        [ 14.4000, 253.9200,   7.2860],\n        [ 14.0900, 177.3100,   7.2510],\n        [ 13.7100,  97.0700,   6.6830],\n        [ 13.0800,  39.7100,   5.8510],\n        [ 12.4100,  10.6500,   5.2540],\n        [ 12.2700,   0.3200,   5.1640],\n        [ 12.2000,   0.0000,   5.0370]], dtype=torch.float64)\n mask tensor([[False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True]])\n control tensor([[ 15.0500, 255.1930,   5.1020,  15.1390, 287.1000,   4.9000],\n        [ 14.9610, 221.4270,   5.3050,  15.0500, 255.1930,   5.1020],\n        [ 14.8720, 186.3800,   5.5070,  14.9610, 221.4270,   5.3050],\n        [ 14.7830, 150.6500,   5.7100,  14.8720, 186.3800,   5.5070],\n        [ 14.6940, 114.8490,   5.9120,  14.7830, 150.6500,   5.7100],\n        [ 14.6060,  34.7280,   6.1140,  14.6940, 114.8490,   5.9120],\n        [ 14.3800,  19.8430,   6.0740,  14.6060,  34.7280,   6.1140],\n        [ 14.1550,   5.7120,   6.0340,  14.3800,  19.8430,   6.0740],\n        [ 13.9290,   0.0000,   5.9940,  14.1550,   5.7120,   6.0340],\n        [ 13.7040,   0.0000,   5.9540,  13.9290,   0.0000,   5.9940]],\n       dtype=torch.float64)\n \n\n\n\nto_t.decode(to_t(tfms2[0]));\n\n\ntfms2[0]\n\n\nMeteo Imp Df  data \n\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-06-15 17:00:00\n14.2200\n224.8000\n5.7990\n\n\n2000-06-15 17:30:00\n14.1100\n195.2800\n6.5770\n\n\n2000-06-15 18:00:00\n14.2300\n244.1700\n6.9310\n\n\n2000-06-15 18:30:00\n14.4000\n253.9200\n7.2860\n\n\n2000-06-15 19:00:00\n14.0900\n177.3100\n7.2510\n\n\n2000-06-15 19:30:00\n13.7100\n97.0700\n6.6830\n\n\n2000-06-15 20:00:00\n13.0800\n39.7100\n5.8510\n\n\n2000-06-15 20:30:00\n12.4100\n10.6500\n5.2540\n\n\n2000-06-15 21:00:00\n12.2700\n0.3200\n5.1640\n\n\n2000-06-15 21:30:00\n12.2000\n0.0000\n5.0370\n\n\n\n\nmask\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-06-15 17:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 17:30:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 18:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 18:30:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 19:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 19:30:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 20:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 20:30:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 21:00:00\nFalse\nTrue\nTrue\n\n\n2000-06-15 21:30:00\nFalse\nTrue\nTrue\n\n\n\n\n\ncontrol\n\n\n\n \nTA_ERA\nSW_IN_ERA\nVPD_ERA\nTA_ERA_lag_1\nSW_IN_ERA_lag_1\nVPD_ERA_lag_1\n\n\ntime\n \n \n \n \n \n \n\n\n\n\n2000-06-15 17:00:00\n15.0500\n255.1930\n5.1020\n15.1390\n287.1000\n4.9000\n\n\n2000-06-15 17:30:00\n14.9610\n221.4270\n5.3050\n15.0500\n255.1930\n5.1020\n\n\n2000-06-15 18:00:00\n14.8720\n186.3800\n5.5070\n14.9610\n221.4270\n5.3050\n\n\n2000-06-15 18:30:00\n14.7830\n150.6500\n5.7100\n14.8720\n186.3800\n5.5070\n\n\n2000-06-15 19:00:00\n14.6940\n114.8490\n5.9120\n14.7830\n150.6500\n5.7100\n\n\n2000-06-15 19:30:00\n14.6060\n34.7280\n6.1140\n14.6940\n114.8490\n5.9120\n\n\n2000-06-15 20:00:00\n14.3800\n19.8430\n6.0740\n14.6060\n34.7280\n6.1140\n\n\n2000-06-15 20:30:00\n14.1550\n5.7120\n6.0340\n14.3800\n19.8430\n6.0740\n\n\n2000-06-15 21:00:00\n13.9290\n0.0000\n5.9940\n14.1550\n5.7120\n6.0340\n\n\n2000-06-15 21:30:00\n13.7040\n0.0000\n5.9540\n13.9290\n0.0000\n5.9940\n\n\n\n\n\n \n\n\n\ntfms3 = TfmdLists(tfms1.items, [*tfms2.fs, MeteoImpDf2Tensor()])\n\n\ntfms3[0]\n\n\n data tensor([[ 14.2200, 224.8000,   5.7990],\n        [ 14.1100, 195.2800,   6.5770],\n        [ 14.2300, 244.1700,   6.9310],\n        [ 14.4000, 253.9200,   7.2860],\n        [ 14.0900, 177.3100,   7.2510],\n        [ 13.7100,  97.0700,   6.6830],\n        [ 13.0800,  39.7100,   5.8510],\n        [ 12.4100,  10.6500,   5.2540],\n        [ 12.2700,   0.3200,   5.1640],\n        [ 12.2000,   0.0000,   5.0370]], dtype=torch.float64)\n mask tensor([[False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True]])\n control tensor([[ 15.0500, 255.1930,   5.1020,  15.1390, 287.1000,   4.9000],\n        [ 14.9610, 221.4270,   5.3050,  15.0500, 255.1930,   5.1020],\n        [ 14.8720, 186.3800,   5.5070,  14.9610, 221.4270,   5.3050],\n        [ 14.7830, 150.6500,   5.7100,  14.8720, 186.3800,   5.5070],\n        [ 14.6940, 114.8490,   5.9120,  14.7830, 150.6500,   5.7100],\n        [ 14.6060,  34.7280,   6.1140,  14.6940, 114.8490,   5.9120],\n        [ 14.3800,  19.8430,   6.0740,  14.6060,  34.7280,   6.1140],\n        [ 14.1550,   5.7120,   6.0340,  14.3800,  19.8430,   6.0740],\n        [ 13.9290,   0.0000,   5.9940,  14.1550,   5.7120,   6.0340],\n        [ 13.7040,   0.0000,   5.9540,  13.9290,   0.0000,   5.9940]],\n       dtype=torch.float64)\n \n\n\n\ntype(tfms3[0])\n\n__main__.MeteoImpTensor\n\n\n\n\n5) Normalize\n\nsource\n\n\nget_stats\n\n get_stats (df, repeat=1, device='cpu')\n\n\nsource\n\n\nMeteoImpNormalize\n\n MeteoImpNormalize (mean_data, std_data, mean_control, std_control)\n\nNormalize/denorm MeteoImpTensor column-wise\n\nnorm = MeteoImpNormalize(*get_stats(hai), *get_stats(hai_era,2))\n\n\ntfms3[0]\n\n\n data tensor([[ 14.2200, 224.8000,   5.7990],\n        [ 14.1100, 195.2800,   6.5770],\n        [ 14.2300, 244.1700,   6.9310],\n        [ 14.4000, 253.9200,   7.2860],\n        [ 14.0900, 177.3100,   7.2510],\n        [ 13.7100,  97.0700,   6.6830],\n        [ 13.0800,  39.7100,   5.8510],\n        [ 12.4100,  10.6500,   5.2540],\n        [ 12.2700,   0.3200,   5.1640],\n        [ 12.2000,   0.0000,   5.0370]], dtype=torch.float64)\n mask tensor([[False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True]])\n control tensor([[ 15.0500, 255.1930,   5.1020,  15.1390, 287.1000,   4.9000],\n        [ 14.9610, 221.4270,   5.3050,  15.0500, 255.1930,   5.1020],\n        [ 14.8720, 186.3800,   5.5070,  14.9610, 221.4270,   5.3050],\n        [ 14.7830, 150.6500,   5.7100,  14.8720, 186.3800,   5.5070],\n        [ 14.6940, 114.8490,   5.9120,  14.7830, 150.6500,   5.7100],\n        [ 14.6060,  34.7280,   6.1140,  14.6940, 114.8490,   5.9120],\n        [ 14.3800,  19.8430,   6.0740,  14.6060,  34.7280,   6.1140],\n        [ 14.1550,   5.7120,   6.0340,  14.3800,  19.8430,   6.0740],\n        [ 13.9290,   0.0000,   5.9940,  14.1550,   5.7120,   6.0340],\n        [ 13.7040,   0.0000,   5.9540,  13.9290,   0.0000,   5.9940]],\n       dtype=torch.float64)\n \n\n\n\nnorm\n\n\n  \n    MeteoImpNormalize -- {'mean_data': tensor([  8.3339, 120.9578,   3.3807], dtype=torch.float64), 'std_data': tensor([  7.9246, 204.0026,   4.3684], dtype=torch.float64), 'mean_control': tensor([  8.1948, 120.6864,   3.3253,   8.1948, 120.6864,   3.3253],\n       dtype=torch.float64), 'std_control': tensor([  7.5459, 187.1730,   3.6871,   7.5459, 187.1730,   3.6871],\n       dtype=torch.float64)}\n  \n  (MeteoImpTensor,object) -&gt; encodes\n\n  (MeteoImpTensor,object) -&gt; decodes\n\n\n\n\n\nnorm(tfms3[0])\n\n\n data tensor([[ 0.7428,  0.5090,  0.5536],\n        [ 0.7289,  0.3643,  0.7317],\n        [ 0.7440,  0.6040,  0.8127],\n        [ 0.7655,  0.6518,  0.8940],\n        [ 0.7264,  0.2762,  0.8860],\n        [ 0.6784, -0.1171,  0.7560],\n        [ 0.5989, -0.3983,  0.5655],\n        [ 0.5144, -0.5407,  0.4288],\n        [ 0.4967, -0.5914,  0.4082],\n        [ 0.4879, -0.5929,  0.3792]], dtype=torch.float64)\n mask tensor([[False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True]])\n control tensor([[ 0.9085,  0.7186,  0.4819,  0.9203,  0.8891,  0.4271],\n        [ 0.8967,  0.5382,  0.5369,  0.9085,  0.7186,  0.4819],\n        [ 0.8849,  0.3510,  0.5917,  0.8967,  0.5382,  0.5369],\n        [ 0.8731,  0.1601,  0.6468,  0.8849,  0.3510,  0.5917],\n        [ 0.8613, -0.0312,  0.7015,  0.8731,  0.1601,  0.6468],\n        [ 0.8496, -0.4592,  0.7563,  0.8613, -0.0312,  0.7015],\n        [ 0.8197, -0.5388,  0.7455,  0.8496, -0.4592,  0.7563],\n        [ 0.7899, -0.6143,  0.7346,  0.8197, -0.5388,  0.7455],\n        [ 0.7599, -0.6448,  0.7238,  0.7899, -0.6143,  0.7346],\n        [ 0.7301, -0.6448,  0.7129,  0.7599, -0.6448,  0.7238]],\n       dtype=torch.float64)\n \n\n\n\ntest_close(norm.decode(norm(tfms3[0]))[0], tfms3[0][0], eps=2e-5)\n\nTest that NormalsParams decode actually works\n\ntfms4 = TfmdLists(tfms3.items, [*tfms3.fs,MeteoImpNormalize(*get_stats(hai),*get_stats(hai_era, 2) ) ])\n\n\ntfms4[0]\n\n\n data tensor([[ 0.7428,  0.5090,  0.5536],\n        [ 0.7289,  0.3643,  0.7317],\n        [ 0.7440,  0.6040,  0.8127],\n        [ 0.7655,  0.6518,  0.8940],\n        [ 0.7264,  0.2762,  0.8860],\n        [ 0.6784, -0.1171,  0.7560],\n        [ 0.5989, -0.3983,  0.5655],\n        [ 0.5144, -0.5407,  0.4288],\n        [ 0.4967, -0.5914,  0.4082],\n        [ 0.4879, -0.5929,  0.3792]], dtype=torch.float64)\n mask tensor([[False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True]])\n control tensor([[ 0.9085,  0.7186,  0.4819,  0.9203,  0.8891,  0.4271],\n        [ 0.8967,  0.5382,  0.5369,  0.9085,  0.7186,  0.4819],\n        [ 0.8849,  0.3510,  0.5917,  0.8967,  0.5382,  0.5369],\n        [ 0.8731,  0.1601,  0.6468,  0.8849,  0.3510,  0.5917],\n        [ 0.8613, -0.0312,  0.7015,  0.8731,  0.1601,  0.6468],\n        [ 0.8496, -0.4592,  0.7563,  0.8613, -0.0312,  0.7015],\n        [ 0.8197, -0.5388,  0.7455,  0.8496, -0.4592,  0.7563],\n        [ 0.7899, -0.6143,  0.7346,  0.8197, -0.5388,  0.7455],\n        [ 0.7599, -0.6448,  0.7238,  0.7899, -0.6143,  0.7346],\n        [ 0.7301, -0.6448,  0.7129,  0.7599, -0.6448,  0.7238]],\n       dtype=torch.float64)\n \n\n\n\ntfms4.decode(tfms4[0])\n\n\n data tensor([[1.4220e+01, 2.2480e+02, 5.7990e+00],\n        [1.4110e+01, 1.9528e+02, 6.5770e+00],\n        [1.4230e+01, 2.4417e+02, 6.9310e+00],\n        [1.4400e+01, 2.5392e+02, 7.2860e+00],\n        [1.4090e+01, 1.7731e+02, 7.2510e+00],\n        [1.3710e+01, 9.7070e+01, 6.6830e+00],\n        [1.3080e+01, 3.9710e+01, 5.8510e+00],\n        [1.2410e+01, 1.0650e+01, 5.2540e+00],\n        [1.2270e+01, 3.2000e-01, 5.1640e+00],\n        [1.2200e+01, 1.4211e-14, 5.0370e+00]], dtype=torch.float64)\n mask tensor([[False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True],\n        [False,  True,  True]])\n control tensor([[ 15.0500, 255.1930,   5.1020,  15.1390, 287.1000,   4.9000],\n        [ 14.9610, 221.4270,   5.3050,  15.0500, 255.1930,   5.1020],\n        [ 14.8720, 186.3800,   5.5070,  14.9610, 221.4270,   5.3050],\n        [ 14.7830, 150.6500,   5.7100,  14.8720, 186.3800,   5.5070],\n        [ 14.6940, 114.8490,   5.9120,  14.7830, 150.6500,   5.7100],\n        [ 14.6060,  34.7280,   6.1140,  14.6940, 114.8490,   5.9120],\n        [ 14.3800,  19.8430,   6.0740,  14.6060,  34.7280,   6.1140],\n        [ 14.1550,   5.7120,   6.0340,  14.3800,  19.8430,   6.0740],\n        [ 13.9290,   0.0000,   5.9940,  14.1550,   5.7120,   6.0340],\n        [ 13.7040,   0.0000,   5.9540,  13.9290,   0.0000,   5.9940]],\n       dtype=torch.float64)\n \n\n\n\n\n6) To Tuple\nFastai likes to work with tuples (in particular for collating)… for now convert to a tuple. Maybe find a way to mimic a tuple in MeteoImpTensor\nAlso duplicate the data, so it becomes training and label\n\nsource\n\n\nToTuple\n\n ToTuple (enc=None, dec=None, split_idx=None, order=None)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nToTuple()\n\n\n  \n    ToTuple\n  \n  (object,object) -&gt; encodes\n\n  (object,object) -&gt; decodes\n\n\n\n\n\ntfms5 = TfmdLists(tfms4.items, [*tfms4.fs,ToTuple])\n\n\ntfms5[0];\n\n\n\nPipeline\n\nGenerators\n\nsource\n\n\n\nas_generator\n\n as_generator (x:collections.abc.Generator|object, iter=False)\n\nMaybe convert iterable to infinite generator\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\ncollections.abc.Generator | object\n\n\n\n\niter\nbool\nFalse\nshould generator return x or iterate over the elements of x\n\n\n\n\ng_var = ['TA', 'SW_IN']\n\n\nisinstance(g_var, Iterable)\n\nTrue\n\n\n\nas_generator(g_var), next(as_generator(g_var))\n\n(&lt;itertools.cycle&gt;, ['TA', 'SW_IN'])\n\n\n\nas_generator([1,2]), next(as_generator([1,2]))\n\n(&lt;itertools.cycle&gt;, [1, 2])\n\n\n\nGap Len Generator\n\nsource\n\n\n\ngen_gap_len\n\n gen_gap_len (min_v=1, max_v=50)\n\n\ng_len = gen_gap_len()\n\n\n[next(g_len) for _ in range(10)]\n\n[20, 9, 32, 49, 38, 25, 26, 14, 3, 43]\n\n\nGamma\nThe gap lengths are drawn from a gamma distribution, so we have a long tail and a min value of 0, but compared to an exponentail distributions we don’t have many gaps of len 0\n\\[ p(x)=\\frac{1}{\\Gamma(k) \\theta^k} x^{k - 1} e^{-\\frac{x}{\\theta}}\\]\n\\[\\begin{align}\\mu &= k\\theta\\\\\nm &= (k-1)\\theta \\end{align}\\] where \\(m\\) is the mode and \\(\\mu\\) is the mean (for \\(k&gt;1\\)), which is what we want\n\nimport matplotlib.pyplot as plt\n\nimport scipy.special as sps\n\n\nmean = 10\n\nscale = mean * .6\nshape = mean/scale\n\nmode = (shape-1)*scale\n\nx = np.arange(0,100)\n\ny = x**(shape-1)*(np.exp(-x/scale) / (sps.gamma(shape)*scale**shape))\n\nplt.plot(x, y)\n\nplt.show()\n\n\n\n\nThis is a very guessestimate of a good probability density distribution of the gap len. The actual measure should come from the fluxnet data\n\nsource\n\n\ngen_gap_len_gamma\n\n gen_gap_len_gamma (mean:float, min_v=1, max_v=50)\n\n\nnext(gen_gap_len(1))\n\n34\n\n\n\ng_len = gen_gap_len_gamma(10)\n\n\ngap_lens_sample = pd.DataFrame([next(g_len) for _ in range(1000)])\ngap_lens_sample.hist(bins=gap_lens_sample[0].max()//2)\n\narray([[&lt;AxesSubplot: title={'center': '0'}&gt;]], dtype=object)\n\n\n\n\n\n\nVar Sel Generator\ndraws a number of variables from a uniform distribution from 1 to the max n of vars and then select the variables with equal probability\n\nsource\n\n\n\ngen_var_sel\n\n gen_var_sel (vars, n_var=None)\n\n\ng_var = gen_var_sel(list(\"abcdefg\"))\n\n\n[next(g_var) for _ in range(5)]\n\n[['d'], ['e'], ['d'], ['a'], ['d']]\n\n\n\n[next(g_var) for _ in range(5)]\n\n[['f'], ['g'], ['c'], ['d'], ['a']]\n\n\n\ng_var1 = gen_var_sel([\"a\", \"bb\", \"ccc\"], 1)\n\n\n[next(g_var1) for _ in range(10)]\n\n[['ccc'], ['bb'], ['a'], ['bb'], ['a'], ['a'], ['bb'], ['a'], ['bb'], ['ccc']]\n\n\n\ng_var = gen_var_sel(['TA', 'VPD', 'SW_IN'])\n\n\n[next(g_var) for _ in range(10)]\n\n[['SW_IN', 'VPD', 'TA'],\n ['SW_IN', 'TA', 'VPD'],\n ['TA', 'VPD', 'SW_IN'],\n ['TA', 'VPD', 'SW_IN'],\n ['TA', 'VPD', 'SW_IN'],\n ['SW_IN', 'TA', 'VPD'],\n ['VPD', 'SW_IN', 'TA'],\n ['VPD', 'SW_IN', 'TA'],\n ['TA', 'VPD', 'SW_IN'],\n ['VPD', 'TA', 'SW_IN']]\n\n\n\nblock_len = 10\ncontrol_lags = [1]\ncontrol_repeat = 1 + len(control_lags)\nblock_ids = list(range(max(control_lags), (len(hai) // block_len) - 1))[:10]\ngap_len = 2\nvar_sel = ['TA','SW_IN']\n\n\nShifts generator\n\nsource\n\n\n\ngen_shifts\n\n gen_shifts (var)\n\nGenerate shifts for a random distribution with variances var\n\nBlock Ids\n\nsource\n\n\n\nget_block_ids\n\n get_block_ids (n_rep:int, total_len:int, block_len:int,\n                var_sel:collections.abc.Iterable|collections.abc.Generator\n                , gap_len:collections.abc.Iterable|collections.abc.Generat\n                or, shifts:collections.abc.Iterable|collections.abc.Genera\n                tor|None=None, offset:int=0)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_rep\nint\n\nnumber of repetitions for each item\n\n\ntotal_len\nint\n\ntotal len dataframe\n\n\nblock_len\nint\n\nlen of a block\n\n\nvar_sel\ncollections.abc.Iterable | collections.abc.Generator\n\nreturns list[str] to select variables\n\n\ngap_len\ncollections.abc.Iterable | collections.abc.Generator\n\nreturns int for gap len\n\n\nshifts\ncollections.abc.Iterable | collections.abc.Generator | None\nNone\nif None make at same distance\n\n\noffset\nint\n0\nstarting point for first item (for allow for control lags and shifts)\n\n\n\n\nget_block_ids(n_rep = 2, total_len = 100, block_len = 10, var_sel = ['TA'], gap_len = 10)\n\n[MeteoImpItem(i=0, shift=-5, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=0, shift=0, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=1, shift=-5, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=1, shift=0, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=2, shift=-5, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=2, shift=0, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=3, shift=-5, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=3, shift=0, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=4, shift=-5, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=4, shift=0, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=5, shift=-5, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=5, shift=0, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=6, shift=-5, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=6, shift=0, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=7, shift=-5, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=7, shift=0, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=8, shift=-5, var_sel=['TA'], gap_len=10),\n MeteoImpItem(i=8, shift=0, var_sel=['TA'], gap_len=10)]\n\n\n\nget_block_ids(n_rep = 3, total_len = 30, block_len = 10, var_sel = gen_var_sel(['TA', 'SW_IN', 'VPD']), gap_len = 10)\n\n[MeteoImpItem(i=0, shift=-5, var_sel=['VPD', 'TA'], gap_len=10),\n MeteoImpItem(i=0, shift=-2, var_sel=['TA', 'VPD'], gap_len=10),\n MeteoImpItem(i=0, shift=1, var_sel=['TA', 'VPD'], gap_len=10),\n MeteoImpItem(i=1, shift=4, var_sel=['TA', 'SW_IN'], gap_len=10),\n MeteoImpItem(i=1, shift=-5, var_sel=['VPD', 'TA'], gap_len=10),\n MeteoImpItem(i=1, shift=-2, var_sel=['TA', 'VPD'], gap_len=10)]\n\n\n\nPipeline\n\nsource\n\n\n\nimp_pipeline\n\n imp_pipeline (df, control, var_sel, gap_len, block_len, control_lags,\n               n_rep, shifts=None, offset=None)\n\n\npipeline, block_ids = imp_pipeline(hai, hai_era, var_sel, gap_len, block_len, control_lags, n_rep=10)\n\n\npipeline\n\n[BlockIndexTransform:\n encodes: (MeteoImpItem,object) -&gt; encodes\n decodes: ,\n BlockDfTransform:\n encodes: (MeteoImpIndex,object) -&gt; encodes\n decodes: ,\n AddGapTransform:\n encodes: (DataControl,object) -&gt; encodes\n decodes: ,\n __main__.MeteoImpDf2Tensor,\n MeteoImpNormalize -- {'mean_data': tensor([  8.3339, 120.9578,   3.3807], dtype=torch.float64), 'std_data': tensor([  7.9246, 204.0026,   4.3684], dtype=torch.float64), 'mean_control': tensor([  8.1948, 120.6864,   3.3253,   8.1948, 120.6864,   3.3253],\n        dtype=torch.float64), 'std_control': tensor([  7.5459, 187.1730,   3.6871,   7.5459, 187.1730,   3.6871],\n        dtype=torch.float64)}:\n encodes: (MeteoImpTensor,object) -&gt; encodes\n decodes: (MeteoImpTensor,object) -&gt; decodes,\n __main__.ToTuple]\n\n\n\npp = Pipeline(pipeline)\n\n\npp\n\nPipeline: BlockIndexTransform -&gt; BlockDfTransform -&gt; AddGapTransform -&gt; MeteoImpDf2Tensor -&gt; MeteoImpNormalize -- {'mean_data': tensor([  8.3339, 120.9578,   3.3807], dtype=torch.float64), 'std_data': tensor([  7.9246, 204.0026,   4.3684], dtype=torch.float64), 'mean_control': tensor([  8.1948, 120.6864,   3.3253,   8.1948, 120.6864,   3.3253],\n       dtype=torch.float64), 'std_control': tensor([  7.5459, 187.1730,   3.6871,   7.5459, 187.1730,   3.6871],\n       dtype=torch.float64)} -&gt; ToTuple\n\n\n\n\nDataloader\nrandom splitter for validation/training set\n\nreset_seed()\n\n\nsplits = EndSplitter()(block_ids) # last 80% is test data\n\nRepeat twice the pipeline since is the same pipeline both for training data and for labels.\nIn theory could optimize the label creation and get the data only from the gap and not the control, but for now it works and the overhead is minimal\n\ndls = TfmdLists(block_ids, pipeline, splits=splits).dataloaders(bs=2)\n\n\ndls.one_batch()\n\n((tensor([[[-0.3298, -0.5929, -0.5640],\n           [-0.3563, -0.5929, -0.6420],\n           [-0.3387, -0.5929, -0.6720],\n           [-0.3261, -0.5929, -0.6722],\n           [-0.3248, -0.5929, -0.6924],\n           [-0.3172, -0.5929, -0.7105],\n           [-0.3071, -0.5929, -0.7066],\n           [-0.2933, -0.5929, -0.6883],\n           [-0.2907, -0.5767, -0.6901],\n           [-0.2882, -0.5386, -0.6929]],\n  \n          [[-0.7097, -0.3020, -0.7189],\n           [-0.6971, -0.3474, -0.7192],\n           [-0.6857, -0.2052, -0.7045],\n           [-0.6630, -0.0902, -0.6656],\n           [-0.6390,  0.0122, -0.6276],\n           [-0.6062,  0.4852, -0.5859],\n           [-0.5709,  0.7880, -0.5388],\n           [-0.5406,  0.5899, -0.5024],\n           [-0.5267,  0.4234, -0.5015],\n           [-0.4901,  0.7530, -0.4832]]], device='cuda:0', dtype=torch.float64),\n  tensor([[[ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [False, False,  True],\n           [False, False,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True]],\n  \n          [[ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [False, False,  True],\n           [False, False,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True]]], device='cuda:0'),\n  tensor([[[-0.3028, -0.6448, -0.5905, -0.2960, -0.6448, -0.5485],\n           [-0.3097, -0.6448, -0.6326, -0.3028, -0.6448, -0.5905],\n           [-0.3166, -0.6448, -0.6743, -0.3097, -0.6448, -0.6326],\n           [-0.3235, -0.6448, -0.7164, -0.3166, -0.6448, -0.6743],\n           [-0.3224, -0.6448, -0.7188, -0.3235, -0.6448, -0.7164],\n           [-0.3213, -0.6448, -0.7213, -0.3224, -0.6448, -0.7188],\n           [-0.3203, -0.6448, -0.7240, -0.3213, -0.6448, -0.7213],\n           [-0.3192, -0.6448, -0.7264, -0.3203, -0.6448, -0.7240],\n           [-0.3182, -0.6401, -0.7288, -0.3192, -0.6448, -0.7264],\n           [-0.3171, -0.6114, -0.7313, -0.3182, -0.6401, -0.7288]],\n  \n          [[-0.5342, -0.5229, -0.6567, -0.5343, -0.5707, -0.6594],\n           [-0.5342, -0.2773, -0.6540, -0.5342, -0.5229, -0.6567],\n           [-0.5021, -0.1783, -0.6084, -0.5342, -0.2773, -0.6540],\n           [-0.4702, -0.0844, -0.5629, -0.5021, -0.1783, -0.6084],\n           [-0.4381,  0.0029, -0.5173, -0.4702, -0.0844, -0.5629],\n           [-0.4060,  0.0821, -0.4717, -0.4381,  0.0029, -0.5173],\n           [-0.3741,  0.1518, -0.4262, -0.4060,  0.0821, -0.4717],\n           [-0.3420,  0.4581, -0.3806, -0.3741,  0.1518, -0.4262],\n           [-0.3166,  0.5191, -0.3532, -0.3420,  0.4581, -0.3806],\n           [-0.2911,  0.5640, -0.3261, -0.3166,  0.5191, -0.3532]]],\n         device='cuda:0', dtype=torch.float64)),\n (tensor([[[-0.3298, -0.5929, -0.5640],\n           [-0.3563, -0.5929, -0.6420],\n           [-0.3387, -0.5929, -0.6720],\n           [-0.3261, -0.5929, -0.6722],\n           [-0.3248, -0.5929, -0.6924],\n           [-0.3172, -0.5929, -0.7105],\n           [-0.3071, -0.5929, -0.7066],\n           [-0.2933, -0.5929, -0.6883],\n           [-0.2907, -0.5767, -0.6901],\n           [-0.2882, -0.5386, -0.6929]],\n  \n          [[-0.7097, -0.3020, -0.7189],\n           [-0.6971, -0.3474, -0.7192],\n           [-0.6857, -0.2052, -0.7045],\n           [-0.6630, -0.0902, -0.6656],\n           [-0.6390,  0.0122, -0.6276],\n           [-0.6062,  0.4852, -0.5859],\n           [-0.5709,  0.7880, -0.5388],\n           [-0.5406,  0.5899, -0.5024],\n           [-0.5267,  0.4234, -0.5015],\n           [-0.4901,  0.7530, -0.4832]]], device='cuda:0', dtype=torch.float64),\n  tensor([[[ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [False, False,  True],\n           [False, False,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True]],\n  \n          [[ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [False, False,  True],\n           [False, False,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True]]], device='cuda:0'),\n  tensor([[[-0.3028, -0.6448, -0.5905, -0.2960, -0.6448, -0.5485],\n           [-0.3097, -0.6448, -0.6326, -0.3028, -0.6448, -0.5905],\n           [-0.3166, -0.6448, -0.6743, -0.3097, -0.6448, -0.6326],\n           [-0.3235, -0.6448, -0.7164, -0.3166, -0.6448, -0.6743],\n           [-0.3224, -0.6448, -0.7188, -0.3235, -0.6448, -0.7164],\n           [-0.3213, -0.6448, -0.7213, -0.3224, -0.6448, -0.7188],\n           [-0.3203, -0.6448, -0.7240, -0.3213, -0.6448, -0.7213],\n           [-0.3192, -0.6448, -0.7264, -0.3203, -0.6448, -0.7240],\n           [-0.3182, -0.6401, -0.7288, -0.3192, -0.6448, -0.7264],\n           [-0.3171, -0.6114, -0.7313, -0.3182, -0.6401, -0.7288]],\n  \n          [[-0.5342, -0.5229, -0.6567, -0.5343, -0.5707, -0.6594],\n           [-0.5342, -0.2773, -0.6540, -0.5342, -0.5229, -0.6567],\n           [-0.5021, -0.1783, -0.6084, -0.5342, -0.2773, -0.6540],\n           [-0.4702, -0.0844, -0.5629, -0.5021, -0.1783, -0.6084],\n           [-0.4381,  0.0029, -0.5173, -0.4702, -0.0844, -0.5629],\n           [-0.4060,  0.0821, -0.4717, -0.4381,  0.0029, -0.5173],\n           [-0.3741,  0.1518, -0.4262, -0.4060,  0.0821, -0.4717],\n           [-0.3420,  0.4581, -0.3806, -0.3741,  0.1518, -0.4262],\n           [-0.3166,  0.5191, -0.3532, -0.3420,  0.4581, -0.3806],\n           [-0.2911,  0.5640, -0.3261, -0.3166,  0.5191, -0.3532]]],\n         device='cuda:0', dtype=torch.float64)))\n\n\n\ndls.device\n\ndevice(type='cuda', index=0)\n\n\n\n@typedispatch\ndef show_batch(x: tuple, y, samples, ctxs=None, max_n=6):\n    return x\n\n\n# dls.show_batch()\n\n\ndls._types\n\n{tuple: [{tuple: [torch.Tensor, torch.Tensor, torch.Tensor]},\n  {tuple: [torch.Tensor, torch.Tensor, torch.Tensor]}]}\n\n\n\nfrom fastcore.foundation import *\n\n\nsource\n\n\nimp_dataloader\n\n imp_dataloader (df, control, var_sel, gap_len, block_len, control_lags,\n                 n_rep, bs, shifts=None, offset=None)\n\n\nimp_dataloader\n\n&lt;function __main__.imp_dataloader(df, control, var_sel, gap_len, block_len, control_lags, n_rep, bs, shifts=None, offset=None)&gt;\n\n\n\ndls = imp_dataloader(hai, hai_era, var_sel, gap_len=10, block_len=200, control_lags=[1], n_rep = 20, bs=10).cpu()\n\n\ndls.one_batch()[0][0].shape\n\ntorch.Size([10, 200, 3])\n\n\n\ndls = dls.cpu()"
  },
  {
    "objectID": "training.html#model",
    "href": "training.html#model",
    "title": "Implement Kalman model using FastAI",
    "section": "Model",
    "text": "Model\n\nData type\nthis is the datatype that is the output of the model, which is a custom class class that both supports fastai processing and has convinience functions\n\nsource\n\n\nMNormalsParams\n\n MNormalsParams (*args)\n\nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.\n\nsource\n\n\nNormalsParams\n\n NormalsParams (*args)\n\nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.\n\nNormalsParams(0,1)\n\n__main__.NormalsParams(mean=0, std=1)\n\n\n\nMNormalsParams(0,1)\n\n__main__.MNormalsParams(mean=0, cov=1)\n\n\n\n\nForward Function\nin order to the a pytorch module we need a forward method to the kalman filter\n\nmodel = KalmanFilterSR.init_random(n_dim_obs = hai.shape[-1], n_dim_state = hai.shape[-1], n_dim_contr = hai_era.shape[-1]*control_repeat)\n\n\ntype(model)\n\nmeteo_imp.kalman.filter.KalmanFilterSR\n\n\n\ncontrol_repeat\n\n2\n\n\n\nmodel\n\n\nKalman Filter (3 obs, 3 state, 6 contr)  $A$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.0937\n0.6706\n0.1638\n\n\nx_1\n0.9272\n0.2620\n0.4967\n\n\nx_2\n0.2630\n0.1175\n0.1694\n\n\n\n\n$Q$\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n0.9201\n0.5029\n0.2247\n\n\nx_1\n0.5029\n1.0777\n1.0181\n\n\nx_2\n0.2247\n1.0181\n1.6707\n\n\n\n\n\n$b$\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.2100\n\n\nx_1\n0.4890\n\n\nx_2\n0.0564\n\n\n\n\n\n$H$\n\n\n\nvariable\nx_0\nx_1\nx_2\n\n\n\n\ny_0\n0.6441\n0.2801\n0.9132\n\n\ny_1\n0.0329\n0.4856\n0.9927\n\n\ny_2\n0.5895\n0.2611\n0.9413\n\n\n\n\n\n$R$\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n1.4459\n0.0555\n1.0762\n\n\ny_1\n0.0555\n0.6715\n0.6483\n\n\ny_2\n1.0762\n0.6483\n2.9768\n\n\n\n\n\n$d$\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.1371\n\n\ny_1\n0.8726\n\n\ny_2\n0.5590\n\n\n\n\n\n$B$\n\n\n\nstate\nc_0\nc_1\nc_2\nc_3\nc_4\nc_5\n\n\n\n\nx_0\n0.6319\n0.6734\n0.7937\n0.6468\n0.5825\n0.4599\n\n\nx_1\n0.7960\n0.9038\n0.9735\n0.6428\n0.3725\n0.2052\n\n\nx_2\n0.0507\n0.4448\n0.5775\n0.7237\n0.5927\n0.3217\n\n\n\n\n\n$m_0$\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.6690\n\n\nx_1\n0.1554\n\n\nx_2\n0.0821\n\n\n\n\n\n$P_0$\n\n\n\nstate\nx_0\nx_1\nx_2\n\n\n\n\nx_0\n1.2632\n0.0221\n0.3865\n\n\nx_1\n0.0221\n0.5081\n0.3983\n\n\nx_2\n0.3865\n0.3983\n1.8304\n\n\n\n\n\n \n\n\n\ninput = dls.one_batch()[0]\n\n\nmodel._predict_filter(*input);\n\n\n\n\nforward’]\nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.\n\nmodel.pred_std\n\nFalse\n\n\n\npred = model.predict(*input)\npred.mean.shape, pred.cov.shape\n\n(torch.Size([10, 200, 3]), torch.Size([10, 200, 3, 3]))\n\n\n\ninput = dls.one_batch()[0]\ntarget = dls.one_batch()[1]\n\n\nmodel.state_dict()\n\nOrderedDict([('A',\n              tensor([[[0.0937, 0.6706, 0.1638],\n                       [0.9272, 0.2620, 0.4967],\n                       [0.2630, 0.1175, 0.1694]]], dtype=torch.float64)),\n             ('H',\n              tensor([[[0.6441, 0.2801, 0.9132],\n                       [0.0329, 0.4856, 0.9927],\n                       [0.5895, 0.2611, 0.9413]]], dtype=torch.float64)),\n             ('B',\n              tensor([[[0.6319, 0.6734, 0.7937, 0.6468, 0.5825, 0.4599],\n                       [0.7960, 0.9038, 0.9735, 0.6428, 0.3725, 0.2052],\n                       [0.0507, 0.4448, 0.5775, 0.7237, 0.5927, 0.3217]]],\n                     dtype=torch.float64)),\n             ('Q_raw',\n              tensor([[[0.4760, 0.0000, 0.0000],\n                       [0.5243, 0.3714, 0.0000],\n                       [0.2343, 0.9991, 0.1775]]], dtype=torch.float64)),\n             ('R_raw',\n              tensor([[[0.8451, 0.0000, 0.0000],\n                       [0.0462, 0.2360, 0.0000],\n                       [0.8950, 0.7419, 0.9471]]], dtype=torch.float64)),\n             ('b',\n              tensor([[[0.2100],\n                       [0.4890],\n                       [0.0564]]], dtype=torch.float64)),\n             ('d',\n              tensor([[[0.1371],\n                       [0.8726],\n                       [0.5590]]], dtype=torch.float64)),\n             ('m0',\n              tensor([[[0.6690],\n                       [0.1554],\n                       [0.0821]]], dtype=torch.float64)),\n             ('P0_raw',\n              tensor([[[0.7309, 0.0000, 0.0000],\n                       [0.0196, 0.0384, 0.0000],\n                       [0.3438, 0.5494, 0.8238]]], dtype=torch.float64))])\n\n\n\ndata = input[0][0]\ndata.shape\n\ntorch.Size([200, 3])\n\n\n\nmask = input[1][0]\ncontrol = input[2][0]\n\n\nmask.shape\n\ntorch.Size([200, 3])\n\n\n\ndata.device\n\ndevice(type='cpu')\n\n\n\ntorch.device\n\ntorch.device\n\n\n\ndata.shape, mask.shape\n\n(torch.Size([200, 3]), torch.Size([200, 3]))\n\n\n\nmodel.predict(data.unsqueeze(0), mask.unsqueeze(0), control.unsqueeze(0));\n\n\nmodel.use_smooth = True\n\n\npred = model(input)\n\n\npred[0].shape\n\ntorch.Size([10, 200, 3])\n\n\n\npred[1].shape\n\ntorch.Size([10, 200, 3, 3])\n\n\n\nmodel.use_smooth = False\n\n\npred_filt = model(input)\n\n\npred_filt[1].shape\n\ntorch.Size([10, 200, 3, 3])\n\n\n\ntype(pred), type(pred_filt)\n\n(__main__.MNormalsParams, __main__.MNormalsParams)\n\n\n\npred_filt.mean.shape, pred_filt.cov.shape\n\n(torch.Size([10, 200, 3]), torch.Size([10, 200, 3, 3]))\n\n\n\ntest_ne(pred, pred_filt)\n\n\n\nLoss Function\nadd support for complete loss (also outside gap) and for filter loss (don’t run the smooher)\nThere are two ways to compute the loss, one is to do it for all predictions the other is for doing it for only the gap - only_gap\nPlay around with flatting + diagonal\n\nmeans, covs = pred\ndata, mask, contr = target\n\n\npred.mean.shape, pred.cov.shape\n\n(torch.Size([10, 200, 3]), torch.Size([10, 200, 3, 3]))\n\n\n\nsource\n\n\nget_only_gap\n\n get_only_gap (mask, *args)\n\nfor each element in arg return only the portion where there is a gap at the time level\n\nsource\n\n\nKalmanLoss\n\n KalmanLoss (only_gap:bool=False, use_std:bool=False,\n             reduction:str='mean', reduction_inbatch:str='sum')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nonly_gap\nbool\nFalse\nloss for all predictions or only gap. Expects predictions only for gap\n\n\nuse_std\nbool\nFalse\nloss on stds otherwise with full cov matrices.\n\n\nreduction\nstr\nmean\none of [‘sum’, ‘mean’, ‘none’] reduction between batches\n\n\nreduction_inbatch\nstr\nsum\none of [‘sum’, ‘mean’, ‘none’] reduction inside a batch\n\n\n\n\ninput  = target = imp_dataloader(hai, hai_era, var_sel, gap_len=5, block_len=10, control_lags=[1], n_rep = 1, bs=2).cpu().one_batch()[0]\n\n\nmodel = KalmanFilterSR.init_random(n_dim_obs = hai.shape[-1], n_dim_state = hai.shape[-1], n_dim_contr = hai_era.shape[-1]*control_repeat)\n\n\nmodel.use_smooth = True\n# model.pred_only_gap = False\n# model.use_conditional = False\npred = model(input)\n\n\npred.cov[0][0].shape\n\ntorch.Size([3, 3])\n\n\n\nKalmanLoss()(pred, target)\n\ntensor(41.3868, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nKalmanLoss(use_std=True)(pred, target)\n\ntensor(42.7013, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nKalmanLoss(reduction='mean')(pred, target)\n\ntensor(41.3868, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nKalmanLoss(reduction_inbatch='mean')(pred, target)\n\ntensor(4.1387, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\ntest_fail(KalmanLoss(reduction_inbatch='fail'), args=(pred, target))\n\n\nOnly Gap\n\nmodel_gap = KalmanFilterSR.init_random(n_dim_obs = hai.shape[-1], n_dim_state = hai.shape[-1], n_dim_contr = hai_era.shape[-1]*control_repeat, pred_only_gap = True, use_conditional=True)\n\n\ninput_gap = target_gap = imp_dataloader(hai, hai_era, var_sel, gap_len=5, block_len=10, control_lags=[1], n_rep = 1, bs=2).cpu().one_batch()[0]\n\n\npred_gap = model_gap(input_gap)\n\n\nKalmanLoss(only_gap=True)(pred_gap, target_gap)\n\ntensor(21.6366, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nKalmanLoss(only_gap=True)(pred_gap, target_gap).backward(retain_graph=True)\n\n\n\n\nMetrics\n\npred0, targ = target[0][0].detach().cpu(), pred[0][0].detach().cpu()\n\n\npred0.shape, targ.shape\n\n(torch.Size([10, 3]), torch.Size([10, 3]))\n\n\nThe shape of the input is very important for the r2score, which cannot be batched and requires a needs to keep the multidimensional input\n\\[ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} \\]\n\nr2_score(pred0, targ)\n\n-7.491135595391619e+30\n\n\n\nr2_score(pred0, targ, multioutput=\"raw_values\")\n\narray([-2.93273242e+03, -2.24734068e+31, -3.74132968e+03])\n\n\n\nr2_score(pred0.flatten(), targ.flatten())\n\n-2.0463836801133035\n\n\nWhile for the rmse is okay to batch and flatten input. The only difference is how the mean is computed, which is a minor difference\n\nmean_squared_error(pred0, targ)\n\n1.0738884633304902\n\n\n\nmean_squared_error(pred0.flatten(), targ.flatten())\n\n1.0738884633304902\n\n\nWrapper around fastai metrics to support masked tensors and normal distributions\nThe problem is that fastai metrics by default flatten everything … so need to reimplement them\n\nmyr2 = skm_to_fastai(r2_score, flatten=False)\n\n\nmyr2(pred[0][0], target[0][0])\n\n-7.491135595391619e+30\n\n\nbut the mask is still flattening the data ….\n\nm = target[1][0]\nm.shape\n\ntorch.Size([10, 3])\n\n\nneed to get the mask as a matrix and not as a vector, so drop columns and rows that are all true and then check the resulting mask is all False\n\nmask_sub = m[:, ~m.all(0)][~m.all(1),:]\nmask_sub\n\ntensor([[False, False],\n        [False, False],\n        [False, False],\n        [False, False],\n        [False, False]])\n\n\n\n~mask_sub.any()\n\ntensor(True)\n\n\n\nm2 = m.clone()\nm2[0,0] = False\nm2[:, ~m2.all(0)][~m2.all(1),:]\n\ntensor([[False,  True],\n        [False, False],\n        [False, False],\n        [False, False],\n        [False, False],\n        [False, False]])\n\n\n\nsource\n\n\nImpMetric\n\n ImpMetric (metric, base_name, only_gap=False, flatten=False)\n\nAverage the values of func taking into account potential different batch sizes\n\nsource\n\n\nimp_rmse\n\n imp_rmse (preds, targs)\n\n\nrmse_mask.name, rmse_gap.name\n\n('rmse', 'rmse_gap')\n\n\n\nrmse_mask(pred, target)\n\n0.747701108455658\n\n\n\nmodel_gap = KalmanFilterSR.init_random(n_dim_obs = hai.shape[-1], n_dim_state = hai.shape[-1], n_dim_contr = hai_era.shape[-1]*control_repeat, pred_only_gap = True, use_conditional=True)\n\n\ninput_gap = target_gap = imp_dataloader(hai, hai_era, var_sel, gap_len=5, block_len=10, control_lags=[1], n_rep = 1, bs=2).cpu().one_batch()[0]\n\n\npred_gap = model_gap(input)\n\n\nrmse_gap(pred_gap, target)\n\n1.09806489944458\n\n\n\nr2_mask.name\n\n'r2'\n\n\n\nr2_mask(pred, target)\n\n-4.9728401966581414e+30\n\n\n\n\nCallback\nsave the model state\n\nsource\n\n\nSaveParams\n\n SaveParams (param_name)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\nSaveParams\n\n SaveParams (param_name)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\ndebug_preds = []\n\n\nclass DebugPredCallback(Callback):\n    order = 0\n    def after_validate(self):\n        if hasattr(self, 'gather_preds'):\n            debug_preds.append(self.gather_preds.preds)\n\n\n\nLearner\n\nobs_cov_history = SaveParams('obs_cov')\n\n\nall_data = CollectDataCallback()\n\n\nmodel = KalmanFilterSR.init_random(n_dim_obs = hai.shape[1], n_dim_state = hai.shape[1], n_dim_contr = hai_era.shape[-1]*control_repeat)#.cuda()\n\n\n# model._set_constraint('obs_cov', model.obs_cov, train=False)\n\n\ndls = imp_dataloader(hai[10_000:11_000], hai_era, ['TA'], gap_len=5, block_len=20, control_lags=[1], n_rep=1, bs=2).cpu()\n\n\ndls.one_batch()[0][0].device\n\ndevice(type='cpu')\n\n\n\ninput, target = dls.one_batch()\n\n\npred = model(input)\nKalmanLoss()(pred, target)\n\ntensor(77.1323, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nFloat64\n\nsource\n\n\n\nFloat64Callback\n\n Float64Callback (after_create=None, before_fit=None, before_epoch=None,\n                  before_train=None, before_batch=None, after_pred=None,\n                  after_loss=None, before_backward=None,\n                  after_cancel_backward=None, after_backward=None,\n                  before_step=None, after_cancel_step=None,\n                  after_step=None, after_cancel_batch=None,\n                  after_batch=None, after_cancel_train=None,\n                  after_train=None, before_validate=None,\n                  after_cancel_validate=None, after_validate=None,\n                  after_cancel_epoch=None, after_epoch=None,\n                  after_cancel_fit=None, after_fit=None)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nrmse\n\n&lt;fastai.metrics.AccumMetric&gt;\n\n\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(), cbs = [Float64Callback] , metrics = rmse_mask)\n\n\nlearn.fit(1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse\ntime\n\n\n\n\n0\n85.788973\n103.703916\n1.533759\n00:03\n\n\n\n\n\n\nOnly gap\n\nlearn_gap = Learner(dls, model_gap, loss_func=KalmanLoss(only_gap=True), cbs = [DebugPredCallback, Float64Callback] , metrics = [rmse_gap])\n\n\npred_gap = learn_gap.model(dls.one_batch()[0])\n\n\nKalmanLoss(only_gap=True)(pred_gap, dls.one_batch()[0])\n\ntensor(6.2605, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nlearn_gap.fit(1, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrmse_gap\ntime\n\n\n\n\n0\n5.226804\n11.659894\n1.231025\n00:02\n\n\n\n\n\n\nlearn.loss"
  },
  {
    "objectID": "training.html#predictions",
    "href": "training.html#predictions",
    "title": "Implement Kalman model using FastAI",
    "section": "Predictions",
    "text": "Predictions\nThe transformation pipeline is not working properly (there is a problem in decode_batch as the _types are more nested than the predictions, which results in an error) + the pipeline is anyway not reproducible + the test dataloaders seems that they are actually not deterministic ….. soo reimplement everything almost from scratch\nsee https://github.com/mone27/meteo_imp/blob/0335003405ec9bd3e3bd2641bc6d7924f34a0788/lib_nbs/kalman/10_fastai.ipynb for all details\n\nPredictions from custom items\n\nsource\n\n\none_batch_with_items\n\n one_batch_with_items (dls, items)\n\nMakes custom dataloader that returns only one batch with items\n\nitems = [MeteoImpItem(1,0, 'TA', 5), MeteoImpItem(2,0, 'TA', 5)]\n\n\ninput, _ = one_batch_with_items(dls, items)\n\n\n# test that there is no shuffling\nbatch0 = one_batch_with_items(dls, items) \nfor _ in range(5):\n    test_close(batch0[0][0], one_batch_with_items(dls, items)[0][0])\n\n\npreds = learn.model(input)\npreds_gap = learn_gap.model(input)\n\n\n\nPredictions Transform Pipeline\nNeed to transform the predictions into a format that can be used (for plotting)\nThe steps are:\n\nconvert covariance to std\nmaybe buffer predictions if they are only for gap\ninverse normalize\nget original target (not transformed)\ntransform to dataframe (with proper index/col names)\n\n\n1) Cov 2 Std\nTransform covariance to std supporting also only gaps predictions\n\nsource\n\n\n\nCovStdTransform\n\n CovStdTransform (enc=None, dec=None, split_idx=None, order=None)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nCovStdTransform()\n\n\n  \n    CovStdTransform\n  \n  (MNormalsParams,object) -&gt; encodes\n(Tensor,object) -&gt; encodes\n(list,object) -&gt; encodes\n\n  \n\n\n\n\ntype(preds)\n\n__main__.MNormalsParams\n\n\n\npreds_0 = learn.model(input)\npreds_1 = CovStdTransform()(preds_0)\npreds_1.std.shape\n\ntorch.Size([2, 20, 3])\n\n\n\npreds_gap_0 = learn_gap.model(input)\npreds_gap_1 = CovStdTransform()(preds_gap_0)\npreds_gap_1.std\n\n[[tensor([0.9525], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;),\n  tensor([0.9526], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;),\n  tensor([0.9527], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;),\n  tensor([0.9522], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;),\n  tensor([0.9515], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;)],\n [tensor([0.9525], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;),\n  tensor([0.9526], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;),\n  tensor([0.9527], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;),\n  tensor([0.9522], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;),\n  tensor([0.9515], dtype=torch.float64, grad_fn=&lt;SqrtBackward0&gt;)]]\n\n\n\n2) Buffer gap only preds\nin addition detach and move to CPU\n\nsource\n\n\n\nbuffer_pred_single\n\n buffer_pred_single (preds:list[torch.Tensor], masks:torch.Tensor)\n\nFor predictions are for gaps only add buffer of Nan so they have same shape of targets\n\nsource\n\n\nbuffer_pred\n\n buffer_pred (preds:list[list[torch.Tensor]], masks:torch.Tensor)\n\nFor predictions are for gaps only add buffer of Nan so they have same shape of targets\n\nsource\n\n\nmaybe_buffer_pred\n\n maybe_buffer_pred (preds, masks)\n\nIf predictions are for gaps only add buffer so they have same shape of targets\n\npreds_2 = maybe_buffer_pred(preds_1, input[1])\ntest_eq(preds_1, preds_2)\n\n\npreds_gap_2 = maybe_buffer_pred(preds_gap_1, input[1])\ntest_eq(preds_2.mean.shape, preds_gap_2.mean.shape)\n\n\nassert (preds_gap_2.mean.isnan() == input[1]).all()\n\n\n3) Inverse Normalize\nhere we use the decode step of the normalizer in the dataloader\n\ndef get_stats_np(*args):\n    stats = get_stats(*args)\n    return (stats[0].numpy(), stats[1].numpy())\n\n\nsource\n\n\n\nInverseNormalize\n\n InverseNormalize (mean, std)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\npreds_3 = InverseNormalize.from_dls(dls)(preds_2)\npreds_2.mean.mean(-2), preds_3.mean.mean(-2)\n\n(tensor([[-0.7011, -0.1037, -0.9301],\n         [-1.0411, -0.0590, -1.3921]], dtype=torch.float64),\n tensor([[ 14.6384, 177.0957,   2.1694],\n         [ 13.4223, 188.0962,   0.4839]], dtype=torch.float64))\n\n\n\npreds_gap_3 = InverseNormalize.from_dls(dls)(preds_gap_2)\ntorch.nanmean(preds_gap_2.mean, -2), torch.nanmean(preds_gap_3.mean, -2)\n\n(tensor([[-0.6435,     nan,     nan],\n         [-0.5958,     nan,     nan]], dtype=torch.float64),\n tensor([[14.8444,     nan,     nan],\n         [15.0151,     nan,     nan]], dtype=torch.float64))\n\n\n\n4) Original target\nwe need the target as a dataframe and not transformed, so we use the first part of the dls pipeline + custom aggregation to a list\n\nsource\n\n\n\norig_target\n\n orig_target (dls, items)\n\n\ntargs = orig_target(dls, items)\nlen(targs), type(targs[0])\n\n(2, __main__.MeteoImpDf)\n\n\n\nfor targ, o_targ in zip(input[1], targs):\n    test_eq(targ.numpy(), o_targ.mask.to_numpy())\n\n\n5) To Dataframe\n\nsource\n\n\n\nNormalsDf\n\n NormalsDf (mean, std)\n\nDataFrames of Normal parameters (mean and std)\n\nsource\n\n\npreds2df\n\n preds2df (preds:__main__.NormalsParams, targs)\n\n\npreds_5 = preds2df(preds_3, targs)\npreds_gap_5 = preds2df(preds_gap_3, targs)\n\n\n\nGet Predictions\nNow combine all the steps above in one function that takes as argument: - model - dataloader - items\nand returns inverse transformed preds, targs and metrics\n\nsource\n\n\nunsqueeze_maybe_list\n\n unsqueeze_maybe_list (x)\n\nadd a dimension in front if Tensor and make a list of x if x is a list\n\n_n_tuple(preds, 0).mean.shape\n\ntorch.Size([1, 20, 3])\n\n\n\n_n_tuple(preds_gap, 0).mean.__len__()\n\n1\n\n\n\n_n_tuple(input, 0)[0].__len__()\n\n1\n\n\n\n_n_tuple(input, 0, False)[0].__len__()\n\n20\n\n\n\nsource\n\n\nPredictMetrics\n\n PredictMetrics (learn)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPredictLossVar\n\n PredictLossVar (only_gap:bool, var:int)\n\nCompute loss only for one variable\n\ninput[1].shape\n\ntorch.Size([2, 20, 3])\n\n\n\nKalmanLoss(False)(preds_0, input)\n\ntensor(74.7852, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nKalmanLoss(True)(preds_gap_0, input)\n\ntensor(4.7129, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nPredictLossVar(True, 0)(preds_gap_0, input)\n\ntensor(4.7129, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nPredictLossVar(False, 0)(preds_0, input)\n\ntensor(74.7852, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nsource\n\n\npredict_items\n\n predict_items (model:meteo_imp.kalman.filter.KalmanFilterBase,\n                dls:fastai.data.core.DataLoaders, items:list[list],\n                metric_fn:Optional[Callable]=None)\n\n\nf_preds, f_targs = predict_items(learn.model, dls, items)\nlen(f_preds)\n\n2\n\n\n\nf_preds, f_targs, loss = predict_items(learn.model, dls, items, KalmanLoss(False))\nloss\n\ntensor(74.7852, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\nf_preds, f_targs = predict_items(learn.model, dls, items + [MeteoImpItem(3,2,'TA', 10)])\nlen(f_preds)\n\n3\n\n\n\nlearn_gap.model.use_conditional = False\n\n\nf_preds_gap, f_targs_gap = predict_items(learn_gap.model, dls, items + [MeteoImpItem(3,4, 'TA', 10)])\nlen(f_preds)\n\n3\n\n\n\nf_preds[0]\n\n\nNormals Df  data \n\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-07-27 19:00:00\n15.8604\n247.1413\n3.6759\n\n\n2000-07-27 19:30:00\n17.1074\n222.8983\n4.3447\n\n\n2000-07-27 20:00:00\n16.7494\n212.7181\n4.5155\n\n\n2000-07-27 20:30:00\n15.1599\n150.3234\n2.6903\n\n\n2000-07-27 21:00:00\n14.7076\n147.0331\n2.2668\n\n\n2000-07-27 21:30:00\n14.6867\n156.1923\n2.2391\n\n\n2000-07-27 22:00:00\n14.7689\n169.1580\n2.3664\n\n\n2000-07-27 22:30:00\n14.7338\n177.7875\n2.3559\n\n\n2000-07-27 23:00:00\n14.5411\n178.0651\n2.1209\n\n\n2000-07-27 23:30:00\n14.4147\n176.9044\n1.9582\n\n\n2000-07-28 00:00:00\n14.3162\n175.7231\n1.8447\n\n\n2000-07-28 00:30:00\n14.2605\n175.4748\n1.7815\n\n\n2000-07-28 01:00:00\n14.3124\n178.9888\n1.8398\n\n\n2000-07-28 01:30:00\n14.2570\n178.2621\n1.7823\n\n\n2000-07-28 02:00:00\n14.2076\n177.5208\n1.7210\n\n\n2000-07-28 02:30:00\n14.1662\n176.4220\n1.6684\n\n\n2000-07-28 03:00:00\n14.0800\n173.6999\n1.5688\n\n\n2000-07-28 03:30:00\n13.8980\n167.9674\n1.3614\n\n\n2000-07-28 04:00:00\n13.5876\n158.9148\n1.0064\n\n\n2000-07-28 04:30:00\n12.9515\n140.7185\n0.2796\n\n\n\n\nstd\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-07-27 19:00:00\n5.1829\n344.4108\n5.2468\n\n\n2000-07-27 19:30:00\n5.1693\n344.0653\n5.2432\n\n\n2000-07-27 20:00:00\n5.1696\n344.0760\n5.2403\n\n\n2000-07-27 20:30:00\n5.1694\n344.0763\n5.2403\n\n\n2000-07-27 21:00:00\n5.1695\n344.0778\n5.2404\n\n\n2000-07-27 21:30:00\n5.1699\n344.0815\n5.2409\n\n\n2000-07-27 22:00:00\n5.1730\n344.1296\n5.2451\n\n\n2000-07-27 22:30:00\n5.3602\n347.8643\n5.4704\n\n\n2000-07-27 23:00:00\n5.3641\n347.8706\n5.4766\n\n\n2000-07-27 23:30:00\n5.3646\n347.8681\n5.4776\n\n\n2000-07-28 00:00:00\n5.3639\n347.8583\n5.4769\n\n\n2000-07-28 00:30:00\n5.3589\n347.7736\n5.4701\n\n\n2000-07-28 01:00:00\n5.1720\n344.0764\n5.2449\n\n\n2000-07-28 01:30:00\n5.1698\n344.0765\n5.2411\n\n\n2000-07-28 02:00:00\n5.1695\n344.0768\n5.2404\n\n\n2000-07-28 02:30:00\n5.1696\n344.0786\n5.2405\n\n\n2000-07-28 03:00:00\n5.1704\n344.0882\n5.2416\n\n\n2000-07-28 03:30:00\n5.1745\n344.1348\n5.2468\n\n\n2000-07-28 04:00:00\n5.1954\n344.3778\n5.2738\n\n\n2000-07-28 04:30:00\n5.3088\n345.7310\n5.4178\n\n\n\n\n\n \n\n\n\nf_preds_gap[0]\n\n\nNormals Df  data \n\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-07-27 19:00:00\nnan\nnan\nnan\n\n\n2000-07-27 19:30:00\nnan\nnan\nnan\n\n\n2000-07-27 20:00:00\nnan\nnan\nnan\n\n\n2000-07-27 20:30:00\nnan\nnan\nnan\n\n\n2000-07-27 21:00:00\nnan\nnan\nnan\n\n\n2000-07-27 21:30:00\nnan\nnan\nnan\n\n\n2000-07-27 22:00:00\nnan\nnan\nnan\n\n\n2000-07-27 22:30:00\n15.1513\nnan\nnan\n\n\n2000-07-27 23:00:00\n15.1126\nnan\nnan\n\n\n2000-07-27 23:30:00\n15.1093\nnan\nnan\n\n\n2000-07-28 00:00:00\n15.0553\nnan\nnan\n\n\n2000-07-28 00:30:00\n15.1090\nnan\nnan\n\n\n2000-07-28 01:00:00\nnan\nnan\nnan\n\n\n2000-07-28 01:30:00\nnan\nnan\nnan\n\n\n2000-07-28 02:00:00\nnan\nnan\nnan\n\n\n2000-07-28 02:30:00\nnan\nnan\nnan\n\n\n2000-07-28 03:00:00\nnan\nnan\nnan\n\n\n2000-07-28 03:30:00\nnan\nnan\nnan\n\n\n2000-07-28 04:00:00\nnan\nnan\nnan\n\n\n2000-07-28 04:30:00\nnan\nnan\nnan\n\n\n\n\nstd\n\n\n\n \nTA\nSW_IN\nVPD\n\n\ntime\n \n \n \n\n\n\n\n2000-07-27 19:00:00\nnan\nnan\nnan\n\n\n2000-07-27 19:30:00\nnan\nnan\nnan\n\n\n2000-07-27 20:00:00\nnan\nnan\nnan\n\n\n2000-07-27 20:30:00\nnan\nnan\nnan\n\n\n2000-07-27 21:00:00\nnan\nnan\nnan\n\n\n2000-07-27 21:30:00\nnan\nnan\nnan\n\n\n2000-07-27 22:00:00\nnan\nnan\nnan\n\n\n2000-07-27 22:30:00\n3.4073\nnan\nnan\n\n\n2000-07-27 23:00:00\n3.4076\nnan\nnan\n\n\n2000-07-27 23:30:00\n3.4079\nnan\nnan\n\n\n2000-07-28 00:00:00\n3.4062\nnan\nnan\n\n\n2000-07-28 00:30:00\n3.4038\nnan\nnan\n\n\n2000-07-28 01:00:00\nnan\nnan\nnan\n\n\n2000-07-28 01:30:00\nnan\nnan\nnan\n\n\n2000-07-28 02:00:00\nnan\nnan\nnan\n\n\n2000-07-28 02:30:00\nnan\nnan\nnan\n\n\n2000-07-28 03:00:00\nnan\nnan\nnan\n\n\n2000-07-28 03:30:00\nnan\nnan\nnan\n\n\n2000-07-28 04:00:00\nnan\nnan\nnan\n\n\n2000-07-28 04:30:00\nnan\nnan\nnan\n\n\n\n\n\n \n\n\n\n\nOnly Gap Context manager\n\nsource\n\n\nonly_gap_ctx\n\n only_gap_ctx (learn, only_gap=True)\n\n\nwith only_gap_ctx(learn):\n    print(learn.model.pred_only_gap)\n    print(learn.loss_func.only_gap)\n    print(learn.metrics[0].only_gap)\nprint(learn.model.pred_only_gap)\n\nTrue\nTrue\nTrue\nFalse\n\n\n\nfrom meteo_imp.kalman.filter import with_settings\n\n\nwith with_settings(learn.model, use_conditional=False, pred_only_gap=True):\n    pred_gap_buff = buffer_pred(learn.model.predict(*input).mean, input[1])[0]\nmask_na = ~pred_gap_buff.isnan()\nwith with_settings(learn.model, use_conditional=False, pred_only_gap=False):\n    pred_ng = learn.model.predict(*input).mean[0]\n\n\ntest_close(pred_gap_buff[mask_na], pred_ng[mask_na])\n\n\n\nPlotting\n\nPlot results\n\nsource\n\n\n\nformat_metric\n\n format_metric (name, val)\n\n\nsource\n\n\nplot_result\n\n plot_result (pred, targ, metrics=None, control_map=None,\n              hide_no_gap=False, n_cols:int=3, bind_interaction:bool=True,\n              units=None, ys=['value', 'value', 'control'], title='',\n              sel=None, error=False, point=True, gap_area=True,\n              control=False, props={})\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npred\n\n\n\n\n\ntarg\n\n\n\n\n\nmetrics\nNoneType\nNone\n\n\n\ncontrol_map\nNoneType\nNone\n\n\n\nhide_no_gap\nbool\nFalse\n\n\n\nn_cols\nint\n3\n\n\n\nbind_interaction\nbool\nTrue\nWhether the sub-plots for each variable should be connected for zooming/panning\n\n\nunits\nNoneType\nNone\n\n\n\nys\nlist\n[‘value’, ‘value’, ‘control’]\n\n\n\ntitle\nstr\n\n\n\n\nsel\nNoneType\nNone\n\n\n\nerror\nbool\nFalse\n\n\n\npoint\nbool\nTrue\n\n\n\ngap_area\nbool\nTrue\n\n\n\ncontrol\nbool\nFalse\n\n\n\nprops\ndict\n{}\n\n\n\n\n\nf_targs[0].data.columns[~f_targs[0].mask.all()]\n\nIndex(['TA'], dtype='object')\n\n\n\nplot_result(f_preds[0], f_targs[0], units=units)\n\n\n\n\n\n\n\nwith only_gap_ctx(learn):\n    f_preds_gap, f_targs_gap = predict_items(learn.model, dls, items)\n    display(plot_result(f_preds_gap[0], f_targs_gap[0]))\n\n\n\n\n\n\n\nwith only_gap_ctx(learn):\n    f_preds_gap, f_targs_gap = predict_items(learn.model, dls, items)\n    display(plot_result(f_preds_gap[0], f_targs_gap[0], hide_no_gap=True))\n\n\n\n\n\n\n\nplot_result(f_preds[0], f_targs[0],control_map=hai_control)\n\n\n\n\n\n\n\nsource\n\n\nplot_results\n\n plot_results (preds, targs, metrics=None, n_cols=1, **kwargs)\n\n\nplot_results(f_preds, f_targs, units=units)\n\n\n\n\n\n\n\nplot_results(f_preds_gap, f_targs_gap, hide_no_gap=True, units=units)\n\n\n\n\n\n\n\nplot_results(f_preds_gap, f_targs_gap, hide_no_gap=True, units=units, control_map=hai_control)\n\n\n\n\n\n\n\nShow Results\n\nrandom.choices(learn.dls.items, k=3)\n\n[MeteoImpItem(i=14, shift=-10, var_sel=['TA'], gap_len=5),\n MeteoImpItem(i=23, shift=-10, var_sel=['TA'], gap_len=5),\n MeteoImpItem(i=17, shift=-10, var_sel=['TA'], gap_len=5)]\n\n\n\nsource\n\n\n\nget_results\n\n get_results (learn, n=3, items=None, dls=None)\n\n\npreds, targs = get_results(learn)\nlen(preds)\n\n[MeteoImpItem(i=42, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=43, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=47, shift=-10, var_sel=['TA'], gap_len=5)]\n\n\n3\n\n\n\nsource\n\n\nshow_results\n\n show_results (learn, n=3, items=None, dls=None, metrics=None, n_cols=1)\n\n\nshow_results(learn, control_map=hai_control)\n\n[MeteoImpItem(i=43, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=47, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=41, shift=-10, var_sel=['TA'], gap_len=5)]\n\n\n\n\n\n\n\n\nshow_results(learn, items=items)\n\n[MeteoImpItem(i=1, shift=0, var_sel=['TA'], gap_len=5), MeteoImpItem(i=2, shift=0, var_sel=['TA'], gap_len=5)]\n\n\n\n\n\n\n\n\n\nInteractive\n\nsource\n\n\nresults_custom_gap\n\n results_custom_gap (learn, df, control, items_idx, var_sel, gap_len,\n                     block_len, shift, control_lags)\n\n\nplot_results(*results_custom_gap(learn, df=hai, control=hai_era,\n                                 items_idx = [800, 801, 804],\n                                 var_sel=['TA'], gap_len=10,\n                                 block_len=200, control_lags=[1], shift=0))\n\n\n\n\n\n\n\nsource\n\n\nCustomGap\n\n CustomGap (learn, df, control)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nCustomGap(learn, hai, hai_era).interact_results()\n\n\n\n\n&lt;function ipywidgets.widgets.interaction._InteractFactory.__call__.&lt;locals&gt;.&lt;lambda&gt;(*args, **kwargs)&gt;\n\n\n\nCustomGap(learn_gap, hai, hai_era).interact_results()\n\n\n\n\n&lt;function ipywidgets.widgets.interaction._InteractFactory.__call__.&lt;locals&gt;.&lt;lambda&gt;(*args, **kwargs)&gt;"
  },
  {
    "objectID": "training.html#extra-training",
    "href": "training.html#extra-training",
    "title": "Implement Kalman model using FastAI",
    "section": "Extra Training",
    "text": "Extra Training\n\nInteractive Sequence\n\nsource\n\n\nInteractiveSequence\n\n InteractiveSequence (s:Sequence, start=0)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nInteractiveSequence([1,2,3])()\n\n\n\n\n\n\n\n\n\nSave model batch Callback\n\n\n\ncopy’]\nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.\n\nsource\n\n\nSaveModelsBatch\n\n SaveModelsBatch (every=None, times_epoch=None)\n\nCallback that tracks the number of iterations done and properly sets training/eval mode\n\nsave_models = SaveModelsBatch(times_epoch=3)\n\n\nlearn = Learner(dls, model, KalmanLoss(only_gap=False), cbs = [save_models, Float64Callback])\n\n\nlearn.fit(1, 1e-1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n42.632274\n103.624711\n00:03\n\n\n\n\n\n\nsave_models.models\n\n[{'train_iter': 1,\n  'model': Kalman Filter\n          N dim obs: 3,\n          N dim state: 3,\n          N dim contr: 6},\n {'train_iter': 10,\n  'model': Kalman Filter\n          N dim obs: 3,\n          N dim state: 3,\n          N dim contr: 6},\n {'train_iter': 19,\n  'model': Kalman Filter\n          N dim obs: 3,\n          N dim state: 3,\n          N dim contr: 6}]\n\n\n\n\n\n\n\n\n[MeteoImpItem(i=43, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=41, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=42, shift=-10, var_sel=['TA'], gap_len=5)]\n[MeteoImpItem(i=43, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=41, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=42, shift=-10, var_sel=['TA'], gap_len=5)]\n[MeteoImpItem(i=43, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=41, shift=-10, var_sel=['TA'], gap_len=5), MeteoImpItem(i=42, shift=-10, var_sel=['TA'], gap_len=5)]\nCPU times: user 6.88 s, sys: 102 ms, total: 6.98 s\nWall time: 3.47 s\n\n\n\nInteractiveSequence(save_model_plots)()"
  },
  {
    "objectID": "training.html#export",
    "href": "training.html#export",
    "title": "Implement Kalman model using FastAI",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nimport altair as alt\nimport altair\nload data for testing\nreset_seed()\nhai = pd.read_parquet(hai_big_path)\nhai_era = pd.read_parquet(hai_era_big_path)\ndls = imp_dataloader(hai, hai_era, var_sel = 'TA', gap_len=5, block_len=100, control_lags = [1], n_rep=1, bs=1).cpu()\nitem = MeteoImpItem(1,2, 'TA', gap_len=10)\nitems = [item]\ntarg = orig_target(dls, items)[0]\ninput, _ = one_batch_with_items(dls, items)"
  },
  {
    "objectID": "results.html#imputation-methods",
    "href": "results.html#imputation-methods",
    "title": "Results",
    "section": "Imputation Methods",
    "text": "Imputation Methods\n\nKalman Filter\ntake a MeteoImp Df and gap fill it using the given model\n\nmodel = torch.load(here(\"analysis/results/trained_13feb/1_gap_varying_12-336_v1.pickle\"))\n\n\npreds, targs = predict_items(model, dls=dls, items = items)\npreds[0].mean[targs[0].mask] = targs[0].data[targs[0].mask]\n\n\nKalmanImputation\n\n_n_tuple\n\n&lt;function meteo_imp.kalman.fastai._n_tuple(x, n, unsqueeze=True)&gt;\n\n\n\ninput[1].shape\n\ntorch.Size([1, 100, 9])\n\n\n\npreds_raw = model(input)\n\n\nlen(preds_raw.mean[0])\n\n10\n\n\n\n_extract_var(preds_raw.mean, 1, 7)\n\n[[tensor([-0.7555], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([-0.7765], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([-0.8153], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([-0.8653], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([-0.8660], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([-0.8712], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([-0.8551], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([-0.8420], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([-0.8442], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([-0.8536], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;)]]\n\n\n\n_extract_var(preds_raw.cov, 1, 7)\n\n[[tensor([[0.0227]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([[0.0307]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([[0.0359]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([[0.0392]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([[0.0409]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([[0.0409]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([[0.0393]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([[0.0361]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([[0.0310]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;),\n  tensor([[0.0230]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;)]]\n\n\n\nitems_all = [MeteoImpItem(1,2, list(hai.columns), gap_len=3)] * 3\n\n\ninput_all, _ = one_batch_with_items(dls, items_all)\n\n\npreds_raw_all = model(input_all)\n\n\n_extract_var(preds_raw_all.mean, 3, 9)\n\n[[tensor([-0.5650], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([-0.5887], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([-0.6259], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;)],\n [tensor([-0.5650], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([-0.5887], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([-0.6259], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;)],\n [tensor([-0.5650], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([-0.5887], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([-0.6259], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;)]]\n\n\n\nbuffer_pred(preds_raw_all.mean, input_all[1]).shape\n\ntorch.Size([3, 100, 9])\n\n\n\n_extract_var(preds_raw_all.cov, 3, 9)\n\n[[tensor([[0.1189]], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([[0.1717]], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([[0.1251]], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;)],\n [tensor([[0.1189]], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([[0.1717]], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([[0.1251]], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;)],\n [tensor([[0.1189]], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([[0.1717]], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;),\n  tensor([[0.1251]], dtype=torch.float64, grad_fn=&lt;SliceBackward0&gt;)]]\n\n\n\nsource\n\n\n\nPredictLossVar\n\n PredictLossVar (only_gap:bool, var:int)\n\nloss (negative log likelihood) for only for one variable for each batch\n\nPredictLossVar(True, 3)(preds_raw_all, input_all)\n\n[tensor(-0.2252, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;),\n tensor(-0.2252, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;),\n tensor(-0.2252, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)]\n\n\n\ninput_all[1].shape\n\ntorch.Size([3, 100, 9])\n\n\n\nsource\n\n\nPredictLikelihoodVar\n\n PredictLikelihoodVar (only_gap:bool, var:int)\n\nmean between timesteps of Likelihood for only for one variable for each batch\n\nPredictLikelihoodVar(True, 3)(preds_raw_all, input_all)\n\n[tensor(1.1569, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.1569, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.1569, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)]\n\n\n\nsource\n\n\nMultiMetrics\n\n MultiMetrics (**metrics)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nKalmanImputation\n\n KalmanImputation (model)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nk_imp = KalmanImputation(model)\n\n\npred = k_imp(item, dls=dls)\n\n\ndisplay_as_row({'pred': pred[45:55],'data': targ.data[45:55], 'mask': targ.mask[45:55]}, hide_idx=False)\n\n\n  pred \n\n\n\n\n \nTA\nSW_IN\nLW_IN\nVPD\nWS\nPA\nP\nSWC\nTS\n\n\ntime\n \n \n \n \n \n \n \n \n \n\n\n\n\n2000-01-04 02:30:00\n2.3467\n0.0000\n304.7310\n0.9520\n5.3900\n96.3000\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 03:00:00\n2.1803\n0.0000\n304.7310\n0.9710\n5.5900\n96.3000\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 03:30:00\n1.8728\n0.0000\n304.7310\n0.9580\n5.6700\n96.2900\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 04:00:00\n1.4769\n0.0000\n304.7310\n0.9380\n6.6100\n96.2600\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 04:30:00\n1.4710\n0.0000\n315.0050\n0.7850\n6.6500\n96.2600\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 05:00:00\n1.4303\n0.0000\n315.0050\n0.5290\n6.1200\n96.2700\n0.0000\n43.0800\n1.3000\n\n\n2000-01-04 05:30:00\n1.5573\n0.0000\n315.0050\n0.4500\n5.5800\n96.2600\n0.2900\n43.1300\n1.3000\n\n\n2000-01-04 06:00:00\n1.6616\n0.0000\n315.0050\n0.4990\n4.7000\n96.2500\n0.0000\n43.1300\n1.2900\n\n\n2000-01-04 06:30:00\n1.6438\n0.0000\n315.0050\n0.3820\n5.0700\n96.2300\n0.0000\n43.1300\n1.2900\n\n\n2000-01-04 07:00:00\n1.5697\n0.0000\n315.0050\n0.3130\n5.2300\n96.2400\n0.0000\n43.1300\n1.2900\n\n\n\n\ndata\n\n\n\n \nTA\nSW_IN\nLW_IN\nVPD\nWS\nPA\nP\nSWC\nTS\n\n\ntime\n \n \n \n \n \n \n \n \n \n\n\n\n\n2000-01-04 02:30:00\n2.1900\n0.0000\n304.7310\n0.9520\n5.3900\n96.3000\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 03:00:00\n2.2700\n0.0000\n304.7310\n0.9710\n5.5900\n96.3000\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 03:30:00\n2.3200\n0.0000\n304.7310\n0.9580\n5.6700\n96.2900\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 04:00:00\n2.3400\n0.0000\n304.7310\n0.9380\n6.6100\n96.2600\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 04:30:00\n2.2400\n0.0000\n315.0050\n0.7850\n6.6500\n96.2600\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 05:00:00\n2.0000\n0.0000\n315.0050\n0.5290\n6.1200\n96.2700\n0.0000\n43.0800\n1.3000\n\n\n2000-01-04 05:30:00\n1.9400\n0.0000\n315.0050\n0.4500\n5.5800\n96.2600\n0.2900\n43.1300\n1.3000\n\n\n2000-01-04 06:00:00\n2.0700\n0.0000\n315.0050\n0.4990\n4.7000\n96.2500\n0.0000\n43.1300\n1.2900\n\n\n2000-01-04 06:30:00\n2.0400\n0.0000\n315.0050\n0.3820\n5.0700\n96.2300\n0.0000\n43.1300\n1.2900\n\n\n2000-01-04 07:00:00\n2.0300\n0.0000\n315.0050\n0.3130\n5.2300\n96.2400\n0.0000\n43.1300\n1.2900\n\n\n\n\n\nmask\n\n\n\n \nTA\nSW_IN\nLW_IN\nVPD\nWS\nPA\nP\nSWC\nTS\n\n\ntime\n \n \n \n \n \n \n \n \n \n\n\n\n\n2000-01-04 02:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 03:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 03:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 04:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 04:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 05:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 05:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 06:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 06:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 07:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n \n\n\n\nKalman Imputation - Specialized Variables\n\nsource\n\n\n\nKalmanImputationVar\n\n KalmanImputationVar (models)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nDetails\n\n\n\n\nmodels\ndataframe with 2 columns model and var\n\n\n\n\nk_impVar = KalmanImputationVar(pd.DataFrame({'model': [model], 'var': 'TA'}))\n\n\npred = k_impVar(var= 'TA', item=item, dls=dls)\n\n\ndisplay_as_row({'pred': pred[45:55],'data': targ.data[45:55], 'mask': targ.mask[45:55]}, hide_idx=False)\n\n\n  pred \n\n\n\n\n \nTA\nSW_IN\nLW_IN\nVPD\nWS\nPA\nP\nSWC\nTS\n\n\ntime\n \n \n \n \n \n \n \n \n \n\n\n\n\n2000-01-04 02:30:00\n2.3467\n0.0000\n304.7310\n0.9520\n5.3900\n96.3000\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 03:00:00\n2.1803\n0.0000\n304.7310\n0.9710\n5.5900\n96.3000\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 03:30:00\n1.8728\n0.0000\n304.7310\n0.9580\n5.6700\n96.2900\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 04:00:00\n1.4769\n0.0000\n304.7310\n0.9380\n6.6100\n96.2600\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 04:30:00\n1.4710\n0.0000\n315.0050\n0.7850\n6.6500\n96.2600\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 05:00:00\n1.4303\n0.0000\n315.0050\n0.5290\n6.1200\n96.2700\n0.0000\n43.0800\n1.3000\n\n\n2000-01-04 05:30:00\n1.5573\n0.0000\n315.0050\n0.4500\n5.5800\n96.2600\n0.2900\n43.1300\n1.3000\n\n\n2000-01-04 06:00:00\n1.6616\n0.0000\n315.0050\n0.4990\n4.7000\n96.2500\n0.0000\n43.1300\n1.2900\n\n\n2000-01-04 06:30:00\n1.6438\n0.0000\n315.0050\n0.3820\n5.0700\n96.2300\n0.0000\n43.1300\n1.2900\n\n\n2000-01-04 07:00:00\n1.5697\n0.0000\n315.0050\n0.3130\n5.2300\n96.2400\n0.0000\n43.1300\n1.2900\n\n\n\n\ndata\n\n\n\n \nTA\nSW_IN\nLW_IN\nVPD\nWS\nPA\nP\nSWC\nTS\n\n\ntime\n \n \n \n \n \n \n \n \n \n\n\n\n\n2000-01-04 02:30:00\n2.1900\n0.0000\n304.7310\n0.9520\n5.3900\n96.3000\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 03:00:00\n2.2700\n0.0000\n304.7310\n0.9710\n5.5900\n96.3000\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 03:30:00\n2.3200\n0.0000\n304.7310\n0.9580\n5.6700\n96.2900\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 04:00:00\n2.3400\n0.0000\n304.7310\n0.9380\n6.6100\n96.2600\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 04:30:00\n2.2400\n0.0000\n315.0050\n0.7850\n6.6500\n96.2600\n0.0000\n43.0800\n1.3100\n\n\n2000-01-04 05:00:00\n2.0000\n0.0000\n315.0050\n0.5290\n6.1200\n96.2700\n0.0000\n43.0800\n1.3000\n\n\n2000-01-04 05:30:00\n1.9400\n0.0000\n315.0050\n0.4500\n5.5800\n96.2600\n0.2900\n43.1300\n1.3000\n\n\n2000-01-04 06:00:00\n2.0700\n0.0000\n315.0050\n0.4990\n4.7000\n96.2500\n0.0000\n43.1300\n1.2900\n\n\n2000-01-04 06:30:00\n2.0400\n0.0000\n315.0050\n0.3820\n5.0700\n96.2300\n0.0000\n43.1300\n1.2900\n\n\n2000-01-04 07:00:00\n2.0300\n0.0000\n315.0050\n0.3130\n5.2300\n96.2400\n0.0000\n43.1300\n1.2900\n\n\n\n\n\nmask\n\n\n\n \nTA\nSW_IN\nLW_IN\nVPD\nWS\nPA\nP\nSWC\nTS\n\n\ntime\n \n \n \n \n \n \n \n \n \n\n\n\n\n2000-01-04 02:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 03:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 03:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 04:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 04:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 05:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 05:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 06:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 06:30:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2000-01-04 07:00:00\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n \n\n\n\n\nMDS\nNeed to call R from python\n\nr.print('here::here(\"R/REddyProc_tools.R\")')\n\n[1] \"here::here(\\\"R/REddyProc_tools.R\\\")\"\n\n\n\n        StrVector with 1 elements.\n        \n\n\n\n'here::here(\"R/REddyProc_tools.R\")'\n\n\n\n\n        \n\n\n\nsource\n\n\nimportr_install\n\n importr_install (pkg)\n\n\nsource\n\n\nsetupR\n\n setupR ()\n\n\nsource\n\n\nR2pd\n\n R2pd (x)\n\n\nsource\n\n\npd2R\n\n pd2R (x)\n\n\nExperiment\nThe time series needs to be at least 90 days long, we we add 45 days before and after the gap\nThere is a problem with conversions of timestamps between R and Python, so convert to string in Python and then back to datetime in R\n\nsource\n\n\n\nadd_buffer\n\n add_buffer (index, inner_index, n)\n\nAdds a buffer of after and after index so length is at least\n\nadd_buffer(hai.index, hai.index[:100], 50)\nadd_buffer(hai.index, hai.index[-50:], 50);\n\n\nsource\n\n\nitem2REddy\n\n item2REddy (item, var, df)\n\nAdd context around item for supporting REddyProc\n\nREddy_df = item2REddy(targ, 'TA', hai)\n\nREddy_df_r = r.toR_timestamp(pd2R(REddy_df))\n\nfilled = R2pd(r.fill_gaps_EProc(REddy_df_r, \"TA\"))\n\nR[write to console]: New sEddyProc class for site 'ID'\n\nR[write to console]: Initialized variable 'TA' with 10 real gaps for gap filling.\n\nR[write to console]: Limited MDS algorithm for gap filling of 'TA.gap_0' with LUT(SW_IN only) and MDC.\n\nR[write to console]: Look up table with window size of 7 days with SW_IN\n\nR[write to console]: 10\n\nR[write to console]: Finished gap filling of 'TA' in 0 seconds. Artificial gaps filled: 4419, real gaps filled: 10, unfilled (long) gaps: 0.\n\n\n\n\nfilled\n\n\n\n\n\n\n\n\nTA_orig\nTA_f\nTA_fqc\nTA_fall\nTA_fall_qc\nTA_fnum\nTA_fsd\nTA_fmeth\nTA_fwin\n\n\n\n\n1\n-0.60\n-0.60\n0.0\n-0.60\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n-0.65\n-0.65\n0.0\n-0.65\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n-0.58\n-0.58\n0.0\n-0.58\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n-0.51\n-0.51\n0.0\n-0.51\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\n-0.49\n-0.49\n0.0\n-0.49\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4415\n3.30\n3.30\n0.0\n3.30\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4416\n3.10\n3.10\n0.0\n3.10\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4417\n3.05\n3.05\n0.0\n3.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4418\n3.05\n3.05\n0.0\n3.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4419\n3.05\n3.05\n0.0\n3.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n4419 rows × 9 columns\n\n\n\n\nREddy_df.set_index(\"TIMESTAMP_END\")\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\nWS\nPA\nP\nSWC\nTS\ngap\n\n\nTIMESTAMP_END\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000-01-01 00:30:00\n-0.60\n0.0\n302.475\n0.222\n2.05\n96.63\n0.0\n43.00\n1.44\n0.0\n\n\n2000-01-01 01:00:00\n-0.65\n0.0\n302.475\n0.122\n2.53\n96.58\n0.0\n43.00\n1.43\n0.0\n\n\n2000-01-01 01:30:00\n-0.58\n0.0\n301.677\n0.090\n3.15\n96.56\n0.0\n43.00\n1.43\n0.0\n\n\n2000-01-01 02:00:00\n-0.51\n0.0\n301.677\n0.110\n3.12\n96.56\n0.0\n43.00\n1.45\n0.0\n\n\n2000-01-01 02:30:00\n-0.49\n0.0\n301.677\n0.102\n3.04\n96.57\n0.0\n43.00\n1.44\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2000-04-01 23:30:00\n3.30\n0.0\n283.856\n1.092\n2.50\n95.06\n0.0\n44.96\n4.32\n0.0\n\n\n2000-04-02 00:00:00\n3.10\n0.0\n283.856\n1.013\n2.09\n95.04\n0.0\n44.96\n4.22\n0.0\n\n\n2000-04-02 00:30:00\n3.05\n0.0\n283.856\n0.964\n2.42\n95.04\n0.0\n44.96\n4.13\n0.0\n\n\n2000-04-02 01:00:00\n3.05\n0.0\n283.856\n0.933\n3.04\n95.04\n0.0\n44.96\n4.04\n0.0\n\n\n2000-04-02 01:30:00\n3.05\n0.0\n251.387\n0.930\n2.96\n95.03\n0.0\n44.95\n3.94\n0.0\n\n\n\n\n4419 rows × 10 columns\n\n\n\n\nfilled = filled.set_index(pd.to_datetime(REddy_df.TIMESTAMP_END))\n\nfilled_item = filled.loc[targ.data.index]\n\npred = targ.data.copy()\npred.loc[~targ.mask[\"TA\"], \"TA\"] = filled_item[\"TA_f\"][~targ.mask[\"TA\"]]\n\n\npred[45:55], targ.data[45:55]\n\n(                           TA  SW_IN    LW_IN    VPD    WS     PA     P  \\\n time                                                                      \n 2000-01-04 02:30:00  1.853612    0.0  304.731  0.952  5.39  96.30  0.00   \n 2000-01-04 03:00:00  1.837083    0.0  304.731  0.971  5.59  96.30  0.00   \n 2000-01-04 03:30:00  1.823985    0.0  304.731  0.958  5.67  96.29  0.00   \n 2000-01-04 04:00:00  1.809805    0.0  304.731  0.938  6.61  96.26  0.00   \n 2000-01-04 04:30:00  1.794282    0.0  315.005  0.785  6.65  96.26  0.00   \n 2000-01-04 05:00:00  1.783447    0.0  315.005  0.529  6.12  96.27  0.00   \n 2000-01-04 05:30:00  1.771525    0.0  315.005  0.450  5.58  96.26  0.29   \n 2000-01-04 06:00:00  1.760362    0.0  315.005  0.499  4.70  96.25  0.00   \n 2000-01-04 06:30:00  1.748410    0.0  315.005  0.382  5.07  96.23  0.00   \n 2000-01-04 07:00:00  1.737933    0.0  315.005  0.313  5.23  96.24  0.00   \n \n                        SWC    TS  \n time                              \n 2000-01-04 02:30:00  43.08  1.31  \n 2000-01-04 03:00:00  43.08  1.31  \n 2000-01-04 03:30:00  43.08  1.31  \n 2000-01-04 04:00:00  43.08  1.31  \n 2000-01-04 04:30:00  43.08  1.31  \n 2000-01-04 05:00:00  43.08  1.30  \n 2000-01-04 05:30:00  43.13  1.30  \n 2000-01-04 06:00:00  43.13  1.29  \n 2000-01-04 06:30:00  43.13  1.29  \n 2000-01-04 07:00:00  43.13  1.29  ,\n                        TA  SW_IN    LW_IN    VPD    WS     PA     P    SWC  \\\n time                                                                         \n 2000-01-04 02:30:00  2.19    0.0  304.731  0.952  5.39  96.30  0.00  43.08   \n 2000-01-04 03:00:00  2.27    0.0  304.731  0.971  5.59  96.30  0.00  43.08   \n 2000-01-04 03:30:00  2.32    0.0  304.731  0.958  5.67  96.29  0.00  43.08   \n 2000-01-04 04:00:00  2.34    0.0  304.731  0.938  6.61  96.26  0.00  43.08   \n 2000-01-04 04:30:00  2.24    0.0  315.005  0.785  6.65  96.26  0.00  43.08   \n 2000-01-04 05:00:00  2.00    0.0  315.005  0.529  6.12  96.27  0.00  43.08   \n 2000-01-04 05:30:00  1.94    0.0  315.005  0.450  5.58  96.26  0.29  43.13   \n 2000-01-04 06:00:00  2.07    0.0  315.005  0.499  4.70  96.25  0.00  43.13   \n 2000-01-04 06:30:00  2.04    0.0  315.005  0.382  5.07  96.23  0.00  43.13   \n 2000-01-04 07:00:00  2.03    0.0  315.005  0.313  5.23  96.24  0.00  43.13   \n \n                        TS  \n time                       \n 2000-01-04 02:30:00  1.31  \n 2000-01-04 03:00:00  1.31  \n 2000-01-04 03:30:00  1.31  \n 2000-01-04 04:00:00  1.31  \n 2000-01-04 04:30:00  1.31  \n 2000-01-04 05:00:00  1.30  \n 2000-01-04 05:30:00  1.30  \n 2000-01-04 06:00:00  1.29  \n 2000-01-04 06:30:00  1.29  \n 2000-01-04 07:00:00  1.29  )\n\n\n\nsource\n\n\ngap_fill_item\n\n gap_fill_item (item, REddy_df, var, filled)\n\n\ngap_fill_item(targ, REddy_df, \"TA\", filled)\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\nWS\nPA\nP\nSWC\nTS\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000-01-03 04:00:00\n0.81\n0.0\n304.148\n0.000\n3.53\n97.00\n0.0\n43.09\n1.37\n\n\n2000-01-03 04:30:00\n0.95\n0.0\n304.382\n0.000\n3.99\n96.98\n0.0\n43.09\n1.37\n\n\n2000-01-03 05:00:00\n1.09\n0.0\n304.382\n0.000\n3.61\n96.96\n0.0\n43.09\n1.37\n\n\n2000-01-03 05:30:00\n1.18\n0.0\n304.382\n0.009\n3.90\n96.93\n0.0\n43.09\n1.37\n\n\n2000-01-03 06:00:00\n1.35\n0.0\n304.382\n0.061\n4.17\n96.91\n0.0\n43.09\n1.38\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n6.53\n95.91\n0.0\n44.12\n1.82\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n5.76\n95.94\n0.0\n44.12\n1.85\n\n\n2000-01-05 04:30:00\n4.11\n0.0\n299.320\n1.746\n5.79\n95.98\n0.0\n44.12\n1.85\n\n\n2000-01-05 05:00:00\n3.77\n0.0\n299.320\n2.065\n6.00\n96.03\n0.0\n44.12\n1.83\n\n\n2000-01-05 05:30:00\n3.36\n0.0\n299.320\n1.911\n4.76\n96.10\n0.0\n44.11\n1.81\n\n\n\n\n100 rows × 9 columns\n\n\n\n\nMDSImputation\n\nsource\n\n\n\nMDSImputation\n\n MDSImputation (var, df)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nmds_imp = MDSImputation('TA', hai)\n\n\nmds_imp(targ)\n\n\n\n\n\n\n\n\nTA\nSW_IN\nLW_IN\nVPD\nWS\nPA\nP\nSWC\nTS\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000-01-03 04:00:00\n0.81\n0.0\n304.148\n0.000\n3.53\n97.00\n0.0\n43.09\n1.37\n\n\n2000-01-03 04:30:00\n0.95\n0.0\n304.382\n0.000\n3.99\n96.98\n0.0\n43.09\n1.37\n\n\n2000-01-03 05:00:00\n1.09\n0.0\n304.382\n0.000\n3.61\n96.96\n0.0\n43.09\n1.37\n\n\n2000-01-03 05:30:00\n1.18\n0.0\n304.382\n0.009\n3.90\n96.93\n0.0\n43.09\n1.37\n\n\n2000-01-03 06:00:00\n1.35\n0.0\n304.382\n0.061\n4.17\n96.91\n0.0\n43.09\n1.38\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2000-01-05 03:30:00\n4.62\n0.0\n330.202\n1.162\n6.53\n95.91\n0.0\n44.12\n1.82\n\n\n2000-01-05 04:00:00\n4.51\n0.0\n330.202\n1.636\n5.76\n95.94\n0.0\n44.12\n1.85\n\n\n2000-01-05 04:30:00\n4.11\n0.0\n299.320\n1.746\n5.79\n95.98\n0.0\n44.12\n1.85\n\n\n2000-01-05 05:00:00\n3.77\n0.0\n299.320\n2.065\n6.00\n96.03\n0.0\n44.12\n1.83\n\n\n2000-01-05 05:30:00\n3.36\n0.0\n299.320\n1.911\n4.76\n96.10\n0.0\n44.11\n1.81\n\n\n\n\n100 rows × 9 columns\n\n\n\n\n\nERA Imputation\n\nsource\n\n\nERAImputation\n\n ERAImputation ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntarg.data.columns\n\nIndex(['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P', 'SWC', 'TS'], dtype='object')\n\n\n\nera_imp = ERAImputation()\n\n\nera_imp(targ)\n\n\n\n\n\n\n\n\nTA\nSW_IN\nVPD\nPA\nP\nWS\nLW_IN\nSWC\nTS\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000-01-03 04:00:00\n1.702\n0.0\n0.693\n97.016\n0.000\n2.948\n304.148\nNaN\nNaN\n\n\n2000-01-03 04:30:00\n1.762\n0.0\n0.691\n97.002\n0.026\n2.989\n304.382\nNaN\nNaN\n\n\n2000-01-03 05:00:00\n1.736\n0.0\n0.697\n96.988\n0.000\n3.015\n304.382\nNaN\nNaN\n\n\n2000-01-03 05:30:00\n1.711\n0.0\n0.703\n96.974\n0.000\n3.041\n304.382\nNaN\nNaN\n\n\n2000-01-03 06:00:00\n1.686\n0.0\n0.709\n96.960\n0.000\n3.066\n304.382\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2000-01-05 03:30:00\n4.945\n0.0\n0.774\n95.910\n0.252\n6.107\n330.202\nNaN\nNaN\n\n\n2000-01-05 04:00:00\n4.789\n0.0\n0.758\n95.926\n0.252\n6.189\n330.202\nNaN\nNaN\n\n\n2000-01-05 04:30:00\n4.632\n0.0\n0.741\n95.942\n0.044\n6.270\n299.320\nNaN\nNaN\n\n\n2000-01-05 05:00:00\n4.251\n0.0\n0.766\n95.988\n0.000\n5.923\n299.320\nNaN\nNaN\n\n\n2000-01-05 05:30:00\n3.869\n0.0\n0.790\n96.035\n0.000\n5.576\n299.320\nNaN\nNaN\n\n\n\n\n100 rows × 9 columns\n\n\n\n\\(\\sigma\\)"
  },
  {
    "objectID": "results.html#metrics",
    "href": "results.html#metrics",
    "title": "Results",
    "section": "Metrics",
    "text": "Metrics\n\nsource\n\nMaskedMetric\n\n MaskedMetric (metric)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nrmse\n\n rmse (y_true, y_pred)\n\n\nsource\n\n\nNormalizedMetric\n\n NormalizedMetric (metric:__main__.MaskedMetric, mean, std)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nnormalize\n\n normalize (x, mean, std)\n\n\nrmse_mask(targ, pred)\n\narray([0.3698457])\n\n\n\n\nValidation Imputations\nchecks that the values of the imputation makes sense\nthis is the average error for ERA\n\nnp.sqrt((hai['TA'] - hai_era.loc[hai.index]['TA_ERA']).pow(2).mean())\n\n1.8708585984196093\n\n\n\nrmse_mask(targ, ERAImputation()(targ))\n\narray([0.29919709])\n\n\n\nerr_era = []\nfor _ in range(100):\n    items = random.choices(dls.items, k = 1)\n    targ = orig_target(dls, items)[0]\n    err_era.append(rmse_mask(targ, ERAImputation()(targ)))\n\n\nnp.array(err_era).mean()\n\n1.3839633283333717"
  },
  {
    "objectID": "results.html#comparison",
    "href": "results.html#comparison",
    "title": "Results",
    "section": "Comparison",
    "text": "Comparison\nTake a variable to be filled, makes an artificial gap of given len, tries to fill with 3 methods and return metrics for each of them, repeat n_rep times\n\nprep visualization\n\nsource\n\n\ngaplen2hours\n\n gaplen2hours (x)\n\n\nsource\n\n\nprep_df\n\n prep_df (df)\n\n\n\n\n\nDetails\n\n\n\n\ndf\nwhat a bad name\n\n\n\n\n\nAggregation\n\nhai.std(axis=0)['TA']\n\n7.924610764367695\n\n\n\nget_stats(hai)\n\n(tensor([8.3339e+00, 1.2096e+02, 3.1150e+02, 3.3807e+00, 3.1800e+00, 9.5962e+01,\n         4.3427e-02, 3.4843e+01, 7.9349e+00], dtype=torch.float64),\n tensor([  7.9246, 204.0026,  41.9557,   4.3684,   1.6254,   0.8552,   0.2803,\n           8.9131,   5.6586], dtype=torch.float64))\n\n\n\\[ \\text{RMSE}_\\text{stand} = \\frac{1}{\\sigma}\\text{RMSE}\\]\n\nsource\n\n\nRMSEAgg\n\n RMSEAgg (df)\n\nAggregate to rmse and normalized rmse\n\nsource\n\n\ntimeseriesAgg\n\n timeseriesAgg (targ, pred, *args)\n\n\n\nImp Methods\n\nsource\n\n\nImpComparison\n\n ImpComparison (models:pandas.core.frame.DataFrame, df, control,\n                block_len, rmse=True, time_series=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nbase_path = here(\"analysis/results/trained_14feb\")\n\n\nsource\n\n\nl_model\n\n l_model (x)\n\n\nmodels_var = [\n    {'var': 'TA',    'model': l_model(\"TA_specialized_gap_12-336_v2.pickle.pickle\")},\n    {'var': 'SW_IN', 'model': l_model(\"SW_IN_specialized_gap_12-336_v2.pickle.pickle\")},\n    {'var': 'LW_IN', 'model': l_model(\"LW_IN_specialized_gap_12-336_v1.pickle\")},\n    {'var': 'VPD',   'model': l_model(\"VPD_specialized_gap_12-336_v1.pickle\")},\n    {'var': 'WS',    'model': l_model(\"VPD_specialized_gap_12-336_v1.pickle\")},\n    {'var': 'PA',    'model': l_model(\"VPD_specialized_gap_12-336_v3.pickle.pickle\")},\n    {'var': 'P',     'model': l_model(\"P_specialized_gap_12-336_v1.pickle\")},\n    {'var': 'TS',    'model': l_model(\"TS_specialized_gap_12-336_v2.pickle.pickle\")},\n    {'var': 'SWC',   'model': l_model(\"SWC_specialized_gap_12-336_v2.pickle.pickle\")},\n]\nmodels_var = pd.DataFrame.from_records(models_var)\n\n\ncomp = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 100)\n\n\ndata_results = comp.compare(gap_len = [5,10,20, 45], var=[\"TA\", \"SW_IN\"], n_rep=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata_results.head()\n\n\n\n\n\n\n\n\nmethod\nvar\ngap_len\nidx_rep\nrmse\nrmse_stand\n\n\n\n\n0\nKalmanFilter\nTA\n2.5\n0\n0.458599\n0.057870\n\n\n3\nKalmanFilter\nTA\n2.5\n1\n0.156532\n0.019753\n\n\n6\nKalmanFilter\nTA\n2.5\n2\n0.174399\n0.022007\n\n\n9\nKalmanFilter\nTA\n2.5\n3\n0.361323\n0.045595\n\n\n12\nKalmanFilter\nTA\n2.5\n4\n0.655402\n0.082705\n\n\n\n\n\n\n\n\ndata_results.columns\n\nIndex(['method', 'var', 'gap_len', 'idx_rep', 'rmse', 'rmse_stand'], dtype='object')\n\n\n\n\nKalman Filters\n\nsource\n\n\nKalmanImpComparison\n\n KalmanImpComparison (models:pandas.core.frame.DataFrame,\n                      df:pandas.core.frame.DataFrame,\n                      control:pandas.core.frame.DataFrame, block_len:int,\n                      rmse:bool=True, time_series:bool=False)\n\nCompare different Kalman filters\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodels\nDataFrame\n\nmodel, gap_single_var,\n\n\ndf\nDataFrame\n\n\n\n\ncontrol\nDataFrame\n\n\n\n\nblock_len\nint\n\n\n\n\nrmse\nbool\nTrue\n\n\n\ntime_series\nbool\nFalse\n\n\n\n\n\nmodels = pd.DataFrame(columns = [\"use_control\", \"gap_single_var\"])\n\n\nbase_path = here(\"analysis/results/trained_13feb\")\n\n\nsource\n\n\nl_model\n\n l_model (x)\n\n\nmodels = [\n    {'use_control': True,  'gap_single_var': True,  'model': l_model(\"1_gap_varying_12-336_v1.pickle\")},\n    {'use_control': True,  'gap_single_var': False, 'model': l_model(\"All_gap_all_30_v1.pickle\")},\n    {'use_control': False, 'gap_single_var': False, 'model': l_model(\"1_gap_varying_336_no_control_v1.pickle\")},\n    {'use_control': False, 'gap_single_var': True,  'model': l_model(\"1_gap_varying_336_no_control_v1.pickle\")},\n]\nmodels = pd.DataFrame.from_records(models)\n\n\nkcomp = KalmanImpComparison(models, hai, hai_era, 100)\n\n\nmodels\n\n\n\n\n\n\n\n\nuse_control\ngap_single_var\nmodel\n\n\n\n\n0\nTrue\nTrue\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n1\nTrue\nFalse\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n2\nFalse\nFalse\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n3\nFalse\nTrue\nKalman Filter\\n N dim obs: 9,\\n N dim state: 18,\\n N dim contr: 14\n\n\n\n\n\n\n\n\nk_results = kcomp.compare(n_rep =3, gap_len = [6, 12, 24], var = list(hai.columns))\n\n\n\n\n\nk_results\n\n\n\n\n\n\n\n\nvar\nloss\nlikelihood\ngap_len\ngap_idx\nuse_control\ngap_single_var\nrmse\nrmse_stand\n\n\n\n\n0\nTA\n-5.280393\n2.671731\n3.0\n0\nTrue\nTrue\n0.131880\n0.016642\n\n\n1\nTA\n-4.566644\n2.487593\n3.0\n1\nTrue\nTrue\n0.663002\n0.083664\n\n\n2\nTA\n-5.032340\n2.686779\n3.0\n2\nTrue\nTrue\n0.390265\n0.049247\n\n\n3\nTA\n-6.865580\n4.571592\n3.0\n0\nTrue\nFalse\n0.599223\n0.075615\n\n\n4\nTA\n-8.085504\n4.509787\n3.0\n1\nTrue\nFalse\n0.146722\n0.018515\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7\nTS\n34.731586\n0.887256\n12.0\n1\nFalse\nFalse\n2.722457\n0.481115\n\n\n8\nTS\n66.913388\n0.769370\n12.0\n2\nFalse\nFalse\n20.535604\n3.629069\n\n\n9\nTS\n7.724566\n1.516849\n12.0\n0\nFalse\nTrue\n1.946988\n0.344073\n\n\n10\nTS\n-0.383060\n1.612699\n12.0\n1\nFalse\nTrue\n0.809863\n0.143120\n\n\n11\nTS\n11.871315\n1.390741\n12.0\n2\nFalse\nTrue\n2.392871\n0.422870\n\n\n\n\n324 rows × 9 columns"
  },
  {
    "objectID": "results.html#plotting",
    "href": "results.html#plotting",
    "title": "Results",
    "section": "Plotting",
    "text": "Plotting\n\nsource\n\nfacet_wrap\n\n facet_wrap (data:pandas.core.frame.DataFrame, plot_fn, col:str,\n             y_labels:list[str]|None=None, n_cols=3,\n             y_resolve='independent')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nfull dataset\n\n\nplot_fn\n\n\nfunction that makes the plot, takes 2 arguments: data and y_label\n\n\ncol\nstr\n\ncolumn to facet\n\n\ny_labels\nlist[str] | None\nNone\ncustom labels y axis\n\n\nn_cols\nint\n3\n\n\n\ny_resolve\nstr\nindependent\n\n\n\n\n\nsource\n\n\nfacet_grid\n\n facet_grid (data:pandas.core.frame.DataFrame, plot_fn, col:str, row:str,\n             y_labels:list[str]|None=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nfull dataset\n\n\nplot_fn\n\n\nfunction that makes the plot, takes 2 arguments: data and y_label\n\n\ncol\nstr\n\ncolumn to facet,\n\n\nrow\nstr\n\n\n\n\ny_labels\nlist[str] | None\nNone\ncustom labels y axis\n\n\n\n\nfrom itertools import product\n\n\ntest_data = pd.DataFrame(list(product(['0','1'], ['a', 'b'])), columns = ['row', 'col'])\ntest_data['text'] = test_data.row + test_data.col\n\n\ndef test_plot(data, *args): return alt.Chart(data).mark_text().encode(text='text')\n\n\nfacet_wrap(test_data, test_plot, col = 'row')\n\n\n\n\n\n\n\nfacet_grid(test_data, test_plot, col = 'col', row='row')\n\n\n\n\n\n\n\n\nThe Plot\n\ndef remove_outliers(data, var, groupby):\n    \"\"\"Simple but maybe computationally inefficent to remove outliers from boxplot \"\"\"\n\n\ndef custom_box_plot(data, x, y, color, xoffset):\n    \"\"\"Boxplot for altair without outliers\"\"\"\n    bar = alt.Chart(data).mark_bar().encode(\n            x=x,\n            y='q1:Q',\n            y2='q2:Q',\n            color=color,\n            xoffset=xoffset\n        ).transform_aggregate(\n            q1=f'q1({y})',\n            q3=f'q1({y})',\n            groupby=[color, xoffset]\n    )\n    \n    return bar\n\n\n# custom_box_plot(data_results, 'gap_len', 'rmse', 'method', 'method')\n\n\nsource\n\n\nthe_plot\n\n the_plot (data)\n\n\nthe_plot(data_results)\n\n\n\n\n\n\n\nsource\n\n\nthe_plot_stand\n\n the_plot_stand (data)\n\n\nsource\n\n\nthe_plot_stand2\n\n the_plot_stand2 (data)\n\n\ndict(zip(scale_meteo.domain, scale_meteo.range))\n\n{'TA': '#1B9E77',\n 'SW_IN': '#D95F02',\n 'LW_IN': '#7570B3',\n 'VPD': '#E7298A',\n 'WS': '#66A61E',\n 'PA': '#E6AB02',\n 'SWC': '#A6761D',\n 'TS': '#666666'}\n\n\n\nthe_plot_stand(data_results)\n\n\n\n\n\n\n\nthe_plot_stand2(data_results)\n\n\n\n\n\n\n\nalt.Color('color')\n\nColor({\n  shorthand: 'color'\n})\n\n\n\nx = alt.X('gap_len'); color = alt.Color('method'); xOffset = alt.XOffset('method'); y = alt.Y('rmse_stand')\n\n\ndata_results.groupby([x.shorthand, color.shorthand, xOffset.shorthand]).agg({y.shorthand: ['median', lambda x: x.quantile(.25), lambda x: x.quantile(.75)]}).columns\n\nMultiIndex([('rmse_stand',     'median'),\n            ('rmse_stand', '&lt;lambda_0&gt;'),\n            ('rmse_stand', '&lt;lambda_1&gt;')],\n           )\n\n\n\ndata_results\n\n\n\n\n\n\n\n\nmethod\nvar\ngap_len\nidx_rep\nrmse\nrmse_stand\n\n\n\n\n0\nKalmanFilter\nTA\n2.5\n0\n0.458599\n0.057870\n\n\n3\nKalmanFilter\nTA\n2.5\n1\n0.156532\n0.019753\n\n\n6\nKalmanFilter\nTA\n2.5\n2\n0.174399\n0.022007\n\n\n9\nKalmanFilter\nTA\n2.5\n3\n0.361323\n0.045595\n\n\n12\nKalmanFilter\nTA\n2.5\n4\n0.655402\n0.082705\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2\nMDS\nSW_IN\n22.5\n0\n187.993906\n0.921527\n\n\n5\nMDS\nSW_IN\n22.5\n1\n106.935300\n0.524186\n\n\n8\nMDS\nSW_IN\n22.5\n2\n29.948688\n0.146805\n\n\n11\nMDS\nSW_IN\n22.5\n3\n170.957608\n0.838017\n\n\n14\nMDS\nSW_IN\n22.5\n4\n73.082151\n0.358241\n\n\n\n\n120 rows × 6 columns\n\n\n\n\ndata_results.groupby(['gap_len', 'method', 'var']).agg(\n        median = pd.NamedAgg(y.shorthand, 'median'),\n        q1 = pd.NamedAgg(y.shorthand,lambda x: x.quantile(.25)),\n        q3 = pd.NamedAgg(y.shorthand,lambda x: x.quantile(.75))).reset_index()\n\n\n\n\n\n\n\n\ngap_len\nmethod\nvar\nmedian\nq1\nq3\n\n\n\n\n0\n2.5\nKalmanFilter\nTA\n0.045595\n0.022007\n0.057870\n\n\n1\n2.5\nKalmanFilter\nSW_IN\n0.128317\n0.050479\n0.241264\n\n\n2\n2.5\nKalmanFilter\nLW_IN\nNaN\nNaN\nNaN\n\n\n3\n2.5\nKalmanFilter\nVPD\nNaN\nNaN\nNaN\n\n\n4\n2.5\nKalmanFilter\nWS\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n103\n22.5\nMDS\nWS\nNaN\nNaN\nNaN\n\n\n104\n22.5\nMDS\nPA\nNaN\nNaN\nNaN\n\n\n105\n22.5\nMDS\nP\nNaN\nNaN\nNaN\n\n\n106\n22.5\nMDS\nSWC\nNaN\nNaN\nNaN\n\n\n107\n22.5\nMDS\nTS\nNaN\nNaN\nNaN\n\n\n\n\n108 rows × 6 columns\n\n\n\n\nsource\n\n\ncustom_boxplot_nooutlier\n\n custom_boxplot_nooutlier (data:pandas.core.frame.DataFrame,\n                           x:altair.vegalite.v5.schema.channels.X,\n                           y:altair.vegalite.v5.schema.channels.Y,\n                           color:altair.vegalite.v5.schema.channels.Color,\n                           xOffset:altair.vegalite.v5.schema.channels.XOff\n                           set)\n\n\nsource\n\n\nthe_plot_stand3\n\n the_plot_stand3 (data)\n\n\nthe_plot_stand3(data_results)\n\n\n\n\n\n\nviolin plots don’t work\n\ndef _the_plot_violin(data, y_label):\n        return alt.Chart(data).mark_area(orient='horizontal').encode(\n        x = alt.X('density:Q', title='gap_len [h]', axis=alt.Axis(labelAngle=0)),\n        y = alt.Y('rmse', title=y_label, scale=alt.Scale(domainMin=-.2 *data['rmse'].mean())),\n        color=alt.Color('method:N', scale=alt.Scale(domain=[\"KalmanFilter\", \"ERA\", \"MDS\"], scheme='dark2')),\n        xOffset=alt.XOffset('method', scale=alt.Scale(domain=[\"KalmanFilter\", \"ERA\", \"MDS\"])),\n        column=alt.Column('gap_len')\n    ).transform_density(\n    'rmse',\n    as_=['rmse', 'density'],\n    groupby=['method', 'gap_len']\n).properties(width=250, height=200)\n\n\nfacet_wrap(data_results, _the_plot_violin, \"var\", y_labels = [f\"RMSE {var} [{units_big[var]}]\" for var in data_results['var'].unique()])\n\n\n\n\n\n\n\n\nGap Length\n\n# 18,20,25,30,40,50,60,70\ngap_len = ImpComparison(models_var, hai, hai_era, block_len=100).compare(gap_len = range(2,30,4), var=['TA', 'SW_IN', 'VPD'], n_rep=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nagg_gap_len\n\n agg_gap_len (data)\n\n\ngap_len_agg = agg_gap_len(gap_len)\n# ).with_column(\n#     (pl.col(\"Q3\") - pl.col(\"Q1\")).alias(\"IRQ\")\n# ).with_columns([\n#     (pl.col(\"Q1\") - 1.5 * pl.col(\"IRQ\")).alias(\"min\"),\n#     (pl.col(\"Q3\") + 1.5 * pl.col(\"IRQ\")).alias(\"max\"),\n# ])\n\n\nrmse_df = _get_era_rmse(hai, hai_era)\n\n\ngap_len_agg = pd.merge(gap_len_agg, rmse_df, on='var')\n\n\n_plot_gap_len(gap_len_agg[gap_len_agg['var'] == 'TA'][gap_len_agg['method'] == 'ERA'], \"label\")\n\n/tmp/ipykernel_101448/837074991.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  _plot_gap_len(gap_len_agg[gap_len_agg['var'] == 'TA'][gap_len_agg['method'] == 'ERA'], \"label\")\n\n\n\n\n\n\n\n\nfacet_wrap(gap_len_agg[gap_len_agg.method == 'KalmanFilter'], _plot_gap_len, 'var')\n\n\n\n\n\n\n\nsource\n\n\nplot_gap_len\n\n plot_gap_len (data, df, control)\n\n\nplot_gap_len(gap_len, hai, hai_era)\n\nValueError: median encoding field is specified without a type; the type cannot be inferred because it does not match any column in the data.\n\n\nalt.VConcatChart(...)\n\n\nwe actually don’t care of different methods\n\n_plot_gap_len(gap_len_agg, y_label=\"\").facet(column='method', row='var').resolve_scale(y='independent')\n\n\n\n\n\n\n\n\nControl\n\nsource\n\n\nplot_compare\n\n plot_compare (data, compare, scale_domain=None)\n\n\nsource\n\n\nplot_control\n\n plot_control (data)\n\n\nplot_control(k_results)\n\nSchemaValidationError: Invalid specification\n\n        altair.vegalite.v5.schema.core.Scale-&gt;0, validating 'type'\n\n        True is not of type 'null'\n        True is not of type 'string'\n        True is not of type 'number'\n        True is not of type 'boolean'\n        True is not of type 'object'\n        True is not of type 'object'\n        \n\n\n\nsource\n\n\nplot_control_loss\n\n plot_control_loss (data)\n\n\nplot_control_loss(k_results)\n\n\nsource\n\n\nplot_control_lh\n\n plot_control_lh (data)\n\n\nplot_control_lh(k_results)\n\n\n\nGap Multiple variables\n\nsource\n\n\nplot_other_var\n\n plot_other_var (data)\n\n\nplot_other_var(k_results)\n\n\n\nSingle var vs Generic\n\nbase_path = here(\"analysis/results/trained_14feb\")\n\n\nsource\n\n\nl_model\n\n l_model (x)\n\n\nmodels_generic = [\n    {'type': 'Generic',  'var': 'TA', 'model': l_model(\"1_gap_varying_12-336_v1.pickle\")},\n    {'type': 'Generic',  'var': 'SW_IN', 'model': l_model(\"1_gap_varying_12-336_v1.pickle\")},\n    {'type': 'Only one var', 'var': 'TA',    'model': l_model(\"TA_specialized_gap_12-336_v2.pickle.pickle\")},\n    {'type': 'Only one var', 'var': 'SW_IN', 'model': l_model(\"SW_IN_specialized_gap_12-336_v2.pickle.pickle\")},\n]\nmodels_generic = pd.DataFrame.from_records(models_generic)\n\n\n@dataclass\nclass Col:\n    col_name: str\n\n\n# dream_models_TA = [\n#     Col('type'),  Col('model'),\n#     'only TA',  l_model(\"trained_4_feb_All_gap_varying_12_v2.pickle\"),\n#     'generic',  l_model(\"trained_4_feb_TA_gap_12_v1.pickle\"),\n# ]\n\n\n# tribble ... not for now\n\n\ncomp_generic = KalmanImpComparison(models_generic, hai, hai_era, 100)\n\n\nk_results_generic = comp_generic.compare(n_rep =3, gap_len = [6, 12, 24], var = ['TA', 'SW_IN'])\n\n\nk_results_generic\n\n\nsource\n\n\nplot_generic\n\n plot_generic (data)\n\n\n_plot_compare(k_results_generic, compare='type')\n\n\nplot_generic(k_results_generic)\n\n\n\nShort gaps vs Generic\n\n# models_short_gap = [\n#     {'type': 'Short gap',  'var': 'TA', 'model': l_model(\"trained_4_feb_All_gap_varying_12_v2.pickle\")},\n#     {'type': 'Long gap',  'var': 'TA', 'model': l_model(\"trained_4_feb_TA_gap_12_v1.pickle\")},\n# ]\n# models_short_gap = pd.DataFrame.from_records(models_short_gap)\n\n\n\nTimeseries\n\ncomp_ts = ImpComparison(models = models_var, df = hai, control = hai_era, block_len = 100, rmse=False, time_series = True)\n\n\nres_ts = comp_ts.compare(gap_len = [5,10,20], var=[\"TA\", 'SW_IN'], n_rep=5)\n\n\nres_ts.columns\n\n\ntest_ts = res_ts.query('method == \"KalmanFilter\" and var == \"TA\"')\n\n\nrow_res = test_ts.iloc[0]\n\n\npred_all = row_res.pred[0]\ntarg =  row_res.targ[0]\nvar = row_res['var']\n\n\ntype(pred_all)\n\n\ntarg.mask[var]\n\n\nlist(row_res.index)\n\n\nnp.argmin(row_res.targ[0].mask['TA'])\n\n\nrow_res.targ[0].mask['TA'].index[47]\n\n\nsource\n\n\nunnest_predictions\n\n unnest_predictions (row_res:pandas.core.series.Series, ctx_len:int=50)\n\n\nunnest_predictions(row_res)\n\n\nres_ts_plot = pd.concat([unnest_predictions(row) for _,row in res_ts.iterrows()])\n\n\nplot_points(res_ts_plot.query('var == \"TA\" and method == \"KalmanFilter\" and idx_rep == 0 and gap_len==2.5'), y= \"measurement\")\n\n\nplot_missing_area(res_ts_plot.query('var == \"TA\" and method == \"KalmanFilter\" and idx_rep == 0 and gap_len==2.5'))\n\n\nplot_line(res_ts_plot.query('var == \"TA\" and idx_rep == 0 and gap_len==2.5'), y= \"mean\", color='method', scale=alt.Scale())\n\n\nplot_error(res_ts_plot.query('var == \"TA\" and idx_rep == 0 and gap_len==2.5').copy(), y= \"mean\", color='method', scale=alt.Scale())\n\n\n_plot_timeseries(res_ts_plot.query('var == \"TA\" and idx_rep == 0 and gap_len==2.5').copy(), \"TA\")\n\n\n_plot_timeseries(res_ts_plot.query('var == \"SW_IN\" and idx_rep == 0 and gap_len==10').copy(), \"SW_IN\")\n\n\ndata_ts = pd.concat([unnest_predictions(row) for _,row in res_ts.iterrows()])\n\n\nsource\n\n\nplot_timeseries\n\n plot_timeseries (data, idx_rep:int|None=None, gap_len:int|None=None,\n                  max_idx:int=3, ctx_len={6.0: 50, 12.0: 50, 168.0: 384},\n                  scale_color=Scale({   domain: ['KalmanFilter', 'ERA',\n                  'MDS'],   scheme: 'dark2' }), compare='method')\n\n\nplot_timeseries(res_ts, idx_rep='random')\n\n\n\nScatter plot\n\ndata_ts.query('var == \"TA\" and gap_len==2.5').copy()\n\n\n_plot_scatter(data_ts.query('var == \"TA\" and gap_len==10 and idx_rep ==0').copy(), x=\"measurement\", y= \"mean\", color='method', scale=alt.Scale())"
  },
  {
    "objectID": "results.html#table",
    "href": "results.html#table",
    "title": "Results",
    "section": "Table",
    "text": "Table\n\nThe Table\nA table where in the rows there is\n\nt = data_results.groupby(['method', 'var', 'gap_len']).agg({'rmse': ['mean', 'std']}).unstack(level=0)\n\nt_idx = t.columns.droplevel()\nt_idx.names = ['RMSE', None]\n\nt2 = t.copy()\nt2.columns = t_idx\n\nt2 = t2.sort_index(axis=1, level=1).swaplevel(axis=1)\nt2\n\nNameError: name 'data_results' is not defined\n\n\n\nrow = t2.iloc[0]\n\nNameError: name 't2' is not defined\n\n\n\nrow\n\nNameError: name 'row' is not defined\n\n\n\nnp.argmin(row.iloc[[0,2,4]]) * 2\n\nNameError: name 'row' is not defined\n\n\n\nsource\n\n\nstyle_the_table\n\n style_the_table (style, cols=[0, 2, 4])\n\n\nsource\n\n\nhighlight_min_method\n\n highlight_min_method (row, props, cols)\n\n\n# #| export\n# renames_table_latex = {name: f\"\\parbox{{2.1cm}}{{{val}}}\" for name, val in \n#                  {'SW_IN': \"Shortwave radiation incoming \\\\textbf{SW IN} [\\si{W/m^2}]\",\n#                'LW_IN': 'Longwave radiation incoming \\\\textbf{LW IN} [$W/m^2$]',\n#                'TA': \"Air Temperature \\\\textbf{TA} [$°C$]\",\n#                'VPD': \"Vapuour Pressure Deficit \\\\textbf{VPD} [$hPa$]\",\n#                'PA': \"Air Pressure \\\\textbf{PA} [$hPa$]\",\n#                'P': \"Precipitation \\\\textbf{P} [$mm$]\",\n#                'WS': \"Wind Speed \\\\textbf{WS} [$m/s$]\",\n#           }.items()}\n\n\nt2.style.pipe(style_the_table)\n\n\nt3 = t2.rename(index = renames_table_latex).style.apply(highlight_min_method, props=\"font-weight: bold\", axis=1).format_index(precision=0).format(precision=3)\n\n\nt3\n\n\nprint(t3.to_latex(convert_css=True, hrules=True, clines=\"skip-last;data\", column_format=\"p{2.1cm}c|rr|rr|rr\", caption=\"caption\", label=\"table\"))\n\n\nsource\n\n\nthe_table\n\n the_table (data, y='rmse', y_name='RMSE')\n\n\nthe_table(data_results)\n\n\nsource\n\n\nthe_table_latex\n\n the_table_latex (table, file, caption='', label='', stand=False)\n\n\nthe_table_latex(the_table(data_results), \"test_table.tex\")\n\n\n\nTable compare\n\nsource\n\n\ntable_compare\n\n table_compare (data, compare:str, y='rmse_stand', compare_ascending=True)\n\n\ntable_compare(k_results, 'use_control')\n\n\n\n\n\n\n\n\nuse_control\nFalse\nTrue\n\n\n\n\nRMSE Standardized\nmean\nstd\nmean\nstd\ndiff.\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\nTA\n3.0\n0.071531\n0.080346\n0.042377\n0.031936\n0.029154\n\n\n6.0\n0.254662\n0.154330\n0.079730\n0.044252\n0.174932\n\n\n12.0\n0.518039\n0.435547\n0.127106\n0.091420\n0.390933\n\n\nSW_IN\n3.0\n0.126355\n0.103091\n0.302880\n0.222587\n0.176525\n\n\n6.0\n0.531083\n0.468922\n0.259730\n0.144197\n0.271353\n\n\n12.0\n0.651510\n0.359853\n0.309855\n0.252639\n0.341655\n\n\nLW_IN\n3.0\n0.117033\n0.081575\n0.218013\n0.157215\n0.100980\n\n\n6.0\n0.260864\n0.127380\n0.210059\n0.164418\n0.050804\n\n\n12.0\n0.648803\n0.225924\n0.531998\n0.237676\n0.116805\n\n\nVPD\n3.0\n0.080883\n0.040920\n0.061606\n0.016222\n0.019277\n\n\n6.0\n0.206404\n0.118284\n0.079571\n0.051189\n0.126833\n\n\n12.0\n0.386051\n0.262311\n0.250002\n0.192650\n0.136050\n\n\nWS\n3.0\n0.300485\n0.158529\n0.502712\n0.560973\n0.202226\n\n\n6.0\n0.533166\n0.333133\n0.456949\n0.250417\n0.076217\n\n\n12.0\n0.576789\n0.276542\n0.572316\n0.360507\n0.004473\n\n\nPA\n3.0\n0.053446\n0.045544\n0.034332\n0.014184\n0.019114\n\n\n6.0\n0.154374\n0.070122\n0.104465\n0.080144\n0.049909\n\n\n12.0\n0.388000\n0.335828\n0.080165\n0.059638\n0.307834\n\n\nP\n3.0\n0.057974\n0.046056\n0.471970\n0.828965\n0.413996\n\n\n6.0\n0.453741\n0.447474\n0.252101\n0.107018\n0.201640\n\n\n12.0\n0.452073\n0.504036\n0.436604\n0.381016\n0.015469\n\n\nSWC\n3.0\n0.057809\n0.039351\n0.037269\n0.029069\n0.020541\n\n\n6.0\n0.260036\n0.172584\n0.039388\n0.030187\n0.220648\n\n\n12.0\n0.383962\n0.195432\n0.270892\n0.302199\n0.113070\n\n\nTS\n3.0\n0.120985\n0.071894\n0.059529\n0.015747\n0.061456\n\n\n6.0\n0.422910\n0.397447\n0.213845\n0.156284\n0.209065\n\n\n12.0\n0.887094\n1.348264\n0.246851\n0.171585\n0.640243\n\n\n\n\n\n\n\n\nd\n\n\n\n\n\n\n\n\nRMSE Standardized\nmean\nstd\nmean\nstd\ndiff.\n\n\n\nuse_control\nFalse\nFalse\nTrue\nTrue\n\n\n\nVariable\nGap [$h$]\n\n\n\n\n\n\n\n\n\nTA\n3.0\n0.071531\n0.080346\n0.042377\n0.031936\n0.029154\n\n\n6.0\n0.254662\n0.154330\n0.079730\n0.044252\n0.174932\n\n\n12.0\n0.518039\n0.435547\n0.127106\n0.091420\n0.390933\n\n\nSW_IN\n3.0\n0.126355\n0.103091\n0.302880\n0.222587\n0.176525\n\n\n6.0\n0.531083\n0.468922\n0.259730\n0.144197\n0.271353\n\n\n12.0\n0.651510\n0.359853\n0.309855\n0.252639\n0.341655\n\n\nLW_IN\n3.0\n0.117033\n0.081575\n0.218013\n0.157215\n0.100980\n\n\n6.0\n0.260864\n0.127380\n0.210059\n0.164418\n0.050804\n\n\n12.0\n0.648803\n0.225924\n0.531998\n0.237676\n0.116805\n\n\nVPD\n3.0\n0.080883\n0.040920\n0.061606\n0.016222\n0.019277\n\n\n6.0\n0.206404\n0.118284\n0.079571\n0.051189\n0.126833\n\n\n12.0\n0.386051\n0.262311\n0.250002\n0.192650\n0.136050\n\n\nWS\n3.0\n0.300485\n0.158529\n0.502712\n0.560973\n0.202226\n\n\n6.0\n0.533166\n0.333133\n0.456949\n0.250417\n0.076217\n\n\n12.0\n0.576789\n0.276542\n0.572316\n0.360507\n0.004473\n\n\nPA\n3.0\n0.053446\n0.045544\n0.034332\n0.014184\n0.019114\n\n\n6.0\n0.154374\n0.070122\n0.104465\n0.080144\n0.049909\n\n\n12.0\n0.388000\n0.335828\n0.080165\n0.059638\n0.307834\n\n\nP\n3.0\n0.057974\n0.046056\n0.471970\n0.828965\n0.413996\n\n\n6.0\n0.453741\n0.447474\n0.252101\n0.107018\n0.201640\n\n\n12.0\n0.452073\n0.504036\n0.436604\n0.381016\n0.015469\n\n\nSWC\n3.0\n0.057809\n0.039351\n0.037269\n0.029069\n0.020541\n\n\n6.0\n0.260036\n0.172584\n0.039388\n0.030187\n0.220648\n\n\n12.0\n0.383962\n0.195432\n0.270892\n0.302199\n0.113070\n\n\nTS\n3.0\n0.120985\n0.071894\n0.059529\n0.015747\n0.061456\n\n\n6.0\n0.422910\n0.397447\n0.213845\n0.156284\n0.209065\n\n\n12.0\n0.887094\n1.348264\n0.246851\n0.171585\n0.640243\n\n\n\n\n\n\n\n\nd.columns\n\nMultiIndex([('mean', False),\n            ( 'std', False),\n            ('mean',  True),\n            ( 'std',  True),\n            (    '',    '')],\n           names=['RMSE Standardized', 'use_control'])\n\n\n\nd.columns\n\nMultiIndex([('rmse_stand', 'mean', False),\n            ('rmse_stand', 'mean',  True),\n            ('rmse_stand',  'std', False),\n            ('rmse_stand',  'std',  True)],\n           names=[None, None, 'use_control'])\n\n\n\nd.iloc[:, 0] -\n\n\n\n\n\n\n\n\n\nrmse_stand\n\n\n\n\nmean\n\n\n\nuse_control\nFalse\nTrue\n\n\nvar\ngap_len\n\n\n\n\n\n\nTA\n3.0\n0.071531\n0.042377\n\n\n6.0\n0.254662\n0.079730\n\n\n12.0\n0.518039\n0.127106\n\n\nSW_IN\n3.0\n0.126355\n0.302880\n\n\n6.0\n0.531083\n0.259730\n\n\n12.0\n0.651510\n0.309855\n\n\nLW_IN\n3.0\n0.117033\n0.218013\n\n\n6.0\n0.260864\n0.210059\n\n\n12.0\n0.648803\n0.531998\n\n\nVPD\n3.0\n0.080883\n0.061606\n\n\n6.0\n0.206404\n0.079571\n\n\n12.0\n0.386051\n0.250002\n\n\nWS\n3.0\n0.300485\n0.502712\n\n\n6.0\n0.533166\n0.456949\n\n\n12.0\n0.576789\n0.572316\n\n\nPA\n3.0\n0.053446\n0.034332\n\n\n6.0\n0.154374\n0.104465\n\n\n12.0\n0.388000\n0.080165\n\n\nP\n3.0\n0.057974\n0.471970\n\n\n6.0\n0.453741\n0.252101\n\n\n12.0\n0.452073\n0.436604\n\n\nSWC\n3.0\n0.057809\n0.037269\n\n\n6.0\n0.260036\n0.039388\n\n\n12.0\n0.383962\n0.270892\n\n\nTS\n3.0\n0.120985\n0.059529\n\n\n6.0\n0.422910\n0.213845\n\n\n12.0\n0.887094\n0.246851\n\n\n\n\n\n\n\n\nsource\n\n\ntable_compare_latex\n\n table_compare_latex (table, file, caption='', label='')\n\n\n\nTable gap single\n\nsource\n\n\ntable_compare3\n\n table_compare3 (data, compare:str, y='rmse_stand',\n                 compare_ascending=True)\n\n\nsource\n\n\ntable_compare3_latex\n\n table_compare3_latex (table, file, caption='', label='')\n\n\n\nGap len table\n\ng = gap_len.groupby(['var', 'gap_len']).agg({'rmse': ['mean']})\n\n\ng\n\n\n(g.droplevel(level=0, axis=1)\n .reset_index()\n .melt(id_vars=['var', 'gap_len'], var_name='rmse')\n .pivot(index = ['var', 'rmse'], columns=['gap_len'])\n .droplevel(level=0, axis=1)\n)\n\n\ng.columns\n\n\nsource\n\n\ntable_gap_len\n\n table_gap_len (data, y='rmse')\n\n\ntable_gap_len(gap_len)\n\nNameError: name 'gap_len' is not defined\n\n\n\nsource\n\n\ntable_gap_len_latex\n\n table_gap_len_latex (table, file, caption='', label='')\n\n\n[f\"{col:.0f}\" for col in list(table_gap_len(gap_len).columns)]\n\n['1', '3', '5', '7', '9', '11', '13']\n\n\n\ntable_gap_len_latex(table_gap_len(gap_len), \"\")\n\n\n\n\n\n\n \n \n1\n3\n5\n7\n9\n11\n13\n\n\nVariable\nRMSE\n \n \n \n \n \n \n \n\n\n\n\n\\parbox{2.1cm}{\\textbf{TA} [\\si{°C}]}\nmean\n1.452\n1.617\n1.169\n1.900\n1.600\n1.656\n0.980\n\n\n\\parbox{2.1cm}{\\textbf{SW\\_IN} [\\si{W/m^2}]}\nmean\n32.173\n47.047\n61.438\n33.085\n48.179\n40.442\n48.532\n\n\n\\parbox{2.1cm}{\\textbf{LW\\_IN} [\\si{W/m^2}]}\nmean\n-\n-\n-\n-\n-\n-\n-\n\n\n\\parbox{2.1cm}{\\textbf{VPD} [\\si{hPa}]}\nmean\n1.162\n0.725\n1.526\n1.711\n2.078\n1.284\n1.759\n\n\n\\parbox{2.1cm}{\\textbf{WS} [\\si{m/s}]}\nmean\n-\n-\n-\n-\n-\n-\n-\n\n\n\\parbox{2.1cm}{\\textbf{PA} [\\si{hPa}]}\nmean\n-\n-\n-\n-\n-\n-\n-\n\n\n\\parbox{2.1cm}{\\textbf{P} [\\si{mm}]}\nmean\n-\n-\n-\n-\n-\n-\n-\n\n\n\\parbox{2.1cm}{\\textbf{SWC} [\\si{\\%}]}\nmean\n-\n-\n-\n-\n-\n-\n-\n\n\n\\parbox{2.1cm}{\\textbf{TS} [\\si{°C}]}\nmean\n-\n-\n-\n-\n-\n-\n-"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "this doesn’t work completely because the decorator is also included in the source..\n\na.__code__.co_code\n\nb'd\\x01S\\x00'\n\n\n\nhashlib.md5(a.__code__.co_code).hexdigest()\n\n'9cf8bd1d73fd245ea1325fbc4055d3d4'\n\n\n\nsource\n\n\n\n cache_disk (base_file, rm_cache=False, verbose=False)\n\nDecorator to cache function output to disk\n\nimport time\nfrom tempfile import tempdir\n\n\ncp = Path(tempdir) / \"test_cache\"\n\n\n@cache_disk(cp)\ndef slow_add(a,b):\n    time.sleep(1)\n    return a + b\n\nthis time is the first time so not from the cache\n\n\n\nCPU times: user 3.34 ms, sys: 1.9 ms, total: 5.23 ms\nWall time: 1 s\n\n\n3\n\n\nnow is much faster beacuse of the cache\n\n\n\nCPU times: user 3 µs, sys: 1 µs, total: 4 µs\nWall time: 4.05 µs\n\n\n3\n\n\nadding comments change the hash, so the function is still cached\n\n@cache_disk(cp)\ndef slow_add(a,b):\n    time.sleep(1)\n    # this is a comment\n    return a + b\n\n\n\n\nCPU times: user 1.2 ms, sys: 1.03 ms, total: 2.22 ms\nWall time: 1 s\n\n\n3\n\n\n\n\n\nCPU times: user 3 µs, sys: 1 µs, total: 4 µs\nWall time: 5.25 µs\n\n\n3"
  },
  {
    "objectID": "utils.html#disk-cache",
    "href": "utils.html#disk-cache",
    "title": "Utils",
    "section": "",
    "text": "this doesn’t work completely because the decorator is also included in the source..\n\na.__code__.co_code\n\nb'd\\x01S\\x00'\n\n\n\nhashlib.md5(a.__code__.co_code).hexdigest()\n\n'9cf8bd1d73fd245ea1325fbc4055d3d4'\n\n\n\nsource\n\n\n\n cache_disk (base_file, rm_cache=False, verbose=False)\n\nDecorator to cache function output to disk\n\nimport time\nfrom tempfile import tempdir\n\n\ncp = Path(tempdir) / \"test_cache\"\n\n\n@cache_disk(cp)\ndef slow_add(a,b):\n    time.sleep(1)\n    return a + b\n\nthis time is the first time so not from the cache\n\n\n\nCPU times: user 3.34 ms, sys: 1.9 ms, total: 5.23 ms\nWall time: 1 s\n\n\n3\n\n\nnow is much faster beacuse of the cache\n\n\n\nCPU times: user 3 µs, sys: 1 µs, total: 4 µs\nWall time: 4.05 µs\n\n\n3\n\n\nadding comments change the hash, so the function is still cached\n\n@cache_disk(cp)\ndef slow_add(a,b):\n    time.sleep(1)\n    # this is a comment\n    return a + b\n\n\n\n\nCPU times: user 1.2 ms, sys: 1.03 ms, total: 2.22 ms\nWall time: 1 s\n\n\n3\n\n\n\n\n\nCPU times: user 3 µs, sys: 1 µs, total: 4 µs\nWall time: 5.25 µs\n\n\n3"
  },
  {
    "objectID": "utils.html#random",
    "href": "utils.html#random",
    "title": "Utils",
    "section": "Random",
    "text": "Random\n\nsource\n\nreset_seed\n\n reset_seed (seed=27)"
  },
  {
    "objectID": "utils.html#testing",
    "href": "utils.html#testing",
    "title": "Utils",
    "section": "Testing",
    "text": "Testing\n\nsource\n\ntest_close\n\n test_close (a, b, eps=1e-05)\n\ntest that a is within eps of b"
  },
  {
    "objectID": "utils.html#standard-scaler",
    "href": "utils.html#standard-scaler",
    "title": "Utils",
    "section": "Standard Scaler",
    "text": "Standard Scaler\nmake a standard scaler that can also inverse transfor standard deviations. see Standardizer for details of implementation\n\nreset_seed()\nxx = np.random.random((4, 10))\n\n\ns = StandardScaler().fit(xx)\n\n\ns.transform(xx)\n\narray([[ 0.07263978,  0.63279488, -0.9975139 ,  0.50899177,  0.15537652,\n         1.45555506,  1.56629646, -1.60237369,  1.51674974,  1.29584745],\n       [ 1.58579521,  0.83086419, -0.68281902,  0.51578245, -0.62395756,\n        -1.19720248, -0.43000476,  1.1539719 , -0.74724819, -0.85525414],\n       [-1.05809926, -1.69049694,  0.0895118 , -1.72684476, -1.08418417,\n         0.32617669, -1.16657374,  0.2345773 ,  0.26525847,  0.64349108],\n       [-0.60033573,  0.22683787,  1.59082112,  0.70207053,  1.55276521,\n        -0.58452927,  0.03028204,  0.21382449, -1.03476002, -1.08408439]])\n\n\n\ns.mean_\n\narray([0.40358703, 0.6758362 , 0.77934606, 0.70748673, 0.34417949,\n       0.62067044, 0.48500116, 0.54921643, 0.34604713, 0.3660338 ])\n\n\n\ns.scale_\n\narray([0.30471427, 0.21926148, 0.04405831, 0.31536161, 0.25229864,\n       0.24649441, 0.26061043, 0.21187396, 0.26093989, 0.22927816])\n\n\n\nsource\n\nStandardScaler.inverse_transform_std\n\n StandardScaler.inverse_transform_std (x_std)\n\n\n\n\n\nDetails\n\n\n\n\nx_std\nstandard deviations"
  },
  {
    "objectID": "utils.html#info-visualization",
    "href": "utils.html#info-visualization",
    "title": "Utils",
    "section": "Info visualization",
    "text": "Info visualization\n\nsource\n\narray2df\n\n array2df (x:torch.Tensor, row_names:Optional[Collection[str]]=None,\n           col_names:Optional[Collection[str]]=None, row_var:str='')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nTensor\n\n2d tensor\n\n\nrow_names\nOptional\nNone\nnames for the row\n\n\ncol_names\nOptional\nNone\nnames for the columns\n\n\nrow_var\nstr\n\nname of the first column (the one with row names). This should describe the values of row_name\n\n\n\n\nimport numpy as np\n\n\na = np.random.rand(2,3,3)\n\n\ndisplay(HTML(f\"&lt;pre&gt; {repr(a)} &lt;/pre&gt;\"))\n\n array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) \n\n\n\na\n\narray([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]])\n\n\n\nsource\n\n\nretrieve_names\n\n retrieve_names (*args)\n\nTries to retrieve the argument name in the call frame, if there are multiple matches name is ’’\n\nsource\n\n\nmaybe_retrieve_callers_name\n\n maybe_retrieve_callers_name (args)\n\nTries to retrieve the argument name in the call frame, if there are multiple matches name is ’’\n\nx, y, z = 1, 2, 3\n\ndef func(*args):\n    return maybe_retrieve_callers_name(args)\n\nprint(func(x,y))\n\n['x', 'y']\n\n\n\nretrieve_names(a,a)\n\n[['_', 'a', '_32'], ['_', 'a', '_32']]\n\n\n\ntrap = io.StringIO()\nwith redirect_stdout(trap):\n    print(\"hello\")\n\ntrap.getvalue()\n\n'hello\\n'\n\n\n\nsource\n\n\nshow_as_row\n\n show_as_row (*os, names:Iterable[str]=None, **kwargs)\n\nShows a interable of tensors on a row\n\nsource\n\n\nrow_items\n\n row_items (**kwargs)\n\n\nsource\n\n\npretty_repr\n\n pretty_repr (o)\n\n\ntype(a)\n\nnumpy.ndarray\n\n\n\nshow_as_row(a,b=a)\n\n\n b array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) a array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) \n\n\n\nfrom itertools import zip_longest\n\n\nzip_longest\n\nitertools.zip_longest\n\n\n\nfunc(a,a)\n\n['a', 'a']\n\n\n\nb = a.copy()\n\n\nshow_as_row([1], [2])\n\n\n #0 [1] #1 [2] \n\n\n\nshow_as_row(a,b)\n\n\n a array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) b array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) \n\n\n\nshow_as_row(a, names='b')\n\n\n b array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) \n\n\n\nshow_as_row(c=a)\n\n\n c array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) \n\n\n\nsource\n\n\ndisplay_as_row\n\n display_as_row (dfs:dict[str,pandas.core.frame.DataFrame], title='',\n                 hide_idx=True, styler=&lt;function _style_df&gt;)\n\ndisplay multiple dataframes in the same row\n\nsource\n\n\nrow_dfs\n\n row_dfs (dfs:dict[str,pandas.core.frame.DataFrame], title='',\n          hide_idx=True, styler=&lt;function _style_df&gt;)\n\n\na = HTML(pd.DataFrame([1,2]).to_html(notebook=True))\n\n\ndisplay_as_row({\"test\": pd.DataFrame([1,2])}, \"hello\")\n\n\nhello  test \n\n\n\n\n0\n\n\n\n\n1\n\n\n2\n\n\n\n\n \n\n\n\ndisplay_as_row({\"test\": pd.DataFrame([1,2])}, \"hello\", hide_idx=False)\n\n\nhello  test \n\n\n\n\n \n0\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n\n\n \n\n\n\ndisplay_as_row({f\"test{i}\": pd.DataFrame([1,2], columns=[i]) for i in range(10)})\n\n\n  test0 \n\n\n\n\n0\n\n\n\n\n1\n\n\n2\n\n\n\n\ntest1\n\n\n\n1\n\n\n\n\n1\n\n\n2\n\n\n\n\n\ntest2\n\n\n\n2\n\n\n\n\n1\n\n\n2\n\n\n\n\n\ntest3\n\n\n\n3\n\n\n\n\n1\n\n\n2\n\n\n\n\n\ntest4\n\n\n\n4\n\n\n\n\n1\n\n\n2\n\n\n\n\n\ntest5\n\n\n\n5\n\n\n\n\n1\n\n\n2\n\n\n\n\n\ntest6\n\n\n\n6\n\n\n\n\n1\n\n\n2\n\n\n\n\n\ntest7\n\n\n\n7\n\n\n\n\n1\n\n\n2\n\n\n\n\n\ntest8\n\n\n\n8\n\n\n\n\n1\n\n\n2\n\n\n\n\n\ntest9\n\n\n\n9\n\n\n\n\n1\n\n\n2"
  },
  {
    "objectID": "utils.html#torch-helpers",
    "href": "utils.html#torch-helpers",
    "title": "Utils",
    "section": "Torch Helpers",
    "text": "Torch Helpers\ninspired from source: https://github.com/pytorch/pytorch/pull/9281\n\nfrom fastcore.test import *\n\n\nsource\n\neye_like\n\n eye_like (x:torch.Tensor)\n\nReturn a tensor with same batch size as x, that has a nxn eye matrix in each sample in batch.\nArgs: x: tensor of shape (B, n, m) or (n,m)\nReturns: tensor of shape (B, n, m) or (n,m) that has the same dtype and device as x.\n\na = torch.ones(3,2,2)\n\n\ntest_close(eye_like(a),\n           torch.tensor([[[1., 0.],\n                          [0., 1.]],\n \n                         [[1., 0.],\n                          [0., 1.]],\n \n                         [[1., 0.],\n                          [0., 1.]]]))\n\n\neye_like(torch.ones(3,3))\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n\n\n\neye_like(torch.ones(2,2,3,3))\n\ntensor([[[[1., 0., 0.],\n          [0., 1., 0.],\n          [0., 0., 1.]],\n\n         [[1., 0., 0.],\n          [0., 1., 0.],\n          [0., 0., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [0., 1., 0.],\n          [0., 0., 1.]],\n\n         [[1., 0., 0.],\n          [0., 1., 0.],\n          [0., 0., 1.]]]])\n\n\n\neye_like(torch.ones(2,3,4))\n\ntensor([[[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [0., 0., 1., 0.]],\n\n        [[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [0., 0., 1., 0.]]])\n\n\n\n\nIs diagonal\n\ne = torch.eye(3,3)\nd = torch.diagonal(e, dim1=-2, dim2=-1)\nd\ntorch.diag_embed(d, dim1=-2, dim2=-1)\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n\n\n\nsource\n\n\nis_diagonal\n\n is_diagonal (x:torch.Tensor)\n\nCheck that tensor is diagonal respect to the last 2 dimensions\n\nis_diagonal(torch.eye(3,3))\n\ntensor(True)\n\n\n\nis_diagonal(torch.ones(3,3))\n\ntensor(False)\n\n\n\nis_diagonal(eye_like(torch.ones(2,3,3)))\n\ntensor(True)"
  },
  {
    "objectID": "utils.html#itertools",
    "href": "utils.html#itertools",
    "title": "Utils",
    "section": "Itertools",
    "text": "Itertools\n\nsource\n\nproduct_dict\n\n product_dict (**kwargs)"
  },
  {
    "objectID": "utils.html#export",
    "href": "utils.html#export",
    "title": "Utils",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "filter_performance_stability.html",
    "href": "filter_performance_stability.html",
    "title": "Filter Perfomance and Stability",
    "section": "",
    "text": "from fastcore.test import *\nfrom fastcore.basics import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.gaussian import *\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.filter import *\nfrom meteo_imp.kalman.filter import get_test_data\n\nimport pykalman\nfrom typing import *\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import Tensor\nfrom torch.distributions import MultivariateNormal\n\nfrom timeit import timeit\nimport polars as pl\nimport altair as alt\n\nfrom tqdm.auto import tqdm\nclass KalmanFilterPerformance():\n    def __init__(self, n_obs=100, n_dim_obs=4, n_dim_state=3, n_dim_contr=3, bs=5, p_missing=.3,\n                 use_sr_filter=True, device='cpu', use_conditional=True, use_batch=True, **kwargs):\n        store_attr()\n        if not use_sr_filter:\n            self.filter = KalmanFilter.init_random(self.n_dim_obs,self.n_dim_state, self.n_dim_contr)\n        else:\n            self.filter = KalmanFilterSR.init_random(self.n_dim_obs,self.n_dim_state, self.n_dim_contr)\n        \n        self.filter.to(device)\n        self.filter.use_conditional = self.use_conditional\n        self.data = get_test_data(self.n_obs, n_dim_obs = n_dim_obs, n_dim_contr=n_dim_contr, p_missing=p_missing, bs=bs, device=device)\n        \n    def get_method(self, method):\n        data, mask, control = self.data\n        method = getattr(self.filter, method)\n        if self.use_batch:\n            return lambda: method(data, mask, control)\n        else:\n            return lambda: [method(d,m,c) for d,m,c in zip(data, mask, control)]\n    def time_method(self, method, rep = 1):\n        method = self.get_method(method)\n        time = timeit('method()', globals={'method': method}, number=rep)\n        return time / rep\nkf = KalmanFilterPerformance(p_missing=0)\nkf.time_method('filter')\n\n0.14900339799987705\nimport itertools\nfrom fastcore.meta import delegates\n# from https://stackoverflow.com/a/5228294\ndef product_dict(**kwargs):\n    keys = kwargs.keys()\n    vals = kwargs.values()\n    for instance in itertools.product(*vals):\n        yield dict(zip(keys, instance))\ndefault_kwargs = {'n_obs':100, 'n_dim_obs':4, 'n_dim_state':3, 'n_dim_contr':3, 'bs':5,\n                       'use_sr_filter': True, 'device':'cpu', 'use_conditional':True, 'use_batch':True}\n@delegates(KalmanFilterPerformance)\ndef perf_comb_params(method,  **kwargs):\n    kwargs = default_kwargs | kwargs\n    kwargs = {key:tuplify(arg) for key, arg in kwargs.items()}\n    arg_sets = product_dict(**kwargs)\n    out = []\n    for arg_set in tqdm(arg_sets):\n        kf = KalmanFilterPerformance(**arg_set)\n        time = kf.time_method(method)\n        out.append({'time': time} | arg_set)\n    return pl.DataFrame(out)\nperf_comb_params('filter')\n\n\n\n\n\n\n\nshape: (1, 10)\nbs\ndevice\nn_dim_contr\nn_dim_obs\nn_dim_state\nn_obs\ntime\nuse_batch\nuse_conditional\nuse_sr_filter\ni64\nstr\ni64\ni64\ni64\ni64\nf64\nbool\nbool\nbool\n5\n\"cpu\"\n3\n4\n3\n100\n0.254086\ntrue\ntrue\ntrue"
  },
  {
    "objectID": "filter_performance_stability.html#performance",
    "href": "filter_performance_stability.html#performance",
    "title": "Filter Perfomance and Stability",
    "section": "Performance",
    "text": "Performance\n\ndef compare_performance(n_obs, n_dim_obs, n_dim_state, n_dim_contr, bs, dtype=torch.float64):\n    kf_cuda = KalmanFilter.init_random(n_dim_obs,n_dim_state, dtype=dtype).cuda()\n    data_cuda, mask_cuda = get_test_data(n_dim_obs,n_dim_state, bs=bs, device=\"cuda\", dtype=dtype)\n    \n    print(\"GPU\")\n\n\n    kf_cuda = KalmanFilter.init_random(n_dim_obs,n_dim_state, dtype=dtype)\n    data_cuda, mask_cuda = get_test_data(n_dim_obs,n_dim_state, bs=bs, dtype=dtype)\n    print(\"CPU\")\n\n    print(\"No batches CPU\")\n\n    print(\"No batches GPU\")\n\n\ncompare_performance(100, 2,2,100)\n\nGPU\n87.9 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nCPU\n7.83 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nNo batches CPU\n12.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nNo batches GPU\n154 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\ncompare_performance(200, 10,10,200)\n\nGPU\n2.04 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nCPU\n7.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nNo batches CPU\n13.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nNo batches GPU\n2.07 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\nFloat64\n\ncompare_performance(100, 2,2,100, dtype=torch.float64)\n\nGPU\n100 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nCPU\n8.29 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nNo batches CPU\n13.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nNo batches GPU\n159 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\ncompare_performance(200, 10,10,200, dtype=torch.float64)\n\nGPU\n2.22 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nCPU\n8.35 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nNo batches CPU\n13.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nNo batches GPU\n2.01 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)"
  },
  {
    "objectID": "filter_performance_stability.html#stability",
    "href": "filter_performance_stability.html#stability",
    "title": "Filter Perfomance and Stability",
    "section": "Stability",
    "text": "Stability\n\nimport polars as pl\nimport altair as alt\nfrom altair import datum\n\n    kSR.Q_raw = torch.nn.Parameter(kSR.Q_raw + eye_like(kSR.Q_raw) * torch.sqrt(torch.tensor(1e-5)))\n    kSR.R_raw = torch.nn.Parameter(kSR.R_raw + eye_like(kSR.R_raw) * torch.sqrt(torch.tensor(1e-5)))\n    kSR.P0_raw = torch.nn.Parameter(kSR.P0_raw + eye_like(kSR.P0_raw) * torch.sqrt(torch.tensor(1e-5)))\n\ndef fuzz_filter_SR(n_iter=10, n_obs=50):\n    reset_seed(27)\n    out = []\n    for n in tqdm(range(n_iter)):\n        k = KalmanFilter.init_random(10,5,8)\n        kSR = KalmanFilterSR.init_from(k)\n        data, mask, control = get_test_data(n_obs,10,8)\n        filt = k.filter(data, mask, control)\n        filtSR = kSR.filter(data, mask, control)\n        for t in range(n_obs):\n            P = filt.cov[:,t]\n            P_C = filtSR.cov[:,t]\n            out.append({'t': t, 'n': n, 'MAE': (P - P_C @ P_C.mT).abs().mean().item()})\n    return pl.DataFrame(out)\n\n\nerr_raw = fuzz_filter_SR(100, 100)\n\n\n\n\n\nerr = err_raw.groupby('t').agg([\n    pl.col('MAE').median().alias(\"median\"),\n    pl.col('MAE').quantile(.75).alias(\"Q3\"),\n    pl.col('MAE').quantile(.25).alias(\"Q1\"),\n    pl.col('MAE').max().alias(\"max\")\n])\n\n\nmedian = alt.Chart(err.to_pandas()).mark_line(color=\"black\"\n           ).encode(\n    x = alt.X('t', title=\"Number of Iterations\"),\n    y = alt.Y('median', axis=alt.Axis(format=\".1e\"), scale=alt.Scale(type=\"log\"), title=\"log MAE\"),\n    # color=datum(\"median\"),\n    strokeDash = datum(\"median\")\n    #, scale=alt.Scale(range=['black']))\n)\n\nQ1 = alt.Chart(err.to_pandas()).mark_line(color='dimgray', strokeDash=[4,6]).encode(x = 't', y = 'Q1', strokeDash=datum(\"quantile\"))\nQ3 = alt.Chart(err.to_pandas()).mark_line(color='dimgray', strokeDash=[4,6]).encode(x = 't', y = 'Q3', strokeDash=datum(\"quantile\"))\nmax = alt.Chart(err.to_pandas()).mark_line(color='black', strokeDash=[2,2]).encode(x = 't', y = 'max', strokeDash=datum(\"max\"))\np = (Q1 + Q3 + max + median).interactive().properties(title=\"Standard Filter vs Square Root Filter (Mean Absolute Error of state cavariances)\")\np\n\n\n\n\n\n\n\nimport vl_convert as vlc\nfrom pyprojroot import here\nbase_path = here(\"manuscript/Master Thesis - Meteorological time series imputation using Kalman filters - Simone Massaro/images/\")\npath = base_path / \"numerical_stability.png\"\npng_data = vlc.vegalite_to_png(vl_spec=p.to_json(), scale=2)\nwith open(path, \"wb\") as f:\n    f.write(png_data)"
  },
  {
    "objectID": "kalman_filter.html",
    "href": "kalman_filter.html",
    "title": "Kalman Filter",
    "section": "",
    "text": "The models uses a latent state variable \\(x\\) that is modelled over time, to impute gaps in \\(y\\)\nThe assumption of the model is that the state variable at time \\(x_t\\) depends only on the last state \\(x_{t-1}\\) and not on the previous states.\n\n\nThe equations of the model are:\n\\[\\begin{align} p(x_t | x_{t-1}) & = \\mathcal{N}(Ax_{t-1} + b, Q) \\\\\np(y_t | x_t) & = \\mathcal{N}(Hx_t + d, R) \\end{align}\\]\nwhere:\n\n\\(A\\) is the A\n\\(b\\) is the bset\n\\(Q\\) is the Q\n\\(H\\) is the obs_trans\n\\(d\\) is the d\n\\(R\\) is the R\n\nin addition the model has also the parameters of the initial state that are used to initialize the filter:\n\nm0\nP0\n\nThe Kalman filter has 3 steps:\n\nfilter (updating the state at time t with observations till time t-1)\nupdate (update the state at time t using the observation at time t)\nsmooth (update the state using the observations at time t+1)\n\nIn case of missing data the update step is skipped.\nAfter smoothing the missing data at time t (\\(y_t\\)) can be imputed from the state (\\(x_t\\)) using this formula: \\[p(y_t|x_t) = \\mathcal{N}(Hx_t + d, R + HP^s_tH^T)\\]\nThe Kalman Filter is an algorithm designed to estimate \\(P(x_t | y_{0:t})\\). As all state transitions and obss are linear with Gaussian distributed noise, these distributions can be represented exactly as Gaussian distributions with mean ms[t] and covs Ps[t]. Similarly, the Kalman Smoother is an algorithm designed to estimate \\(P(x_t | y_{0:t-1})\\)"
  },
  {
    "objectID": "kalman_filter.html#introduction",
    "href": "kalman_filter.html#introduction",
    "title": "Kalman Filter",
    "section": "",
    "text": "The models uses a latent state variable \\(x\\) that is modelled over time, to impute gaps in \\(y\\)\nThe assumption of the model is that the state variable at time \\(x_t\\) depends only on the last state \\(x_{t-1}\\) and not on the previous states.\n\n\nThe equations of the model are:\n\\[\\begin{align} p(x_t | x_{t-1}) & = \\mathcal{N}(Ax_{t-1} + b, Q) \\\\\np(y_t | x_t) & = \\mathcal{N}(Hx_t + d, R) \\end{align}\\]\nwhere:\n\n\\(A\\) is the A\n\\(b\\) is the bset\n\\(Q\\) is the Q\n\\(H\\) is the obs_trans\n\\(d\\) is the d\n\\(R\\) is the R\n\nin addition the model has also the parameters of the initial state that are used to initialize the filter:\n\nm0\nP0\n\nThe Kalman filter has 3 steps:\n\nfilter (updating the state at time t with observations till time t-1)\nupdate (update the state at time t using the observation at time t)\nsmooth (update the state using the observations at time t+1)\n\nIn case of missing data the update step is skipped.\nAfter smoothing the missing data at time t (\\(y_t\\)) can be imputed from the state (\\(x_t\\)) using this formula: \\[p(y_t|x_t) = \\mathcal{N}(Hx_t + d, R + HP^s_tH^T)\\]\nThe Kalman Filter is an algorithm designed to estimate \\(P(x_t | y_{0:t})\\). As all state transitions and obss are linear with Gaussian distributed noise, these distributions can be represented exactly as Gaussian distributions with mean ms[t] and covs Ps[t]. Similarly, the Kalman Smoother is an algorithm designed to estimate \\(P(x_t | y_{0:t-1})\\)"
  },
  {
    "objectID": "kalman_filter.html#kalman-filter-base-1",
    "href": "kalman_filter.html#kalman-filter-base-1",
    "title": "Kalman Filter",
    "section": "Kalman Filter Base",
    "text": "Kalman Filter Base\n\nsource\n\nKalmanFilterBase\n\n KalmanFilterBase (A:torch.Tensor, H:torch.Tensor, B:torch.Tensor,\n                   Q:torch.Tensor, R:torch.Tensor, b:torch.Tensor,\n                   d:torch.Tensor, m0:torch.Tensor, P0:torch.Tensor,\n                   n_dim_state:int=None, n_dim_obs:int=None,\n                   n_dim_contr:int=None,\n                   var_names:Optional[Iterable[str]]=None,\n                   contr_names:Optional[Iterable[str]]=None,\n                   cov_checker:meteo_imp.gaussian.CheckPosDef|None=None,\n                   use_conditional:bool=False, use_control:bool=True,\n                   use_smooth:bool=True, pred_only_gap:bool=False,\n                   pred_std:bool=False)\n\nBase class for handling Kalman Filter implementation in PyTorch\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nA\nTensor\n\n[n_dim_state,n_dim_state] \\(A\\), state transition matrix\n\n\nH\nTensor\n\n[n_dim_obs, n_dim_state] \\(H\\), observation matrix\n\n\nB\nTensor\n\n[n_dim_state, n_dim_contr] \\(B\\) control matrix\n\n\nQ\nTensor\n\n[n_dim_state, n_dim_state] \\(Q\\), state trans covariance matrix\n\n\nR\nTensor\n\n[n_dim_obs, n_dim_obs] \\(R\\), observations covariance matrix\n\n\nb\nTensor\n\n[n_dim_state] \\(b\\), state transition offset\n\n\nd\nTensor\n\n[n_dim_obs] \\(d\\), observations offset\n\n\nm0\nTensor\n\n[n_dim_state] \\(m_0\\)\n\n\nP0\nTensor\n\n[n_dim_state, n_dim_state] \\(P_0\\)\n\n\nn_dim_state\nint\nNone\nNumber of dimensions for state - default infered from parameters\n\n\nn_dim_obs\nint\nNone\nNumber of dimensions for observations - default infered from parameters\n\n\nn_dim_contr\nint\nNone\nNumber of dimensions for control - default infered from parameters\n\n\nvar_names\nOptional\nNone\nNames of variables for printing\n\n\ncontr_names\nOptional\nNone\nNames of control variables for printing\n\n\ncov_checker\nmeteo_imp.gaussian.CheckPosDef | None\nNone\nCheck covariance at every step\n\n\nuse_conditional\nbool\nFalse\nUse conditional distribution for gaps that don’t have all variables missing\n\n\nuse_control\nbool\nTrue\nUse the control in the filter\n\n\nuse_smooth\nbool\nTrue\nUse smoother for predictions (otherwise is filter only)\n\n\npred_only_gap\nbool\nFalse\nit True predictions are only for the gap\n\n\npred_std\nbool\nFalse\nreturn only stds and not covariances\n\n\n\n\n\nConstructors\nGiving all the parameters manually to the KalmanFilterBase init method is not convenient, hence we are having some methods that help initize the class\ndue to a bug in fastcore cannot subclass after creating class methods\n\nsource\n\n\nKalmanFilter\n\n KalmanFilter (A:torch.Tensor, H:torch.Tensor, B:torch.Tensor,\n               Q:torch.Tensor, R:torch.Tensor, b:torch.Tensor,\n               d:torch.Tensor, m0:torch.Tensor, P0:torch.Tensor,\n               n_dim_state:int=None, n_dim_obs:int=None,\n               n_dim_contr:int=None,\n               var_names:Optional[Iterable[str]]=None,\n               contr_names:Optional[Iterable[str]]=None,\n               cov_checker:meteo_imp.gaussian.CheckPosDef|None=None,\n               use_conditional:bool=False, use_control:bool=True,\n               use_smooth:bool=True, pred_only_gap:bool=False,\n               pred_std:bool=False)\n\nBase class for handling Kalman Filter implementation in PyTorch\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nA\nTensor\n\n[n_dim_state,n_dim_state] \\(A\\), state transition matrix\n\n\nH\nTensor\n\n[n_dim_obs, n_dim_state] \\(H\\), observation matrix\n\n\nB\nTensor\n\n[n_dim_state, n_dim_contr] \\(B\\) control matrix\n\n\nQ\nTensor\n\n[n_dim_state, n_dim_state] \\(Q\\), state trans covariance matrix\n\n\nR\nTensor\n\n[n_dim_obs, n_dim_obs] \\(R\\), observations covariance matrix\n\n\nb\nTensor\n\n[n_dim_state] \\(b\\), state transition offset\n\n\nd\nTensor\n\n[n_dim_obs] \\(d\\), observations offset\n\n\nm0\nTensor\n\n[n_dim_state] \\(m_0\\)\n\n\nP0\nTensor\n\n[n_dim_state, n_dim_state] \\(P_0\\)\n\n\nn_dim_state\nint\nNone\nNumber of dimensions for state - default infered from parameters\n\n\nn_dim_obs\nint\nNone\nNumber of dimensions for observations - default infered from parameters\n\n\nn_dim_contr\nint\nNone\nNumber of dimensions for control - default infered from parameters\n\n\nvar_names\nOptional\nNone\nNames of variables for printing\n\n\ncontr_names\nOptional\nNone\nNames of control variables for printing\n\n\ncov_checker\nmeteo_imp.gaussian.CheckPosDef | None\nNone\nCheck covariance at every step\n\n\nuse_conditional\nbool\nFalse\nUse conditional distribution for gaps that don’t have all variables missing\n\n\nuse_control\nbool\nTrue\nUse the control in the filter\n\n\nuse_smooth\nbool\nTrue\nUse smoother for predictions (otherwise is filter only)\n\n\npred_only_gap\nbool\nFalse\nit True predictions are only for the gap\n\n\npred_std\nbool\nFalse\nreturn only stds and not covariances\n\n\n\n\nsource\n\n\nKalmanFilterSR\n\n KalmanFilterSR (A:torch.Tensor, H:torch.Tensor, B:torch.Tensor,\n                 Q:torch.Tensor, R:torch.Tensor, b:torch.Tensor,\n                 d:torch.Tensor, m0:torch.Tensor, P0:torch.Tensor,\n                 n_dim_state:int=None, n_dim_obs:int=None,\n                 n_dim_contr:int=None,\n                 var_names:Optional[Iterable[str]]=None,\n                 contr_names:Optional[Iterable[str]]=None,\n                 cov_checker:meteo_imp.gaussian.CheckPosDef|None=None,\n                 use_conditional:bool=False, use_control:bool=True,\n                 use_smooth:bool=True, pred_only_gap:bool=False,\n                 pred_std:bool=False)\n\nBase class for handling Kalman Filter implementation in PyTorch\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nA\nTensor\n\n[n_dim_state,n_dim_state] \\(A\\), state transition matrix\n\n\nH\nTensor\n\n[n_dim_obs, n_dim_state] \\(H\\), observation matrix\n\n\nB\nTensor\n\n[n_dim_state, n_dim_contr] \\(B\\) control matrix\n\n\nQ\nTensor\n\n[n_dim_state, n_dim_state] \\(Q\\), state trans covariance matrix\n\n\nR\nTensor\n\n[n_dim_obs, n_dim_obs] \\(R\\), observations covariance matrix\n\n\nb\nTensor\n\n[n_dim_state] \\(b\\), state transition offset\n\n\nd\nTensor\n\n[n_dim_obs] \\(d\\), observations offset\n\n\nm0\nTensor\n\n[n_dim_state] \\(m_0\\)\n\n\nP0\nTensor\n\n[n_dim_state, n_dim_state] \\(P_0\\)\n\n\nn_dim_state\nint\nNone\nNumber of dimensions for state - default infered from parameters\n\n\nn_dim_obs\nint\nNone\nNumber of dimensions for observations - default infered from parameters\n\n\nn_dim_contr\nint\nNone\nNumber of dimensions for control - default infered from parameters\n\n\nvar_names\nOptional\nNone\nNames of variables for printing\n\n\ncontr_names\nOptional\nNone\nNames of control variables for printing\n\n\ncov_checker\nmeteo_imp.gaussian.CheckPosDef | None\nNone\nCheck covariance at every step\n\n\nuse_conditional\nbool\nFalse\nUse conditional distribution for gaps that don’t have all variables missing\n\n\nuse_control\nbool\nTrue\nUse the control in the filter\n\n\nuse_smooth\nbool\nTrue\nUse smoother for predictions (otherwise is filter only)\n\n\npred_only_gap\nbool\nFalse\nit True predictions are only for the gap\n\n\npred_std\nbool\nFalse\nreturn only stds and not covariances\n\n\n\n\nRandom parameters\n\nkB = KalmanFilterBase.init_random(3,4, 3, dtype=torch.float64)\nkB\n\nKalman Filter\n        N dim obs: 3,\n        N dim state: 4,\n        N dim contr: 3\n\n\n\nkB.Q\n\ntensor([[[1.5725, 0.3829, 0.0843, 0.3637],\n         [0.3829, 1.4430, 0.3358, 1.1988],\n         [0.0843, 0.3358, 1.7816, 0.7411],\n         [0.3637, 1.1988, 0.7411, 1.6773]]], dtype=torch.float64,\n       grad_fn=&lt;UnsafeViewBackward0&gt;)\n\n\n\nkB.Q_C\n\ntensor([[[1.2540, 0.0000, 0.0000, 0.0000],\n         [0.3053, 1.1618, 0.0000, 0.0000],\n         [0.0672, 0.2714, 1.3052, 0.0000],\n         [0.2901, 0.9556, 0.3542, 0.7446]]], dtype=torch.float64,\n       grad_fn=&lt;DiagonalScatterBackward0&gt;)\n\n\n\ntest_close(kB.Q_C @ kB.Q_C.mT, kB.Q, eps=2e-5)\n\n\nkB.P0 = to_posdef(torch.rand(1,3,3))\n\ncheck that assigment works :)\n\nkB.P0 = to_posdef(torch.rand(4, 4, dtype=torch.float64))\n\n\nkB.P0_C\n\ntensor([[0.9349, 0.0000, 0.0000, 0.0000],\n        [0.3928, 1.0748, 0.0000, 0.0000],\n        [0.7406, 0.7533, 0.8326, 0.0000],\n        [0.5903, 0.0391, 0.8217, 0.9638]], dtype=torch.float64,\n       grad_fn=&lt;DiagonalScatterBackward0&gt;)\n\n\n\nkB = KalmanFilterBase.init_random(3,4, 3, dtype=torch.float64)\nkB\n\nKalman Filter\n        N dim obs: 3,\n        N dim state: 4,\n        N dim contr: 3\n\n\n\nlist(kB.named_parameters())\n\n[('A',\n  Parameter containing:\n  tensor([[[0.1751, 0.5375, 0.7676, 0.7450],\n           [0.1204, 0.4777, 0.5823, 0.3786],\n           [0.8484, 0.2317, 0.3969, 0.7088],\n           [0.0270, 0.6178, 0.6097, 0.9128]]], dtype=torch.float64,\n         requires_grad=True)),\n ('H',\n  Parameter containing:\n  tensor([[[0.4984, 0.1597, 0.4458, 0.7349],\n           [0.1070, 0.9531, 0.1942, 0.6683],\n           [0.9186, 0.7123, 0.1806, 0.5045]]], dtype=torch.float64,\n         requires_grad=True)),\n ('B',\n  Parameter containing:\n  tensor([[[0.2827, 0.1138, 0.9378],\n           [0.5594, 0.9364, 0.5136],\n           [0.8592, 0.7647, 0.5183],\n           [0.2376, 0.5618, 0.5096]]], dtype=torch.float64, requires_grad=True)),\n ('Q_raw',\n  Parameter containing:\n  tensor([[[0.4590, 0.0000, 0.0000, 0.0000],\n           [0.5348, 0.5296, 0.0000, 0.0000],\n           [0.0025, 0.0749, 0.4603, 0.0000],\n           [0.0588, 0.5416, 0.9309, 0.5859]]], dtype=torch.float64,\n         requires_grad=True)),\n ('R_raw',\n  Parameter containing:\n  tensor([[[0.1393, 0.0000, 0.0000],\n           [0.0249, 0.7613, 0.0000],\n           [0.7829, 0.4311, 0.9199]]], dtype=torch.float64, requires_grad=True)),\n ('b',\n  Parameter containing:\n  tensor([[[0.4661],\n           [0.3918],\n           [0.0571],\n           [0.9529]]], dtype=torch.float64, requires_grad=True)),\n ('d',\n  Parameter containing:\n  tensor([[[0.9334],\n           [0.5645],\n           [0.0695]]], dtype=torch.float64, requires_grad=True)),\n ('m0',\n  Parameter containing:\n  tensor([[[0.8234],\n           [0.3850],\n           [0.3380],\n           [0.4376]]], dtype=torch.float64, requires_grad=True)),\n ('P0_raw',\n  Parameter containing:\n  tensor([[[0.4467, 0.0000, 0.0000, 0.0000],\n           [0.4818, 0.9502, 0.0000, 0.0000],\n           [0.4770, 0.3478, 0.9812, 0.0000],\n           [0.2460, 0.9602, 0.6476, 0.2647]]], dtype=torch.float64,\n         requires_grad=True))]\n\n\n\nkB.state_dict()\n\nOrderedDict([('A',\n              tensor([[[0.1751, 0.5375, 0.7676, 0.7450],\n                       [0.1204, 0.4777, 0.5823, 0.3786],\n                       [0.8484, 0.2317, 0.3969, 0.7088],\n                       [0.0270, 0.6178, 0.6097, 0.9128]]], dtype=torch.float64)),\n             ('H',\n              tensor([[[0.4984, 0.1597, 0.4458, 0.7349],\n                       [0.1070, 0.9531, 0.1942, 0.6683],\n                       [0.9186, 0.7123, 0.1806, 0.5045]]], dtype=torch.float64)),\n             ('B',\n              tensor([[[0.2827, 0.1138, 0.9378],\n                       [0.5594, 0.9364, 0.5136],\n                       [0.8592, 0.7647, 0.5183],\n                       [0.2376, 0.5618, 0.5096]]], dtype=torch.float64)),\n             ('Q_raw',\n              tensor([[[0.4590, 0.0000, 0.0000, 0.0000],\n                       [0.5348, 0.5296, 0.0000, 0.0000],\n                       [0.0025, 0.0749, 0.4603, 0.0000],\n                       [0.0588, 0.5416, 0.9309, 0.5859]]], dtype=torch.float64)),\n             ('R_raw',\n              tensor([[[0.1393, 0.0000, 0.0000],\n                       [0.0249, 0.7613, 0.0000],\n                       [0.7829, 0.4311, 0.9199]]], dtype=torch.float64)),\n             ('b',\n              tensor([[[0.4661],\n                       [0.3918],\n                       [0.0571],\n                       [0.9529]]], dtype=torch.float64)),\n             ('d',\n              tensor([[[0.9334],\n                       [0.5645],\n                       [0.0695]]], dtype=torch.float64)),\n             ('m0',\n              tensor([[[0.8234],\n                       [0.3850],\n                       [0.3380],\n                       [0.4376]]], dtype=torch.float64)),\n             ('P0_raw',\n              tensor([[[0.4467, 0.0000, 0.0000, 0.0000],\n                       [0.4818, 0.9502, 0.0000, 0.0000],\n                       [0.4770, 0.3478, 0.9812, 0.0000],\n                       [0.2460, 0.9602, 0.6476, 0.2647]]], dtype=torch.float64))])\n\n\n\n\nFrom filter\n\n\n\n\ninit_from’]\nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.\n\nk1 = KalmanFilter.init_random(3,4,3)\nk2 = KalmanFilterSR.init_from(k1)\nfor p1, p2 in zip(k1.parameters(), k2.parameters()):\n    test_close(p1,p2, eps=1e-3) #noise added by contraints\n\n\n\nGet Info\n\nsource\n\n\nKalmanFilterBase.get_info\n\n KalmanFilterBase.get_info ()\n\n\nkB\n\n\nKalman Filter (3 obs, 4 state, 3 contr)  $A$ \n\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n0.1751\n0.5375\n0.7676\n0.7450\n\n\nx_1\n0.1204\n0.4777\n0.5823\n0.3786\n\n\nx_2\n0.8484\n0.2317\n0.3969\n0.7088\n\n\nx_3\n0.0270\n0.6178\n0.6097\n0.9128\n\n\n\n\n$Q$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n0.9002\n0.5074\n0.0024\n0.0558\n\n\nx_1\n0.5074\n1.2712\n0.0756\n0.5690\n\n\nx_2\n0.0024\n0.0756\n0.9072\n0.9246\n\n\nx_3\n0.0558\n0.5690\n0.9246\n2.2208\n\n\n\n\n\n$b$\n\n\n\nstate\noffset\n\n\n\n\nx_0\n0.4661\n\n\nx_1\n0.3918\n\n\nx_2\n0.0571\n\n\nx_3\n0.9529\n\n\n\n\n\n$H$\n\n\n\nvariable\nx_0\nx_1\nx_2\nx_3\n\n\n\n\ny_0\n0.4984\n0.1597\n0.4458\n0.7349\n\n\ny_1\n0.1070\n0.9531\n0.1942\n0.6683\n\n\ny_2\n0.9186\n0.7123\n0.1806\n0.5045\n\n\n\n\n\n$R$\n\n\n\nvariable\ny_0\ny_1\ny_2\n\n\n\n\ny_0\n0.5856\n0.0190\n0.5991\n\n\ny_1\n0.0190\n1.3107\n0.5130\n\n\ny_2\n0.5991\n0.5130\n2.3748\n\n\n\n\n\n$d$\n\n\n\nvariable\noffset\n\n\n\n\ny_0\n0.9334\n\n\ny_1\n0.5645\n\n\ny_2\n0.0695\n\n\n\n\n\n$B$\n\n\n\nstate\nc_0\nc_1\nc_2\n\n\n\n\nx_0\n0.2827\n0.1138\n0.9378\n\n\nx_1\n0.5594\n0.9364\n0.5136\n\n\nx_2\n0.8592\n0.7647\n0.5183\n\n\nx_3\n0.2376\n0.5618\n0.5096\n\n\n\n\n\n$m_0$\n\n\n\nstate\nmean\n\n\n\n\nx_0\n0.8234\n\n\nx_1\n0.3850\n\n\nx_2\n0.3380\n\n\nx_3\n0.4376\n\n\n\n\n\n$P_0$\n\n\n\nstate\nx_0\nx_1\nx_2\nx_3\n\n\n\n\nx_0\n0.8860\n0.4535\n0.4490\n0.2316\n\n\nx_1\n0.4535\n1.8632\n0.6740\n1.3448\n\n\nx_2\n0.4490\n0.6740\n2.0374\n1.2929\n\n\nx_3\n0.2316\n1.3448\n1.2929\n2.0978\n\n\n\n\n\n \n\n\n\n\nTest data\n\nreset_seed()\ndata, mask, control = get_test_data()\nshow_as_row(data, mask, control)\n\n\n data tensor([[[0.8775,    nan,    nan],\n         [0.6706,    nan, 0.9272],\n         [   nan, 0.4967,    nan],\n         [   nan,    nan,    nan],\n         [   nan,    nan, 0.4760],\n         [0.7606, 0.7759, 0.5243],\n         [0.3714, 0.0426, 0.2343],\n         [0.9991, 0.1775,    nan],\n         [0.6734,    nan, 0.6468],\n         [0.5825, 0.4599, 0.7960]],\n\n        [[0.9038, 0.9735, 0.6428],\n         [0.3725, 0.2052,    nan],\n         [0.4448, 0.5775, 0.7237],\n         [0.5927,    nan, 0.6441],\n         [   nan, 0.9132, 0.0329],\n         [0.4856, 0.9927, 0.5895],\n         [0.2611, 0.9413, 0.1371],\n         [0.8726, 0.5590, 0.8451],\n         [0.1253, 0.9434, 0.0462],\n         [0.2360, 0.0239, 0.8950]]], dtype=torch.float64)\n mask tensor([[[ True, False, False],\n         [ True, False,  True],\n         [False,  True, False],\n         [False, False, False],\n         [False, False,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True, False],\n         [ True, False,  True],\n         [ True,  True,  True]],\n\n        [[ True,  True,  True],\n         [ True,  True, False],\n         [ True,  True,  True],\n         [ True, False,  True],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True]]])\n control tensor([[[0.9201, 0.6883, 0.5342],\n         [0.7847, 0.3137, 0.1778],\n         [0.5838, 0.9799, 0.3611],\n         [0.3155, 0.7475, 0.5450],\n         [0.5641, 0.2493, 0.8323],\n         [0.9723, 0.1883, 0.3605],\n         [0.5344, 0.3443, 0.7696],\n         [0.3410, 0.7553, 0.3177],\n         [0.0315, 0.5209, 0.6514],\n         [0.3131, 0.4510, 0.3550]],\n\n        [[0.4790, 0.0676, 0.3606],\n         [0.7299, 0.6713, 0.3134],\n         [0.7460, 0.1291, 0.4653],\n         [0.5693, 0.9906, 0.8288],\n         [0.9039, 0.5240, 0.6277],\n         [0.3574, 0.0076, 0.6530],\n         [0.8667, 0.9368, 0.8667],\n         [0.6749, 0.3526, 0.6618],\n         [0.0837, 0.7188, 0.7247],\n         [0.3211, 0.4898, 0.9030]]], dtype=torch.float64)"
  },
  {
    "objectID": "kalman_filter.html#filter",
    "href": "kalman_filter.html#filter",
    "title": "Kalman Filter",
    "section": "Filter",
    "text": "Filter\n\nFilter predict\nProbability of state at time t given state a time t-1\n\\(p(x_t) = \\mathcal{N}(x_t; m_t^-, P_t^-)\\) where:\n\npredicted state mean: \\(m_t^- = Am_{t-1} + B c_t + b\\)\npredicted state covariance: \\(P_t^- = AP_{t-1}A^T + Q\\)\n\n\nA, Q, b, B, m_pr,P_pr= (k.A, k.Q, k.b, k.B,torch.concat([k.m0]*2), torch.concat([k.P0]*2))\n\n\nm_pr.shape, P_pr.shape, A.shape\n\n(torch.Size([2, 4, 1]), torch.Size([2, 4, 4]), torch.Size([1, 4, 4]))\n\n\n\nCovariance\nimplement \\(P_t^- = AP_{t-1}A^T + Q\\)\n\nP_m = _filter_predict_cov_stand(A, Q, P_pr)\nP_m\n\ntensor([[[6.2873, 3.7655, 4.8718, 5.8589],\n         [3.7655, 4.8551, 4.7314, 5.1009],\n         [4.8718, 4.7314, 5.9373, 6.0160],\n         [5.8589, 5.1009, 6.0160, 8.5954]],\n\n        [[6.2873, 3.7655, 4.8718, 5.8589],\n         [3.7655, 4.8551, 4.7314, 5.1009],\n         [4.8718, 4.7314, 5.9373, 6.0160],\n         [5.8589, 5.1009, 6.0160, 8.5954]]], dtype=torch.float64,\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nMean\n\n\nPredict\n\nB.shape\n\ntorch.Size([1, 4, 3])\n\n\n\ncontrol.shape\n\ntorch.Size([2, 10, 3])\n\n\n\nB[0].shape\n\ntorch.Size([4, 3])\n\n\n\nB[0] @ control[0, 0].unsqueeze(-1)\n\ntensor([[0.7217],\n        [0.9572],\n        [0.5742],\n        [1.2138]], dtype=torch.float64, grad_fn=&lt;MmBackward0&gt;)\n\n\n\nm_m, P_m = _filter_predict(\n    A, Q, b, B,\n    m_pr,P_pr, control[:, 0, :].unsqueeze(-1))\n\n\nshow_as_row(m_m, P_m)\n\n\n m_m tensor([[[2.4986],\n         [2.2429],\n         [1.8833],\n         [3.6164]],\n\n        [[2.1377],\n         [1.5964],\n         [1.5371],\n         [2.8315]]], dtype=torch.float64, grad_fn=)\n P_m tensor([[[6.2873, 3.7655, 4.8718, 5.8589],\n         [3.7655, 4.8551, 4.7314, 5.1009],\n         [4.8718, 4.7314, 5.9373, 6.0160],\n         [5.8589, 5.1009, 6.0160, 8.5954]],\n\n        [[6.2873, 3.7655, 4.8718, 5.8589],\n         [3.7655, 4.8551, 4.7314, 5.1009],\n         [4.8718, 4.7314, 5.9373, 6.0160],\n         [5.8589, 5.1009, 6.0160, 8.5954]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\n(m_m.shape, P_m.shape,)\n\n(torch.Size([2, 4, 1]), torch.Size([2, 4, 4]))\n\n\n\n\n\nFilter update\nProbability of state at time t given the observations at time t\n\\(p(x_t|y_t) = \\mathcal{N}(x_t; m_t, P_t)\\) where:\n\npredicted obs mean: \\(z_t = Hm_t^- + d\\)\nprediced obs covariance: \\(S_t = HP_t^-H^T + R\\)\nkalman gain\\(K_t = P_t^-H^TS_t^{-1}\\)\ncorrected state mean: \\(m_t = m_t^- + K_t(y_t - z_t)\\)\ncorrected state covariance: \\(P_t = (I-K_tH)P_t^-\\)\n\nif the observation are missing this step is skipped and the corrected state is equal to the predicted state\nNeed to figure out the Nans for the gradients …\n\nKalman Gain\nDon’t compute the inverse of the matrix, but use cholesky_solve to invert the matrix\n\nH, d, R, obs = k.H, k.d, k.R, data[:,0,:].unsqueeze(-1)\n\n\nK = _filter_update_k_gain(H, R, P_m)\nK\n\ntensor([[[ 0.1177,  0.2313,  0.0612],\n         [-0.5573,  0.6582,  0.0288],\n         [-0.3189,  0.5919, -0.0174],\n         [ 0.3269,  0.3056, -0.0378]],\n\n        [[ 0.1177,  0.2313,  0.0612],\n         [-0.5573,  0.6582,  0.0288],\n         [-0.3189,  0.5919, -0.0174],\n         [ 0.3269,  0.3056, -0.0378]]], dtype=torch.float64,\n       grad_fn=&lt;TransposeBackward0&gt;)\n\n\n\ntest_close(_filter_update_k_gain(H, R, P_m), P_m @ H.mT @ torch.inverse(H @ P_m @ H.mT + R))\n\n\n\nCovariance\n\nP = _filter_update_cov(H, K, P_m)\nP\n\ntensor([[[ 2.1074, -0.2158,  0.3602,  0.1911],\n         [-0.2158,  0.4808,  0.0472, -0.1503],\n         [ 0.3602,  0.0472,  0.8035, -0.0177],\n         [ 0.1911, -0.1503, -0.0177,  0.8448]],\n\n        [[ 2.1074, -0.2158,  0.3602,  0.1911],\n         [-0.2158,  0.4808,  0.0472, -0.1503],\n         [ 0.3602,  0.0472,  0.8035, -0.0177],\n         [ 0.1911, -0.1503, -0.0177,  0.8448]]], dtype=torch.float64,\n       grad_fn=&lt;UnsafeViewBackward0&gt;)\n\n\n\n\nMean\n\nz = H @ m_m + d; z\n(obs - z)\n\ntensor([[[-3.3343],\n         [    nan],\n         [    nan]],\n\n        [[-2.4181],\n         [-5.0754],\n         [-3.9145]]], dtype=torch.float64, grad_fn=&lt;SubBackward0&gt;)\n\n\n\nm = _filter_update_mean(H, d, K, m_m, obs)\nm\n\ntensor([[[    nan],\n         [    nan],\n         [    nan],\n         [    nan]],\n\n        [[ 0.4396],\n         [-0.5094],\n         [-0.6276],\n         [ 0.6379]]], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;)\n\n\n\nm, P = _filter_update(H, d, R, m_m, P_m, obs)\nshow_as_row(m, P)\nm.shape, P.shape\n\n\n m tensor([[[    nan],\n         [    nan],\n         [    nan],\n         [    nan]],\n\n        [[ 0.4396],\n         [-0.5094],\n         [-0.6276],\n         [ 0.6379]]], dtype=torch.float64, grad_fn=)\n P tensor([[[ 2.1074, -0.2158,  0.3602,  0.1911],\n         [-0.2158,  0.4808,  0.0472, -0.1503],\n         [ 0.3602,  0.0472,  0.8035, -0.0177],\n         [ 0.1911, -0.1503, -0.0177,  0.8448]],\n\n        [[ 2.1074, -0.2158,  0.3602,  0.1911],\n         [-0.2158,  0.4808,  0.0472, -0.1503],\n         [ 0.3602,  0.0472,  0.8035, -0.0177],\n         [ 0.1911, -0.1503, -0.0177,  0.8448]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n(torch.Size([2, 4, 1]), torch.Size([2, 4, 4]))\n\n\nthere are nan in the output because there are nan in the observations\nThe next functions adds the support for missing obsevations by also using a mask\n\n\nMissing observations\nIf all the observations at time \\(t\\) are missing the correct step is skipped and the filtered state at time \\(t\\) () is the same of the filtered state.\nIf only some observations are missing a variation of equation can be used.\n\\(y^{ng}_t\\) is a vector containing the observations that are not missing at time \\(t\\).\nIt can be expressed as a linear transformation of \\(y_t\\)\n\\[ y^{ng}_t = My_t\\]\nwhere \\(M\\) is a mask matrix that is used to select the subset of \\(y_t\\) that is observed. \\(M \\in \\mathbb{R}^{n_{ng} \\times n}\\) and is made of columns which are made of all zeros but for an entry 1 at row corresponding to the non-missing observation. hence:\n\\[ p(y^{ng}_t) = \\mathcal{N}(M\\mu_{y_t},  M\\Sigma_{y_t}M^T)\\]\nfrom which you can derive\n\\[ p(y^{ng}_t|x_t) = p(MHx_t + Mb, MRM^T)  \\tag{1}\\]\nThen the posterior \\(p(x_t|y_t^{ng})\\) can be computed similarly of equation @filter_correct as:\n\\[ p(x_t|y^{ng}_t) = \\mathcal{N}(x_t; m_t, P_t)  \\tag{2}\\]\nwhere:\n\npredicted obs mean: \\(z_t = MHm_t^- + Md\\)\npredicted obs covariance: \\(S_t = MHP_t^-(MH)^T + MRM^T\\)\nKalman gain \\(K_t = P_t^-(MH)^TS_t^{-1}\\)\ncorrected state mean: \\(m_t = m_t^- + K_t(My_t - z_t)\\)\ncorrected state covariance: \\(P_t = (I-K_tMH)P_t^-\\)\n\n\nDetails implementation\nFor the implementation the matrix multiplication \\(MH\\) can be replaced with H[m] where m is the mask for the rows for H and \\(MRM^T\\) with R[m][:,m]\n\nH, R, d,obs, mm = k.H, k.R, k.d, data[:,0,:].unsqueeze(-1), mask[:,0,:].unsqueeze(-1)\n\n\nm = torch.tensor([False,True,True]) # mask batch\nM = torch.tensor([[[0,1,0], # mask matrix\n                  [0,0,1]]], dtype=torch.float64)\nshow_as_row(m, M, H, R)\n\n\n m tensor([False,  True,  True])\n M tensor([[[0., 1., 0.],\n         [0., 0., 1.]]], dtype=torch.float64)\n H Parameter containing:\ntensor([[[0.1349, 0.0519, 0.1180, 0.9767],\n         [0.1679, 0.8635, 0.3753, 0.9760],\n         [0.2125, 0.8049, 0.2124, 0.6794]]], dtype=torch.float64,\n       requires_grad=True)\n R tensor([[[1.6259, 1.1183, 0.8535],\n         [1.1183, 1.2831, 0.9935],\n         [0.8535, 0.9935, 2.4550]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\nM @ M.mT\n\ntensor([[[1., 0.],\n         [0., 1.]]], dtype=torch.float64)\n\n\n\nM @ H, H[:, m]\n\n(tensor([[[0.1679, 0.8635, 0.3753, 0.9760],\n          [0.2125, 0.8049, 0.2124, 0.6794]]], dtype=torch.float64,\n        grad_fn=&lt;UnsafeViewBackward0&gt;),\n tensor([[[0.1679, 0.8635, 0.3753, 0.9760],\n          [0.2125, 0.8049, 0.2124, 0.6794]]], dtype=torch.float64,\n        grad_fn=&lt;IndexBackward0&gt;))\n\n\n\nM @ R @ M.mT, R[:,m][:,:,m]\n\n(tensor([[[1.2831, 0.9935],\n          [0.9935, 2.4550]]], dtype=torch.float64,\n        grad_fn=&lt;UnsafeViewBackward0&gt;),\n tensor([[[1.2831, 0.9935],\n          [0.9935, 2.4550]]], dtype=torch.float64, grad_fn=&lt;IndexBackward0&gt;))\n\n\nBy using partially missing observations _filter_update cannot be easily batched as the shape of the intermediate variables depends on the number of observed variables. So the idea is to divide the batch in blocks that share the same number of variables missing.\n\nmask_values, indices = torch.unique(mask[:,1,:], dim=0, return_inverse=True)\nmask_values, indices\n\n(tensor([[ True, False,  True],\n         [ True,  True, False]]),\n tensor([0, 1]))\n\n\n\n\nUpdate mask\n\nmm = mask[0,0,:]\n\n\nH[:, mm].shape, d[:, mm].shape, R[:, mm][:, :,mm].shape, obs[:, mm].shape\n\n(torch.Size([1, 1, 4]),\n torch.Size([1, 1, 1]),\n torch.Size([1, 1, 1]),\n torch.Size([2, 1, 1]))\n\n\n\nshow_as_row(*_filter_update_mask(H, d, R, m_m, P_m, obs, mask[0, 0, :] ))\n\n\n #0 tensor([[[0.7184],\n         [0.7151],\n         [0.0696],\n         [1.1526]],\n\n        [[0.8467],\n         [0.4883],\n         [0.2217],\n         [1.0446]]], dtype=torch.float64, grad_fn=)\n #1 tensor([[[2.3679, 0.4016, 0.8785, 0.4342],\n         [0.4016, 1.9680, 1.3041, 0.4451],\n         [0.8785, 1.3041, 1.8687, 0.4890],\n         [0.4342, 0.4451, 0.4890, 1.0874]],\n\n        [[2.3679, 0.4016, 0.8785, 0.4342],\n         [0.4016, 1.9680, 1.3041, 0.4451],\n         [0.8785, 1.3041, 1.8687, 0.4890],\n         [0.4342, 0.4451, 0.4890, 1.0874]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\nm, P = _filter_update_mask(H, d, R, m_m, P_m, obs, mask[0, 0, :] )\nm.shape, P.shape\n\n(torch.Size([2, 4, 1]), torch.Size([2, 4, 4]))\n\n\n\n\nUpdate mask batch\n\nm, P = _filter_update_mask_batch(H, d, R, m_m, P_m, obs, mask[:,0,:] )\nshow_as_row(m, P)\nm.shape, P.shape\n\n\n m tensor([[[ 0.7184],\n         [ 0.7151],\n         [ 0.0696],\n         [ 1.1526]],\n\n        [[ 0.4396],\n         [-0.5094],\n         [-0.6276],\n         [ 0.6379]]], dtype=torch.float64, grad_fn=)\n P tensor([[[ 2.3679,  0.4016,  0.8785,  0.4342],\n         [ 0.4016,  1.9680,  1.3041,  0.4451],\n         [ 0.8785,  1.3041,  1.8687,  0.4890],\n         [ 0.4342,  0.4451,  0.4890,  1.0874]],\n\n        [[ 2.1074, -0.2158,  0.3602,  0.1911],\n         [-0.2158,  0.4808,  0.0472, -0.1503],\n         [ 0.3602,  0.0472,  0.8035, -0.0177],\n         [ 0.1911, -0.1503, -0.0177,  0.8448]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n(torch.Size([2, 4, 1]), torch.Size([2, 4, 4]))\n\n\n\nm.sum().backward(retain_graph=True) # check that pytorch can compute gradients with the whole batch and gradients aren't nan\nH.grad\n\ntensor([[[-1.1769, -2.7620, -0.9158, -2.4954],\n         [-2.0431,  0.8269,  0.5074, -1.5867],\n         [ 0.1472,  0.0285,  0.1012,  0.0356]]], dtype=torch.float64)\n\n\n\n\n\n\nFilter All\nThe resursive version of the kalman filter is apperently breaking pytorch gradients calculations so a workaround is needed. During the loop the states are saved in a python list and then at the end they are combined back into a tensor. The last line of the function does:\n\nconvert lists to tensors\ncorrect order dimensions\n\n\nfilt_state, pred_state  = k._filter_all(data, mask, control)\n\n\n(ms, Ps), (m_ms, P_ms) = filt_state, pred_state\n\nPredictions at time 0 for both batches\n\nshow_as_row(*map(Self.shape(), (m_ms, P_ms, ms, Ps,)))\n\n\n #0 torch.Size([2, 10, 4, 1])\n #1 torch.Size([2, 10, 4, 4])\n #2 torch.Size([2, 10, 4, 1])\n #3 torch.Size([2, 10, 4, 4])\n \n\n\n\nshow_as_row(*map(lambda x:x[0][0], (m_ms, P_ms, ms, Ps,)))\n\n\n #0 tensor([[0.6498],\n        [0.3257],\n        [0.8462],\n        [0.7727]], dtype=torch.float64, grad_fn=)\n #1 tensor([[0.7511, 0.6229, 0.6414, 0.4561],\n        [0.6229, 1.2138, 0.6974, 0.9224],\n        [0.6414, 0.6974, 1.4388, 1.4051],\n        [0.4561, 0.9224, 1.4051, 3.1638]], dtype=torch.float64,\n       grad_fn=)\n #2 tensor([[0.6392],\n        [0.3073],\n        [0.8191],\n        [0.7180]], dtype=torch.float64, grad_fn=)\n #3 tensor([[0.6695, 0.4821, 0.4340, 0.0368],\n        [0.4821, 0.9707, 0.3394, 0.1987],\n        [0.4340, 0.3394, 0.9115, 0.3391],\n        [0.0368, 0.1987, 0.3391, 1.0092]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\n\nFilter\nThe filter methods wraps _filter_all but in addition:\n\nreturns only filtered state\nremove last dimensions from mean\n\n\nsource\n\n\nKalmanFilter.filter\n\n KalmanFilter.filter (obs:torch.Tensor, mask:torch.Tensor,\n                      control:torch.Tensor)\n\nFilter observation\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nobs\nTensor\n([n_batches], n_obs, [self.n_dim_obs]) where n_batches and n_dim_obs dimensions can be omitted if 1\n\n\nmask\nTensor\n([n_batches], n_obs, [self.n_dim_obs]) where n_batches and n_dim_obs dimensions can be omitted if 1\n\n\ncontrol\nTensor\n([n_batches], n_obs, [self.n_dim_contr])\n\n\nReturns\nListMultiNormal\nFiltered state (n_batches, n_obs, self.n_dim_state)\n\n\n\n\nfilt = k.filter(data, mask, control)\nfilt.mean.shape, filt.cov.shape\n\n(torch.Size([2, 10, 4, 1]), torch.Size([2, 10, 4, 4]))"
  },
  {
    "objectID": "kalman_filter.html#smooth",
    "href": "kalman_filter.html#smooth",
    "title": "Kalman Filter",
    "section": "Smooth",
    "text": "Smooth\n\nSmooth update step\ncompute the probability of the state at time t given all the observations\n\\(p(x_t|Y) = \\mathcal{N}(x_t; m_t^s, P_t^s)\\) where:\n\nKalman smoothing gain: \\(G_t = P_tA^T(P_{t+1}^-)^{-1}\\)\nsmoothed mean: \\(m_t^s = m_t + G_t(m_{t+1}^s - m_{t+1}^-)\\)\nsmoothed covariance: \\(P_t^s = P_t + G_t(P_{t+1}^s - P_{t+1}^-)G_t^T\\)\n\n\ntest_close(_smooth_gain(A, filt_state.cov, pred_state.cov), filt_state.cov @ A.mT @ torch.inverse(pred_state.cov))\n\n\nK_p = _smooth_gain(A, filt_state[:,0].cov, pred_state[:,0].cov)\nK_p.shape\n\ntorch.Size([2, 4, 4])\n\n\n\n_smooth_mean(K_p, filt_state[:,0].mean, pred_state[:,0].mean, filt_state[:,0].mean)\n\ntensor([[[ 0.6254],\n         [ 0.2922],\n         [ 0.7965],\n         [ 0.6964]],\n\n        [[-0.1829],\n         [-0.7088],\n         [-0.0211],\n         [ 0.2570]]], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;)\n\n\n\n_smooth_cov(K_p, filt_state[:,0].cov, pred_state[:,0].cov, filt_state[:,0].cov)\n\ntensor([[[ 0.5327,  0.3321,  0.2094, -0.1773],\n         [ 0.3321,  0.8061,  0.0930, -0.0363],\n         [ 0.2094,  0.0930,  0.5427, -0.0125],\n         [-0.1773, -0.0363, -0.0125,  0.6738]],\n\n        [[ 0.3342,  0.0671,  0.0652, -0.2001],\n         [ 0.0671,  0.3478, -0.1115, -0.1245],\n         [ 0.0652, -0.1115,  0.4549, -0.0025],\n         [-0.2001, -0.1245, -0.0025,  0.6981]]], dtype=torch.float64,\n       grad_fn=&lt;DivBackward0&gt;)\n\n\n\nshow_as_row(*_smooth_update(A, filt_state[:, 0], pred_state[:, 0], filt_state[:, 0]))\n\n\n #0 tensor([[[ 0.6254],\n         [ 0.2922],\n         [ 0.7965],\n         [ 0.6964]],\n\n        [[-0.1829],\n         [-0.7088],\n         [-0.0211],\n         [ 0.2570]]], dtype=torch.float64, grad_fn=)\n #1 tensor([[[ 0.5327,  0.3321,  0.2094, -0.1773],\n         [ 0.3321,  0.8061,  0.0930, -0.0363],\n         [ 0.2094,  0.0930,  0.5427, -0.0125],\n         [-0.1773, -0.0363, -0.0125,  0.6738]],\n\n        [[ 0.3342,  0.0671,  0.0652, -0.2001],\n         [ 0.0671,  0.3478, -0.1115, -0.1245],\n         [ 0.0652, -0.1115,  0.4549, -0.0025],\n         [-0.2001, -0.1245, -0.0025,  0.6981]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\nshow_as_row(*map(Self.shape(), _smooth_update(A, MNormal(m_m, P_m), MNormal(m_m, P_m), MNormal(m_m, P_m))))\n\n\n #0 torch.Size([2, 4, 1])\n #1 torch.Size([2, 4, 4])\n \n\n\n\n\nSmooth\n\nsmooth_state = _smooth(k.A,  filt_state, pred_state)\n\n\nshow_as_row(smooth_state.mean[0][0], smooth_state.cov[0][0])\n\n\n #0 tensor([[-0.0785],\n        [-0.5839],\n        [-0.0959],\n        [ 0.0031]], dtype=torch.float64, grad_fn=)\n #1 tensor([[ 0.4578,  0.2300,  0.1755, -0.1481],\n        [ 0.2300,  0.6676,  0.0346, -0.0171],\n        [ 0.1755,  0.0346,  0.5665,  0.0642],\n        [-0.1481, -0.0171,  0.0642,  0.7652]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\nshow_as_row(smooth_state.mean.shape, smooth_state.cov.shape)\n\n\n #0 torch.Size([2, 10, 4, 1])\n #1 torch.Size([2, 10, 4, 4])\n \n\n\n\n\nKalmanFilter method\n\nsource\n\n\nKalmanFilter.smooth\n\n KalmanFilter.smooth (obs:torch.Tensor, mask:torch.Tensor,\n                      control:torch.Tensor)\n\nKalman Filter Smoothing\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nobs\nTensor\n\n\n\nmask\nTensor\n\n\n\ncontrol\nTensor\n\n\n\nReturns\nListMultiNormal\n[n_timesteps, n_dim_state] smoothed state\n\n\n\n\nsmoothed_state = k.smooth(data, mask, control)\n\n\nshow_as_row(smoothed_state.mean.shape, smoothed_state.cov.shape)\n\n\n #0 torch.Size([2, 10, 4, 1])\n #1 torch.Size([2, 10, 4, 4])\n \n\n\n\nsmoothed_state.mean.sum().backward(retain_graph=True)\nA.grad\n\ntensor([[[-4.9495, -5.2939, -7.8642,  0.2546],\n         [ 0.2445,  8.3747, 13.5693, -7.4543],\n         [-5.4811, -5.2497, -6.6092, -1.5187],\n         [-1.5874,  2.1945,  3.6410, -2.3143]]], dtype=torch.float64)"
  },
  {
    "objectID": "kalman_filter.html#predict-1",
    "href": "kalman_filter.html#predict-1",
    "title": "Kalman Filter",
    "section": "Predict",
    "text": "Predict\nThe prediction at time t (\\(y_t\\)) are computed rom the state (\\(x_t\\)) using this formula: \\[p(y_t|x_t) = \\mathcal{N}(Hx_t + d, R + HP^s_tH^T)\\]\nthis works both if the state was filtered or smoother\nThis add the supports for conditional predictions, which means that at the time (t) when we are making the predictions some of the variables have been actually observed. Since the model prediction is a normal distribution we can condition on the observed values and thus improve the predictions. See conditional_gaussian\nIn order to have conditional predictions that make sense it’s not possible to return the full covariance matrix for the predictions but only the standard deviations\n\ntest_m = torch.tensor(\n    [[True, True, True,],\n    [False, True, True],\n    [False, False, False]]\n)\n\n\ntorch.logical_xor(test_m.all(-1), test_m.any(-1))\n\ntensor([False,  True, False])\n\n\n\nA = torch.rand(2,2,3,3)\n\n\n(A @ A).shape\n\ntorch.Size([2, 2, 3, 3])\n\n\npredict can be vectorized across both the batch and the timesteps, except for timesteps that require conditional predictions\n\nsmoothed_state.mean.shape, smoothed_state.cov.shape\n\n(torch.Size([2, 10, 4, 1]), torch.Size([2, 10, 4, 4]))\n\n\n\n(k.H @ smoothed_state.mean).shape\n\ntorch.Size([2, 10, 3, 1])\n\n\n\npred_obs0 = k._obs_from_state(smoothed_state)\npred_obs0.mean.shape\n\ntorch.Size([2, 10, 3])\n\n\n\npred_obs0.cov.shape\n\ntorch.Size([2, 10, 3, 3])\n\n\n\nsource\n\nKalmanFilter.predict\n\n KalmanFilter.predict (obs, mask, control, smooth=True)\n\nPredicted observations at all times\n\npred = k.predict(data, mask, control)\n\n\npred.mean.shape, pred.std.shape\n\n(torch.Size([2, 10, 3]), torch.Size([2, 10, 3]))\n\n\nGradients …\n\ndef get_grad_mask(x):\n    \"filter gradient after sub the masks value with x\"\n    d = data.clone()\n    d[~mask] = x\n    k.predict(data, mask, control).mean.sum().backward(retain_graph=True)\n    grad = k.R_raw.grad.clone()\n    k.zero_grad() \n    return grad\n\n\nget_grad_mask(10)\n\ntensor([[[ -2.5082,   0.0000,   0.0000],\n         [-11.7667,  -0.1206,   0.0000],\n         [ -7.6345,  -8.3390,  -4.0905]]], dtype=torch.float64)\n\n\n\ntest_close(get_grad_mask(1), get_grad_mask(10))\n\n\nsource\n\n\nKalmanFilter.predict\n\n KalmanFilter.predict (obs, mask, control, smooth=True)\n\nPredicted observations at all times\n\n@patch\ndef predict_times(self: KalmanFilter, times, obs, mask=None, smooth=True, check_args=None):\n    \"\"\"Predicted observations at specific times \"\"\"\n    state = self.smooth(obs, mask, check_args) if smooth else self.filter(obs, mask, check_args)\n    obs, mask = self._parse_obs(obs, mask)\n    times = array1d(times)\n    \n    n_timesteps = obs.shape[0]\n    n_features = obs.shape[1] if len(obs.shape) &gt; 1 else 1\n    \n    if times.max() &gt; n_timesteps or times.min() &lt; 0:\n        raise ValueError(f\"provided times range from {times.min()} to {times.max()}, which is outside allowed range : 0 to {n_timesteps}\")\n\n    means = torch.empty((times.shape[0], n_features), dtype=obs.dtype, device=obs.device)\n    stds = torch.empty((times.shape[0], n_features), dtype=obs.dtype, device=obs.device) \n    for i, t in enumerate(times):\n        mean, std = self._obs_from_state(\n            state.mean[t],\n            state.cov[t],\n            {'t': t, **check_args} if check_args is not None else None\n        )\n        \n        means[i], stds[i] = _get_cond_pred(ListNormal(mean, std), obs[t], mask[t])\n    \n    return ListNormal(means, stds)"
  },
  {
    "objectID": "kalman_filter.html#filter-2",
    "href": "kalman_filter.html#filter-2",
    "title": "Kalman Filter",
    "section": "Filter",
    "text": "Filter\n\nFilter predict\n\nCovariance\nImplement the numerical stable version of the covariance update\n\nA, Q, b, B, m_pr,P_pr= (k.A, k.Q, k.b, k.B,torch.concat([k.m0]*2), torch.concat([k.P0]*2))\n\n\nQ_C = kSR.Q_C\n\n\n_filter_predict_cov_stand(kSR.A, kSR.Q_C @ kSR.Q_C.mT, P_pr)\n\ntensor([[[1.9504, 2.3535, 2.1727, 2.2936],\n         [2.3535, 5.8436, 5.0464, 6.0831],\n         [2.1727, 5.0464, 6.2940, 6.0969],\n         [2.2936, 6.0831, 6.0969, 8.1538]],\n\n        [[1.9504, 2.3535, 2.1727, 2.2936],\n         [2.3535, 5.8436, 5.0464, 6.0831],\n         [2.1727, 5.0464, 6.2940, 6.0969],\n         [2.2936, 6.0831, 6.0969, 8.1538]]], dtype=torch.float64,\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\nP_pr_C = torch.linalg.cholesky(P_pr)\n\n\\[W = \\begin{bmatrix}AC_{t-1}&C_Q\\end{bmatrix}\\]\n\nW = torch.concat([A @ P_pr_C, Q_C.expand_as(P_pr_C)], dim=-1)\nW.shape\n\ntorch.Size([2, 4, 8])\n\n\n\nP_m_C = torch.linalg.qr(W.mT).R.mT\n\n\nP_m_C\n\ntensor([[[-2.5709,  0.0000,  0.0000,  0.0000],\n         [-1.7904,  1.1590,  0.0000,  0.0000],\n         [-1.9907,  1.3402,  1.2995,  0.0000],\n         [-2.2985,  0.5614,  0.8125, -1.0728]],\n\n        [[-2.5709,  0.0000,  0.0000,  0.0000],\n         [-1.7904,  1.1590,  0.0000,  0.0000],\n         [-1.9907,  1.3402,  1.2995,  0.0000],\n         [-2.2985,  0.5614,  0.8125, -1.0728]]], dtype=torch.float64,\n       grad_fn=&lt;TransposeBackward0&gt;)\n\n\n\nP_m_C @ P_m_C.mT\n\ntensor([[[6.6096, 4.6031, 5.1178, 5.9093],\n         [4.6031, 4.5489, 5.1174, 4.7660],\n         [5.1178, 5.1174, 7.4476, 6.3838],\n         [5.9093, 4.7660, 6.3838, 7.4094]],\n\n        [[6.6096, 4.6031, 5.1178, 5.9093],\n         [4.6031, 4.5489, 5.1174, 4.7660],\n         [5.1178, 5.1174, 7.4476, 6.3838],\n         [5.9093, 4.7660, 6.3838, 7.4094]]], dtype=torch.float64,\n       grad_fn=&lt;UnsafeViewBackward0&gt;)\n\n\n\nP_m = _filter_predict_cov_stand(A, Q_C @ Q_C.mT, P_pr)\n\n\ntest_close(P_m, P_m_C @ P_m_C.mT)\n\n\n(P_m - P_m_C @ P_m_C.mT).max()\n\ntensor(8.8818e-16, dtype=torch.float64, grad_fn=&lt;MaxBackward1&gt;)\n\n\n\ntest_P_m_C = torch.linalg.cholesky(P_m)\n\n\n(test_P_m_C @ test_P_m_C.mT - P_m_C @ P_m_C.mT).max()\n\ntensor(8.8818e-16, dtype=torch.float64, grad_fn=&lt;MaxBackward1&gt;)\n\n\n\n(test_P_m_C - P_m_C).max()\n\ntensor(5.1418, dtype=torch.float64, grad_fn=&lt;MaxBackward1&gt;)\n\n\nCholesky decomposition is not unique! but the solution is correct\n\nP_m_C = _filter_predict_cov_SR(A, Q_C, P_pr_C)\ntest_close(P_m_C @ P_m_C.mT, _filter_predict_cov_stand(A, Q_C @ Q_C.mT, P_pr))\n\n\ndef fuzz_filter_predict_cov_SR(n=10):\n    for _ in range(n):\n        kSR = KalmanFilterSR.init_random(5,10,4)\n        A, Q_C, b, B, m_pr,P_pr = (kSR.A.unsqueeze(0), kSR.Q_C.unsqueeze(0), kSR.b.unsqueeze(-1),\n                                                  kSR.B.unsqueeze(0),\n                                                  torch.stack([kSR.m0]*2).unsqueeze(-1),\n                                                  torch.stack([kSR.P0]*2))\n        P_pr_C = torch.linalg.cholesky(P_pr)\n        P_m_C = _filter_predict_cov_SR(A, Q_C, P_pr_C)\n        test_close(P_m_C @ P_m_C.mT, _filter_predict_cov_stand(A, Q_C @ Q_C.mT, P_pr), eps=5e-13)\n\n\nfuzz_filter_predict_cov_SR()\n\n\n\nPredict\n\nm_m, P_m_C = _filter_predict_SR(kSR.A, kSR.Q_C, kSR.b, kSR.B, m_pr, P_pr_C, control[:,0].unsqueeze(-1)) \nshow_as_row(m_m, P_m_C)\n\n\n m_m tensor([[[1.9549],\n         [2.4811],\n         [3.3275],\n         [3.5545]],\n\n        [[1.1647],\n         [2.3109],\n         [2.4892],\n         [2.8133]]], dtype=torch.float64, grad_fn=)\n P_m_C tensor([[[-1.3966,  0.0000,  0.0000,  0.0000],\n         [-1.6852, -1.7331,  0.0000,  0.0000],\n         [-1.5557, -1.3991,  1.3843,  0.0000],\n         [-1.6423, -1.9130,  0.6253, -1.1858]],\n\n        [[-1.3966,  0.0000,  0.0000,  0.0000],\n         [-1.6852, -1.7331,  0.0000,  0.0000],\n         [-1.5557, -1.3991,  1.3843,  0.0000],\n         [-1.6423, -1.9130,  0.6253, -1.1858]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\nis_posdef(P_m_C @ P_m_C.mT).all()\n\ntensor(True)\n\n\n\n\n\nFilter Update\n\ndef is_sr(x_C, x): return torch.allclose(x_C @ x_C.mT, x)\n\n\nsource\n\n\ncat_2d\n\n cat_2d (x)\n\n\n\n\n\nDetails\n\n\n\n\nx\nmatrix as list of list of Tensor\n\n\n\n\nx = [[]]\n\n\nExample calculations\n\nH, R, R_C, obs = kSR.H, kSR.R, kSR.R_C, data[:,0,:].unsqueeze(-1)\nP_m = P_m_C @ P_m_C.mT\n\n# use standard filter to compute expected result\nS = H @ P_m @ H.mT + R\nS_C = torch.linalg.cholesky(S)\nK = _filter_update_k_gain(H, R, P_m)\nK_bar = K @ S_C\nP_stand = _filter_update_cov(H, K, P_m)\nP_C_stand = torch.linalg.cholesky(P_stand)\n\n\nassert all([is_sr(R_C, R), is_sr(S_C, S)])\n\n\\[M = \\begin{bmatrix} R^{1/2} & H(P^-)^{1/2} \\\\ 0 & (P^-)^{1/2} \\end{bmatrix}\\]\n\nM = cat_2d([[R_C.expand(2,-1,-1)              , H @ P_m_C], \n            [torch.zeros_like((H @ P_m_C).mT),  P_m_C]])\n\n\nM[0]\n\ntensor([[ 1.2478,  0.0000,  0.0000, -1.2662, -1.1753,  0.7242, -0.3448],\n        [ 0.3190,  1.2801,  0.0000, -2.1349, -1.5926,  1.0952, -0.3843],\n        [ 0.0341,  0.9601,  0.7766, -3.2461, -2.1290,  0.8250, -0.5179],\n        [ 0.0000,  0.0000,  0.0000, -1.3966,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000, -1.6852, -1.7331,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000, -1.5557, -1.3991,  1.3843,  0.0000],\n        [ 0.0000,  0.0000,  0.0000, -1.6423, -1.9130,  0.6253, -1.1858]],\n       dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\\[V = \\begin{bmatrix} S^{1/2} & 0 \\\\ \\bar{K} & P^{1/2} \\end{bmatrix}\\]\n\n# V from standard filter\nV_stand = cat_2d([[S_C,   torch.zeros_like(K_bar.mT)],\n                  [K_bar, P_C_stand]])\n\n\nV_stand[0]\n\ntensor([[ 2.2770,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 2.5904,  1.8631,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 3.2634,  2.2593,  1.3379,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.7766,  0.5205,  0.6151,  0.8355,  0.0000,  0.0000,  0.0000],\n        [ 1.8316,  0.8659,  0.9168, -0.1000,  0.9427,  0.0000,  0.0000],\n        [ 2.0275,  0.9734,  0.2654, -0.0859,  0.2527,  1.0461,  0.0000],\n        [ 2.2790,  0.9606,  0.6923, -0.4813,  0.4183,  0.2013,  1.0540]],\n       dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\ntest_close(M @ M.mT, V_stand @ V_stand.mT)\n\n\nV_sr = torch.linalg.qr(M.mT).R.mT\n\n\nV_sr[0]\n\ntensor([[-2.2770,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-2.5904, -1.8631,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-3.2634, -2.2593, -1.3379,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.7766, -0.5205, -0.6151,  0.8355,  0.0000,  0.0000,  0.0000],\n        [-1.8316, -0.8659, -0.9168, -0.1000,  0.9427,  0.0000,  0.0000],\n        [-2.0275, -0.9734, -0.2654, -0.0859,  0.2527, -1.0461,  0.0000],\n        [-2.2790, -0.9606, -0.6923, -0.4813,  0.4183, -0.2013, -1.0540]],\n       dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\ntest_close(V_sr @ V_sr.mT, V_stand @ V_stand.mT)\n\n\nn_dim_obs = R_C.shape[-1]\nP_C = V_sr[:, n_dim_obs:, n_dim_obs:]\nP_C\n\ntensor([[[ 0.8355,  0.0000,  0.0000,  0.0000],\n         [-0.1000,  0.9427,  0.0000,  0.0000],\n         [-0.0859,  0.2527, -1.0461,  0.0000],\n         [-0.4813,  0.4183, -0.2013, -1.0540]],\n\n        [[ 0.8355,  0.0000,  0.0000,  0.0000],\n         [-0.1000,  0.9427,  0.0000,  0.0000],\n         [-0.0859,  0.2527, -1.0461,  0.0000],\n         [-0.4813,  0.4183, -0.2013, -1.0540]]], dtype=torch.float64,\n       grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nis_sr(P_C, P_stand)\n\nTrue\n\n\n\\(P_C\\) computed with the QR decomposition is not the same from the cholesky decomposition, but that’s okay! They are all valid square roots of the posterior covariance\n\n(P_C == P_C_stand).all()\n\ntensor(False)\n\n\n\n(H @ P_m_C).shape\n\ntorch.Size([2, 3, 4])\n\n\n\n\nCovariance\n\nsource\n\n\n\ntensor_info\n\n tensor_info (x)\n\n\nP_C, S_C = _filter_update_cov_SR(kSR.H, kSR.R_C, P_m_C)\n\n\ndef fuzz_filter_update_cov_SR(n=10):\n    errs = []\n    for _ in range(n):\n        kSR = KalmanFilterSR.init_random(5,10,4)\n        H, R_C, P_m = (kSR.H, kSR.R_C, torch.cat([kSR.P0]*5))\n        R = R_C @ R_C.mT\n        P_m_C = torch.linalg.cholesky(P_m)\n        P_C, _ = _filter_update_cov_SR(H, R_C, P_m_C)\n        K = _filter_update_k_gain(H, R, P_m)\n        P_stand = _filter_update_cov(H, K, P_m)\n        errs.append((P_C @ P_C.mT -  P_stand).abs().max().item())\n    return torch.tensor(errs)\n\n\nerr = fuzz_filter_update_cov_SR(100)\nassert err.max() &lt; torch.tensor(1e-10)\nerr.median(), err.max()\n\n(tensor(2.8588e-15), tensor(1.0214e-14))\n\n\n\nKalman Gain\nDon’t compute the inverse of the matrix, but use cholesky_solve to invert the matrix\n\nS = kSR.H @ P_m_C @ P_m_C.mT @ kSR.H.mT + kSR.R\n\n\ntest_close(kSR.R, kSR.R_C @ kSR.R_C.mT)\n\n\ntest_close(S_C @ S_C.mT, S)\n\n\n_filter_update_k_gain_SR(H, P_m_C, S_C)\n\ntensor([[[-0.0014, -0.2782,  0.4598],\n         [ 0.2389, -0.3662,  0.6852],\n         [ 0.2854,  0.2819,  0.1984],\n         [ 0.3866, -0.1119,  0.5174]],\n\n        [[-0.0014, -0.2782,  0.4598],\n         [ 0.2389, -0.3662,  0.6852],\n         [ 0.2854,  0.2819,  0.1984],\n         [ 0.3866, -0.1119,  0.5174]]], dtype=torch.float64,\n       grad_fn=&lt;TransposeBackward0&gt;)\n\n\n\ntest_close(_filter_update_k_gain(H, R, P_m), _filter_update_k_gain_SR(H, P_m_C, S_C))\n\n\ndef fuzz_kalman_gain_SR(n=10):\n    errs = []\n    for _ in range(n):\n        kSR = KalmanFilterSR.init_random(5,10,4)\n        H, R_C, P_m = (kSR.H, kSR.R_C, torch.cat([kSR.P0]*5))\n        R = R_C @ R_C.mT\n        P_m_C = torch.linalg.cholesky(P_m)\n        P_C, S_C = _filter_update_cov_SR(H, R_C, P_m_C)\n        K_stand = _filter_update_k_gain(H, R, P_m)\n        K = _filter_update_k_gain_SR(H, P_m_C, S_C)\n        errs.append((K_stand - K).abs().max().item())\n    return torch.tensor(errs)\n\n\nerr = fuzz_kalman_gain_SR(100)\nassert err.max() &lt; torch.tensor(1e-10)\nerr.median(), err.max()\n\n(tensor(4.5797e-15), tensor(2.5202e-14))\n\n\n\n\nMeasurement update\n\nm, P_C = _filter_update_SR(H, d, R_C, m_m, P_m_C, obs)\nshow_as_row(m, P_C)\nm.shape, P_C.shape\n\n\n m tensor([[[   nan],\n         [   nan],\n         [   nan],\n         [   nan]],\n\n        [[0.1197],\n         [0.3457],\n         [0.5033],\n         [0.6044]]], dtype=torch.float64, grad_fn=)\n P_C tensor([[[ 0.8355,  0.0000,  0.0000,  0.0000],\n         [-0.1000,  0.9427,  0.0000,  0.0000],\n         [-0.0859,  0.2527, -1.0461,  0.0000],\n         [-0.4813,  0.4183, -0.2013, -1.0540]],\n\n        [[ 0.8355,  0.0000,  0.0000,  0.0000],\n         [-0.1000,  0.9427,  0.0000,  0.0000],\n         [-0.0859,  0.2527, -1.0461,  0.0000],\n         [-0.4813,  0.4183, -0.2013, -1.0540]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n(torch.Size([2, 4, 1]), torch.Size([2, 4, 4]))\n\n\n\nget_test_data(1, 5,4, bs=1)[1].shape\n\ntorch.Size([1, 1, 5])\n\n\n\ndef fuzz_filter_update_SR(n=10):\n    errs = {'mean': [], 'cov': []}\n    for _ in range(n):\n        kSR = KalmanFilterSR.init_random(5,10,4)\n        H, d, R, R_C, m_m, P_m = (kSR.H, kSR.d, kSR.R, kSR.R_C, torch.cat([kSR.m0]*5), torch.cat([kSR.P0]*5))\n        obs = torch.randn_like(H @ m_m)\n        P_m_C = torch.linalg.cholesky(P_m)\n        mSR, P_m_C = _filter_update_SR(H, d, R_C, m_m, P_m_C, obs)\n        m, P_C = _filter_update(H, d, R, m_m, P_m, obs)\n        errs['mean'].append((mSR - m).abs().max().item())\n        errs['cov'].append((P_C - P_m_C @ P_m_C.mT).abs().max().item())\n    return pd.DataFrame(errs)\n\nerr = fuzz_filter_update_SR(100)\n\nerr.median(), err.max()\n\n(mean    1.049161e-14\n cov     2.740863e-15\n dtype: float64,\n mean    5.573320e-14\n cov     1.099121e-14\n dtype: float64)\n\n\n\n\nMissing observations\n\nUpdate mask\nHere need to compute the square root of \\(R\\), because cannot apply the mask to \\(R^{1/2}\\)\n\nR\n\ntensor([[[1.5571, 0.3980, 0.0426],\n         [0.3980, 1.7403, 1.2398],\n         [0.0426, 1.2398, 1.5260]]], dtype=torch.float64,\n       grad_fn=&lt;UnsafeViewBackward0&gt;)\n\n\n\nis_posdef(R)\n\ntensor([True])\n\n\n\nR_m = torch.tensor([[1.5571,  0.0426], [0.0426, 1.5259]])\n\n\nR_m\n\ntensor([[1.5571, 0.0426],\n        [0.0426, 1.5259]])\n\n\n\nis_posdef(R_m)\n\ntensor(True)\n\n\n\nm = [True, False, True]\n\n\nis_posdef(R[:, m,:][:, :, m])\n\ntensor([True])\n\n\n\nH_m, d_m, R_m, R_C_m, obs_m, = H[:, m,:], d[:, m,:], R[:, m,:][:, :,m], R_C[:, m,:][:, :,m], obs[:, m]\n\n\nR2 = R_m\nR2_C_m = torch.linalg.cholesky(R_m)\n\n\nis_sr(R_C_m, R_m)\n\nFalse\n\n\n\n_filter_update_SR(H_m, d_m, R_C_m, m_m, P_m_C, obs_m)[0] - _filter_update(H_m, d_m, R_m, m_m, P_m_C @ P_m_C.mT, obs_m)[0]\n\ntensor([[[    nan],\n         [    nan],\n         [    nan],\n         [    nan]],\n\n        [[-0.1152],\n         [-0.1834],\n         [-0.1472],\n         [-0.1785]]], dtype=torch.float64, grad_fn=&lt;SubBackward0&gt;)\n\n\n\n_filter_update_SR(H_m, d_m, R2_C_m, m_m, P_m_C, obs_m)[0] - _filter_update(H_m, d_m, R_m, m_m, P_m_C @ P_m_C.mT, obs_m)[0]\n\ntensor([[[        nan],\n         [        nan],\n         [        nan],\n         [        nan]],\n\n        [[-4.4409e-16],\n         [-4.4409e-16],\n         [ 0.0000e+00],\n         [-8.8818e-16]]], dtype=torch.float64, grad_fn=&lt;SubBackward0&gt;)\n\n\n\nshow_as_row(*_filter_update_mask_SR(H, d, R_C, m_m, P_m_C, obs, mask[0, 0, :] ))\n\n\n #0 tensor([[[1.3313],\n         [1.0104],\n         [1.6996],\n         [1.7246]],\n\n        [[0.7732],\n         [1.3875],\n         [1.4670],\n         [1.6642]]], dtype=torch.float64, grad_fn=)\n #1 tensor([[[ 1.1441,  0.0000,  0.0000,  0.0000],\n         [ 0.7349,  1.3175,  0.0000,  0.0000],\n         [ 0.4355,  0.5900, -1.1768,  0.0000],\n         [ 0.3596,  1.0472, -0.3473, -1.1330]],\n\n        [[ 1.1441,  0.0000,  0.0000,  0.0000],\n         [ 0.7349,  1.3175,  0.0000,  0.0000],\n         [ 0.4355,  0.5900, -1.1768,  0.0000],\n         [ 0.3596,  1.0472, -0.3473, -1.1330]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\nm, P_C = _filter_update_mask_SR(H, d, R_C, m_m, P_m_C, obs, mask[0, 0, :] )\nm.shape, P_C.shape\n\n(torch.Size([2, 4, 1]), torch.Size([2, 4, 4]))\n\n\n\nmask[0,0].shape\n\ntorch.Size([3])\n\n\n\ndef fuzz_filter_update_SR(n=10):\n    errs = {'mean': [], 'cov': []}\n    for _ in range(n):\n        kSR = KalmanFilterSR.init_random(5,10,4)\n        H, d, R, R_C, m_m, P_m = (kSR.H, kSR.d, kSR.R, kSR.R_C, torch.cat([kSR.m0]*5), torch.cat([kSR.P0]*5))\n        obs, mask, _  = get_test_data(1, 5, 4, bs=5)\n        obs, mask = obs[:,0].unsqueeze(-1), mask[0,0]\n        \n        P_m_C = torch.linalg.cholesky(P_m)\n        mSR, P_m_C = _filter_update_mask_SR(H, d, R, m_m, P_m_C, obs, mask)\n        m, P_C = _filter_update_mask(H, d, R, m_m, P_m, obs, mask)\n        errs['mean'].append((mSR - m).abs().max().item())\n        errs['cov'].append((P_C - P_m_C @ P_m_C.mT).abs().max().item())\n    return pd.DataFrame(errs)\n\nerr = fuzz_filter_update_SR(100)\n\nerr.median(), err.max()\n\n(mean    3.996803e-15\n cov     1.970646e-15\n dtype: float64,\n mean    3.996803e-15\n cov     7.827072e-15\n dtype: float64)\n\n\n\n\nUpdate mask batch\n\nm, P_C = _filter_update_mask_batch_SR(H, d, R, m_m, P_m_C, obs, mask[:,0,:] )\nshow_as_row(m, P_C)\nm.shape, P_C.shape\n\n\n m tensor([[[1.3685],\n         [1.0982],\n         [1.7967],\n         [1.8337]],\n\n        [[0.1197],\n         [0.3457],\n         [0.5033],\n         [0.6044]]], dtype=torch.float64, grad_fn=)\n P_C tensor([[[ 1.1607,  0.0000,  0.0000,  0.0000],\n         [ 0.8022,  1.3585,  0.0000,  0.0000],\n         [ 0.5153,  0.6769, -1.2081,  0.0000],\n         [ 0.4512,  1.1387, -0.3915, -1.1430]],\n\n        [[ 0.8355,  0.0000,  0.0000,  0.0000],\n         [-0.1000,  0.9427,  0.0000,  0.0000],\n         [-0.0859,  0.2527, -1.0461,  0.0000],\n         [-0.4813,  0.4183, -0.2013, -1.0540]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n(torch.Size([2, 4, 1]), torch.Size([2, 4, 4]))\n\n\n\nm.sum().backward(retain_graph=True) # check that pytorch can compute gradients with the whole batch and gradients aren't nan\nH.grad\n\ntensor([[[-5.3359, -5.2561, -7.0879, -7.6131],\n         [ 0.0176, -0.2548, -0.2340, -0.2578],\n         [-0.2513, -0.9494, -1.2821, -1.5227]]], dtype=torch.float64)\n\n\n\n\n\n\nFilter All\nThe resursive version of the kalman filter is apperently breaking pytorch gradients calculations so a workaround is needed. During the loop the states are saved in a python list and then at the end they are combined back into a tensor. The last line of the function does:\n\nconvert lists to tensors\ncorrect order dimensions\n\n\nfilt_stateSR, pred_stateSR  = kSR._filter_all(data, mask, control)\n\n\n(ms, P_Cs), (m_ms, P_m_Cs) = filt_stateSR, pred_stateSR\n\nPredictions at time 0 for both batches\n\nshow_as_row(*map(Self.shape(), (m_ms, P_m_Cs, ms, P_Cs,)))\n\n\n #0 torch.Size([2, 10, 4, 1])\n #1 torch.Size([2, 10, 4, 4])\n #2 torch.Size([2, 10, 4, 1])\n #3 torch.Size([2, 10, 4, 4])\n \n\n\n\nshow_as_row(*map(lambda x:x[0][0], (m_ms, P_m_Cs, ms, P_Cs,)))\n\n\n #0 tensor([[0.4499],\n        [0.8575],\n        [0.2647],\n        [0.4293]], dtype=torch.float64, grad_fn=)\n #1 tensor([[1.2562, 0.0000, 0.0000, 0.0000],\n        [0.3804, 0.9666, 0.0000, 0.0000],\n        [0.9536, 0.7909, 1.2431, 0.0000],\n        [0.4487, 0.4993, 0.4600, 1.2599]], dtype=torch.float64,\n       grad_fn=)\n #2 tensor([[0.5189],\n        [0.9208],\n        [0.4203],\n        [0.5421]], dtype=torch.float64, grad_fn=)\n #3 tensor([[-1.1638,  0.0000,  0.0000,  0.0000],\n        [-0.2344, -0.9142,  0.0000,  0.0000],\n        [-0.5963, -0.5742, -1.1218,  0.0000],\n        [-0.1702, -0.3039, -0.2622,  1.2088]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\n\nFilter\nThe filter methods wraps _filter_all but in addition:\n\nreturns only filtered state\nremove last dimensions from mean\n\n\nsource\n\n\nKalmanFilterSR.filter\n\n KalmanFilterSR.filter (obs:torch.Tensor, mask:torch.Tensor,\n                        control:torch.Tensor)\n\nFilter observation\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nobs\nTensor\n([n_batches], n_obs, [self.n_dim_obs]) where n_batches and n_dim_obs dimensions can be omitted if 1\n\n\nmask\nTensor\n([n_batches], n_obs, [self.n_dim_obs]) where n_batches and n_dim_obs dimensions can be omitted if 1\n\n\ncontrol\nTensor\n([n_batches], n_obs, [self.n_dim_contr])\n\n\nReturns\nListMultiNormal\nFiltered state (n_batches, n_obs, self.n_dim_state)\n\n\n\n\nfiltSR = kSR.filter(data, mask, control)\nfiltSR.mean.shape, filtSR.cov.shape\n\n(torch.Size([2, 10, 4, 1]), torch.Size([2, 10, 4, 4]))\n\n\n\ndef fuzz_filter_SR(n=10):\n    errs = {'mean': [], 'cov': []}\n    for _ in range(n):\n        kSR = KalmanFilterSR.init_random(5,10,4)\n        k = KalmanFilter.init_from(kSR)\n        dat = get_test_data(20, 5, 4)\n        mean, cov = k.filter(*dat)\n        meanSR, covSR = kSR.filter(*dat)\n        covSR = covSR @ covSR.mT\n        errs['mean'].append((meanSR - mean).abs().max().item())\n        errs['cov'].append((covSR - cov).abs().max().item())\n    return pd.DataFrame(errs)\n\nerr = fuzz_filter_SR(10)\n\nerr.median(), err.max()\n\n(mean    8.108847e-12\n cov     8.149592e-12\n dtype: float64,\n mean    4.446576e-11\n cov     1.261309e-10\n dtype: float64)"
  },
  {
    "objectID": "kalman_filter.html#smooth-2",
    "href": "kalman_filter.html#smooth-2",
    "title": "Kalman Filter",
    "section": "Smooth",
    "text": "Smooth\n\nSmooth update step\ncompute the probability of the state at time t given all the observations\n\\(p(x_t|Y) = \\mathcal{N}(x_t; m_t^s, P_t^s)\\) where:\n\nKalman smoothing gain: \\(G_t = P_tA^T(P_{t+1}^-)^{-1}\\)\nsmoothed mean: \\(m_t^s = m_t + G_t(m_{t+1}^s - m_{t+1}^-)\\)\nsmoothed covariance: \\(P_t^s = P_t + G_t(P_{t+1}^s - P_{t+1}^-)G_t^T\\)\n\n\nfilt_state = ListMNormal(filt_stateSR.mean, filt_stateSR.cov @ filt_stateSR.cov.mT)\npred_state = ListMNormal(pred_stateSR.mean, pred_stateSR.cov @ pred_stateSR.cov.mT)\n\n\nK_p = _smooth_gain_SR(kSR.A, filt_stateSR[:, 0].cov, pred_stateSR[:, 0].cov)\n\n\ntest_close(\n    _smooth_gain_SR(kSR.A, filt_stateSR[:, 0].cov, pred_stateSR[:, 0].cov),\n    _smooth_gain(kSR.A, filt_state[:, 0].cov, pred_state[:, 0].cov)\n)\n\n\ntest_close(filt_state[0,0].cov, filt_stateSR[0,0].cov @ filt_stateSR[0,0].cov.mT)\n\n\ntest_close(pred_state[0,0].cov, pred_stateSR[0,0].cov @ pred_stateSR[0,0].cov.mT)\n\n\ndef fuzz_gain_SR(n=10):\n    errs = {'K': []}\n    for _ in range(n):\n        kSR = KalmanFilterSR.init_random(5,10,4)\n        k = KalmanFilter.init_from(kSR)\n        dat = get_test_data(2, 5, 4)\n        (_, f_cov), (_, p_cov) = k._filter_all(*dat)\n        (_, f_covSR), (_, p_covSR) = kSR._filter_all(*dat)\n        K = _smooth_gain(k.A, f_cov[:, 0], p_cov[:, 0])\n        K_SR = _smooth_gain_SR(k.A, f_covSR[:, 0], p_covSR[:, 0])\n        errs['K'].append((K - K_SR).abs().max().item())\n    return pd.DataFrame(errs)\n\nerr = fuzz_gain_SR(10)\n\nerr.median(), err.max()\n\n(K    1.920686e-14\n dtype: float64,\n K    4.352074e-14\n dtype: float64)\n\n\n\nm_p, P_p = _smooth_update_SR(kSR.A, filt_stateSR[:, 0, :], pred_stateSR[:, 0, :], filt_stateSR[:, 0, :])\n\n\ntest_close(filt_state[:, 0, :].mean, filt_stateSR[:, 0, :].mean)\n\n\n_smooth_update(kSR.A, filt_state[0,0],  pred_state[0,0] , filt_state[0,0] )\n\nMultiNormal(mean=tensor([[[0.6325],\n         [0.9899],\n         [0.5680],\n         [0.6284]]], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;), cov=tensor([[[ 0.7480, -0.0959, -0.0944, -0.2622],\n         [-0.0959,  0.6667,  0.1856,  0.0380],\n         [-0.0944,  0.1856,  0.9192, -0.0280],\n         [-0.2622,  0.0380, -0.0280,  1.3021]]], dtype=torch.float64,\n       grad_fn=&lt;DivBackward0&gt;))\n\n\n\n_smooth_update_SR(kSR.A, filt_stateSR[0,0],  pred_stateSR[0,0], filt_stateSR[0,0])\n\nMultiNormal(mean=tensor([[[0.6325],\n         [0.9899],\n         [0.5680],\n         [0.6284]]], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;), cov=tensor([[[-1.8502, -2.4971, -5.2813, -3.0804],\n         [-2.4971, -1.6908, -4.8194, -2.8887],\n         [-5.2813, -4.8194, -9.7933, -6.0797],\n         [-3.0804, -2.8887, -6.0797, -2.6434]]], dtype=torch.float64,\n       grad_fn=&lt;DivBackward0&gt;))\n\n\n\ntest_close((filt_stateSR.mean, pred_stateSR.mean), (filt_state.mean, pred_state.mean))\n\n\ntest_close(\n    _smooth_update_SR(kSR.A, filt_stateSR[:, 0], pred_stateSR[:, 0], filt_state[:, 0]),\n    _smooth_update(kSR.A, filt_state[:, 0], pred_state[:, 0], filt_state[:, 0])\n)\n\n\ndef fuzz_update_SR(n=10):\n    errs = {'mean': [], 'cov': []}\n    for _ in range(n):\n        kSR = KalmanFilterSR.init_random(5,10,4)\n        k = KalmanFilter.init_from(kSR)\n        dat = get_test_data(2, 5, 4)\n        f_state, p_state = k._filter_all(*dat)\n        f_stateSR, p_stateSR = kSR._filter_all(*dat)\n        mean, cov = _smooth_update(k.A, f_state[:, 0], p_state[:, 0], f_state[:, 1])\n        meanSR, covSR = _smooth_update_SR(k.A, f_stateSR[:, 0], p_stateSR[:, 0], f_state[:, 1])\n        errs['mean'].append((meanSR - mean).abs().max().item())\n        errs['cov'].append((covSR - cov).abs().max().item())\n    return pd.DataFrame(errs)\n\nerr = fuzz_update_SR(10)\n\nerr.median(), err.max()\n\n(mean    1.154632e-14\n cov     2.771117e-13\n dtype: float64,\n mean    3.375078e-14\n cov     8.242296e-13\n dtype: float64)\n\n\n\n\nSmooth\n\nsmooth_state = _smooth_SR(kSR.A,  filt_stateSR, pred_stateSR)\n\n\nis_sr(filt_stateSR.cov, filt_state.cov)\n\nTrue\n\n\n\ns_mean, s_cov =  _smooth(kSR.A,  filt_state, pred_state)\ns_meanSR, s_covSR =  _smooth_SR(kSR.A,  filt_stateSR, pred_stateSR)\ntest_close(s_cov, s_covSR)\n(s_cov - s_covSR).median(), (s_cov - s_covSR).max()\n\n(tensor(0., dtype=torch.float64, grad_fn=&lt;MedianBackward0&gt;),\n tensor(3.5527e-15, dtype=torch.float64, grad_fn=&lt;MaxBackward1&gt;))\n\n\n\nshow_as_row(smooth_state.mean[0][0], smooth_state.cov[0][0])\n\n\n #0 tensor([[-0.3279],\n        [ 0.4029],\n        [-0.7501],\n        [-0.0098]], dtype=torch.float64, grad_fn=)\n #1 tensor([[ 0.9163, -0.0046,  0.0978, -0.1494],\n        [-0.0046,  0.7132,  0.2841,  0.0932],\n        [ 0.0978,  0.2841,  1.1262,  0.0916],\n        [-0.1494,  0.0932,  0.0916,  1.3623]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\nshow_as_row(smooth_state.mean.shape, smooth_state.cov.shape)\n\n\n #0 torch.Size([2, 10, 4, 1])\n #1 torch.Size([2, 10, 4, 4])\n \n\n\n\n\nKalmanFilter method\n\nmask\n\ntensor([[[ True, False, False],\n         [ True, False,  True],\n         [False,  True, False],\n         [False, False, False],\n         [False, False,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True, False],\n         [ True, False,  True],\n         [ True,  True,  True]],\n\n        [[ True,  True,  True],\n         [ True,  True, False],\n         [ True,  True,  True],\n         [ True, False,  True],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True]]])\n\n\n\ntorch.argwhere((~mask).any(-1).any(0)).min()\n\ntensor(0)\n\n\n\ntorch.ones(1,2,0)\n\ntensor([], size=(1, 2, 0))\n\n\n\nsource\n\n\nKalmanFilterSR.smooth\n\n KalmanFilterSR.smooth (obs:torch.Tensor, mask:torch.Tensor,\n                        control:torch.Tensor)\n\nKalman Filter Smoothing\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nobs\nTensor\n\n\n\nmask\nTensor\n\n\n\ncontrol\nTensor\n\n\n\nReturns\nListMultiNormal\n[n_timesteps, n_dim_state] smoothed state\n\n\n\n\nsmoothed_state = kSR.smooth(data, mask, control)\n\n\nshow_as_row(smoothed_state.mean.shape, smoothed_state.cov.shape)\n\n\n #0 torch.Size([2, 10, 4, 1])\n #1 torch.Size([2, 10, 4, 4])\n \n\n\n\nsmoothed_state.cov.isnan().any()\n\ntensor(False)\n\n\n\nsmoothed_state.mean.sum().backward(retain_graph=True)\nA.grad\n\ntensor([[[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]], dtype=torch.float64)\n\n\n\nsmoothed_state_stand = k.smooth(data, mask, control)\n\n\n(smoothed_state.mean - smoothed_state_stand.mean).max()\n(smoothed_state.cov - smoothed_state_stand.cov).max()\n\ntensor(0.6755, dtype=torch.float64, grad_fn=&lt;MaxBackward1&gt;)\n\n\n\ndef fuzz_smooth_SR(n=10):\n    errs = {'mean': [], 'cov': []}\n    for _ in range(n):\n        kSR = KalmanFilterSR.init_random(5,10,4)\n        k = KalmanFilter.init_from(kSR)\n        dat = get_test_data(20, 5, 4)\n        mean, cov = k.smooth(*dat)\n        meanSR, covSR = kSR.smooth(*dat)\n        errs['mean'].append((meanSR - mean).abs().max().item())\n        errs['cov'].append((covSR - cov).abs().max().item())\n    return pd.DataFrame(errs)\n\nerr = fuzz_smooth_SR(10)\n\nerr.median(), err.max()\n\n(mean    9.733464e-12\n cov     8.838374e-12\n dtype: float64,\n mean    4.984400e-09\n cov     4.896766e-09\n dtype: float64)"
  },
  {
    "objectID": "kalman_filter.html#predict-3",
    "href": "kalman_filter.html#predict-3",
    "title": "Kalman Filter",
    "section": "Predict",
    "text": "Predict\npredict can be vectorized across both the batch and the timesteps, except for timesteps that require conditional predictions\n\nObs from State\n\nsmoothed_state.mean.shape, smoothed_state.cov.shape\n\n(torch.Size([2, 10, 4, 1]), torch.Size([2, 10, 4, 4]))\n\n\n\npred_obs0 = kSR._obs_from_state(smoothed_state)\npred_obs0.mean.shape, pred_obs0.cov.shape\n\n(torch.Size([2, 10, 3, 1]), torch.Size([2, 10, 3, 3]))\n\n\n\npred_obs0.cov.isnan().any()\n\ntensor(False)\n\n\n\ngap_mask = ~mask.all(-1)\n\n\ngap_mask.shape\n\ntorch.Size([2, 10])\n\n\nPredict has various modes:\n\npred_only_gap is True, returns predictions only where the mask is False\n\nuse_conditional returns a list (for each batch) of list (for each time stamp) of Tensors of shape [1, gap_len]\nuse_conditional is False, returns a list (for each batch) of Tensor of shape [n_times_gap, n_dim_obs]\n\n\n\n\nMasked Batch\n\ngap_mask = ~mask.all(-1)\n\n\nfrom pprint import pp\n\n\npp(_masked2batch(mask[gap_mask], mask)[0])\n\n[tensor([False, False]),\n tensor([False]),\n tensor([False, False]),\n tensor([False, False, False]),\n tensor([False, False]),\n tensor([False]),\n tensor([False])]\n\n\n\nstr(_masked2batch(mask[gap_mask], mask)[0])\n\n'[tensor([False, False]), tensor([False]), tensor([False, False]), tensor([False, False, False]), tensor([False, False]), tensor([False]), tensor([False])]'\n\n\n\nshow_as_row(all_mask = mask[0] , only_gap=_masked2batch(mask[gap_mask], mask)[0])\n\n\n all_mask tensor([[ True, False, False],\n        [ True, False,  True],\n        [False,  True, False],\n        [False, False, False],\n        [False, False,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True, False],\n        [ True, False,  True],\n        [ True,  True,  True]])\n only_gap [tensor([False, False]),\n tensor([False]),\n tensor([False, False]),\n tensor([False, False, False]),\n tensor([False, False]),\n tensor([False]),\n tensor([False])]\n \n\n\n\n\nPredict\n\nsource\n\n\nKalmanFilterSR.predict\n\n KalmanFilterSR.predict (obs, mask, control, smooth=True)\n\nPredicted observations at all times\n\n\nExploration\n\nsource\n\n\nwith_settings\n\n with_settings (k, **kwargs)\n\n\nsource\n\n\nreplacing_ctx\n\n replacing_ctx (*args)\n\n\nwith with_settings(kSR, use_conditional=False, pred_only_gap=False):\n    pred_mean, pred_cov = kSR.predict(data, mask, control)\nshow_as_row(mean= pred_mean.shape, cov = pred_cov.shape)\n\n\n mean torch.Size([2, 10, 3])\n cov torch.Size([2, 10, 3, 3])\n \n\n\n\nwith with_settings(kSR, use_conditional=False, pred_only_gap=True):\n    pred_mean, pred_cov = kSR.predict(data, mask, control)\nshow_as_row(mean= pred_mean[0], cov = pred_cov[0])\n\n\n mean [tensor([-0.3666,  0.3765], dtype=torch.float64, grad_fn=),\n tensor([-0.0844], dtype=torch.float64, grad_fn=),\n tensor([0.1625, 0.3119], dtype=torch.float64, grad_fn=),\n tensor([-0.1102, -0.5474, -0.1901], dtype=torch.float64,\n       grad_fn=),\n tensor([-0.1439, -0.4996], dtype=torch.float64, grad_fn=),\n tensor([0.4966], dtype=torch.float64, grad_fn=),\n tensor([0.0202], dtype=torch.float64, grad_fn=)]\n cov [tensor([[2.5518, 2.1682],\n        [2.1682, 2.8766]], dtype=torch.float64, grad_fn=),\n tensor([[2.3344]], dtype=torch.float64, grad_fn=),\n tensor([[1.9095, 0.5371],\n        [0.5371, 3.0221]], dtype=torch.float64, grad_fn=),\n tensor([[2.0929, 1.1465, 0.8395],\n        [1.1465, 2.8696, 2.5367],\n        [0.8395, 2.5367, 3.5092]], dtype=torch.float64,\n       grad_fn=),\n tensor([[1.9076, 0.8393],\n        [0.8393, 2.3729]], dtype=torch.float64, grad_fn=),\n tensor([[2.7991]], dtype=torch.float64, grad_fn=),\n tensor([[2.2841]], dtype=torch.float64, grad_fn=)]\n \n\n\n\nwith with_settings(kSR, use_conditional=True, pred_only_gap=True):\n    pred_mean, pred_cov = kSR.predict(data, mask, control)\nshow_as_row(mean= pred_mean[0], cov = pred_cov[0])\n\n\n mean [tensor([0.0292, 0.6235], dtype=torch.float64, grad_fn=),\n tensor([0.4156], dtype=torch.float64, grad_fn=),\n tensor([0.3672, 0.7999], dtype=torch.float64, grad_fn=),\n tensor([-0.1102, -0.5474, -0.1901], dtype=torch.float64,\n       grad_fn=),\n tensor([-0.0686, -0.1575], dtype=torch.float64, grad_fn=),\n tensor([0.4823], dtype=torch.float64, grad_fn=),\n tensor([0.1720], dtype=torch.float64, grad_fn=)]\n cov [tensor([[2.5518, 2.1682],\n        [2.1682, 2.8766]], dtype=torch.float64, grad_fn=),\n tensor([[2.3344]], dtype=torch.float64, grad_fn=),\n tensor([[1.9095, 0.5371],\n        [0.5371, 3.0221]], dtype=torch.float64, grad_fn=),\n tensor([[2.0929, 1.1465, 0.8395],\n        [1.1465, 2.8696, 2.5367],\n        [0.8395, 2.5367, 3.5092]], dtype=torch.float64,\n       grad_fn=),\n tensor([[1.9076, 0.8393],\n        [0.8393, 2.3729]], dtype=torch.float64, grad_fn=),\n tensor([[2.7991]], dtype=torch.float64, grad_fn=),\n tensor([[2.2841]], dtype=torch.float64, grad_fn=)]\n \n\n\n\nwith with_settings(kSR, use_conditional = True, pred_only_gap = False):\n    test_fail(kSR.predict, [data, mask, control]) # this params combination is invalid\n\n\nGap only prediction\ncopy paste from other notebook to make visualization easier\n\nsource\n\n\n\nbuffer_pred_single\n\n buffer_pred_single (preds:list[torch.Tensor], masks:torch.Tensor)\n\nFor predictions are for gaps only add buffer of Nan so they have same shape of targets\n\nsource\n\n\nbuffer_pred\n\n buffer_pred (preds:list[list[torch.Tensor]], masks:torch.Tensor)\n\nFor predictions are for gaps only add buffer of Nan so they have same shape of targets\n\nwith with_settings(kSR, use_conditional=False, pred_only_gap=True):\n    pred_m_gap, _ = kSR.predict(data, mask, control)\n    \nwith with_settings(kSR, use_conditional=False, pred_only_gap=False):\n    pred_m, _ = kSR.predict(data, mask, control)\n\nsoooooooo this is a problem!!! those should be the same\n\nshow_as_row(gap = buffer_pred(pred_m_gap, mask), no_gap = pred_m)\n\n\n gap tensor([[[    nan, -0.3666,  0.3765],\n         [    nan, -0.0844,     nan],\n         [ 0.1625,     nan,  0.3119],\n         [-0.1102, -0.5474, -0.1901],\n         [-0.1439, -0.4996,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,  0.4966],\n         [    nan,  0.0202,     nan],\n         [    nan,     nan,     nan]],\n\n        [[    nan,     nan,     nan],\n         [    nan,     nan,  0.2059],\n         [    nan,     nan,     nan],\n         [    nan, -0.3611,     nan],\n         [ 0.1032,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan]]], dtype=torch.float64)\n no_gap tensor([[[ 0.0322, -0.3666,  0.3765],\n         [ 0.1475, -0.0844,  0.4320],\n         [ 0.1625, -0.0796,  0.3119],\n         [-0.1102, -0.5474, -0.1901],\n         [-0.1439, -0.4996,  0.0158],\n         [ 0.1736, -0.0500,  0.1638],\n         [ 0.0998, -0.2115,  0.2026],\n         [ 0.2642,  0.0727,  0.4966],\n         [ 0.2667,  0.0202,  0.5999],\n         [ 0.5621,  0.4854,  1.0911]],\n\n        [[ 0.0523, -0.3558,  0.1773],\n         [ 0.0572, -0.2074,  0.2059],\n         [ 0.0361, -0.2809,  0.0575],\n         [-0.0671, -0.3611,  0.1202],\n         [ 0.1032, -0.2193, -0.1928],\n         [-0.1298, -0.5874, -0.3787],\n         [ 0.0450, -0.2554, -0.2029],\n         [ 0.0938, -0.0932,  0.3782],\n         [ 0.2229, -0.1440, -0.1133],\n         [ 0.3722,  0.3017,  1.1004]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\nstate = kSR.smooth(data, mask, control)\n\n\nstate.mean.shape\n\ntorch.Size([2, 10, 4, 1])\n\n\n\ngap_mask = ~mask.all(-1)\n# this destroy batches! so need to do some magic after\nstate_gap = state[gap_mask]\n\n\nstate_gap.mean.shape\n\ntorch.Size([10, 4, 1])\n\n\n\npred_obs = kSR._obs_from_state(state)\npred_obs.mean.squeeze_(-1)\n\ntensor([[[ 0.0322, -0.3666,  0.3765],\n         [ 0.1475, -0.0844,  0.4320],\n         [ 0.1625, -0.0796,  0.3119],\n         [-0.1102, -0.5474, -0.1901],\n         [-0.1439, -0.4996,  0.0158],\n         [ 0.1736, -0.0500,  0.1638],\n         [ 0.0998, -0.2115,  0.2026],\n         [ 0.2642,  0.0727,  0.4966],\n         [ 0.2667,  0.0202,  0.5999],\n         [ 0.5621,  0.4854,  1.0911]],\n\n        [[ 0.0523, -0.3558,  0.1773],\n         [ 0.0572, -0.2074,  0.2059],\n         [ 0.0361, -0.2809,  0.0575],\n         [-0.0671, -0.3611,  0.1202],\n         [ 0.1032, -0.2193, -0.1928],\n         [-0.1298, -0.5874, -0.3787],\n         [ 0.0450, -0.2554, -0.2029],\n         [ 0.0938, -0.0932,  0.3782],\n         [ 0.2229, -0.1440, -0.1133],\n         [ 0.3722,  0.3017,  1.1004]]], dtype=torch.float64,\n       grad_fn=&lt;SqueezeBackward3&gt;)\n\n\n\npred_obs_gap = kSR._obs_from_state(state_gap)\npred_obs_gap.mean.squeeze_(-1)\n\ntensor([[ 0.0322, -0.3666,  0.3765],\n        [ 0.1475, -0.0844,  0.4320],\n        [ 0.1625, -0.0796,  0.3119],\n        [-0.1102, -0.5474, -0.1901],\n        [-0.1439, -0.4996,  0.0158],\n        [ 0.2642,  0.0727,  0.4966],\n        [ 0.2667,  0.0202,  0.5999],\n        [ 0.0572, -0.2074,  0.2059],\n        [-0.0671, -0.3611,  0.1202],\n        [ 0.1032, -0.2193, -0.1928]], dtype=torch.float64,\n       grad_fn=&lt;SqueezeBackward3&gt;)\n\n\n\n(pred_obs[gap_mask].mean == pred_obs_gap.mean).all() # so far good\n\ntensor(True)\n\n\n\npred_obs.mean.shape\n\ntorch.Size([2, 10, 3])\n\n\n\nmask.shape\n\ntorch.Size([2, 10, 3])\n\n\n\nshow_as_row(pred = buffer_pred(_masked2batch(pred_obs_gap.mean, mask), mask), mask = mask, all = pred_obs.mean)\n\n\n pred tensor([[[    nan, -0.3666,  0.3765],\n         [    nan, -0.0844,     nan],\n         [ 0.1625,     nan,  0.3119],\n         [-0.1102, -0.5474, -0.1901],\n         [-0.1439, -0.4996,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,  0.4966],\n         [    nan,  0.0202,     nan],\n         [    nan,     nan,     nan]],\n\n        [[    nan,     nan,     nan],\n         [    nan,     nan,  0.2059],\n         [    nan,     nan,     nan],\n         [    nan, -0.3611,     nan],\n         [ 0.1032,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan]]], dtype=torch.float64)\n mask tensor([[[ True, False, False],\n         [ True, False,  True],\n         [False,  True, False],\n         [False, False, False],\n         [False, False,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True, False],\n         [ True, False,  True],\n         [ True,  True,  True]],\n\n        [[ True,  True,  True],\n         [ True,  True, False],\n         [ True,  True,  True],\n         [ True, False,  True],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True]]])\n all tensor([[[ 0.0322, -0.3666,  0.3765],\n         [ 0.1475, -0.0844,  0.4320],\n         [ 0.1625, -0.0796,  0.3119],\n         [-0.1102, -0.5474, -0.1901],\n         [-0.1439, -0.4996,  0.0158],\n         [ 0.1736, -0.0500,  0.1638],\n         [ 0.0998, -0.2115,  0.2026],\n         [ 0.2642,  0.0727,  0.4966],\n         [ 0.2667,  0.0202,  0.5999],\n         [ 0.5621,  0.4854,  1.0911]],\n\n        [[ 0.0523, -0.3558,  0.1773],\n         [ 0.0572, -0.2074,  0.2059],\n         [ 0.0361, -0.2809,  0.0575],\n         [-0.0671, -0.3611,  0.1202],\n         [ 0.1032, -0.2193, -0.1928],\n         [-0.1298, -0.5874, -0.3787],\n         [ 0.0450, -0.2554, -0.2029],\n         [ 0.0938, -0.0932,  0.3782],\n         [ 0.2229, -0.1440, -0.1133],\n         [ 0.3722,  0.3017,  1.1004]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\npred_gap_buff = buffer_pred(_masked2batch(pred_obs_gap.mean, mask), mask)\nmask_na = ~pred_gap_buff.isnan()\n\n\ntest_close(pred_gap_buff[mask_na], pred_obs.mean[mask_na])\n\n\nwith with_settings(kSR, use_conditional=False, pred_only_gap=True):\n    pred_gap_buff = buffer_pred(kSR.predict(data, mask, control).mean, mask)\nmask_na = ~pred_gap_buff.isnan()\nwith with_settings(kSR, use_conditional=False, pred_only_gap=False):\n    pred_ng = kSR.predict(data, mask, control).mean\n\n\ntest_close(pred_gap_buff[mask_na], pred_ng[mask_na])\n\n\nshow_as_row(pred = pred_gap_buff, mask = mask, all = pred_ng)\n\n\n pred tensor([[[    nan, -0.3666,  0.3765],\n         [    nan, -0.0844,     nan],\n         [ 0.1625,     nan,  0.3119],\n         [-0.1102, -0.5474, -0.1901],\n         [-0.1439, -0.4996,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,  0.4966],\n         [    nan,  0.0202,     nan],\n         [    nan,     nan,     nan]],\n\n        [[    nan,     nan,     nan],\n         [    nan,     nan,  0.2059],\n         [    nan,     nan,     nan],\n         [    nan, -0.3611,     nan],\n         [ 0.1032,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan]]], dtype=torch.float64)\n mask tensor([[[ True, False, False],\n         [ True, False,  True],\n         [False,  True, False],\n         [False, False, False],\n         [False, False,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True, False],\n         [ True, False,  True],\n         [ True,  True,  True]],\n\n        [[ True,  True,  True],\n         [ True,  True, False],\n         [ True,  True,  True],\n         [ True, False,  True],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True],\n         [ True,  True,  True]]])\n all tensor([[[ 0.0322, -0.3666,  0.3765],\n         [ 0.1475, -0.0844,  0.4320],\n         [ 0.1625, -0.0796,  0.3119],\n         [-0.1102, -0.5474, -0.1901],\n         [-0.1439, -0.4996,  0.0158],\n         [ 0.1736, -0.0500,  0.1638],\n         [ 0.0998, -0.2115,  0.2026],\n         [ 0.2642,  0.0727,  0.4966],\n         [ 0.2667,  0.0202,  0.5999],\n         [ 0.5621,  0.4854,  1.0911]],\n\n        [[ 0.0523, -0.3558,  0.1773],\n         [ 0.0572, -0.2074,  0.2059],\n         [ 0.0361, -0.2809,  0.0575],\n         [-0.0671, -0.3611,  0.1202],\n         [ 0.1032, -0.2193, -0.1928],\n         [-0.1298, -0.5874, -0.3787],\n         [ 0.0450, -0.2554, -0.2029],\n         [ 0.0938, -0.0932,  0.3782],\n         [ 0.2229, -0.1440, -0.1133],\n         [ 0.3722,  0.3017,  1.1004]]], dtype=torch.float64,\n       grad_fn=)\n \n\n\n\nConditional\n\nwith with_settings(kSR, use_conditional=False, pred_only_gap=True):\n    pred_gap = buffer_pred(kSR.predict(data, mask, control).mean, mask)\nwith with_settings(kSR, use_conditional=True, pred_only_gap=True):\n    pred_gap_cond = buffer_pred(kSR.predict(data, mask, control).mean, mask)\n\n\nshow_as_row(no_conditional = pred_gap, conditional = pred_gap_cond)\n\n\n no_conditional tensor([[[    nan, -0.3666,  0.3765],\n         [    nan, -0.0844,     nan],\n         [ 0.1625,     nan,  0.3119],\n         [-0.1102, -0.5474, -0.1901],\n         [-0.1439, -0.4996,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,  0.4966],\n         [    nan,  0.0202,     nan],\n         [    nan,     nan,     nan]],\n\n        [[    nan,     nan,     nan],\n         [    nan,     nan,  0.2059],\n         [    nan,     nan,     nan],\n         [    nan, -0.3611,     nan],\n         [ 0.1032,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan]]], dtype=torch.float64)\n conditional tensor([[[    nan,  0.0292,  0.6235],\n         [    nan,  0.4156,     nan],\n         [ 0.3672,     nan,  0.7999],\n         [-0.1102, -0.5474, -0.1901],\n         [-0.0686, -0.1575,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,  0.4823],\n         [    nan,  0.1720,     nan],\n         [    nan,     nan,     nan]],\n\n        [[    nan,     nan,     nan],\n         [    nan,     nan,  0.5191],\n         [    nan,     nan,     nan],\n         [    nan,  0.2014,     nan],\n         [ 0.6513,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan],\n         [    nan,     nan,     nan]]], dtype=torch.float64)\n \n\n\n\nmask[gap_mask]\n\ntensor([[ True, False, False],\n        [ True, False,  True],\n        [False,  True, False],\n        [False, False, False],\n        [False, False,  True],\n        [ True,  True, False],\n        [ True, False,  True],\n        [ True,  True, False],\n        [ True, False,  True],\n        [False,  True,  True]])\n\n\n\ndata[gap_mask]\n\ntensor([[0.8775,    nan,    nan],\n        [0.6706,    nan, 0.9272],\n        [   nan, 0.4967,    nan],\n        [   nan,    nan,    nan],\n        [   nan,    nan, 0.4760],\n        [0.9991, 0.1775,    nan],\n        [0.6734,    nan, 0.6468],\n        [0.3725, 0.2052,    nan],\n        [0.5927,    nan, 0.6441],\n        [   nan, 0.9132, 0.0329]], dtype=torch.float64)\n\n\n\ndata[gap_mask]\n\ntensor([[0.8775,    nan,    nan],\n        [0.6706,    nan, 0.9272],\n        [   nan, 0.4967,    nan],\n        [   nan,    nan,    nan],\n        [   nan,    nan, 0.4760],\n        [0.9991, 0.1775,    nan],\n        [0.6734,    nan, 0.6468],\n        [0.3725, 0.2052,    nan],\n        [0.5927,    nan, 0.6441],\n        [   nan, 0.9132, 0.0329]], dtype=torch.float64)\n\n\n\nassert is_posdef(pred_cov[0][0]).all()\n\n\nstate = kSR.smooth(data, mask, control)\nassert not state.cov.isnan().any()\nkSR.use_sr_pred = False\npred_obs = kSR._obs_from_state(state)\nassert not pred_obs.cov.isnan().any()\n\n\ncov2std(pred_obs.cov).isnan().any()\n\ntensor(False)\n\n\n\nassert not kSR.predict(data, mask, control).cov.isnan().any()\n\n\nassert not kSR.predict(data, mask, control, smooth=False).cov.isnan().any()\n\n\nkSR._predict_filter(data, mask, control).std.shape\n\ntorch.Size([2, 10, 3, 3])\n\n\n\nkSR.use_conditional = False\n\n\npred = kSR.predict(data, mask, control, smooth=True)\n\n\npred.mean.shape, pred.std.shape\n\nAttributeError: 'ListMultiNormal' object has no attribute 'std'\n\n\n\nfilt_state, pred_state = kSR._filter_all(data, mask, control)\nmean, cov = kSR._obs_from_state(pred_state)\n\n\nis_posdef(cov @ cov.mT)\n\n\npred"
  },
  {
    "objectID": "kalman_filter.html#debug-nan",
    "href": "kalman_filter.html#debug-nan",
    "title": "Kalman Filter",
    "section": "Debug nan",
    "text": "Debug nan\n\nimport polars as pl\nimport altair as alt\n\n\nkSR.predict(*get_test_data(200)).cov.isnan().any()\n\n\ndata.shape\n\n\nreset_seed()\n\n\nSR Filter\n\nnan = [{'nan':kSR.predict(*get_test_data(n), smooth=smooth).cov.isnan().any().item(), 'n': n, 'rep': rep, 'smooth': smooth}\n       for n in [10, 20, 30, 40, 50, 55, 60, 70, 100, 150, 200] for rep in range(10) for smooth in [True, False]]\n\n\nnan_df = pl.DataFrame(nan).groupby(['nan', 'n', 'smooth']).count().to_pandas()\n\n\nalt.Chart(nan_df).mark_line().encode(x='n:Q', y='count', color='nan', column='smooth')\n\n\n\nStandard Filter\n\nk = KalmanFilter.init_from(kSR)\n\n\nnan = [{'nan':k.predict(*get_test_data(n), smooth=smooth).std.isnan().any().item(), 'n': n, 'rep': rep, 'smooth': smooth}\n       for n in [10, 20, 30, 40, 50, 55, 60, 70, 100, 150, 200] for rep in range(10) for smooth in [True, False]]\n\n\nnan_df = pl.DataFrame(nan).groupby(['nan', 'n', 'smooth']).count().to_pandas()\n\n\nalt.Chart(nan_df).mark_line().encode(x='n:Q', y='count', color='nan', column='smooth')\n\nso the standard filter is working better than the SR for the smoothing (with this parameter setting), so there is a way to make the sr smoother not that bad\nBut I just want to see how we have nan\nSo the problem is that we have a negative number on the diagonal … so is not positive definite and even the standard deviation is nan\n\nfor i in range(20):\n    dat = get_test_data(10)\n    pred = kSR.predict(*dat)\n    if pred.cov.isnan().any():\n        print(i)\n        break\n\n\nk = KalmanFilter.init_from(kSR)\n\n\nfor p1, p2 in zip(k.parameters(), kSR.parameters()):\n    test_close(p1,p2)\n\n\nf_state_stand = k.filter(*dat)\ns_state_stand = k.smooth(*dat)\npred_stand = k.predict(*dat)\n\n\npred_stand[1, -1].std\n\n\n(f_state_stand.mean - filt_state.mean).mean()\n\n\nfilt_state = kSR.filter(*dat)\n\n\ns_state = kSR.smooth(*dat)\n\n\nis_posdef(s_state.cov)\n\n\nis_posdef(s_state[1, -1].cov)\n\n\nfilt_state[1,-1].cov\n\n\nkSR.H @ s_state[1, -1].cov @ kSR.H.mT\n\n\nkSR.use_sr_pred = False\npred_obs = kSR._obs_from_state(s_state)\npred_obs[1,-1].cov\n\n\nkSR.predict(*dat).\n\n\npred.std[1,-1]"
  },
  {
    "objectID": "kalman_filter.html#additional",
    "href": "kalman_filter.html#additional",
    "title": "Kalman Filter",
    "section": "Additional",
    "text": "Additional\n\nConstructors\n\nSimple parameters\n\nsource\n\n\n\nKalmanFilter.init_simple\n\n KalmanFilter.init_simple (n_dim, dtype=torch.float64)\n\nSimplest version of kalman filter parameters\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_dim\n\n\nn_dim_obs and n_dim_state\n\n\ndtype\ndtype\ntorch.float64\n\n\n\n\n\nKalmanFilter.init_simple(2).state_dict()\n\n\nLocal slope\nLocal slope models are an extentions of local level model that in the state variable keep track of also the slope\nGiven \\(n\\) as the number of dimensions of the observations\nThe transition matrix (A) is:\n\\[A = \\left[\\begin{array}{cc}I & I \\\\ 0 & I\\end{array}\\right]\\]\nwhere:\n\n\\(I \\in \\mathbb{R}^{n \\times n}\\)\n\\(A \\in \\mathbb{R}^{2n \\times 2n}\\)\n\nthe state \\(x \\in \\mathbb{R}^{2N \\times 1}\\) where the upper half keep track of the level and the lower half of the slope. \\(A \\in \\mathbb{R}^2N \\times 2N\\)\nthe observation matrix (H) is:\n\\[H = \\left[\\begin{array}{cc}I & 0 \\end{array}\\right]\\]\nFor the multivariate case the 1 are replaced with an identiy matrix\nassuming that the control has the same dimensions of the observations then if we are doing a local slope model we have \\(B \\in \\mathbb{R}^{state \\times contr}\\): \\[ B = \\begin{bmatrix} -I & I \\\\ 0 & 0 \\end{bmatrix}\\]\n\n\n\n\ninit_local_slope_pca’]\nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.\n\nKalmanFilter.init_local_slope_pca(2,2,pd.DataFrame([[1,2], [2,4]])).state_dict()"
  },
  {
    "objectID": "kalman_filter.html#export",
    "href": "kalman_filter.html#export",
    "title": "Kalman Filter",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "Fluxnet/hainich.html",
    "href": "Fluxnet/hainich.html",
    "title": "Fluxnet Hainich",
    "section": "",
    "text": "['PA', 'P', 'WS', 'WD', 'LW_IN', 'NETRAD']\n\n['PA', 'P', 'WS', 'WD', 'LW_IN', 'NETRAD']\n_def_meteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\n\nmeteo_vars_big = {f\"{var}_F\" : var for var in ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'P']} | {'SWC_F_MDS_1': 'SWC', 'TS_F_MDS_1': 'TS'}\n\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nunits_big = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    'VPD': 'hPa',\n    'PA': 'hPa',\n    'P': 'mm',\n    'WS': 'm s-1',\n    'LW_IN': 'W m-2',\n    'TS': '°C',\n    'SWC': '%'\n    \n    # 'NETRAD': 'W m-2',\n}\n\nhai_path_raw = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\"\nhai_path = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4_float32.parquet\"\nhai_path64 = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4_float64.parquet\"\nhai_big_path = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4_float64_big.parquet\"\nread_col_names(hai_path_raw)\n\nIndex(['TIMESTAMP_START', 'TIMESTAMP_END', 'TA_F_MDS', 'TA_F_MDS_QC', 'TA_ERA',\n       'TA_F', 'TA_F_QC', 'SW_IN_POT', 'SW_IN_F_MDS', 'SW_IN_F_MDS_QC',\n       ...\n       'GPP_DT_CUT_MEAN', 'GPP_DT_CUT_SE', 'GPP_DT_CUT_05', 'GPP_DT_CUT_16',\n       'GPP_DT_CUT_25', 'GPP_DT_CUT_50', 'GPP_DT_CUT_75', 'GPP_DT_CUT_84',\n       'GPP_DT_CUT_95', 'RECO_SR'],\n      dtype='object', length=238)\ncol_types(read_col_names(hai_path_raw)[:10]) # only for 10 cols for testing\n\n{'TIMESTAMP_START': 'str',\n 'TIMESTAMP_END': 'str',\n 'TA_F_MDS': numpy.float32,\n 'TA_F_MDS_QC': None,\n 'TA_ERA': numpy.float32,\n 'TA_F': numpy.float32,\n 'TA_F_QC': None,\n 'SW_IN_POT': numpy.float32,\n 'SW_IN_F_MDS': numpy.float32,\n 'SW_IN_F_MDS_QC': None}\nsource"
  },
  {
    "objectID": "Fluxnet/hainich.html#era",
    "href": "Fluxnet/hainich.html#era",
    "title": "Fluxnet Hainich",
    "section": "ERA",
    "text": "ERA\n\n_def_meteo_vars\n\n{'TA_F': 'TA', 'SW_IN_F': 'SW_IN', 'VPD_F': 'VPD'}\n\n\n\n\n\nCPU times: user 41 s, sys: 32.4 ms, total: 41.1 s\nWall time: 41.3 s\n\n\n\nhai_era.to_parquet(hai_era_path)\n\n\n\n\nCPU times: user 21.2 ms, sys: 10.3 ms, total: 31.5 ms\nWall time: 18.4 ms\n\n\n\nhai_era64 = read_fluxnet_csv(hai_era_path_raw, None, meteo_vars = era_vars, num_dtype=np.float64)\n\n\nhai_era64.to_parquet(hai_era_path64)\n\n\n\n\nCPU times: user 39.8 s, sys: 65.6 ms, total: 39.9 s\nWall time: 40.1 s\n\n\n\nControl map"
  },
  {
    "objectID": "Fluxnet/hainich.html#plotting",
    "href": "Fluxnet/hainich.html#plotting",
    "title": "Fluxnet Hainich",
    "section": "Plotting",
    "text": "Plotting\nScales for consistent colors for plotting variables\n\nunits_big.keys()\n\ndict_keys(['TA', 'SW_IN', 'VPD', 'PA', 'P', 'WS', 'LW_IN', 'TS', 'SWC'])\n\n\n\ndf = pd.DataFrame({'vars' : units_big.keys()})\n\n\nscale_meteo\n\nScale({\n  domain: ['TA', 'SW_IN', 'LW_IN', 'VPD', 'WS', 'PA', 'SWC', 'TS', 'P'],\n  range: ['#1B9E77', '#D95F02', '#7570B3', '#E7298A', '#66A61E', '#E6AB02', '#A6761D', '#666666']\n})\n\n\n\nalt.Chart(df).mark_rect().encode(x = 'vars', color = alt.Color('vars', scale= scale_meteo))\n\n\n\n\n\n\nif we remove one variable the order doesn’t change\n\nalt.Chart(df[df.vars != 'SW_IN']).mark_rect().encode(x = 'vars', color = alt.Color('vars', scale= scale_meteo))"
  },
  {
    "objectID": "Fluxnet/hainich.html#export",
    "href": "Fluxnet/hainich.html#export",
    "title": "Fluxnet Hainich",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "Fluxnet/gap_finder.html",
    "href": "Fluxnet/gap_finder.html",
    "title": "Find Gaps in Fluxnet data",
    "section": "",
    "text": "test_file_zip = here() / Path(\"../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip\")\ntest_file = here() / Path(\"../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4/FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\ntmp_dir = Path(\"/tmp\")\nout_dir = here() / Path(\"../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\ntest_file_zip\n\nPosixPath('/home/simone/Documents/uni/Thesis/GPFA_imputation/../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip')\nunzip the file and load it lazily with polars\nzipf = zipfile.ZipFile(test_file_zip).extract('FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv', path=tmp_dir)\nzipf\n\n'/tmp/FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv'\n# load columns names\ncol_names = pl.read_csv(zipf, n_rows=1).columns\ntypes = {\n    **{\n        # pl.Uint8 should be enough for a QC flag, but some columns are floats in the csv ...\n        col_name: pl.Float32 if col_name.endswith(\"_QC\") else pl.Float64 for col_name in col_names\n    },\n    \"TIMESTAMP_START\": pl.Int64,\n    \"TIMESTAMP_END\": pl.Int64\n}\n# load df with correct types\ndf = pl.scan_csv(zipf, null_values=[\"-9999\", \"-9999.99\"], dtypes=types)\ndf.head().collect()\n\n\n\n\nshape: (5, 238)\nTIMESTAMP_START\nTIMESTAMP_END\nTA_F_MDS\nTA_F_MDS_QC\nTA_ERA\nTA_F\nTA_F_QC\nSW_IN_POT\nSW_IN_F_MDS\nSW_IN_F_MDS_QC\nSW_IN_ERA\nSW_IN_F\nSW_IN_F_QC\nLW_IN_F_MDS\nLW_IN_F_MDS_QC\nLW_IN_ERA\nLW_IN_F\nLW_IN_F_QC\nLW_IN_JSB\nLW_IN_JSB_QC\nLW_IN_JSB_ERA\nLW_IN_JSB_F\nLW_IN_JSB_F_QC\nVPD_F_MDS\nVPD_F_MDS_QC\nVPD_ERA\nVPD_F\nVPD_F_QC\nPA\nPA_ERA\nPA_F\nPA_F_QC\nP\nP_ERA\nP_F\nP_F_QC\nWS\n...\nRECO_DT_VUT_75\nRECO_DT_VUT_84\nRECO_DT_VUT_95\nRECO_DT_CUT_REF\nRECO_DT_CUT_USTAR50\nRECO_DT_CUT_MEAN\nRECO_DT_CUT_SE\nRECO_DT_CUT_05\nRECO_DT_CUT_16\nRECO_DT_CUT_25\nRECO_DT_CUT_50\nRECO_DT_CUT_75\nRECO_DT_CUT_84\nRECO_DT_CUT_95\nGPP_DT_VUT_REF\nGPP_DT_VUT_USTAR50\nGPP_DT_VUT_MEAN\nGPP_DT_VUT_SE\nGPP_DT_VUT_05\nGPP_DT_VUT_16\nGPP_DT_VUT_25\nGPP_DT_VUT_50\nGPP_DT_VUT_75\nGPP_DT_VUT_84\nGPP_DT_VUT_95\nGPP_DT_CUT_REF\nGPP_DT_CUT_USTAR50\nGPP_DT_CUT_MEAN\nGPP_DT_CUT_SE\nGPP_DT_CUT_05\nGPP_DT_CUT_16\nGPP_DT_CUT_25\nGPP_DT_CUT_50\nGPP_DT_CUT_75\nGPP_DT_CUT_84\nGPP_DT_CUT_95\nRECO_SR\ni64\ni64\nf64\nf32\nf64\nf64\nf32\nf64\nf64\nf32\nf64\nf64\nf32\nf64\nf32\nf64\nf64\nf32\nf64\nf32\nf64\nf64\nf32\nf64\nf32\nf64\nf64\nf32\nf64\nf64\nf64\nf32\nf64\nf64\nf64\nf32\nf64\n...\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n200001010000\n200001010030\n-0.6\n0.0\n-0.349\n-0.6\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nnull\nnull\n302.475\n302.475\n2.0\n270.92\n0.0\n307.452\n270.92\n0.0\n0.222\n0.0\n0.403\n0.222\n0.0\n96.63\n96.671\n96.63\n0.0\n0.0\n0.011\n0.0\n0.0\n2.05\n...\n1.04549\n1.04774\n1.04848\n1.02086\n1.02913\n1.02528\n0.006221\n0.938402\n0.965572\n1.0197\n1.03409\n1.04549\n1.04669\n1.04848\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.23379\n200001010030\n200001010100\n-0.65\n0.0\n-0.39\n-0.65\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nnull\nnull\n302.475\n302.475\n2.0\n271.265\n0.0\n307.452\n271.265\n0.0\n0.122\n0.0\n0.399\n0.122\n0.0\n96.58\n96.676\n96.58\n0.0\n0.0\n0.011\n0.0\n0.0\n2.53\n...\n1.04422\n1.04647\n1.0472\n1.01962\n1.02788\n1.02403\n0.006213\n0.937264\n0.964401\n1.01847\n1.03283\n1.04422\n1.04542\n1.0472\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.23238\n200001010100\n200001010130\n-0.58\n0.0\n-0.43\n-0.58\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nnull\nnull\n301.677\n301.677\n2.0\n271.958\n0.0\n306.541\n271.958\n0.0\n0.09\n0.0\n0.396\n0.09\n0.0\n96.56\n96.682\n96.56\n0.0\n0.0\n0.0\n0.0\n0.0\n3.15\n...\n1.046\n1.04825\n1.04898\n1.02135\n1.02963\n1.02577\n0.006224\n0.938856\n0.96604\n1.0202\n1.03459\n1.046\n1.0472\n1.04898\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.23238\n200001010130\n200001010200\n-0.51\n0.0\n-0.439\n-0.51\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nnull\nnull\n301.677\n301.677\n2.0\n272.292\n0.0\n306.541\n272.292\n0.0\n0.11\n0.0\n0.411\n0.11\n0.0\n96.56\n96.673\n96.56\n0.0\n0.0\n0.0\n0.0\n0.0\n3.12\n...\n1.04777\n1.05003\n1.05076\n1.02308\n1.03137\n1.02751\n0.006234\n0.940447\n0.967676\n1.02193\n1.03634\n1.04777\n1.04897\n1.05076\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.23521\n200001010200\n200001010230\n-0.49\n0.0\n-0.449\n-0.49\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nnull\nnull\n301.677\n301.677\n2.0\n272.481\n0.0\n306.541\n272.481\n0.0\n0.102\n0.0\n0.426\n0.102\n0.0\n96.57\n96.665\n96.57\n0.0\n0.0\n0.0\n0.0\n0.0\n3.04\n...\n1.04827\n1.05054\n1.05127\n1.02357\n1.03187\n1.02801\n0.006237\n0.940901\n0.968143\n1.02242\n1.03684\n1.04827\n1.04948\n1.05127\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.23379\ncolumns selection, interested only in QC columns to find gaps\ndf.head().select(pl.col(\"^.*_QC$\")).collect().columns\n\n['TA_F_MDS_QC',\n 'TA_F_QC',\n 'SW_IN_F_MDS_QC',\n 'SW_IN_F_QC',\n 'LW_IN_F_MDS_QC',\n 'LW_IN_F_QC',\n 'LW_IN_JSB_QC',\n 'LW_IN_JSB_F_QC',\n 'VPD_F_MDS_QC',\n 'VPD_F_QC',\n 'PA_F_QC',\n 'P_F_QC',\n 'WS_F_QC',\n 'CO2_F_MDS_QC',\n 'TS_F_MDS_1_QC',\n 'TS_F_MDS_2_QC',\n 'TS_F_MDS_3_QC',\n 'TS_F_MDS_4_QC',\n 'TS_F_MDS_5_QC',\n 'SWC_F_MDS_1_QC',\n 'SWC_F_MDS_2_QC',\n 'SWC_F_MDS_3_QC',\n 'G_F_MDS_QC',\n 'LE_F_MDS_QC',\n 'H_F_MDS_QC',\n 'NEE_CUT_REF_QC',\n 'NEE_VUT_REF_QC',\n 'NEE_CUT_USTAR50_QC',\n 'NEE_VUT_USTAR50_QC',\n 'NEE_CUT_MEAN_QC',\n 'NEE_VUT_MEAN_QC',\n 'NEE_CUT_05_QC',\n 'NEE_CUT_16_QC',\n 'NEE_CUT_25_QC',\n 'NEE_CUT_50_QC',\n 'NEE_CUT_75_QC',\n 'NEE_CUT_84_QC',\n 'NEE_CUT_95_QC',\n 'NEE_VUT_05_QC',\n 'NEE_VUT_16_QC',\n 'NEE_VUT_25_QC',\n 'NEE_VUT_50_QC',\n 'NEE_VUT_75_QC',\n 'NEE_VUT_84_QC',\n 'NEE_VUT_95_QC']\nThe goal is to find where the data is missing in the dataset (which means that it has been gap-filled) and find:\nwe filter out the rows where there is no gap (QC =0)\nthen find the start and end of gap by comparing with the original row number of the previous entry\ndf.select(\n        [pl.col(\"TA_F_QC\"), \"TIMESTAMP_START\"]\n    ).with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    ).filter(\n        pl.col(\"TA_F_QC\") != 0\n    ).slice(10, 10).collect()\n\n\n\n\nshape: (10, 3)\nTA_F_QC\nTIMESTAMP_START\nrow_num\nf32\ni64\nu32\n1.0\n200112201230\n34537\n1.0\n200206051100\n42550\n1.0\n200211081330\n50043\n1.0\n200404200930\n75427\n2.0\n200404201000\n75428\n2.0\n200404201030\n75429\n2.0\n200404201100\n75430\n2.0\n200404201130\n75431\n2.0\n200404201200\n75432\n2.0\n200404201230\n75433\ndf.select(\n        [pl.col(\"TA_F_QC\"), \"TIMESTAMP_START\"]\n    ).with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    ).filter(\n        pl.col(\"TA_F_QC\") != 0\n    ).slice(10, 15).with_columns([\n        (pl.col(\"row_num\") - pl.col(\"row_num\").shift() ).alias(\"before\"),\n        (pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\")).alias(\"after\"),\n    ]).collect()\n\n\n\n\nshape: (15, 5)\nTA_F_QC\nTIMESTAMP_START\nrow_num\nbefore\nafter\nf32\ni64\nu32\nu32\nu32\n1.0\n200112201230\n34537\nnull\n8013\n1.0\n200206051100\n42550\n8013\n7493\n1.0\n200211081330\n50043\n7493\n25384\n1.0\n200404200930\n75427\n25384\n1\n2.0\n200404201000\n75428\n1\n1\n2.0\n200404201030\n75429\n1\n1\n2.0\n200404201100\n75430\n1\n1\n2.0\n200404201130\n75431\n1\n1\n2.0\n200404201200\n75432\n1\n1\n2.0\n200404201230\n75433\n1\n1\n1.0\n200404201300\n75434\n1\n46940\n1.0\n200612241100\n122374\n46940\n1\n1.0\n200612241130\n122375\n1\n1\n1.0\n200612241200\n122376\n1\n1\n1.0\n200612241230\n122377\n1\nnull\ndf.select(\n        [pl.col(\"TA_F_QC\"), \"TIMESTAMP_START\"]\n    ).with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    ).filter(\n        pl.col(\"TA_F_QC\") != 0\n    ).slice(10, 15).with_columns([\n        (pl.col(\"row_num\") - pl.col(\"row_num\").shift() ).alias(\"before\"),\n        (pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\")).alias(\"after\"),\n    ]).filter(\n        (pl.col(\"before\") != 1) | (pl.col(\"after\") != 1)\n    ).with_column(\n        (pl.when((pl.col(\"before\") != 1) & (pl.col(\"after\") != 1))\n        .then(1)\n        .otherwise(pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\") + 1)\n        .alias(\"gap_len\"))\n    ).collect()\n\n\n\n\nshape: (7, 6)\nTA_F_QC\nTIMESTAMP_START\nrow_num\nbefore\nafter\ngap_len\nf32\ni64\nu32\nu32\nu32\nu32\n1.0\n200112201230\n34537\nnull\n8013\n1\n1.0\n200206051100\n42550\n8013\n7493\n1\n1.0\n200211081330\n50043\n7493\n25384\n1\n1.0\n200404200930\n75427\n25384\n1\n8\n1.0\n200404201300\n75434\n1\n46940\n46941\n1.0\n200612241100\n122374\n46940\n1\n4\n1.0\n200612241230\n122377\n1\nnull\nnull\ngaps = df.select(\n        [pl.col(\"TA_F_QC\"), \"TIMESTAMP_START\"]\n    ).with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    ).filter(\n        pl.col(\"TA_F_QC\") != 0\n    ).with_columns([\n        (pl.col(\"row_num\") - pl.col(\"row_num\").shift() ).alias(\"before\"),\n        (pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\")).alias(\"after\"),\n    ]).filter(\n        (pl.col(\"before\") != 1) | (pl.col(\"after\") != 1)\n    ).with_column(\n        (pl.when((pl.col(\"before\") != 1) & (pl.col(\"after\") != 1))\n        .then(1)\n        .otherwise(pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\") + 1)\n        .alias(\"gap_len\"))\n    ).filter(\n        pl.col(\"before\") != 1\n    ).select(\n        [\"TIMESTAMP_START\", \"gap_len\"]\n    ).collect()\ngaps\n\n\n\n\nshape: (25, 2)\nTIMESTAMP_START\ngap_len\ni64\nu32\n200007261130\n1\n200007271230\n1\n200008021000\n1\n200008161200\n1\n200008231030\n1\n200009201200\n1\n200012040100\n1\n200107121630\n1\n200108190930\n1\n200110231230\n1\n200112201230\n1\n200206051100\n1\n200211081330\n1\n200404200930\n8\n200612241100\n15\n200904021230\n1\n200912281130\n341\n201003161130\n1490\n201007010000\n22\n201104211030\n1\n201105051030\n1\n201108111030\n1008\n201206042230\n4\n201209250930\n47\n201210090930\n3\ngaps.select(pl.col(\"gap_len\").sum())\n\n\n\n\nshape: (1, 1)\ngap_len\nu32\n2954\ndf.select(\n        [pl.col(\"TA_F_QC\"), \"TIMESTAMP_START\"]\n    ).with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    ).filter(\n        pl.col(\"TA_F_QC\") != 0\n    ).collect().shape\n\n(2954, 3)\nit works!\ndef find_gap(df, col_name):\n    return df.select(\n        [col_name, pl.col(\"TIMESTAMP_END\").alias(\"gap_start\")]\n    ).with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    ).filter(\n        pl.col(col_name) != 0\n    ).with_columns([\n        (pl.col(\"row_num\") - pl.col(\"row_num\").shift() ).alias(\"before\"),\n        (pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\")).alias(\"after\"),\n    ]).filter(\n        (pl.col(\"before\") != 1) | (pl.col(\"after\") != 1)\n    ).with_column(\n        (pl.when((pl.col(\"before\") != 1) & (pl.col(\"after\") != 1))\n        .then(pl.col(\"gap_start\"))\n        .otherwise(pl.col(\"gap_start\").shift(-1))\n        .alias(\"gap_end\"))\n    ).filter(\n        pl.col(\"before\") != 1\n    ).select(\n        [\"gap_start\", \"gap_end\", pl.lit(col_name).alias(\"variable\")]\n    )\nsource"
  },
  {
    "objectID": "Fluxnet/gap_finder.html#find-gaps",
    "href": "Fluxnet/gap_finder.html#find-gaps",
    "title": "Find Gaps in Fluxnet data",
    "section": "Find gaps",
    "text": "Find gaps\n\ndef _find_gap_df(df, col_name):\n    \"Find gaps with a df with a single QC column\"\n    return df.filter(\n        pl.col(col_name) != 0\n    ).with_columns([\n        (pl.col(\"row_num\") - pl.col(\"row_num\").shift() ).alias(\"before\"),\n        (pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\")).alias(\"after\"),\n    ]).filter(\n        (pl.col(\"before\") != 1) | (pl.col(\"after\") != 1)\n    ).with_column(\n        (pl.when((pl.col(\"before\") != 1) & (pl.col(\"after\") != 1))\n        .then(pl.col(\"start\"))\n        .otherwise(pl.col(\"start\").shift(-1))\n        .alias(\"gap_end\"))\n    ).filter(\n        pl.col(\"before\") != 1\n    ).select(\n        [pl.col(\"start\").alias(\"gap_start\"), \"gap_end\"]\n    )\n\n\ndef find_gap_variable(df, col_name):\n    \n    # row numembering has to happen before filtering\n    df = df.with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    )\n    \n    # start with null values\n    dff = df.filter(\n            pl.col(col_name).is_null()\n        )\n    gaps = [\n        _find_gap_df(dff, col_name).with_columns(\n            pl.lit(None).alias(\"gap_value\")\n        )]\n    \n    \n    # all other values\n    # here the QC flags are merged together as we we not interested in the QC alg only if there is a gap\n    dff = df.filter(\n            ~pl.col(col_name).is_null()\n        )\n    gaps.append(\n        _find_gap_df(dff, col_name).with_columns(\n            pl.lit(1).alias(\"gap_value\")\n        ))\n    \n    return pl.concat(gaps)\n\n\nfind_gap_variable(df, \"SW_IN_F_QC\").collect().head()\n\n\ngaps_all.groupby(\"variable\").agg(pl.col(\"gap_len\").sum() / df.collect().shape[0])\n\n\ndef download_and_find_gaps(urls, download_dir, out_dir, tmp_dir):\n    site_infos = []\n    for url in tqdm(urls):\n        file_zip = download_fluxnet(url, download_dir)\n        file, site_info = find_gaps_fluxnet_archive(file_zip, out_dir, tmp_dir)\n        site_infos.append(site_info)\n        print(file)\n        \n    return pl.concat(site_infos)\n\n\nsource\n\ndownload_and_find_gaps\n\n download_and_find_gaps (urls, download_dir, out_dir, tmp_dir)\n\n\nurls = [\"https://ftp.fluxdata.org/.fluxnet_downloads_86523/FLUXNET2015/FLX_AR-SLu_FLUXNET2015_FULLSET_2009-2011_1-4.zip?=mone27\",\n\"https://ftp.fluxdata.org/.fluxnet_downloads_86523/FLUXNET2015/FLX_AR-Vir_FLUXNET2015_FULLSET_2009-2012_1-4.zip?=mone27\"]\n\n\ndownload_and_find_gaps(urls, download_dir, out_dir, tmp_dir)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPFA Imputation",
    "section": "",
    "text": "This is developed by Simone Massaro as a master thesis in Ecosystem Analysis and Modelling at the University of Göttingen, Germany\nThe aim is to impute gaps into the meteorological time series for Eddy-Covariance applications.\nModels developed: - Gaussian Processes Factor Analysis (GPFA) - space state models (Kalman Filter)"
  },
  {
    "objectID": "index.html#repository-structure",
    "href": "index.html#repository-structure",
    "title": "GPFA Imputation",
    "section": "Repository Structure",
    "text": "Repository Structure"
  },
  {
    "objectID": "index.html#other-imputation-methods",
    "href": "index.html#other-imputation-methods",
    "title": "GPFA Imputation",
    "section": "Other imputation methods",
    "text": "Other imputation methods"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "GPFA Imputation",
    "section": "License",
    "text": "License\nThe code under model_imp.kalman.filter has the following license\n```All code contained except that in pykalman/utils.py is released under the license below. All code in pykalman/utils.py is released under the license contained therein.\nNew BSD License\nCopyright (c) 2012 Daniel Duckworth. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of Daniel Duckworth nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ```"
  },
  {
    "objectID": "gaussian.html",
    "href": "gaussian.html",
    "title": "Gaussian Distributions",
    "section": "",
    "text": "from fastcore.test import *\nimport altair as alt"
  },
  {
    "objectID": "gaussian.html#normal-parameters",
    "href": "gaussian.html#normal-parameters",
    "title": "Gaussian Distributions",
    "section": "Normal Parameters",
    "text": "Normal Parameters\n\nNormal\n\nimport torch\n\n\nsource\n\n\nListNormal.detach\n\n ListNormal.detach ()\n\nDetach both mean and cov at once\n\nln = ListNormal(torch.rand(10), torch.rand(10))\n\n\nln[5]\n\nNormal(mean=tensor(0.2285), std=tensor(0.9300))\n\n\n\n\nMultivariate Normal\n\n\n\nListMultiNormal.detach\n\n ListMultiNormal.detach ()\n\nDetach both mean and cov at once\n\nListMNormal(torch.rand(2,10), torch.rand(2,10,10))[1]\n\nMultiNormal(mean=tensor([0.0076, 0.7078, 0.1849, 0.8875, 0.0598, 0.5685, 0.9130, 0.4167, 0.7761,\n        0.8239]), cov=tensor([[0.5052, 0.1173, 0.9735, 0.2103, 0.6431, 0.2104, 0.4656, 0.0827, 0.1011,\n         0.5391],\n        [0.3806, 0.4621, 0.5505, 0.0553, 0.1300, 0.3820, 0.2128, 0.1168, 0.1066,\n         0.3580],\n        [0.3908, 0.8341, 0.5620, 0.8415, 0.5727, 0.3466, 0.1083, 0.6935, 0.1600,\n         0.1613],\n        [0.4332, 0.6150, 0.0203, 0.4674, 0.6139, 0.7792, 0.4534, 0.4927, 0.2141,\n         0.2313],\n        [0.5573, 0.6323, 0.8267, 0.7420, 0.7631, 0.2128, 0.1779, 0.7642, 0.0040,\n         0.8567],\n        [0.2587, 0.3077, 0.2753, 0.3718, 0.1282, 0.9497, 0.7606, 0.3450, 0.7562,\n         0.8713],\n        [0.4206, 0.1561, 0.3089, 0.5148, 0.9883, 0.3910, 0.0613, 0.1449, 0.1414,\n         0.2631],\n        [0.9216, 0.2119, 0.9350, 0.4848, 0.3025, 0.9027, 0.3520, 0.2612, 0.6861,\n         0.0352],\n        [0.3255, 0.7473, 0.8152, 0.7568, 0.3317, 0.2959, 0.8844, 0.4175, 0.2749,\n         0.8595],\n        [0.8556, 0.1053, 0.9823, 0.1468, 0.9170, 0.9112, 0.8187, 0.6592, 0.1459,\n         0.9400]]))"
  },
  {
    "objectID": "gaussian.html#positive-definite",
    "href": "gaussian.html#positive-definite",
    "title": "Gaussian Distributions",
    "section": "Positive Definite",
    "text": "Positive Definite\nThe covariance matrices need to be positive definite Those are utilities functions to check is a matrix is positive definite and to make any matrix positive definite\n\nOther libraries\nMost libraries that implement Kalman Filters use manually specified parameters, which often don’t have the issue of the positive definite constraint (eg. pykalman)\nFrom statsmodels statespace models: &gt;Cholesky decomposition […] requires that the matrix be positive definite. While this should generally be true, it may not be in every case. source\nwhich seems to mean that they take into account the fact that during the filter calculations may not be positive definite\n\nA = torch.rand(2,3,3) # batched random matrix used for testing\n\n\n\nSymmetry\n\nsource\n\n\nis_symmetric\n\n is_symmetric (value, atol=1e-05)\n\n\nis_symmetric(A)\n\ntensor([False, False])\n\n\n\nsource\n\n\nsymmetric_upto_batched\n\n symmetric_upto_batched (value, start=-8)\n\n\nsource\n\n\nsymmetric_upto\n\n symmetric_upto (value, start=-8)\n\n\nsymmetric_upto_batched(A)\n\ntensor([0, 0])\n\n\n\nis posdef\nDefault pytorch check (uses symmetry + cholesky decomposition)\n\nsource\n\n\n\nis_posdef\n\n is_posdef (cov)\n\n\nis_posdef(A)\n\ntensor([False, False])\n\n\ncheck if it is pos definite using eigenvalues. Positive definite matrix have all positive eigenvalues\n\ntorch.linalg.eigvalsh(A)\n\ntensor([[0.1633, 0.4556, 1.0072],\n        [0.0982, 0.3418, 1.7830]])\n\n\n\nsource\n\n\nis_posdef_eigv\n\n is_posdef_eigv (cov)\n\n\nis_posdef_eigv(A)\n\n(tensor([True, True]),\n tensor([[0.1633, 0.4556, 1.0072],\n         [0.0982, 0.3418, 1.7830]]))\n\n\nNote that is_posdef and is_posdef_eigv can return different values, in general is_posdef_eigv is more tollerant\n\n\nPytorch constraint\ntransform any matrix \\(A\\) into a positive definite matrix (\\(PD\\)) using the following formula\n\\(PD = AA^T + aI\\)\nwhere \\(AA^T\\) is a positive semi-definite matrix and \\(a\\) is a small positive number that is added on the diagonal to ensure that the resulting matrix is positive definite (not semi-definite)\nthe inverse transformation uses cholesky decomposition\nAnother approach would be to multiple to lower triangular matrix, but they’d require a positive diagonal, which is harderd to obtain see https://en.wikipedia.org/wiki/Definite_matrix#Cholesky_decomposition\nThe API inspired by gpytorch constraints\n\nfrom meteo_imp.utils import *\n\n\nsource\n\n\ninv_softplus\n\n inv_softplus (x)\n\n\nsource\n\n\nbatch_diag_embed\n\n batch_diag_embed (x)\n\n\nsource\n\n\nbatch_diag_scatter\n\n batch_diag_scatter (input, src)\n\n\nsource\n\n\nbatch_diagonal\n\n batch_diagonal (x)\n\n\nsource\n\n\nPosDef\n\n PosDef (min_diag:float=1e-05)\n\nPositive Definite Constraint for PyTorch parameters\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmin_diag\nfloat\n1e-05\nmin value for diagonal to ensure num stability\n\n\n\n\nconstraint = PosDef()\n\nposdef = constraint.transform(A)\n\n\nA = torch.randn(2, 3,3)\ntriang = constraint.transform_triangular(A)\np_diag = constraint.transform_pos_diag(triang)\ncho_fact = constraint.transform_cho_factor(A)\nposdef = constraint.transform(A)\nshow_as_row(A, triang, p_diag, cho_fact, posdef)\n\n\n A tensor([[[-0.0246,  0.4967,  0.1831],\n         [-1.5187,  0.5129, -0.9096],\n         [ 1.9180,  0.8267, -0.9243]],\n\n        [[-0.1214, -0.5275,  0.6791],\n         [ 0.0395,  1.2408,  1.2185],\n         [-0.9288, -0.0625, -0.1523]]])\n triang tensor([[[-0.0246,  0.0000,  0.0000],\n         [-1.5187,  0.5129,  0.0000],\n         [ 1.9180,  0.8267, -0.9243]],\n\n        [[-0.1214,  0.0000,  0.0000],\n         [ 0.0395,  1.2408,  0.0000],\n         [-0.9288, -0.0625, -0.1523]]])\n p_diag tensor([[[ 0.6809,  0.0000,  0.0000],\n         [-1.5187,  0.9821,  0.0000],\n         [ 1.9180,  0.8267,  0.3342]],\n\n        [[ 0.6343,  0.0000,  0.0000],\n         [ 0.0395,  1.4948,  0.0000],\n         [-0.9288, -0.0625,  0.6199]]])\n cho_fact tensor([[[ 0.6809,  0.0000,  0.0000],\n         [-1.5187,  0.9821,  0.0000],\n         [ 1.9180,  0.8267,  0.3342]],\n\n        [[ 0.6343,  0.0000,  0.0000],\n         [ 0.0395,  1.4948,  0.0000],\n         [-0.9288, -0.0625,  0.6199]]])\n posdef tensor([[[ 0.4637, -1.0342,  1.3060],\n         [-1.0342,  3.2711, -2.1010],\n         [ 1.3060, -2.1010,  4.4736]],\n\n        [[ 0.4024,  0.0250, -0.5892],\n         [ 0.0250,  2.2361, -0.1301],\n         [-0.5892, -0.1301,  1.2509]]])\n \n\n\n\nshow_as_row(is_posdef(torch.stack([posdef,A])), is_posdef_eigv(torch.stack([posdef,A])), is_symmetric(torch.stack([posdef,A])))\n\n\n #0 tensor([[ True,  True],\n        [False, False]])\n #1 (tensor([[ True,  True],\n        [False, False]]),\n tensor([[[ 4.5415e-03,  1.6879e+00,  6.5159e+00],\n         [ 9.9948e-02,  1.5298e+00,  2.2596e+00]],\n\n        [[-3.1540e+00,  7.2122e-01,  1.9967e+00],\n         [-1.0659e+00,  7.8085e-01,  1.2522e+00]]]))\n #2 tensor([[ True,  True],\n        [False, False]])\n \n\n\n\ntest_eq(is_posdef(posdef).all(), True)\n\n\ntest_close(posdef, constraint.transform(constraint.inverse_transform(posdef)))\n\n\nsymmetric_upto(posdef[0])\n\n-8\n\n\n\nis_posdef_eigv(to_posdef(torch.rand(1000, 1000)))[0]\n\ntensor(False)\n\n\n\n\nFuzzer\n\nrun_fuzzer = True # temporly disable for performance reasons\n\n\ndef random_posdef(bs=10,n=100,n_range=(0,1), **kwargs):\n    A = torch.rand(bs,n,n, **kwargs)  * (n_range[1]-n_range[0]) + n_range[0]\n    return PosDef().transform(A)\n\n\n# fuzzer\ndef fuzz_posdef(bs=10,n=100,n_range=(0,1), **kwargs):\n    posdef = random_posdef(bs, n, **kwargs)\n    return pd.DataFrame(\n        {'n': [n], 'range': str(n_range), 'n_samples': bs,\n         'posdef': is_posdef(posdef).sum().item() / bs,\n         'sym': is_symmetric(posdef).sum().item() / bs, \n         'posdef_eigv': is_posdef_eigv(posdef)[0].sum().item() / bs\n    })\n\n\nfuzz_posdef()\n\n\n\n\n\n\n\n\nn\nrange\nn_samples\nposdef\nsym\nposdef_eigv\n\n\n\n\n0\n100\n(0, 1)\n10\n0.9\n1.0\n0.6\n\n\n\n\n\n\n\n\nn_min, n_max = -1, 1\nA = torch.rand(2,100,100)  * (n_max-n_min) + n_min\n\n\nis_posdef(to_posdef(A))\n\ntensor([False, False])\n\n\n\nma = torch.tensor([[1., 7],\n                   [-3, 4]])\n\n\nis_posdef(to_posdef(ma))\n\ntensor(True)\n\n\n\nfuzz_posdef(device='cuda')\n\n\n\n\n\n\n\n\nn\nrange\nn_samples\nposdef\nsym\nposdef_eigv\n\n\n\n\n0\n100\n(0, 1)\n10\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n# %time fuzz_posdef(bs=100, device='cuda')\n\n\nrate_posdef = pd.concat([fuzz_posdef(n=n, bs=100, n_range=n_range, device='cuda') \n               for n in [10, 100]\n               for n_range in [(-1,1),(0,1)]])\n\n\nimport altair as alt\nfrom altair import datum\n\n\nrate_posdef.head()\n\n\n\n\n\n\n\n\nn\nrange\nn_samples\nposdef\nsym\nposdef_eigv\n\n\n\n\n0\n10\n(-1, 1)\n100\n1.00\n1.0\n1.0\n\n\n0\n10\n(0, 1)\n100\n1.00\n1.0\n1.0\n\n\n0\n100\n(-1, 1)\n100\n0.98\n1.0\n1.0\n\n\n0\n100\n(0, 1)\n100\n0.97\n1.0\n1.0\n\n\n\n\n\n\n\n\ndef _plot_var(df, var, x='n:N', row='range', y_domain=(0,1), height=70, width=50):\n    bar = alt.Chart(df).mark_bar().encode(\n        x = alt.X('n:N'),\n        y = alt.Y(var, scale=alt.Scale(domain=y_domain)),\n        color = 'n:N',\n    ).properties(height=height, width=width, ) \n    \n    text = alt.Chart(df).mark_text(dy=10, color='white').encode(\n        x = alt.X('n:N'),\n        y = alt.Y(var),\n        text = alt.Text(var, format=\".2f\")\n    )\n    \n    return (bar + text).facet(\n        row=row).properties(title=var, )\n\n\ndef _plot_var_box(df, var, x='n:N', row='range', column='noise:N', height=70, width=50, title=''):\n    box = alt.Chart(df).mark_boxplot().encode(\n        x = alt.X(x),\n        y = alt.Y(var),\n        color = x,\n    ).properties(height=height, width=width) \n\n    # text = alt.Chart(df).mark_text(dy=10, color='white').encode(\n    #     x = alt.X('n:N'),\n    #     y = alt.Y(var),\n    #     text = alt.Text(var, format=\".2f\")\n    # )\n    \n    return (box).facet(\n        column=column,\n        row=row).properties(title=title)\n\n\nfrom IPython import display\nimport vl_convert as vlc\nfrom functools import partial\n\n\nGeneration of Random positive definite matrices\n\ndef plot_posdef_simulation(n_s, range_s, bs=100, **kwargs):\n    if not run_fuzzer: return\n    rate_posdef = pd.concat([fuzz_posdef(n=n, bs=bs, n_range=range, device='cuda', **kwargs) \n               for n in n_s for range in range_s])\n    \n    print(rate_posdef)\n    vl_spec = alt.hconcat(*[_plot_var(rate_posdef, var) for var in ['posdef', 'posdef_eigv']]).to_json()\n    # workaround for bug in vegalite see https://github.com/altair-viz/altair/issues/2742\n    svg = vlc.vegalite_to_svg(vl_spec, vl_version='v5.3')\n    display.display(display.HTML(svg))\n\n\nplot_posdef_simulation(n_s = [10, 100], range_s = [(-1, 1)], bs=1000)\n\n     n    range  n_samples  posdef  sym  posdef_eigv\n0   10  (-1, 1)       1000   1.000  1.0        1.000\n0  100  (-1, 1)       1000   0.972  1.0        0.998\n\n\nrange0.00.51.0posdef(-1, 1)10100n1.000.97posdefrange0.00.51.0posdef_eigv(-1, 1)10100n1.001.00posdef_eigv10100n\n\n\nLet’s go big by using a matrix 1000x1000\n\nplot_posdef_simulation(n_s = [1000], range_s = [(10, 20)], bs=100)\n\n      n     range  n_samples  posdef  sym  posdef_eigv\n0  1000  (10, 20)        100     0.0  1.0          0.0\n\n\nrange0.00.51.0posdef(10, 20)1000n0.00posdefrange0.00.51.0posdef_eigv(10, 20)1000n0.00posdef_eigv1000n\n\n\nfor a standard noise on the diagonal less than half of the random matrices that are 1000 in size are positive definite.\nLet’s have a look at one of such matrices\n\nposdef = random_posdef(100, 1000)\nnot_pd = posdef[torch.argwhere(~is_posdef_eigv(posdef)[0])[0]]\n\nThis should be positive definite but actually it’s not …\n\nnot_pd\n\ntensor([[[4.8334e-01, 5.3647e-01, 5.7627e-01,  ..., 4.4425e-01,\n          3.0689e-01, 2.0315e-01],\n         [5.3647e-01, 2.2643e+00, 1.1408e+00,  ..., 1.1138e+00,\n          4.2245e-01, 3.3727e-01],\n         [5.7627e-01, 1.1408e+00, 1.8559e+00,  ..., 1.3060e+00,\n          1.3603e+00, 7.1770e-01],\n         ...,\n         [4.4425e-01, 1.1138e+00, 1.3060e+00,  ..., 3.4489e+02,\n          2.5251e+02, 2.4902e+02],\n         [3.0689e-01, 4.2245e-01, 1.3603e+00,  ..., 2.5251e+02,\n          3.3394e+02, 2.4621e+02],\n         [2.0315e-01, 3.3727e-01, 7.1770e-01,  ..., 2.4902e+02,\n          2.4621e+02, 3.2393e+02]]])\n\n\ntrying with float64 (for memory constraint on the GPU only using a 700x700 matrix)\n\nplot_posdef_simulation(n_s = [700], range_s = [(-.1, 1)], bs=100)\n\n     n      range  n_samples  posdef  sym  posdef_eigv\n0  700  (-0.1, 1)        100     0.0  1.0          0.0\n\n\nrange0.00.51.0posdef(-0.1, 1)700n0.00posdefrange0.00.51.0posdef_eigv(-0.1, 1)700n0.00posdef_eigv700n\n\n\n\nplot_posdef_simulation(n_s = [700], range_s = [(-.1, 1)], bs=100, dtype=torch.float64)\n\n     n      range  n_samples  posdef  sym  posdef_eigv\n0  700  (-0.1, 1)        100     0.0  1.0          0.0\n\n\nrange0.00.51.0posdef(-0.1, 1)700n0.00posdefrange0.00.51.0posdef_eigv(-0.1, 1)700n0.00posdef_eigv700n\n\n\nAll matrices now are positive definite\n\n\nMultiplication\ncheck is multiplication of matrices is not breaking the positive definite constraint\nIf \\(A\\) and \\(B\\) are both positive definite matrices \\(ABA\\) is also positive definite https://en.wikipedia.org/wiki/Definite_matrix#Multiplication\n\ndef fuzz_op(op, # operation that takes 2 pos def matrices and return one pos def matrix\n            fn_check = is_posdef,\n                  n=100, # size of matrix\n                  max_t=1000, # number of multiplications\n                  noise=1e-5, # noise to add on diagonal\n                  bs=10, # batch size\n                  n_range=(0,1), # range of random numbers\n                  **kwargs):\n    pd1 = random_posdef(bs, n, noise, n_range, **kwargs)\n    pd2 = random_posdef(bs, n, noise, n_range,**kwargs)\n    stop_times = torch.zeros(bs, **kwargs)\n    \n    for t in torch.arange(max_t):\n        pd1 = op(pd1, pd2)\n        check = fn_check(pd1)\n        stop_times[torch.logical_and(stop_times == 0, ~check)] = t\n        if not check.any(): break\n         \n    stop_times[stop_times == 0] = t\n    return pd.DataFrame(\n        {'n': [n], 'noise': f\"{noise:.0e}\", 'range': str(n_range), 'n_samples': bs, 'last_t': t.item(),\n         'mean_stop': stop_times.mean().item(),\n         'std_stop': stop_times.std().item(),\n         'stop_times': [stop_times.cpu().numpy()]})\n\n\nfuzz_multiply = partial(fuzz_op, lambda pd1, pd2: pd2 @ pd1 @ pd2)\nfuzz_multiply_eigv = partial(fuzz_multiply, fn_check = lambda pd1: is_posdef_eigv(pd1)[0])\n\n\ndef plot_multiply_simulation(n_s, noise_s, max_mult=1000, bs=100, **kwargs):\n    mult = pd.concat([fuzz_multiply(n=n, noise=noise, bs=bs, device='cuda', **kwargs) \n               for n in n_s for noise in noise_s]).explode('stop_times')\n    \n    mult_eigv = pd.concat([fuzz_multiply(n=n, noise=noise, bs=bs, device='cuda', **kwargs) \n               for n in n_s for noise in noise_s]).explode('stop_times')\n    \n    vl_spec = alt.hconcat(*[_plot_var_box(df, 'stop_times') for df in [mult, mult_eigv]]).to_json()\n    # workaround for bug in vegalite see https://github.com/altair-viz/altair/issues/2742\n    svg = vlc.vegalite_to_svg(vl_spec, vl_version='v5.3')\n    display.display(display.HTML(svg))\n    return (mult, mult_eigv)\n\n\nplot_multiply_simulation(n_s=[2,3,10, 100], noise_s=[1e-3, 1e-4, 1e-5], bs=100);\n\nTypeError: random_posdef() takes from 0 to 3 positional arguments but 4 were given\n\n\n\n\nAddition\ncheck is multiplication of matrices is not breaking the positive definite constraint\nIf \\(A\\) and \\(B\\) are both positive definite matrices \\(A+B\\) is also positive definite https://en.wikipedia.org/wiki/Definite_matrix#Addition\n\npd1 = random_posdef(10, 100)\npd2 = random_posdef(10, 100)\n\n\nis_posdef(pd1 + pd2).all()\n\n\nfuzz_add = partial(fuzz_op, lambda pd1, pd2: pd1 + pd2)\n\n\ndef plot_add_simulation(n_s, noise_s, max_ts=[1000], bs=100, **kwargs):\n    add = pd.concat([fuzz_add(n=n, noise=noise, bs=bs, max_t=max_t, device='cuda', **kwargs) \n               for n in n_s for noise in noise_s for max_t in max_ts]).explode('stop_times')\n    \n    vl_spec = _plot_var_box(add, var='stop_times', height=150, width=150).to_json()\n\n    svg = vlc.vegalite_to_svg(vl_spec, vl_version='v5.3')\n    display.display(display.HTML(svg))\n\n\ncache_disk(\"add_plot\")(lambda: plot_add_simulation(n_s=[50, 100, 150], noise_s=[1e-3, 1e-4, 1e-5], bs=100, max_ts=[1e5]))()\n\n\n\nNumpy posdef\n\nimport numpy as np\n\n\narr = np.random.rand(2,3,3)\n\n\narr.shape\n\n\narr.transpose(0,2,1) == np.moveaxis(arr, -1, -2)\n\n\ndef to_posdef_np(x, noise=1e-5):\n    return x @ np.moveaxis(x, -1, -2) + (noise * np.eye(x.shape[-1], dtype=arr.dtype))\n\n\nto_posdef_np(arr)\n\n\n# fuzzer\ndef fuzz_posdef_np(n=100, noise=1e-5, bs=10, range=(0,1), dtype=np.float32):\n    A = np.random.rand(bs,n,n).astype(dtype)  * (range[1]-range[0]) + range[0]\n    posdef = torch.from_numpy(to_posdef_np(A, noise))\n    return pd.DataFrame(\n        {'n': [n], 'noise': f\"{noise:.0e}\", 'range': str(range), 'n_samples': bs,\n         'posdef': is_posdef(posdef).sum().item() / bs,\n         'sym': is_symmetric(posdef).sum().item() / bs, \n         'posdef_eigv': is_posdef_eigv(posdef)[0].sum().item() / bs\n    })\n\n\nfuzz_posdef_np(n=1000, dtype=np.float32)\n\n\n\n\nChecker Positive Definite\nThis is to help finding matrices that aren’t positive definite and debug the issues. Returns a detailed dataframe row with info about the matrix and optionally logs everything to a global object\n\nsource\n\n\nCheckPosDef\n\n CheckPosDef (do_check:bool=False, use_log:bool=True, warning:bool=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndo_check\nbool\nFalse\nset to True to actually check matrix\n\n\nuse_log\nbool\nTrue\nkeep internal log\n\n\nwarning\nbool\nTrue\nshow a warning if a matrix is not pos def\n\n\n\n\nCheckPosDef(True).check(A)\n\n\nCheckPosDef(True).check(A[0])\n\n\nchecker = CheckPosDef(True)\n\nchecker.check(A, my_arg=\"my arg\") # this will be another col in the log\n\n\nchecker.log\n\n\nchecker.add_args(show=\"only once\")\nchecker.check(posdef)\nchecker.check(A)\nchecker.log\n\n\nB = torch.rand(2,3,3) # a batch of matrices\n\n\nis_symmetric(B).shape\n\n\nchecker.check(B)\n\n\ntest_close(B[0] @ A, (B @ A)[0]) # example batched matrix multiplication\n\n\n\nDiagonal Positive Definite Contraint\nthis is a simpler contraint that make the matrix diagonal and positive definite, by forcing it to have positive numbers on the diagonal.\ngiven a vector matrix \\(a\\) it is transformed into a diagonal positive definite matrix using:\n\\(A_{diag\\ pos\\ def} = a^2 I\\)\nthe inverse transformation is the square root of the diagonal\n\nfrom meteo_imp.utils import *\n\n\nsource\n\n\nto_diagposdef\n\n to_diagposdef (x)\n\n\nsource\n\n\nDiagPosDef\n\n DiagPosDef ()\n\nDiagonal Positive Definite Constraint for PyTorch parameters\n\nDiagPosDef().transform(torch.rand(3))\n\n\nDiagPosDef().transform(torch.rand(2, 3))\n\n\nv = -1.2 * torch.ones(2,3)\n\n\nDiagPosDef().inverse_transform(DiagPosDef().transform(v))\n\n\nto_diagposdef(torch.rand(3,3))\n\n\ndpd_const = DiagPosDef()\na = torch.rand(3)\n\n\ndpd_const.transform(a)\n\n\ntest_close(a, dpd_const.inverse_transform(dpd_const.transform(a)))"
  },
  {
    "objectID": "gaussian.html#conditional-predictions",
    "href": "gaussian.html#conditional-predictions",
    "title": "Gaussian Distributions",
    "section": "Conditional Predictions",
    "text": "Conditional Predictions\nTherefore we need to compute the conditional distribution of a normal 1\n\\[ X = \\left[\\begin{array}{c} x \\\\ o \\end{array} \\right] \\]\n\\[ p(X) = N\\left(\\left[ \\begin{array}{c} \\mu_x \\\\ \\mu_o \\end{array} \\right], \\left[\\begin{array}{cc} \\Sigma_{xx} & \\Sigma_{xo} \\\\ \\Sigma_{ox} & \\Sigma_{oo} \\end{array} \\right]\\right)\\]\nwhere \\(x\\) is a vector of variable that need to predicted and \\(o\\) is a vector of the variables that have been observed\nthen the conditional distribution is:\n\\[p(x|o) = N(\\mu_x + \\Sigma_{xo}\\Sigma_{oo}^{-1}(o - \\mu_o), \\Sigma_{xx} - \\Sigma_{xo}\\Sigma_{oo}^{-1}\\Sigma_{ox})\\]\n\nsource\n\nconditional_guassian\n\n conditional_guassian (μ:torch.Tensor, Σ:torch.Tensor, obs:torch.Tensor,\n                       mask:torch.Tensor)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nμ\nTensor\nmean with shape [n_vars]\n\n\nΣ\nTensor\ncov with shape [n_vars, n_vars]\n\n\nobs\nTensor\nObservations with shape [n_obs], where n_obs = sum(idx)\n\n\nmask\nTensor\nBoolean tensor specifying for each variable is observed (True) or not (False). Shape [n_vars]\n\n\nReturns\nListMultiNormal\nDistribution conditioned on observations. shape [n_vars - n_obs]\n\n\n\n\n# example distribution with only 2 variables\nμ = torch.tensor([.5, 1.])\nΣ = torch.tensor([[1., .5], [.5 ,1.]])\n\n\nmask = torch.tensor([True, False]) # second variable is the observed one\n\nobs = torch.tensor([5.]) # value of second variable\n\ngauss_cond = conditional_guassian(μ, Σ, obs, mask)\n\n# hardcoded values to test that the code is working, see also for alternative implementation https://python.quantecon.org/multivariate_normal.html\ntest_close(3.25, gauss_cond.mean.item())\ntest_close(.75, gauss_cond.cov.item())\n\n\n\nBatches\ncannot have proper batch support, or at least not in a straigthforward way as the shape of the output would be different for the different batches.\nso using a for-loop to temporarly fix the situation\n\nsource\n\n\ncond_gaussian_batched\n\n cond_gaussian_batched (dist:__main__.ListMultiNormal, obs, mask)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndist\nListMultiNormal\n\n\n\nobs\n\nthis needs to have the same shape of the mask !!!\n\n\nmask\n\n\n\n\nReturns\nList\nlists of distributions for element in the batch\n\n\n\n\nreset_seed(10)\nmean = torch.rand(2,3) # batch\ncov = to_posdef(torch.rand(2,3,3))\nmask = torch.rand(2,3) &gt; .3\nobs = torch.rand(2,3)\n\n\nconditional_gaussian_batched(mean, cov, obs, mask)\n\n\nmask.shape, obs.shape\n\n\nassert mean.shape == mask.shape\nassert mean.dim() == 2\n\n\nobs.shape\n\n\nmean_x = mean[~mask]\nmean_o = mean[mask]\n\n\nmask\n\n\nmean_x\n\n\ncov.shape\n\n\ncov[~mask]\n\n\ncov\n\n\ncov[0][~mask[0], ~mask[0]]\n\n\ncov[0][mask[0],:][:, mask[0]].shape\n\n\n\nPerformance\nanalysis of the performance of inverting a positive definite matrix\nUse cholesky decomposition and cholesky_solve to improve performance of matrix inversion\nsee the Probabilist machine learning course from uni Tübigen, specifically the code from the Gaussian Regression Notebook for details\nThis is the direct implementation of the equations\n\ndef _conditional_guassian_base(\n                         μ: Tensor, # mean with shape `[n_vars]`\n                         Σ: Tensor, # cov with shape `[n_vars, n_vars] `\n                         obs: Tensor, # Observations with shape `[n_vars]`\n                         idx: Tensor # Boolean tensor specifying for each variable is observed (True) or not (False). Shape `[n_vars]`\n                        ) -&gt; ListNormal: # Distribution conditioned on observations\n    μ_x = μ[~idx]\n    μ_o = μ[idx]\n    \n    Σ_xx = Σ[~idx,:][:, ~idx]\n    Σ_xo = Σ[~idx,:][:, idx]\n    Σ_ox = Σ[idx,:][:, ~idx]\n    Σ_oo = Σ[idx,:][:, idx]\n    \n    Σ_oo_inv = torch.linalg.inv(Σ_oo)\n    \n    mean = μ_x + Σ_xo@Σ_oo_inv@(obs - μ_o)\n    cov = Σ_xx - Σ_xo@Σ_oo_inv@Σ_ox\n    \n    return ListNormal(mean, cov)\n\nfaster version\n\nn_var = 5\nmean = torch.rand(n_var, dtype=torch.float64)\ncov = to_posdef(torch.rand(n_var, n_var, dtype=torch.float64))\ndist = MultivariateNormal(mean, cov)\nidx = torch.rand(n_var, dtype=torch.float64) &gt; .5\nobs = torch.rand(n_var, dtype=torch.float64)[idx]\n\n\ntorch.linalg.inv(cov)\n\n\n(torch.linalg.inv(cov) - cholesky_inverse(torch.linalg.cholesky(cov))).max()\n\n\ntest_close(torch.linalg.inv(cov), cholesky_inverse(torch.linalg.cholesky(cov)), eps=1e-2)\n\n\nreset_seed()\nA = to_posdef(torch.rand(1000, 1000, dtype=torch.float64)) + torch.eye(1000) * 1e-3 # noise to ensure is positive definite\n\n\nis_symmetric(A)\n\n\nis_posdef(A)\n\nThe second version is way faster\n\ntest_close(conditional_guassian(mean, cov, obs, idx).mean, _conditional_guassian_base(mean, cov, obs, idx).mean)\n\n\nB = to_posdef(torch.rand(n_var, n_var, dtype=torch.float64))\n\n\nB @ torch.inverse(cov)\n\n\ntorch.cholesky_solve(cholesky(cov), B)"
  },
  {
    "objectID": "gaussian.html#helper",
    "href": "gaussian.html#helper",
    "title": "Gaussian Distributions",
    "section": "Helper",
    "text": "Helper\n\ncov2std\n\nx = torch.stack([torch.eye(3)*i for i in  range(1,4)])\n\n\nx\n\n\ntorch.diagonal(x, dim1=1, dim2=2)\n\n\nsource\n\n\ncov2std\n\n cov2std (x)\n\nconvert cov of array of covariances to array of stddev"
  },
  {
    "objectID": "gaussian.html#export",
    "href": "gaussian.html#export",
    "title": "Gaussian Distributions",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "gaussian.html#footnotes",
    "href": "gaussian.html#footnotes",
    "title": "Gaussian Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://cs.nyu.edu/~roweis/notes/gaussid.pdf eq, 5a, 5d↩︎"
  }
]