[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GPFA Imputation",
    "section": "",
    "text": "This is developed by Simone Massaro as a master thesis in Ecosystem Analysis and Modelling at the University of Göttingen, Germany\nThe aim is to impute gaps into the meteorological time series for Eddy-Covariance applications.\nModels developed: - Gaussian Processes Factor Analysis (GPFA) - space state models (Kalman Filter)"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "GPFA Imputation",
    "section": "License",
    "text": "License\nThe code under model_imp.kalman.filter has the following license\n```All code contained except that in pykalman/utils.py is released under the license below. All code in pykalman/utils.py is released under the license contained therein.\nNew BSD License\nCopyright (c) 2012 Daniel Duckworth. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of Daniel Duckworth nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ```"
  },
  {
    "objectID": "gaussian.html",
    "href": "gaussian.html",
    "title": "Gaussian Distributions Utils",
    "section": "",
    "text": "from fastcore.test import *"
  },
  {
    "objectID": "gaussian.html#normal-parameters",
    "href": "gaussian.html#normal-parameters",
    "title": "Gaussian Distributions Utils",
    "section": "Normal Parameters",
    "text": "Normal Parameters\n\nNormal\n\nimport torch\n\n\nsource\n\n\nListNormal.detach\n\n ListNormal.detach ()\n\nDetach both mean and cov at once\n\nListNormal(torch.rand(10), torch.rand(10))[1]\n\nNormal(mean=tensor(0.4322), std=tensor(0.0450))\n\n\n\n\nMultivariate Normal\n\n\n\nListMultiNormal.detach\n\n ListMultiNormal.detach ()\n\nDetach both mean and cov at once\n\nListMNormal(torch.rand(2,10), torch.rand(2,10,10))[1]\n\nMultiNormal(mean=tensor([0.0229, 0.4637, 0.1999, 0.9229, 0.1036, 0.5479, 0.4829, 0.2972, 0.9683,\n        0.9519]), cov=tensor([[0.0374, 0.5933, 0.5046, 0.2290, 0.7677, 0.5853, 0.0074, 0.3667, 0.6025,\n         0.8323],\n        [0.1473, 0.9688, 0.2277, 0.1073, 0.8594, 0.6005, 0.5353, 0.9306, 0.1585,\n         0.5595],\n        [0.4591, 0.4949, 0.6432, 0.0630, 0.6998, 0.2254, 0.9016, 0.7314, 0.9867,\n         0.1340],\n        [0.9946, 0.6823, 0.8888, 0.3387, 0.9628, 0.6730, 0.4966, 0.8522, 0.3186,\n         0.3757],\n        [0.4234, 0.1553, 0.3145, 0.0338, 0.3227, 0.2926, 0.5984, 0.6980, 0.2820,\n         0.8597],\n        [0.3834, 0.1146, 0.0706, 0.0280, 0.0724, 0.6593, 0.0788, 0.0611, 0.8379,\n         0.8059],\n        [0.5841, 0.7931, 0.8659, 0.2980, 0.7672, 0.9661, 0.6043, 0.1415, 0.9610,\n         0.4112],\n        [0.6061, 0.2894, 0.0213, 0.1567, 0.8274, 0.6500, 0.9066, 0.3175, 0.2687,\n         0.3781],\n        [0.1840, 0.7383, 0.7131, 0.8098, 0.1568, 0.8432, 0.5871, 0.9134, 0.2612,\n         0.5477],\n        [0.3704, 0.0043, 0.8054, 0.9980, 0.4867, 0.3591, 0.5260, 0.1704, 0.8769,\n         0.2887]]))"
  },
  {
    "objectID": "gaussian.html#positive-definite",
    "href": "gaussian.html#positive-definite",
    "title": "Gaussian Distributions Utils",
    "section": "Positive Definite",
    "text": "Positive Definite\nThe covariance matrices need to be positive definite Those are utilities functions to check is a matrix is positive definite and to make any matrix positive definite\n\nOther libraries\nMost libraries that implement Kalman Filters use manually specified parameters, which often don’t have the issue of the positive definite constraint (eg. pykalman)\nFrom statsmodels statespace models: >Cholesky decomposition […] requires that the matrix be positive definite. While this should generally be true, it may not be in every case. source\nwhich seems to mean that they take into account the fact that during the filter calculations may not be positive definite\n\nA = torch.rand(2,3,3) # batched random matrix used for testing\n\n\n\nSymmetry\n\nsource\n\n\nis_symmetric\n\n is_symmetric (value, atol=1e-05)\n\n\nis_symmetric(A)\n\ntensor([False, False])\n\n\n\nsource\n\n\nsymmetric_upto_batched\n\n symmetric_upto_batched (value, start=-8)\n\n\nsource\n\n\nsymmetric_upto\n\n symmetric_upto (value, start=-8)\n\n\nsymmetric_upto_batched(A)\n\ntensor([0, 0])\n\n\n\nis posdef\nDefault pytorch check (uses symmetry + cholesky decomposition)\n\nsource\n\n\n\nis_posdef\n\n is_posdef (cov)\n\n\nis_posdef(A)\n\ntensor([False, False])\n\n\ncheck if it is pos definite using eigenvalues. Positive definite matrix have all positive eigenvalues\n\ntorch.linalg.eigvalsh(A)\n\ntensor([[ 0.1256,  0.4735,  1.0205],\n        [-0.4617,  0.2060,  1.6219]])\n\n\n\nsource\n\n\nis_posdef_eigv\n\n is_posdef_eigv (cov)\n\n\nis_posdef_eigv(A)\n\n(tensor([ True, False]),\n tensor([[ 0.1256,  0.4735,  1.0205],\n         [-0.4617,  0.2060,  1.6219]]))\n\n\n\n\nPytorch constraint\nNote that is_posdef and is_posdef_eigv can return different values, in general is_posdef_eigv is more tollerant\ntransform any matrix \\(A\\) into a positive definite matrix (\\(PD\\)) using the following formula\n\\(PD = AA^T + aI\\)\nwhere \\(AA^T\\) is a positive semi-definite matrix and \\(a\\) is a small positive number that is added on the diagonal to ensure that the resulting matrix is positive definite (not semi-definite)\nthe inverse transformation uses cholesky decomposition\nAnother approach would be to multiple to lower triangular matrix, but they’d require a positive diagonal, which is harderd to obtain see https://en.wikipedia.org/wiki/Definite_matrix#Cholesky_decomposition\nThe API inspired by gpytorch constraints\n\nfrom meteo_imp.utils import *\n\n/opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/pyprojroot/pyprojroot.py:51: UserWarning: Path doesn't exist: /home/runner/work/meteo_imp/meteo_imp/.cache\n  warnings.warn(\"Path doesn't exist: {}\".format(path))\n\n\n\nsource\n\n\nPosDef\n\n PosDef (noise=1e-05)\n\nPositive Definite Constraint for PyTorch parameters\n\nconstraint = PosDef()\n\nposdef = constraint.transform(A)\n\n\nconstraint.noise\n\n1e-05\n\n\n\nA\n\ntensor([[[0.9268, 0.5991, 0.2592],\n         [0.0158, 0.4280, 0.4779],\n         [0.2563, 0.1366, 0.2649]],\n\n        [[0.7287, 0.4174, 0.5989],\n         [0.8998, 0.5918, 0.8489],\n         [0.0034, 0.4378, 0.0456]]])\n\n\n\nposdef\n\ntensor([[[1.2850, 0.3949, 0.3880],\n         [0.3949, 0.4118, 0.1891],\n         [0.3880, 0.1891, 0.1546]],\n\n        [[1.0639, 1.4111, 0.2125],\n         [1.4111, 1.8805, 0.3009],\n         [0.2125, 0.3009, 0.1938]]])\n\n\n\nshow_as_row(is_posdef(torch.stack([posdef,A])), is_posdef_eigv(torch.stack([posdef,A])), is_symmetric(torch.stack([posdef,A])))\n\n\n  tensor([[ True,  True],\n        [False, False]]) \n\n\n\ntest_eq(is_posdef(posdef).all(), True)\n\n\nconstraint.inverse_transform(posdef)\n\ntensor([[[1.1336, 0.0000, 0.0000],\n         [0.3483, 0.5390, 0.0000],\n         [0.3423, 0.1296, 0.1434]],\n\n        [[1.0315, 0.0000, 0.0000],\n         [1.3681, 0.0944, 0.0000],\n         [0.2060, 0.2012, 0.3330]]])\n\n\n\ntest_close(posdef, constraint.transform(constraint.inverse_transform(posdef)), eps=2e-5)\n\n\nsymmetric_upto(posdef[0])\n\n-8\n\n\n\nreset_seed()\n\n\n\nFuzzer\n\nrun_fuzzer = True # temporly disable for performance reasons\n\n\ndef random_posdef(bs=10,n=100,noise=1e-5,n_range=(0,1), **kwargs):\n    A = torch.rand(bs,n,n, **kwargs)  * (n_range[1]-n_range[0]) + n_range[0]\n    return PosDef(noise).transform(A)\n\n\n# fuzzer\ndef fuzz_posdef(bs=10,n=100,noise=1e-5,n_range=(0,1), **kwargs):\n    posdef = random_posdef(bs, n, noise, **kwargs)\n    return pd.DataFrame(\n        {'n': [n], 'noise': f\"{noise:.0e}\", 'range': str(n_range), 'n_samples': bs,\n         'posdef': is_posdef(posdef).sum().item() / bs,\n         'sym': is_symmetric(posdef).sum().item() / bs, \n         'posdef_eigv': is_posdef_eigv(posdef)[0].sum().item() / bs\n    })\n\n\nfuzz_posdef()\n\n\n\n\n\n  \n    \n      \n      n\n      noise\n      range\n      n_samples\n      posdef\n      sym\n      posdef_eigv\n    \n  \n  \n    \n      0\n      100\n      1e-05\n      (0, 1)\n      10\n      1.0\n      1.0\n      1.0\n    \n  \n\n\n\n\n\nn_min, n_max = -1, 1\nA = torch.rand(2,100,100)  * (n_max-n_min) + n_min\n\n\nis_posdef(to_posdef(A))\n\ntensor([True, True])\n\n\n\nma = torch.tensor([[1, 7],\n                   [-3, 4]])\n\n\nis_posdef(to_posdef(ma))\n\ntensor(True)\n\n\n\nfuzz_posdef(device='cuda')\n\n\n\n\n\n  \n    \n      \n      n\n      noise\n      range\n      n_samples\n      posdef\n      sym\n      posdef_eigv\n    \n  \n  \n    \n      0\n      100\n      1e-05\n      (0, 1)\n      10\n      1.0\n      1.0\n      1.0\n    \n  \n\n\n\n\n\n# %time fuzz_posdef(bs=100, device='cuda')\n\n\nrate_posdef = pd.concat([fuzz_posdef(n=n, noise=noise, bs=100, n_range=n_range, device='cuda') \n               for n in [10, 100]\n               for noise in [1e-2, 1e-5, 1e-7]\n               for n_range in [(-1,1),(0,1)]])\n\n\nimport altair as alt\nfrom altair import datum\n\n\nrate_posdef.head()\n\n\n\n\n\n  \n    \n      \n      n\n      noise\n      range\n      n_samples\n      posdef\n      sym\n      posdef_eigv\n    \n  \n  \n    \n      0\n      10\n      1e-02\n      (-1, 1)\n      100\n      1.0\n      1.0\n      1.0\n    \n    \n      0\n      10\n      1e-02\n      (0, 1)\n      100\n      1.0\n      1.0\n      1.0\n    \n    \n      0\n      10\n      1e-05\n      (-1, 1)\n      100\n      1.0\n      1.0\n      1.0\n    \n    \n      0\n      10\n      1e-05\n      (0, 1)\n      100\n      1.0\n      1.0\n      1.0\n    \n    \n      0\n      10\n      1e-07\n      (-1, 1)\n      100\n      1.0\n      1.0\n      1.0\n    \n  \n\n\n\n\n\ndef _plot_var(df, var, x='n:N', row='range', column='noise:N', y_domain=(0,1), height=70, width=50):\n    bar = alt.Chart(df).mark_bar().encode(\n        x = alt.X('n:N'),\n        y = alt.Y(var, scale=alt.Scale(domain=y_domain)),\n        color = 'n:N',\n    ).properties(height=height, width=width, ) \n    \n    text = alt.Chart(df).mark_text(dy=10, color='white').encode(\n        x = alt.X('n:N'),\n        y = alt.Y(var),\n        text = alt.Text(var, format=\".2f\")\n    )\n    \n    return (bar + text).facet(\n        column=column,\n        row=row).properties(title=var, )\n\n\ndef _plot_var_box(df, var, x='n:N', row='range', column='noise:N', height=70, width=50, title=''):\n    box = alt.Chart(df).mark_boxplot().encode(\n        x = alt.X(x),\n        y = alt.Y(var),\n        color = x,\n    ).properties(height=height, width=width) \n\n    # text = alt.Chart(df).mark_text(dy=10, color='white').encode(\n    #     x = alt.X('n:N'),\n    #     y = alt.Y(var),\n    #     text = alt.Text(var, format=\".2f\")\n    # )\n    \n    return (box).facet(\n        column=column,\n        row=row).properties(title=title)\n\n\nfrom IPython import display\nimport vl_convert as vlc\nfrom functools import partial\n\n\nGeneration of Random positive definite matrices\n\ndef plot_posdef_simulation(n_s, noise_s, range_s, bs=100, **kwargs):\n    if not run_fuzzer: return\n    rate_posdef = pd.concat([fuzz_posdef(n=n, noise=noise, bs=bs, n_range=range, device='cuda', **kwargs) \n               for n in n_s for noise in noise_s for range in range_s])\n    \n    vl_spec = alt.hconcat(*[_plot_var(rate_posdef, var) for var in ['posdef', 'posdef_eigv']]).to_json()\n    # workaround for bug in vegalite see https://github.com/altair-viz/altair/issues/2742\n    svg = vlc.vegalite_to_svg(vl_spec, vl_version='v5.3')\n    display.display(display.HTML(svg))\n\n\nplot_posdef_simulation(n_s = [10, 100], noise_s = [1e-2, 1e-5, 1e-7], range_s = [(-1, 1)], bs=1000)\n\nrangenoise0.00.51.0posdef(-1, 1)1e-021e-051e-0710100n10100n10100n1.001.001.001.001.000.96posdefrangenoise0.00.51.0posdef_eigv(-1, 1)1e-021e-051e-0710100n10100n10100n1.001.001.000.991.000.95posdef_eigv10100n\n\n\nLet’s go big by using a matrix 1000x1000\n\nplot_posdef_simulation(n_s = [1000], noise_s = [1e-3, 1e-4, 1e-5], range_s = [(-.1, 1)], bs=100)\n\nrangenoise0.00.51.0posdef(-0.1, 1)1e-031e-041e-051000n1000n1000n1.000.620.43posdefrangenoise0.00.51.0posdef_eigv(-0.1, 1)1e-031e-041e-051000n1000n1000n0.580.500.58posdef_eigv1000n\n\n\nfor a standard noise on the diagonal less than half of the random matrices that are 1000 in size are positive definite.\nLet’s have a look at one of such matrices\n\nposdef = random_posdef(100, 1000)\nnot_pd = posdef[torch.argwhere(~is_posdef_eigv(posdef)[0])[0]]\n\nThis should be positive definite but actually it’s not …\n\nnot_pd\n\ntensor([[[335.6587, 255.2734, 259.6248,  ..., 246.4368, 257.4700, 255.6423],\n         [255.2734, 339.6811, 265.9074,  ..., 248.7385, 259.2237, 258.6045],\n         [259.6248, 265.9074, 348.2930,  ..., 256.4056, 264.2218, 258.5888],\n         ...,\n         [246.4368, 248.7385, 256.4056,  ..., 322.1650, 248.4815, 249.9758],\n         [257.4700, 259.2237, 264.2218,  ..., 248.4815, 336.6406, 257.6953],\n         [255.6423, 258.6045, 258.5888,  ..., 249.9758, 257.6953, 341.0876]]])\n\n\ntrying with float64 (for memory constraint on the GPU only using a 700x700 matrix)\n\nplot_posdef_simulation(n_s = [700], noise_s = [1e-3, 1e-4, 1e-5], range_s = [(-.1, 1)], bs=100)\n\nrangenoise0.00.51.0posdef(-0.1, 1)1e-031e-041e-05700n700n700n1.000.890.74posdefrangenoise0.00.51.0posdef_eigv(-0.1, 1)1e-031e-041e-05700n700n700n0.720.670.66posdef_eigv700n\n\n\n\nplot_posdef_simulation(n_s = [700], noise_s = [1e-4, 1e-5], range_s = [(-.1, 1)], bs=100, dtype=torch.float64)\n\nrangenoise0.00.51.0posdef(-0.1, 1)1e-041e-05700n700n1.001.00posdefrangenoise0.00.51.0posdef_eigv(-0.1, 1)1e-041e-05700n700n1.001.00posdef_eigv700n\n\n\nAll matrices now are positive definite\n\n\nMultiplication\ncheck is multiplication of matrices is not breaking the positive definite constraint\nIf \\(A\\) and \\(B\\) are both positive definite matrices \\(ABA\\) is also positive definite https://en.wikipedia.org/wiki/Definite_matrix#Multiplication\n\ndef fuzz_op(op, # operation that takes 2 pos def matrices and return one pos def matrix\n            fn_check = is_posdef,\n                  n=100, # size of matrix\n                  max_t=1000, # number of multiplications\n                  noise=1e-5, # noise to add on diagonal\n                  bs=10, # batch size\n                  n_range=(0,1), # range of random numbers\n                  **kwargs):\n    pd1 = random_posdef(bs, n, noise, n_range, **kwargs)\n    pd2 = random_posdef(bs, n, noise, n_range,**kwargs)\n    stop_times = torch.zeros(bs, **kwargs)\n    \n    for t in torch.arange(max_t):\n        pd1 = op(pd1, pd2)\n        check = fn_check(pd1)\n        stop_times[torch.logical_and(stop_times == 0, ~check)] = t\n        if not check.any(): break\n         \n    stop_times[stop_times == 0] = t\n    return pd.DataFrame(\n        {'n': [n], 'noise': f\"{noise:.0e}\", 'range': str(n_range), 'n_samples': bs, 'last_t': t.item(),\n         'mean_stop': stop_times.mean().item(),\n         'std_stop': stop_times.std().item(),\n         'stop_times': [stop_times.cpu().numpy()]})\n\n\nfuzz_multiply = partial(fuzz_op, lambda pd1, pd2: pd2 @ pd1 @ pd2)\nfuzz_multiply_eigv = partial(fuzz_multiply, fn_check = lambda pd1: is_posdef_eigv(pd1)[0])\n\n\ndef plot_multiply_simulation(n_s, noise_s, max_mult=1000, bs=100, **kwargs):\n    mult = pd.concat([fuzz_multiply(n=n, noise=noise, bs=bs, device='cuda', **kwargs) \n               for n in n_s for noise in noise_s]).explode('stop_times')\n    \n    mult_eigv = pd.concat([fuzz_multiply(n=n, noise=noise, bs=bs, device='cuda', **kwargs) \n               for n in n_s for noise in noise_s]).explode('stop_times')\n    \n    vl_spec = alt.hconcat(*[_plot_var_box(df, 'stop_times') for df in [mult, mult_eigv]]).to_json()\n    # workaround for bug in vegalite see https://github.com/altair-viz/altair/issues/2742\n    svg = vlc.vegalite_to_svg(vl_spec, vl_version='v5.3')\n    display.display(display.HTML(svg))\n    return (mult, mult_eigv)\n\n\nplot_multiply_simulation(n_s=[2,3,10, 100], noise_s=[1e-3, 1e-4, 1e-5], bs=100);\n\nrangenoise0510stop_times(0, 1)1e-031e-041e-052310100n2310100n2310100n8.004.001.002.001.004.002.001.006.002.002.005.003.005.003.006.004.001.002.004.003.005.005.002.005.001.001.001.002.004.001.003.008.001.001.003.003.002.003.001.001.003.003.001.003.001.003.005.001.002.001.007.001.002.002.002.006.003.002.002.002.005.004.001.001.002.002.001.006.002.002.003.002.007.001.006.001.003.009.003.002.002.002.002.005.001.004.007.001.0011.003.004.001.003.003.001.003.002.005.004.001.002.001.003.002.003.002.003.003.002.001.001.001.001.002.001.002.003.001.001.001.001.001.001.002.002.001.002.001.004.001.001.001.002.002.001.003.001.001.003.001.001.002.001.001.003.001.002.001.001.001.001.001.001.001.002.001.003.004.001.001.005.002.001.002.003.002.004.002.002.002.001.001.004.003.002.002.001.001.002.001.004.003.001.001.001.002.001.002.001.001.002.001.001.001.002.002.001.002.002.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.003.003.001.004.002.005.002.001.001.002.003.001.005.004.001.002.004.002.003.006.002.004.002.003.003.005.008.004.0013.002.009.002.001.004.002.0011.004.001.001.003.003.002.002.004.004.001.003.002.002.004.001.004.001.002.005.001.001.001.002.001.002.002.001.005.005.005.003.005.003.005.002.001.001.004.001.004.003.002.004.001.004.001.001.004.003.007.005.002.003.001.008.002.001.002.002.003.002.002.002.003.002.002.001.003.002.002.003.001.001.001.001.001.002.005.001.003.001.003.002.003.003.002.002.001.004.002.002.001.002.001.002.003.003.001.005.001.002.003.001.001.002.002.001.001.001.003.001.003.002.001.001.003.001.004.003.002.003.003.001.001.002.001.002.001.002.005.001.002.001.002.002.002.002.002.001.003.001.001.003.005.003.003.001.001.003.001.004.001.003.002.001.003.003.003.002.001.001.002.002.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.004.001.003.002.001.004.002.004.001.003.002.003.006.001.005.002.001.001.001.001.002.002.003.003.005.002.002.002.001.005.003.005.002.003.001.001.001.002.003.003.004.002.002.003.003.004.001.004.006.001.004.002.005.002.001.001.001.004.002.002.004.009.001.008.005.002.004.004.004.001.005.003.005.002.001.002.003.001.001.003.003.001.005.0012.001.003.001.002.002.007.002.003.001.003.003.001.003.004.001.002.001.002.003.001.002.002.001.002.001.001.001.003.001.003.002.001.002.001.001.005.001.001.002.001.003.002.002.002.003.002.005.003.002.002.001.002.001.002.002.001.002.001.001.002.003.002.002.001.002.001.002.002.001.001.003.001.001.001.001.003.002.001.002.001.004.001.001.003.002.001.001.002.001.002.001.001.001.005.005.001.002.001.001.001.003.001.001.003.002.001.002.001.003.003.003.002.002.004.001.002.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00rangenoise010stop_times(0, 1)1e-031e-041e-052310100n2310100n2310100n2.001.002.003.003.002.003.001.001.004.002.003.002.002.003.007.004.002.004.003.008.002.001.008.001.004.002.002.002.001.003.003.002.001.003.003.001.007.002.002.007.005.007.001.001.002.008.001.003.005.007.001.003.006.003.002.006.006.002.005.002.004.0013.001.005.0010.001.004.001.006.004.004.002.003.003.003.008.003.002.001.003.004.001.002.002.005.006.006.002.001.002.004.004.001.003.002.001.001.003.006.004.004.002.002.002.001.002.001.001.001.002.002.001.002.002.002.001.001.001.001.003.005.002.003.003.002.002.002.001.002.003.002.002.001.004.003.001.001.001.002.003.002.002.002.002.006.001.002.002.001.003.002.001.002.001.002.002.001.001.001.003.001.001.002.002.001.002.004.001.003.005.001.002.001.001.003.001.001.003.002.002.002.001.002.002.003.002.002.001.001.003.001.002.002.001.001.002.001.002.002.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.005.001.002.004.006.001.003.002.001.006.003.003.004.002.001.005.0010.002.004.002.002.005.001.002.005.001.001.0018.003.002.002.001.001.002.001.004.006.003.001.003.0014.002.003.002.005.001.002.002.001.005.002.001.001.002.001.001.002.002.003.005.001.002.003.005.004.008.002.001.005.002.003.001.001.004.001.001.002.002.005.006.004.002.003.002.005.004.009.004.001.002.002.002.005.006.001.002.002.005.006.002.003.004.002.001.002.001.002.001.001.002.006.001.001.001.002.001.003.001.002.001.004.002.002.001.001.003.001.001.002.001.005.001.002.002.001.001.002.002.001.002.001.001.001.003.002.002.001.004.002.003.002.002.001.003.002.002.001.001.001.004.001.002.002.002.002.002.001.001.002.001.001.002.003.001.001.003.001.001.001.003.001.001.001.002.001.002.001.003.004.001.003.003.003.001.002.003.002.001.002.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.002.003.002.001.001.003.004.002.004.001.003.002.004.004.002.001.002.004.004.006.003.005.005.001.002.003.003.0010.002.001.002.002.002.002.001.002.008.005.004.001.002.001.002.001.003.002.001.001.001.004.003.003.006.002.001.007.004.001.003.003.007.005.005.003.005.002.001.003.003.002.003.001.003.008.003.003.003.0016.001.002.008.002.002.004.001.001.004.002.006.005.003.005.004.008.002.004.007.001.004.002.004.002.001.005.002.003.002.002.003.002.001.005.001.001.002.001.002.003.002.001.002.002.002.001.001.001.002.002.002.001.001.001.003.001.002.002.002.002.002.001.001.002.002.003.002.005.002.001.002.004.002.004.001.003.001.002.001.003.001.001.001.001.001.002.002.003.002.001.002.004.001.004.001.001.002.001.004.002.003.002.003.001.001.001.003.002.001.002.002.003.002.002.002.003.002.003.002.001.001.004.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.002310100n\n\n\n\n\nAddition\ncheck is multiplication of matrices is not breaking the positive definite constraint\nIf \\(A\\) and \\(B\\) are both positive definite matrices \\(A+B\\) is also positive definite https://en.wikipedia.org/wiki/Definite_matrix#Addition\n\npd1 = random_posdef(10, 100)\npd2 = random_posdef(10, 100)\n\n\nis_posdef(pd1 + pd2).all()\n\ntensor(True)\n\n\n\nfuzz_add = partial(fuzz_op, lambda pd1, pd2: pd1 + pd2)\n\n\n\n\nCPU times: user 5.06 s, sys: 43.7 ms, total: 5.1 s\nWall time: 4.11 s\n\n\n\n\n\n\n  \n    \n      \n      n\n      noise\n      range\n      n_samples\n      last_t\n      mean_stop\n      std_stop\n      stop_times\n    \n  \n  \n    \n      0\n      100\n      1e-05\n      (0, 1)\n      10\n      12471.0\n      8596.5\n      2447.240234\n      [12417.0, 9269.0, 12471.0, 6977.0, 8576.0, 748...\n    \n  \n\n\n\n\n\ndef plot_add_simulation(n_s, noise_s, max_ts=[1000], bs=100, **kwargs):\n    add = pd.concat([fuzz_add(n=n, noise=noise, bs=bs, max_t=max_t, device='cuda', **kwargs) \n               for n in n_s for noise in noise_s for max_t in max_ts]).explode('stop_times')\n    \n    vl_spec = _plot_var_box(add, var='stop_times', height=150, width=150).to_json()\n\n    svg = vlc.vegalite_to_svg(vl_spec, vl_version='v5.3')\n    display.display(display.HTML(svg))\n\n\ncache_disk(\"add_plot\")(lambda: plot_add_simulation(n_s=[50, 100, 150], noise_s=[1e-3, 1e-4, 1e-5], bs=100, max_ts=[1e5]))()\n\nrangenoise020,00040,00060,00080,000100,000stop_times(0, 1)1e-031e-041e-0550100150n50100150n50100150n50100150n\n\n\n\n\nNumpy posdef\n\nimport numpy as np\n\n\narr = np.random.rand(2,3,3)\n\n\narr.shape\n\n\narr.transpose(0,2,1) == np.moveaxis(arr, -1, -2)\n\n\ndef to_posdef_np(x, noise=1e-5):\n    return x @ np.moveaxis(x, -1, -2) + (noise * np.eye(x.shape[-1], dtype=arr.dtype))\n\n\nto_posdef_np(arr)\n\n\n# fuzzer\ndef fuzz_posdef_np(n=100, noise=1e-5, bs=10, range=(0,1), dtype=np.float32):\n    A = np.random.rand(bs,n,n).astype(dtype)  * (range[1]-range[0]) + range[0]\n    posdef = torch.from_numpy(to_posdef_np(A, noise))\n    return pd.DataFrame(\n        {'n': [n], 'noise': f\"{noise:.0e}\", 'range': str(range), 'n_samples': bs,\n         'posdef': is_posdef(posdef).sum().item() / bs,\n         'sym': is_symmetric(posdef).sum().item() / bs, \n         'posdef_eigv': is_posdef_eigv(posdef)[0].sum().item() / bs\n    })\n\n\nfuzz_posdef_np(n=1000, dtype=np.float32)\n\n\n\n\nChecker Positive Definite\nThis is to help finding matrices that aren’t positive definite and debug the issues. Returns a detailed dataframe row with info about the matrix and optionally logs everything to a global object\n\nsource\n\n\nCheckPosDef\n\n CheckPosDef (do_check:bool=False, use_log:bool=True, warning:bool=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndo_check\nbool\nFalse\nset to True to actually check matrix\n\n\nuse_log\nbool\nTrue\nkeep internal log\n\n\nwarning\nbool\nTrue\nshow a warning if a matrix is not pos def\n\n\n\n\nCheckPosDef(True).check(A)\n\n\nCheckPosDef(True).check(A[0])\n\n\nchecker = CheckPosDef(True)\n\nchecker.check(A, my_arg=\"my arg\") # this will be another col in the log\n\n\nchecker.log\n\n\nchecker.add_args(show=\"only once\")\nchecker.check(posdef)\nchecker.check(A)\nchecker.log\n\n\nB = torch.rand(2,3,3) # a batch of matrices\n\n\nchecker.check(B)\n\n\ntest_close(B[0] @ A, (B @ A)[0]) # example batched matrix multiplication\n\n\n\nDiagonal Positive Definite Contraint\nthis is a simpler contraint that make the matrix diagonal and positive definite, by forcing it to have positive numbers on the diagonal.\ngiven a random vector \\(a\\) it is transformed into a diagonal positive definite matrix using:\n\\(A_{diag\\ pos\\ def} = e^a I\\)\nthe inverse transformation is the log of the diagonal\n\nsource\n\n\nDiagPosDef\n\n DiagPosDef ()\n\nDiagonal Positive Definite Constraint for PyTorch parameters\n\ndpd_const = DiagPosDef()\na = torch.rand(3)\n\n\ndpd_const.transform(a)\n\ntensor([[1.1899, 0.0000, 0.0000],\n        [0.0000, 2.0571, 0.0000],\n        [0.0000, 0.0000, 1.4777]])\n\n\n\ntest_close(a, dpd_const.inverse_transform(dpd_const.transform(a)))"
  },
  {
    "objectID": "gaussian.html#conditional-predictions",
    "href": "gaussian.html#conditional-predictions",
    "title": "Gaussian Distributions Utils",
    "section": "Conditional Predictions",
    "text": "Conditional Predictions\nTherefore we need to compute the conditional distribution of a normal 1\n\\[ X = \\left[\\begin{array}{c} x \\\\ o \\end{array} \\right] \\]\n\\[ p(X) = N\\left(\\left[ \\begin{array}{c} \\mu_x \\\\ \\mu_o \\end{array} \\right], \\left[\\begin{array}{cc} \\Sigma_{xx} & \\Sigma_{xo} \\\\ \\Sigma_{ox} & \\Sigma_{oo} \\end{array} \\right]\\right)\\]\nwhere \\(x\\) is a vector of variable that need to predicted and \\(o\\) is a vector of the variables that have been observed\nthen the conditional distribution is:\n\\[p(x|o) = N(\\mu_x + \\Sigma_{xo}\\Sigma_{oo}^{-1}(o - \\mu_o), \\Sigma_{xx} - \\Sigma_{xo}\\Sigma_{oo}^{-1}\\Sigma_{ox})\\]\n\nsource\n\nconditional_guassian\n\n conditional_guassian (μ:torch.Tensor, Σ:torch.Tensor, obs:torch.Tensor,\n                       mask:torch.Tensor)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nμ\nTensor\nmean with shape [n_vars]\n\n\nΣ\nTensor\ncov with shape [n_vars, n_vars]\n\n\nobs\nTensor\nObservations with shape [n_obs], where n_obs = sum(idx)\n\n\nmask\nTensor\nBoolean tensor specifying for each variable is observed (True) or not (False). Shape [n_vars]\n\n\nReturns\nListMultiNormal\nDistribution conditioned on observations. shape [n_vars - n_obs]\n\n\n\n\n# example distribution with only 2 variables\nμ = torch.tensor([.5, 1.])\nΣ = torch.tensor([[1., .5], [.5 ,1.]])\n\n\nmask = torch.tensor([True, False]) # second variable is the observed one\n\nobs = torch.tensor([5.]) # value of second variable\n\ngauss_cond = conditional_guassian(μ, Σ, obs, mask)\n\n# hardcoded values to test that the code is working, see also for alternative implementation https://python.quantecon.org/multivariate_normal.html\ntest_close(3.25, gauss_cond.mean.item())\ntest_close(.75, gauss_cond.cov.item())\n\n\n\nBatches\ncannot have proper batch support, or at least not in a straigthforward way as the shape of the output would be different for the different batches.\nso using a for-loop to temporarly fix the situation\n\nsource\n\n\ncond_gaussian_batched\n\n cond_gaussian_batched (dist:__main__.ListMultiNormal, obs, mask)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndist\nListMultiNormal\n\n\n\nobs\n\nthis needs to have the same shape of the mask !!!\n\n\nmask\n\n\n\n\nReturns\nList\nlists of distributions for element in the batch\n\n\n\n\nreset_seed(10)\nmean = torch.rand(2,3) # batch\ncov = to_posdef(torch.rand(2,3,3))\nmask = torch.rand(2,3) > .3\nobs = torch.rand(2,3)\n\n\nconditional_gaussian_batched(mean, cov, obs, mask)\n\n\nmask.shape, obs.shape\n\n\nassert mean.shape == mask.shape\nassert mean.dim() == 2\n\n\nobs.shape\n\n\nmean_x = mean[~mask]\nmean_o = mean[mask]\n\n\nmask\n\n\nmean_x\n\n\ncov.shape\n\n\ncov[~mask]\n\n\ncov\n\n\ncov[0][~mask[0], ~mask[0]]\n\n\ncov[0][mask[0],:][:, mask[0]].shape\n\n\n\nPerformance\nanalysis of the performance of inverting a positive definite matrix\nUse cholesky decomposition and cholesky_solve to improve performance of matrix inversion\nsee the Probabilist machine learning course from uni Tübigen, specifically the code from the Gaussian Regression Notebook for details\nThis is the direct implementation of the equations\n\ndef _conditional_guassian_base(\n                         μ: Tensor, # mean with shape `[n_vars]`\n                         Σ: Tensor, # cov with shape `[n_vars, n_vars] `\n                         obs: Tensor, # Observations with shape `[n_vars]`\n                         idx: Tensor # Boolean tensor specifying for each variable is observed (True) or not (False). Shape `[n_vars]`\n                        ) -> ListNormal: # Distribution conditioned on observations\n    μ_x = μ[~idx]\n    μ_o = μ[idx]\n    \n    Σ_xx = Σ[~idx,:][:, ~idx]\n    Σ_xo = Σ[~idx,:][:, idx]\n    Σ_ox = Σ[idx,:][:, ~idx]\n    Σ_oo = Σ[idx,:][:, idx]\n    \n    Σ_oo_inv = torch.linalg.inv(Σ_oo)\n    \n    mean = μ_x + Σ_xo@Σ_oo_inv@(obs - μ_o)\n    cov = Σ_xx - Σ_xo@Σ_oo_inv@Σ_ox\n    \n    return ListNormal(mean, cov)\n\nfaster version\n\nn_var = 5\nmean = torch.rand(n_var, dtype=torch.float64)\ncov = to_posdef(torch.rand(n_var, n_var, dtype=torch.float64))\ndist = MultivariateNormal(mean, cov)\nidx = torch.rand(n_var, dtype=torch.float64) > .5\nobs = torch.rand(n_var, dtype=torch.float64)[idx]\n\n\ntorch.linalg.inv(cov)\n\n\n(torch.linalg.inv(cov) - cholesky_inverse(torch.linalg.cholesky(cov))).max()\n\n\ntest_close(torch.linalg.inv(cov), cholesky_inverse(torch.linalg.cholesky(cov)), eps=1e-2)\n\n\nreset_seed()\nA = to_posdef(torch.rand(1000, 1000, dtype=torch.float64)) + torch.eye(1000) * 1e-3 # noise to ensure is positive definite\n\n\nis_symmetric(A)\n\n\nis_posdef(A)\n\nThe second version is way faster\n\ntest_close(conditional_guassian(mean, cov, obs, idx).mean, _conditional_guassian_base(mean, cov, obs, idx).mean)\n\n\nB = to_posdef(torch.rand(n_var, n_var, dtype=torch.float64))\n\n\nB @ torch.inverse(cov)\n\n\ntorch.cholesky_solve(cholesky(cov), B)"
  },
  {
    "objectID": "gaussian.html#helper",
    "href": "gaussian.html#helper",
    "title": "Gaussian Distributions Utils",
    "section": "Helper",
    "text": "Helper\n\ncov2std\n\nx = torch.stack([torch.eye(3)*i for i in  range(1,4)])\n\n\nx\n\n\ntorch.diagonal(x, dim1=1, dim2=2)\n\n\nsource\n\n\ncov2std\n\n cov2std (x)\n\nconvert cov of array of covariances to array of stddev"
  },
  {
    "objectID": "gaussian.html#export",
    "href": "gaussian.html#export",
    "title": "Gaussian Distributions Utils",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n cache_disk (base_file, rm_cache=False)\n\nDecorator to cache function output to disk\n\nimport time\nfrom tempfile import tempdir\n\n\ncp = Path(tempdir) / \"test_cache\"\n\n\n@cache_disk(cp)\ndef slow_add(a,b):\n    time.sleep(1)\n    return a + b\n\nthis time is the first time so not from the cache\n\n\n\nCPU times: user 3 µs, sys: 1 µs, total: 4 µs\nWall time: 5.48 µs\n\n\n3\n\n\nnow is much faster beacuse of the cache\n\n\n\nCPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 5.01 µs\n\n\n3\n\n\nadding comments change the hash, so the function is still cached\n\n@cache_disk(cp)\ndef slow_add(a,b):\n    time.sleep(1)\n    # this is a comment\n    return a + b\n\n\n\n\nCPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 4.77 µs\n\n\n3\n\n\n\n\n\nCPU times: user 5 µs, sys: 1 µs, total: 6 µs\nWall time: 7.39 µs\n\n\n3\n\n\n\nsource\n\n\n\n\n reset_seed (seed=27)\n\n\nsource\n\n\n\n\n reset_seed (seed=27)"
  },
  {
    "objectID": "utils.html#testing",
    "href": "utils.html#testing",
    "title": "Utils",
    "section": "Testing",
    "text": "Testing\n\nsource\n\ntest_close\n\n test_close (a, b, eps=1e-05)\n\ntest that a is within eps of b"
  },
  {
    "objectID": "utils.html#standard-scaler",
    "href": "utils.html#standard-scaler",
    "title": "Utils",
    "section": "Standard Scaler",
    "text": "Standard Scaler\nmake a standard scaler that can also inverse transfor standard deviations. see Standardizer for details of implementation\n\nreset_seed()\nxx = np.random.random((4, 10))\n\n\ns = StandardScaler().fit(xx)\n\n\ns.transform(xx)\n\narray([[ 0.07263978,  0.63279488, -0.9975139 ,  0.50899177,  0.15537652,\n         1.45555506,  1.56629646, -1.60237369,  1.51674974,  1.29584745],\n       [ 1.58579521,  0.83086419, -0.68281902,  0.51578245, -0.62395756,\n        -1.19720248, -0.43000476,  1.1539719 , -0.74724819, -0.85525414],\n       [-1.05809926, -1.69049694,  0.0895118 , -1.72684476, -1.08418417,\n         0.32617669, -1.16657374,  0.2345773 ,  0.26525847,  0.64349108],\n       [-0.60033573,  0.22683787,  1.59082112,  0.70207053,  1.55276521,\n        -0.58452927,  0.03028204,  0.21382449, -1.03476002, -1.08408439]])\n\n\n\ns.mean_\n\narray([0.40358703, 0.6758362 , 0.77934606, 0.70748673, 0.34417949,\n       0.62067044, 0.48500116, 0.54921643, 0.34604713, 0.3660338 ])\n\n\n\ns.scale_\n\narray([0.30471427, 0.21926148, 0.04405831, 0.31536161, 0.25229864,\n       0.24649441, 0.26061043, 0.21187396, 0.26093989, 0.22927816])\n\n\n\nsource\n\nStandardScaler.inverse_transform_std\n\n StandardScaler.inverse_transform_std (x_std)\n\n\n\n\n\nDetails\n\n\n\n\nx_std\nstandard deviations"
  },
  {
    "objectID": "utils.html#info-visualization",
    "href": "utils.html#info-visualization",
    "title": "Utils",
    "section": "Info visualization",
    "text": "Info visualization\n\nsource\n\narray2df\n\n array2df (x:torch.Tensor, row_names:Optional[Collection[str]]=None,\n           col_names:Optional[Collection[str]]=None, row_var:str='')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nTensor\n\n2d tensor\n\n\nrow_names\nOptional\nNone\nnames for the row\n\n\ncol_names\nOptional\nNone\nnames for the columns\n\n\nrow_var\nstr\n\nname of the first column (the one with row names). This should describe the values of row_name\n\n\n\n\nimport numpy as np\n\n\na = np.random.rand(2,3,3)\n\n\ndisplay(HTML(f\"<pre> {repr(a)} </pre>\"))\n\n array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) \n\n\n\na\n\narray([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]])\n\n\n\nsource\n\n\nretrieve_names\n\n retrieve_names (*args)\n\nTries to retrieve the argument name in the call frame, if there are multiple matches name is ’’\n\nsource\n\n\nmaybe_retrieve_callers_name\n\n maybe_retrieve_callers_name (args)\n\nTries to retrieve the argument name in the call frame, if there are multiple matches name is ’’\n\nx, y, z = 1, 2, 3\n\ndef func(*args):\n    return maybe_retrieve_callers_name(args)\n\nprint(func(x,y))\n\n['x', 'y']\n\n\n\nretrieve_names(a,a)\n\n[['_', 'a', '_32'], ['_', 'a', '_32']]\n\n\n\nsource\n\n\nshow_as_row\n\n show_as_row (*os:Iterable, names:Iterable[str]=None, **kwargs)\n\nShows a interable of tensors on a row\n\nshow_as_row(a,a)\n\n\n a array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) \n\n\n\nfunc(a,a)\n\n['a', 'a']\n\n\n\nshow_as_row(a, names='b')\n\n\n b array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) \n\n\n\nshow_as_row(c=a)\n\n\n c array([[[0.96646567, 0.58332229, 0.09242191],\n        [0.0136295 , 0.83693011, 0.9147879 ],\n        [0.70458626, 0.3870066 , 0.7056939 ]],\n\n       [[0.92331116, 0.28815289, 0.68401985],\n        [0.5202925 , 0.87736578, 0.92388931],\n        [0.48923016, 0.59621396, 0.26427542]]]) \n\n\n\nsource\n\n\ndisplay_as_row\n\n display_as_row (dfs:dict[str,pandas.core.frame.DataFrame], title='',\n                 styler=<function _style_df>)\n\ndisplay multiple dataframes in the same row\n\nsource\n\n\nrow_dfs\n\n row_dfs (dfs:dict[str,pandas.core.frame.DataFrame], title='',\n          styler=<function _style_df>)\n\n\na = HTML(pd.DataFrame([1,2]).to_html(notebook=True))\n\n\ndisplay_as_row({\"test\": pd.DataFrame([1,2])}, \"hello\")\n\n\ndisplay_as_row({f\"test{i}\": pd.DataFrame([1,2]) for i in range(10)})"
  },
  {
    "objectID": "utils.html#tensor-shapes",
    "href": "utils.html#tensor-shapes",
    "title": "Utils",
    "section": "Tensor shapes",
    "text": "Tensor shapes\n\nsource\n\narray2d\n\n array2d (X)\n\nReturns at least 2-d array with data from X\n\nsource\n\n\narray1d\n\n array1d (X)\n\nReturns at least 1-d array with data from X\n\nsource\n\n\nlast_dims\n\n last_dims (X:torch.Tensor, t:int, ndims:int=2)\n\nExtract the final dimensions of X\nExtract the final ndim dimensions at index t if X has >= ndim + 1 dimensions, otherwise return X.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nTensor\n\n\n\n\nt\nint\n\nindex to use for the ndims + 1th dimension\n\n\nndims\nint\n2\nnumber of dimensions in the array desired\n\n\nReturns\narray with dimension ndims\n\nthe final ndims dimensions indexed by t\n\n\n\n\nsource\n\n\ndetermine_dimensionality\n\n determine_dimensionality (variables, default)\n\nDerive the dimensionality of the state space\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nvariables\nlist of ({None, array}, conversion function, index)\nvariables, functions to convert them to arrays, and indices in thosearrays to derive dimensionality from.\n\n\ndefault\n\n\n\n\nReturns\nint\ndimensionality of state space as derived from variables or default."
  },
  {
    "objectID": "utils.html#export",
    "href": "utils.html#export",
    "title": "Utils",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "Fluxnet/gap_finder.html",
    "href": "Fluxnet/gap_finder.html",
    "title": "Find Gaps in Fluxnet data",
    "section": "",
    "text": "test_file_zip = here() / Path(\"../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4.zip\")\ntest_file = here() / Path(\"../fluxnet/FLX_DE-Hai_FLUXNET2015_FULLSET_2000-2012_1-4/FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\")\ntmp_dir = Path(\"/tmp\")\nout_dir = here() / Path(\"../fluxnet/gap_stat\")\ndownload_dir = Path(\"/run/media/simone/Simone DATI/fluxnet_all\")\nunzip the file and load it lazily with polars\ncolumns selection, interested only in QC columns to find gaps\nThe goal is to find where the data is missing in the dataset (which means that it has been gap-filled) and find:\nwe filter out the rows where there is no gap (QC =0)\nthen find the start and end of gap by comparing with the original row number of the previous entry\nit works!\nsource"
  },
  {
    "objectID": "Fluxnet/gap_finder.html#find-gaps",
    "href": "Fluxnet/gap_finder.html#find-gaps",
    "title": "Find Gaps in Fluxnet data",
    "section": "Find gaps",
    "text": "Find gaps\n\ndef _find_gap_df(df, col_name):\n    \"Find gaps with a df with a single QC column\"\n    return df.filter(\n        pl.col(col_name) != 0\n    ).with_columns([\n        (pl.col(\"row_num\") - pl.col(\"row_num\").shift() ).alias(\"before\"),\n        (pl.col(\"row_num\").shift(-1) - pl.col(\"row_num\")).alias(\"after\"),\n    ]).filter(\n        (pl.col(\"before\") != 1) | (pl.col(\"after\") != 1)\n    ).with_column(\n        (pl.when((pl.col(\"before\") != 1) & (pl.col(\"after\") != 1))\n        .then(pl.col(\"start\"))\n        .otherwise(pl.col(\"start\").shift(-1))\n        .alias(\"gap_end\"))\n    ).filter(\n        pl.col(\"before\") != 1\n    ).select(\n        [pl.col(\"start\").alias(\"gap_start\"), \"gap_end\"]\n    )\n\n\ndef find_gap_variable(df, col_name):\n    \n    # row numembering has to happen before filtering\n    df = df.with_column(\n        pl.first().cumcount().alias(\"row_num\")\n    )\n    \n    # start with null values\n    dff = df.filter(\n            pl.col(col_name).is_null()\n        )\n    gaps = [\n        _find_gap_df(dff, col_name).with_columns(\n            pl.lit(None).alias(\"gap_value\")\n        )]\n    \n    \n    # all other values\n    # here the QC flags are merged together as we we not interested in the QC alg only if there is a gap\n    dff = df.filter(\n            ~pl.col(col_name).is_null()\n        )\n    gaps.append(\n        _find_gap_df(dff, col_name).with_columns(\n            pl.lit(1).alias(\"gap_value\")\n        ))\n    \n    return pl.concat(gaps)\n\n\nfind_gap_variable(df, \"SW_IN_F_QC\").collect().head()\n\nNotFoundError: start\n\n\n\ndef find_all_gaps(df):\n    return pl.concat(\n        [find_gap(df, col_name) for col_name in df.select(pl.col(\"^.*_QC$\")).columns]\n    )\n\n\nsource\n\nfind_all_gaps\n\n find_all_gaps (df)\n\n\ngaps_all = find_all_gaps(df).collect()\n\n\ngaps_all.groupby(\"variable\").agg(pl.col(\"gap_len\").sum() / df.collect().shape[0])\n\n\n\n\nshape: (45, 2)\n\n\n\n\nvariable\n\n\ngap_len\n\n\n\n\nstr\n\n\nf64\n\n\n\n\n\n\n\"NEE_VUT_84_QC\"\n\n\n0.722569\n\n\n\n\n\"NEE_VUT_16_QC\"\n\n\n0.725254\n\n\n\n\n\"NEE_CUT_16_QC\"\n\n\n0.727048\n\n\n\n\n\"NEE_VUT_50_QC\"\n\n\n0.6978\n\n\n\n\n\"NEE_VUT_95_QC\"\n\n\n0.742042\n\n\n\n\n\"TA_F_MDS_QC\"\n\n\n0.012959\n\n\n\n\n\"H_F_MDS_QC\"\n\n\n0.198801\n\n\n\n\n\"LW_IN_F_QC\"\n\n\n0.243714\n\n\n\n\n\"TS_F_MDS_2_QC\"\n\n\n0.007857\n\n\n\n\n\"SWC_F_MDS_2_QC...\n\n\n0.242968\n\n\n\n\n\"CO2_F_MDS_QC\"\n\n\n0.097889\n\n\n\n\n\"NEE_CUT_75_QC\"\n\n\n0.709597\n\n\n\n\n...\n\n\n...\n\n\n\n\n\"SW_IN_F_MDS_QC...\n\n\n0.015802\n\n\n\n\n\"LW_IN_F_MDS_QC...\n\n\n0.243714\n\n\n\n\n\"SW_IN_F_QC\"\n\n\n0.015802\n\n\n\n\n\"NEE_CUT_95_QC\"\n\n\n0.745916\n\n\n\n\n\"LW_IN_JSB_QC\"\n\n\n0.022492\n\n\n\n\n\"PA_F_QC\"\n\n\n0.016109\n\n\n\n\n\"NEE_VUT_MEAN_Q...\n\n\n0.96898\n\n\n\n\n\"SWC_F_MDS_1_QC...\n\n\n0.015516\n\n\n\n\n\"TS_F_MDS_4_QC\"\n\n\n0.850741\n\n\n\n\n\"NEE_VUT_USTAR5...\n\n\n0.703832\n\n\n\n\n\"NEE_VUT_75_QC\"\n\n\n0.70968\n\n\n\n\n\"TS_F_MDS_1_QC\"\n\n\n0.006352\n\n\n\n\n\n\n\n\ndef download_and_find_gaps(urls, download_dir, out_dir, tmp_dir):\n    site_infos = []\n    for url in tqdm(urls):\n        file_zip = download_fluxnet(url, download_dir)\n        file, site_info = find_gaps_fluxnet_archive(file_zip, out_dir, tmp_dir)\n        site_infos.append(site_info)\n        print(file)\n        \n    return pl.concat(site_infos)\n\n\nsource\n\n\ndownload_and_find_gaps\n\n download_and_find_gaps (urls, download_dir, out_dir, tmp_dir)\n\n\nurls = [\"https://ftp.fluxdata.org/.fluxnet_downloads_86523/FLUXNET2015/FLX_AR-SLu_FLUXNET2015_FULLSET_2009-2011_1-4.zip?=mone27\",\n\"https://ftp.fluxdata.org/.fluxnet_downloads_86523/FLUXNET2015/FLX_AR-Vir_FLUXNET2015_FULLSET_2009-2012_1-4.zip?=mone27\"]\n\n\ndownload_and_find_gaps(urls, download_dir, out_dir, tmp_dir)\n\n\n\n\nFLX_AR-SLu_FLUXNET2015_FULLSET_HH_2009-2011_1-4\nFLX_AR-Vir_FLUXNET2015_FULLSET_HH_2009-2012_1-4\n\n\n\n\n\nshape: (2, 3)\n\n\n\n\nstart\n\n\nend\n\n\nsite\n\n\n\n\ni64\n\n\ni64\n\n\nstr\n\n\n\n\n\n\n200901010030\n\n\n201201010000\n\n\n\"AR-SLu\"\n\n\n\n\n200901010030\n\n\n201301010000\n\n\n\"AR-Vir\""
  },
  {
    "objectID": "Fluxnet/hainich.html",
    "href": "Fluxnet/hainich.html",
    "title": "Fluxnet Hainich",
    "section": "",
    "text": "_def_meteo_vars = {\n    \"TA_F\": \"TA\",\n    \"SW_IN_F\": \"SW_IN\",\n    # \"LW_IN_F\": \"LW_IN\",\n    \"VPD_F\": \"VPD\",\n    #\"PA\": \"PA\"\n}\n\n\nunits = {\n    'TA': '°C',\n    'SW_IN': 'W m-2',\n    # 'LW_IN': 'W m-2',\n    'VPD': 'hPa'\n}\n\nhai_path = here(\"data\") / \"FLX_DE-Hai_FLUXNET2015_FULLSET_HH_2000-2012_1-4.csv\"\n\n/opt/hostedtoolcache/Python/3.10.9/x64/lib/python3.10/site-packages/pyprojroot/pyprojroot.py:51: UserWarning: Path doesn't exist: /home/runner/work/meteo_imp/meteo_imp/data\n  warnings.warn(\"Path doesn't exist: {}\".format(path))\nsource"
  },
  {
    "objectID": "Fluxnet/hainich.html#export",
    "href": "Fluxnet/hainich.html#export",
    "title": "Fluxnet Hainich",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "simplegp_imputation.html",
    "href": "simplegp_imputation.html",
    "title": "Simple GP Imputation",
    "section": "",
    "text": "source\n\n\n\n SimpleGP (train_x, train_y, likelihood)\n\nExact GP implemnetation using GPyTorch\n\nk = SimpleGP(torch.tensor([1,2,3]), torch.tensor([1,2,3]), gpytorch.likelihoods.GaussianLikelihood())\n\n\nk\n\nSimpleGP(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): ConstantMean()\n  (covar_module): ScaleKernel(\n    (base_kernel): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n    (raw_outputscale_constraint): Positive()\n  )\n)\n\n\n\nk.covar_module.base_kernel.lengthscale.item()\n\n0.6931471824645996\n\n\n\nk.covar_module.outputscale.item()\n\n0.6931471824645996\n\n\n\nsource\n\n\n\n\n SimpleGP.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nk.get_info()\n\n{'lengthscale':    lengthscale\n 0     0.693147,\n 'outputscale':    outputscale\n 0     0.693147,\n 'likelihood':       noise\n 0  0.693247}"
  },
  {
    "objectID": "simplegp_imputation.html#learner",
    "href": "simplegp_imputation.html#learner",
    "title": "Simple GP Imputation",
    "section": "Learner",
    "text": "Learner\n\nsource\n\nSimpleGPLearner\n\n SimpleGPLearner (X:torch.Tensor, T:torch.Tensor=None)\n\nLearner for a simple GP process. It handles only 1 dimensional time series\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nTensor\n\n(n_obs) Univariate time series\n\n\nT\nTensor\nNone\n(n_obs) Vector of time of observations.\n\n\n\n\nX = torch.tensor([1.,2,3,4])\n\n\nl = SimpleGPLearner(X)\n\n\nl.train()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n\n\n\nl.predict(torch.tensor([5, 7]))\n\nNormalParameters(mean=tensor([5.5955, 6.4173]), std=tensor([0.1980, 0.6468]))\n\n\nImputation\nThe imputation using simple GPs make a separate GP process for each variable, which are completely independent\n\nsource\n\n\nSimpleGPImputationExplorer\n\n SimpleGPImputationExplorer (data:pandas.core.frame.DataFrame, cuda=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nobserved data with missing data as NA\n\n\ncuda\nbool\nFalse\nUse GPU?\n\n\n\n\nfd = GPFADataTest.generate(2, 7).add_gap(3, [\"x1\"]).add_gap(2, [\"x0\"])\n\n\ngp_imp = SimpleGPImputationExplorer(fd.data)\n\n\ngp_imp\n\nSimple GP Imputation Explorer:\n    N obs: 7\n    N features 2 (x0, x1)\n    N missing observations 5\n\n\n\ngp_imp.fit()\n\n\n\n\n\n\n\nSimple GP Imputation Explorer:\n    N obs: 7\n    N features 2 (x0, x1)\n    N missing observations 5\n\n\n\ngp_imp.learners[0].predict(torch.tensor([3.]))\n\nNormalParameters(mean=tensor([-0.1003]), std=tensor([0.2664]))\n\n\n\ngp_imp.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      0.322117\n      0.294301\n    \n    \n      1\n      -0.119194\n      0.224219\n    \n    \n      2\n      -0.193881\n      -0.017484\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n    \n    \n      5\n      0.540541\n      NaN\n    \n    \n      6\n      -0.410130\n      -0.271418\n    \n  \n\n\n\n\n\ngp_imp.train_data\n\ntensor([[ 0.3221,  0.2943],\n        [-0.1192,  0.2242],\n        [-0.1939, -0.0175],\n        [-0.4101, -0.2714]])\n\n\n\ngp_imp.predict()\n\n\n\n\n\n  \n    \n      \n      mean\n      time\n      std\n      variable\n    \n  \n  \n    \n      0\n      0.105723\n      0.0\n      0.232544\n      x0\n    \n    \n      1\n      -0.109509\n      1.0\n      0.232544\n      x0\n    \n    \n      2\n      -0.145935\n      2.0\n      0.232544\n      x0\n    \n    \n      3\n      -0.100289\n      3.0\n      0.266372\n      x0\n    \n    \n      4\n      -0.100289\n      4.0\n      0.266372\n      x0\n    \n    \n      5\n      -0.100289\n      5.0\n      0.266372\n      x0\n    \n    \n      6\n      -0.251402\n      6.0\n      0.232544\n      x0\n    \n    \n      0\n      0.294510\n      0.0\n      0.023458\n      x1\n    \n    \n      1\n      0.220237\n      1.0\n      0.023208\n      x1\n    \n    \n      2\n      -0.014538\n      2.0\n      0.023457\n      x1\n    \n    \n      3\n      -0.220033\n      3.0\n      0.083291\n      x1\n    \n    \n      4\n      -0.293843\n      4.0\n      0.150042\n      x1\n    \n    \n      5\n      -0.293429\n      5.0\n      0.122185\n      x1\n    \n    \n      6\n      -0.270590\n      6.0\n      0.023595\n      x1"
  },
  {
    "objectID": "simplegp_imputation.html#results",
    "href": "simplegp_imputation.html#results",
    "title": "Simple GP Imputation",
    "section": "results",
    "text": "results\n\nself = gp_imp\ninfos = [learner.model.get_info() for learner in self.learners]\n\n\ninfos\n\n[{'lengthscale':    lengthscale\n  0     0.192713,\n  'outputscale':    outputscale\n  0     0.366132,\n  'likelihood':       noise\n  0  0.384582},\n {'lengthscale':    lengthscale\n  0     1.679326,\n  'outputscale':    outputscale\n  0     0.910373,\n  'likelihood':       noise\n  0  0.004235}]\n\n\n\nsource\n\nSimpleGPImputationExplorer.model_info\n\n SimpleGPImputationExplorer.model_info ()\n\nCombine parameters of different kernels into one output\n\ngp_imp.model_info()\n\n{'lengthscale':   variable  lengthscale\n 0       x0     0.192713\n 0       x1     1.679326,\n 'outputscale':   variable  outputscale\n 0       x0     0.366132\n 0       x1     0.910373,\n 'likelihood':   variable     noise\n 0       x0  0.384582\n 0       x1  0.004235}\n\n\n\nsource\n\n\nSimpleGPImputationExplorer.to_result\n\n SimpleGPImputationExplorer.to_result (data_complete, units=None)\n\n\nImputationResult??\n\n\nInit signature:\nImputationResult(\n    data_imputed,\n    data_complete,\n    model_info,\n    units=None,\n    metrics_all_data=True,\n)\nDocstring:      <no docstring>\nSource:        \nclass ImputationResult:\n    def __init__(self,\n                 data_imputed, #imputed data in tidy format\n                 data_complete, # complete data in tidy format\n                 model_info, # learner for parameters display\n                 units = None, # units for plots\n                 metrics_all_data = True # Compute metrics only for gap or for all data?\n                ):\n        store_attr()\nFile:           ~/Documents/uni/Thesis/GPFA_imputation/gpfa_imputation/results.py\nType:           type\nSubclasses:     \n\n\n\n\n\ngp_imp.to_result(fd.data_compl_tidy).display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      x0\n      0.2123\n    \n    \n      x1\n      0.7596\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n    \n  \n  \n    \n      x0\n      0.2954\n    \n    \n      x1\n      0.1396\n    \n  \n\n \n\n\n\nModel Info  lengthscale \n\n  \n    \n      variable\n      lengthscale\n    \n  \n  \n    \n      x0\n      0.1927\n    \n    \n      x1\n      1.6793\n    \n  \n\n  outputscale \n\n  \n    \n      variable\n      outputscale\n    \n  \n  \n    \n      x0\n      0.3661\n    \n    \n      x1\n      0.9104\n    \n  \n\n  likelihood \n\n  \n    \n      variable\n      noise\n    \n  \n  \n    \n      x0\n      0.3846\n    \n    \n      x1\n      0.0042"
  },
  {
    "objectID": "kalman/imputation.html",
    "href": "kalman/imputation.html",
    "title": "Imputation Kalman Model",
    "section": "",
    "text": "source\n\n\n\n KalmanImputation (data:pandas.core.frame.DataFrame,\n                   model:meteo_imp.kalman.model.KalmanModel=<class\n                   'meteo_imp.kalman.model.KalmanModel'>, **kwargs)\n\nImputation using a kalman model\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\n\n\n\nmodel\nKalmanModel\nKalmanModel\na subclass of KalmanModel to be used as model\n\n\nkwargs\n\n\n\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\n\n\nreset_seed(1)\ndata = MeteoDataTest.generate_gpfa(2, 5).add_random_missing()\n\n\ndata.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      0.023263\n      NaN\n    \n    \n      1\n      0.219627\n      0.268028\n    \n    \n      2\n      -0.039892\n      0.063075\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      -0.645490\n      -0.144866\n    \n  \n\n\n\n\n\nk_imp = KalmanImputation(data.data)\n\nValueError: could not determine the shape of object type 'DataFrame'\n\n\n\nk_imp.train_data\n\n\nk_imp.train_data[k_imp.train_idx]\n\ntensor([[ 0.8847,  0.9976],\n        [ 0.1895,  0.0048],\n        [-1.4328, -1.0024]])\n\n\n\nk_imp.fit(10, lr=0.1)\n\nstarting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<__main__.KalmanImputation>\n\n\n\nk_imp.impute()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0\n      x0\n      -0.031254\n      0.473841\n    \n    \n      1\n      1\n      x0\n      0.219627\n      0.000000\n    \n    \n      2\n      2\n      x0\n      -0.039892\n      0.000000\n    \n    \n      3\n      3\n      x0\n      -0.280452\n      0.514002\n    \n    \n      4\n      4\n      x0\n      -0.645490\n      0.000000\n    \n    \n      5\n      0\n      x1\n      0.109620\n      0.259775\n    \n    \n      6\n      1\n      x1\n      0.268028\n      0.000000\n    \n    \n      7\n      2\n      x1\n      0.063075\n      0.000000\n    \n    \n      8\n      3\n      x1\n      -0.000474\n      0.280925\n    \n    \n      9\n      4\n      x1\n      -0.144866\n      0.000000\n    \n  \n\n\n\n\n\ndata.data.shape\n\n\nk_imp.impute(pred_all=True)\n\n\n\n\nsource\n\n\n\n\n\n KalmanImputation.to_result (data_compl, var_names=None, units=None,\n                             pred_all=False)\n\n\nX = np.hstack([np.arange(0,3.), np.arange(3., 0, -1)]).reshape(6, 1)\n\n\nres = k_imp.to_result(data.data_compl_tidy)\n\n\nres.display_results()"
  },
  {
    "objectID": "kalman/imputation.html#debug",
    "href": "kalman/imputation.html#debug",
    "title": "Imputation Kalman Model",
    "section": "Debug",
    "text": "Debug\n\nfrom meteo_imp.data import hai\n\n\ntd = MeteoDataTest(hai)\n\n\ntd.add_gap(10, 'TA', 10)\n\n\ni_hai = KalmanImputation(td.data)\n\n\ni_hai.fit()"
  },
  {
    "objectID": "kalman/imputation.html#export",
    "href": "kalman/imputation.html#export",
    "title": "Imputation Kalman Model",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/old statsmodels - statespace.html",
    "href": "kalman/old statsmodels - statespace.html",
    "title": "[Not Working] StatsModels Kalman filter",
    "section": "",
    "text": "import pandas as pd\nfrom fastcore.basics import store_attr\nimport statsmodels.api as sm\nimport numpy as np\nfrom meteo_imp.results import ImputationResult\nto work properly with the statsmodel the dataframe can have missing data, but the data should be have the index properly set\nFor testing purposes use models from statsmodel"
  },
  {
    "objectID": "kalman/old statsmodels - statespace.html#introduction",
    "href": "kalman/old statsmodels - statespace.html#introduction",
    "title": "[Not Working] StatsModels Kalman filter",
    "section": "Introduction",
    "text": "Introduction\nThe model uses a Kalman filter with the simplest possible equations\nThe notation used is from @durbin_time_2012 since the one used in the statsmodel library that is used for this implementation.\nThe state of the model changes only due to the process noise (the design matrix is one) The transition matrix is also one\nThe general notation is:\n\\[\\begin{split}y_t & = Z_t \\alpha_t + d_t + \\varepsilon_t \\\\\n\\alpha_{t+1} & = T_t \\alpha_t + c_t + R_t \\eta_t \\\\\\end{split}\\]\nwhere \\[\\begin{split}\\varepsilon_t \\sim N(0, H_t) \\\\\n\\eta_t \\sim N(0, Q_t) \\\\\\end{split}\\]\n\nZ : design \\((k\\_endog \\times k\\_states \\times nobs)\\)\nd : obs_intercept \\(k\\_endog \\times nobs)\\)\nH : obs_cov \\(k\\_endog \\times k\\_endog \\times nobs)\\)\nT : transition \\(k\\_states \\times k\\_states \\times nobs)\\)\nc : state_intercept \\(k\\_states \\times nobs)\\)\nR : selection \\(k\\_states \\times k\\_posdef \\times nobs)\\)\nQ : state_cov \\(k\\_posdef \\times k\\_posdef \\times nobs)\\)\n\n\nLocal Level\nThe random walker makes the following assumptions:\n\nthe design matrix is one, which means that the change over time of the state is only due to the random process noise\nthe transition matrix is one, which means that the observations are equal to the state plus a random measurement noise\nthe process and measurement noise don’t change over time\n\nHence the equations of the model are:\n\\[\\begin{split}\ny_t & = \\alpha_t +  \\varepsilon \\\\\n\\alpha_{t+1} & = \\alpha_t + \\eta\n\\end{split}\\]\nThe parameters of the models are \\(\\varepsilon\\) and \\(\\eta\\) which are estimated by maximising the log likelihood\nthis first implementation considers only a univariate scenario"
  },
  {
    "objectID": "kalman/old statsmodels - statespace.html#implementation",
    "href": "kalman/old statsmodels - statespace.html#implementation",
    "title": "[Not Working] StatsModels Kalman filter",
    "section": "Implementation",
    "text": "Implementation\n\nimport numpy as np\nimport statsmodels.api as sm\nfrom fastcore.basics import patch\nimport pandas as pd\n\n\n\"\"\"\nUnivariate Random Walker or local level model\n\"\"\"\nclass UnivarLocalLevel(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog, **kwargs):\n        # Model order\n        k_states = k_posdef = 1\n\n        # Initialize the statespace\n        super(UnivarLocalLevel, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states, **kwargs\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1])\n        self.ssm['transition'] = np.array([1])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['epsilon', 'eta']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*2\n    \n    # This is to guarantee that the parameters are positive\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(UnivarLocalLevel, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm['state_cov', 0, 0] = params[1]\n\n\nY = np.arange(0,10)\n\n\nrw = UnivarLocalLevel(Y)\n\n\nres = rw.fit()\n\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  1.87798D+00    |proj g|=  2.84223D-01\n\nAt iterate    5    f=  1.27704D+00    |proj g|=  2.20201D-04\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      7     13      1     0     0   1.860D-06   1.277D+00\n  F =   1.2770446799271464     \n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n\n\n This problem is unconstrained.\n\n\n\nres.summary()\n\n\n\nStatespace Model Results\n\n  Dep. Variable:            y           No. Observations:     10   \n\n\n  Model:           UnivarRandomWalker   Log Likelihood      -12.770\n\n\n  Date:             Sat, 12 Nov 2022    AIC                 29.541 \n\n\n  Time:                 19:06:40        BIC                 29.935 \n\n\n  Sample:                   0           HQIC                28.690 \n\n\n                           - 10                                    \n\n\n  Covariance Type:         opg                                     \n\n\n\n\n             coef     std err      z      P>|z|  [0.025    0.975]  \n\n\n  epsilon  1.454e-11     1.061  1.37e-11  1.000    -2.079     2.079\n\n\n  eta         1.0000  1.66e+05  6.03e-06  1.000 -3.25e+05  3.25e+05\n\n\n\n\n  Ljung-Box (L1) (Q):     0.38   Jarque-Bera (JB):   2.70 \n\n\n  Prob(Q):                0.54   Prob(JB):           0.26 \n\n\n  Heteroskedasticity (H): 1.00   Skew:               -1.34\n\n\n  Prob(H) (two-sided):    1.00   Kurtosis:           2.79 \n\nWarnings:[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\nres.params\n\narray([1.45422542e-11, 9.99987933e-01])\n\n\n\npredict = res.get_prediction()\n\n\npredict.se_mean\n\narray([1.00000000e+03, 9.99993967e-01, 9.99993967e-01, 9.99993967e-01,\n       9.99993967e-01, 9.99993967e-01, 9.99993967e-01, 9.99993967e-01,\n       9.99993967e-01, 9.99993967e-01])\n\n\n\nres.get_prediction(1, 10)\n\n<statsmodels.tsa.statespace.mlemodel.PredictionResultsWrapper>"
  },
  {
    "objectID": "kalman/test_filter_pykalman.html",
    "href": "kalman/test_filter_pykalman.html",
    "title": "Compare PyKalman",
    "section": "",
    "text": "from fastcore.test import *\nfrom fastcore.basics import *\nfrom meteo_imp.utils import *\nfrom meteo_imp.gaussian import *\nfrom meteo_imp.data_preparation import MeteoDataTest\nfrom meteo_imp.kalman.filter import *\n\nimport pykalman\nfrom typing import *\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import Tensor\nfrom torch.distributions import MultivariateNormal"
  },
  {
    "objectID": "kalman/test_filter_pykalman.html#performance",
    "href": "kalman/test_filter_pykalman.html#performance",
    "title": "Compare PyKalman",
    "section": "Performance",
    "text": "Performance\n\nreset_seed(12)\n\n\nSmall filter\n\nkf_cuda = KalmanFilter.init_random(3,3).cuda()\ndata_cuda, mask_cuda = get_test_data(200,3, bs=500, device=\"cuda\")\n\n\nkf = KalmanFilter.init_random(2,2)\ndata, mask = get_test_data(100,2, bs=200)\n\n\nkf = KalmanFilter.init_random(3,3)\ndata, mask = get_test_data(50,3, bs=200)\n\n\n\n\n4.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n# no batches\n\n8.21 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)"
  },
  {
    "objectID": "kalman/test_filter_pykalman.html#introduction",
    "href": "kalman/test_filter_pykalman.html#introduction",
    "title": "Compare PyKalman",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "kalman/test_filter_pykalman.html#kalmanfilter",
    "href": "kalman/test_filter_pykalman.html#kalmanfilter",
    "title": "Compare PyKalman",
    "section": "KalmanFilter",
    "text": "KalmanFilter\nThe Kalman Filter is an algorithm designed to estimate \\(P(x_t | y_{0:t})\\). As all state transitions and obss are linear with Gaussian distributed noise, these distributions can be represented exactly as Gaussian distributions with mean filt_state_means[t] and covs filt_state_covs[t]. Similarly, the Kalman Smoother is an algorithm designed to estimate \\(P(x_t | y_{0:T-1})\\)\n\nMain class\n\nk = KalmanFilter.init_random(1,1)\n\n\nk.obs_cov\n\ntensor([[0.2187]], grad_fn=<AddBackward0>)\n\n\n\nk.init_state_cov\n\ntensor([[0.4346]], grad_fn=<AddBackward0>)\n\n\n\nlist(k.named_parameters())\n\n\nk = KalmanFilter()\n# pykalman reference implementation\npyk = pykalman.KalmanFilter()\n\n\nX = torch.tensor([1.,2,3])\nnX = X.numpy()\nX\n\n2 dimensional dobs\n\nX2 = torch.tensor([[i, 10. * i] for i in range(1,5)])\nnX2 = X2.numpy()\n\n\nk2 = KalmanFilter(transition_matrices = torch.eye(2), obs_matrices=torch.eye(2))\n\npyk2 = pykalman.KalmanFilter(n_dim_obs = 2, n_dim_state=2)\n\n\nX2\n\n\nobs_mask = torch.ones_like(X2, dtype=torch.bool)\nobs_mask[0, 1] = False # only one value missing\nobs_mask[2, :] = False # whole row missing\n# Xm X missing\nX2m = X2.clone()\nX2m[~obs_mask] = torch.nan\n# nXm Numpy X missing\nnX2m = np.ma.masked_array(X2.numpy(), mask = ~obs_mask.numpy())\n\n\nobs_mask\n\n\nX2m\n\n\nnX2m\n\n\nKalmanFilter.init_random(3,6, dtype=torch.float64)\n\n\n\nTester\n\nfrom fastcore.basics import *\n\n\nto_posdef = PosDef().transform\n\nclass KalmanFilterTester():\n    \"\"\"utility class to compare meteo_imp torch implementation with pykalman one\"\"\"\n    \n    torch2pyk = {\n        'trans_matrix':    'transition_matrices',\n        'trans_off':       'transition_offsets',        \n        'trans_cov':       'transition_covariance',        \n        'obs_matrix':      'observation_matrices',\n        'obs_off':         'observation_offsets',          \n        'obs_cov':         'observation_covariance',            \n        'init_state_mean': 'initial_state_mean',        \n        'init_state_cov':  'initial_state_covariance',\n    }\n    \n    def __init__(self,\n                 n_dim_state = 3,\n                n_dim_obs = 3,\n                n_obs = 10,\n                p_missing = .3,\n                seed=None,\n                dtype=torch.float32,\n                nan_mask = True\n                ):\n        store_attr(but='seed')\n        if seed: reset_seed(seed)\n        \n        self.random_init()\n    \n    def random_init(self):\n        self.params = {\n            'trans_matrix': torch.rand(self.n_dim_state, self.n_dim_state, dtype=self.dtype),\n            'trans_off':  torch.rand(self.n_dim_state, dtype=self.dtype),        \n            'trans_cov':      to_posdef(torch.rand(self.n_dim_state, self.n_dim_state, dtype=self.dtype)),        \n            'obs_matrix':        torch.rand(self.n_dim_obs, self.n_dim_state, dtype=self.dtype),\n            'obs_off':         torch.rand(self.n_dim_obs, dtype=self.dtype),          \n            'obs_cov':             to_posdef(torch.rand(self.n_dim_obs, self.n_dim_obs, dtype=self.dtype)),            \n            'init_state_mean':  torch.rand(self.n_dim_state, dtype=self.dtype),        \n            'init_state_cov':   to_posdef(torch.rand(self.n_dim_state, self.n_dim_state, dtype=self.dtype)),\n        }\n        self.params_pyk = {self.torch2pyk[name]: param.numpy() for name, param in self.params.items()}\n        \n        self.filter = KalmanFilter(**self.params)\n        self.filter_pyk = pykalman.standard.KalmanFilter(**self.params_pyk)\n        \n        \n        self.data = torch.rand(self.n_obs, self.n_dim_obs, dtype=self.dtype).unsqueeze(0) # batch dim\n        self.mask = (torch.rand(self.n_obs, self.n_dim_obs) > self.p_missing).unsqueeze(0) \n        if self.nan_mask: self.data[~self.mask] = torch.nan # ensure that the data cannot be used\n        self.data_pyk = np.ma.masked_array(self.data.squeeze(0).numpy(), mask = ~self.mask.squeeze(0).numpy())\n\n\ntst = KalmanFilterTester()\n\n\ntst64 = KalmanFilterTester(dtype=torch.float64)\n\n\ntst.data.shape\n\ntorch.Size([1, 10, 3])\n\n\n\ntst.data_pyk.shape\n\n(10, 3)\n\n\n\ntest_close(tst.params.values(), tst.params_pyk.values())\n\n\n\nFilter\n\nFilter predict\n\nfrom datetime import datetime\ndef _filter_predict(transition_matrix, transition_cov,\n                    transition_offset, current_state_mean,\n                    current_state_cov, check_args=None):\n    r\"\"\"Calculate the mean and cov of $P(x_{t+1} | z_{0:t})$\"\"\"\n    pred_state_mean = transition_matrix @ current_state_mean + transition_offset\n    pred_state_cov =  transition_matrix @ current_state_cov @ transition_matrix.T + transition_cov\n\n    if check_args is not None: check_posdef(pred_state_cov, 'filter_predict', **check_args)\n    \n    return (pred_state_mean, pred_state_cov)\n\n\ndef _filter_predict2(transition_matrix, transition_covariance,\n                    transition_offset, current_state_mean,\n                    current_state_covariance):\n    predicted_state_mean = (\n        torch.matmul(transition_matrix, current_state_mean)\n        + transition_offset\n    )\n    predicted_state_covariance = (\n        torch.matmul(transition_matrix,\n               torch.matmul(current_state_covariance,\n                      transition_matrix.T))\n        + transition_covariance\n    )\n\n    return (predicted_state_mean, predicted_state_covariance)\n\n\n(\n    (tst.params['transition_matrices'] @ tst.params['initial_state_cov'] ) -\n    torch.matmul(tst.params['transition_matrices'], tst.params['initial_state_cov'])\n)\n\n\n(\n    (tst.params['transition_matrices'] @ tst.params['initial_state_cov']  @ tst.params['transition_matrices'].T) -\n    torch.matmul(tst.params['transition_matrices'], torch.matmul(tst.params['initial_state_cov'], tst.params['transition_matrices'].T))\n)\n\n\n(\n    (tst.params['transition_matrices'] @ tst.params['initial_state_cov']  @ tst.params['transition_matrices'].T + tst.params['transition_cov']) -\n    (torch.matmul(tst.params['transition_matrices'], torch.matmul(tst.params['initial_state_cov'], tst.params['transition_matrices'].T)) + tst.params['transition_cov'])\n)\n\n\ntrans_m = np.eye(2)\ntrans_cov = np.eye(2)\ntrans_off = np.zeros((2,2))\ncurr_mean = np.ones((2,1))\ncurr_cov = np.zeros((2,2))\n\n\n_filter_predict(torch.tensor(trans_m) , torch.tensor(trans_cov), torch.tensor(trans_off), torch.tensor(curr_mean), torch.tensor(curr_cov))\n\n\ntest_close(\n    pykalman.standard._filter_predict(trans_m , trans_cov, trans_off, curr_mean, curr_cov),\n    _filter_predict(torch.tensor(trans_m) , torch.tensor(trans_cov), torch.tensor(trans_off), torch.tensor(curr_mean), torch.tensor(curr_cov)))\n\n\npred_pyk = pykalman.standard._filter_predict(\n   tst.params_pyk['transition_matrices'],\n   tst.params_pyk['transition_covariance'],\n   tst.params_pyk['transition_offsets'],\n   tst.params_pyk['initial_state_mean'],\n   tst.params_pyk['initial_state_covariance'],\n)\n\n\npred_torch = _filter_predict(\n   tst.params['transition_matrices'],\n   tst.params['transition_cov'],\n   tst.params['transition_offsets'],\n   tst.params['initial_state_mean'],\n   tst.params['initial_state_cov'],\n\n)\n\n\npred_pyk[0] - pred_torch[0].numpy()\n\n\ntest_close(pred_pyk[0], pred_torch[0])\n\n\npred_pyk[1] - pred_torch[1].numpy()\n\n\ntest_close(pred_pyk[1], pred_torch[1])\n\n\npykalman.standard._filter_predict??\n\n\ntst.params_pyk['transition_matrices'].shape\n\n\ntest_close(\n   pykalman.standard._filter_predict(\n       tst.params_pyk['transition_matrices'],\n       tst.params_pyk['transition_covariance'],\n       tst.params_pyk['transition_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n    ),\n    \n    _filter_predict2(\n       tst.params['transition_matrices'],\n       tst.params['transition_cov'],\n       tst.params['transition_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n    \n    )\n)\n\n\ntest_close(\n    \n    _filter_predict2(\n       tst.params['transition_matrices'],\n       tst.params['transition_cov'],\n       tst.params['transition_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n    ),\n    _filter_predict(\n       tst.params['transition_matrices'],\n       tst.params['transition_cov'],\n       tst.params['transition_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n    )\n)\n\n\ntest_close(\n   pykalman.standard._filter_predict(\n       tst.params_pyk['transition_matrices'],\n       tst.params_pyk['transition_covariance'],\n       tst.params_pyk['transition_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n    ),\n    \n    _filter_predict(\n       tst.params['transition_matrices'],\n       tst.params['transition_cov'],\n       tst.params['transition_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n    \n    )\n)\n\nThe issues seems is was from check_posdef but torch.matmul and @ are correctly working as intended\n\n\nFilter correct\n\npykalman.standard._filter_correct??\n\n\ntst = KalmanFilterTester(p_missing=0, nan_mask=False)\n\n\nobservation_matrix = tst.params_pyk['observation_matrices']\nobservation_covariance = tst.params_pyk['observation_covariance']\npredicted_state_mean = tst.params_pyk['initial_state_mean']\npredicted_state_covariance = tst.params_pyk['initial_state_covariance']\nobservation_offset = tst.params_pyk['observation_offsets']\nobservation = tst.data_pyk[0]\n\n\nobservation\n\n\nobs_matrix = tst.params['obs_matrices']\nobs_cov = tst.params['obs_cov']\npred_state_mean = tst.params['initial_state_mean']\npred_state_cov = tst.params['initial_state_cov']\nobs_offset = tst.params['obs_offsets']\nobs = tst.data[0]\n\nPred obs mean\n\npredicted_observation_mean = (\n    np.dot(observation_matrix,\n           predicted_state_mean)\n    + observation_offset\n)\n\n\npred_obs_mean = obs_matrix @ pred_state_mean + obs_offset\n\n\npred_obs_mean\n\n\ntest_close(pred_obs_mean, predicted_observation_mean)\n\nPred obs cov\n\npredicted_observation_covariance = (\n    np.dot(observation_matrix,\n           np.dot(predicted_state_covariance,\n                  observation_matrix.T))\n    + observation_covariance\n)\n\n\npred_obs_cov = obs_matrix @ pred_state_cov @ obs_matrix.T + obs_cov\n\n\npred_obs_cov\n\n\ntest_close(predicted_observation_covariance, pred_obs_cov)\n\nKalman gain\n\nkalman_gain = (\n    np.dot(predicted_state_covariance,\n           np.dot(observation_matrix.T,\n                  np.linalg.pinv(predicted_observation_covariance)))\n)\n\n\nkalman_gain_torch = pred_state_cov @ obs_matrix.T @ torch.inverse(pred_obs_cov)\n\n\nkalman_gain_torch\n\n\ntest_close(kalman_gain, kalman_gain_torch)\n\ncorr state mean\n\ncorrected_state_mean = (\n    predicted_state_mean\n    + np.dot(kalman_gain, observation - predicted_observation_mean))\n\n\ncorr_state_mean = pred_state_mean + kalman_gain_torch @ (obs - pred_obs_mean)\n\n\ncorr_state_mean\n\n\ntest_close(corrected_state_mean, corr_state_mean)\n\ncorr state cov\n\ncorrected_state_covariance = (\n    predicted_state_covariance\n    - np.dot(kalman_gain,\n             np.dot(observation_matrix,\n                    predicted_state_covariance))\n)\n\n\ncorr_state_cov = pred_state_cov - kalman_gain_torch @ obs_matrix @ pred_state_cov\n\n\ncorr_state_cov\n\n\ntest_close(corrected_state_covariance, corr_state_cov)\n\n\ndef print_info(xs, name=''):\n    for x in listify(xs):\n        print(f\"{name} - shape: {x.shape}, type {x.dtype}, mean {x.mean()}\")\n\n\ndef _filter_correct(obs_matrix,\n                    obs_cov,\n                    obs_offset,\n                    pred_state_mean,\n                    pred_state_cov,\n                    obs,\n                    mask,\n                    check_args=None):\n    if mask.all():\n        pred_obs_mean = obs_matrix @ pred_state_mean + obs_offset\n        pred_obs_cov = obs_matrix @ pred_state_cov @ obs_matrix.T + obs_cov\n        \n        kalman_gain = pred_state_cov @ obs_matrix.T @ torch.cholesky_inverse(torch.linalg.cholesky(pred_obs_cov))\n\n        corrected_state_mean = pred_state_mean + kalman_gain @ (obs - pred_obs_mean)\n        corrected_state_cov = pred_state_cov - kalman_gain @ obs_matrix @ pred_state_cov\n    else:\n        n_dim_state = pred_state_cov.shape[0]\n        n_dim_obs = obs_matrix.shape[0]\n        kalman_gain = torch.zeros((n_dim_state, n_dim_obs))\n\n        corrected_state_mean = pred_state_mean\n        corrected_state_cov = pred_state_cov\n        \n    if check_args is not None: check_posdef(pred_state_cov, 'filter_correct', **check_args)\n    return (kalman_gain, corrected_state_mean,\n            corrected_state_cov)\n\n\nnp.any(np.ma.getmask(observation))\n\n\ndef _pyk_filter_correct(observation_matrix, observation_covariance,\n                    observation_offset, predicted_state_mean,\n                    predicted_state_covariance, observation):\n    if not np.any(np.ma.getmask(observation)):\n        predicted_observation_mean = (\n            np.dot(observation_matrix,\n                   predicted_state_mean)\n            + observation_offset\n        )\n        print_info(predicted_observation_mean, 'pred_obs_mean')\n        predicted_observation_covariance = (\n            np.dot(observation_matrix,\n                   np.dot(predicted_state_covariance,\n                          observation_matrix.T))\n            + observation_covariance\n        )\n        print_info(predicted_observation_covariance, 'pred_obs_cov')\n\n        kalman_gain = (\n            np.dot(predicted_state_covariance,\n                   np.dot(observation_matrix.T,\n                          np.linalg.pinv(predicted_observation_covariance)))\n        )\n        print_info(kalman_gain, 'kalman_gain')\n\n        corrected_state_mean = (\n            predicted_state_mean\n            + np.dot(kalman_gain, observation - predicted_observation_mean)\n        )\n        print_info(corrected_state_mean, 'corr_state_mean')\n        corrected_state_covariance = (\n            predicted_state_covariance\n            - np.dot(kalman_gain,\n                     np.dot(observation_matrix,\n                            predicted_state_covariance))\n        )\n        print_info(corrected_state_covariance, 'corr_state_cov')\n    else:\n        n_dim_state = predicted_state_covariance.shape[0]\n        n_dim_obs = observation_matrix.shape[0]\n        kalman_gain = np.zeros((n_dim_state, n_dim_obs))\n\n        corrected_state_mean = predicted_state_mean\n        corrected_state_covariance = predicted_state_covariance\n\n    return (kalman_gain, corrected_state_mean,\n            corrected_state_covariance)\n\n\ntst = KalmanFilterTester() # need nan\n\n\n_filter_correct(\n   tst.params['obs_matrices'],\n   tst.params['obs_cov'],\n   tst.params['obs_offsets'],\n   tst.params['initial_state_mean'],\n   tst.params['initial_state_cov'],\n   tst.data[0],\n   tst.mask[0]\n)\n\n\ntst.data\n\n\ntst.mask\n\n\ntst.mask\n\n\npykalman.standard._filter_correct(\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0]\n    )\n\n\n_pyk_filter_correct(\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0]\n    )\n\n\npykalman.standard._filter_correct(\n   tst.params['obs_matrices'].numpy(),\n   tst.params['obs_cov'].numpy(),\n   tst.params['obs_offsets'].numpy(),\n   tst.params['initial_state_mean'].numpy(),\n   tst.params['initial_state_cov'].numpy(),\n   np.ma.masked_array(tst.data[0], mask=[True, True, True])\n)\n\n\n_pyk_filter_correct(\n   tst.params['obs_matrices'].numpy(),\n   tst.params['obs_cov'].numpy(),\n   tst.params['obs_offsets'].numpy(),\n   tst.params['initial_state_mean'].numpy(),\n   tst.params['initial_state_cov'].numpy(),\n   np.ma.masked_array(tst.data[0], mask=[True, True, True])\n)\n\n\n(pykalman.standard._filter_correct(\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0]\n    )[0] -\n    \n    _filter_correct(\n       tst.params['obs_matrices'],\n       tst.params['obs_cov'],\n       tst.params['obs_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n       tst.data[0],\n       tst.mask[0]\n    )[0].numpy())\n\n\nargs_np = {\n    'obs_m': np.eye(2),\n    'obs_cov': np.eye(2),\n    'obs_off': np.zeros((2,1)),\n    'pred_state_mean': np.ones(2),\n    'pred_state_cov': np.eye(2),\n    'obs': np.ones((2,1)),\n}\n\nargs_torch = {k: torch.tensor(v) for k,v in args_np.items()}\n\n\n_filter_correct(*args_torch.values(), mask=torch.ones_like(args_torch['obs'], dtype=torch.bool))\n\n\ntest_close(\n    pykalman.standard._filter_correct(*args_np.values()),\n    _filter_correct(*args_torch.values(), mask=torch.ones_like(args_torch['obs'], dtype=torch.bool)))\n\n\ntest_close(\n   pykalman.standard._filter_correct(\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0]\n    ),\n    \n    _filter_correct(\n       tst.params['obs_matrices'],\n       tst.params['obs_cov'],\n       tst.params['obs_offsets'],\n       tst.params['initial_state_mean'],\n       tst.params['initial_state_cov'],\n       tst.data[0],\n       tst.mask[0]\n    )\n)\n\n\nobs_matrix, obs_cov, obs_offset,pred_state_mean,pred_state_cov,obs, mask = k.obs_matrices, k.obs_cov, k.obs_offsets, k.init_state_mean, k.init_state_cov, data, mask\n\n\nmask.all(-1)\n\ntensor([[ True, False,  True, False, False,  True, False, False,  True, False],\n        [False, False,  True, False, False,  True, False, False,  True, False]])\n\n\n\nmask\n\ntensor([[[ True,  True,  True],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [False, False,  True],\n         [False,  True, False],\n         [ True,  True,  True],\n         [False,  True,  True],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [False, False, False]],\n\n        [[False,  True,  True],\n         [ True, False,  True],\n         [ True,  True,  True],\n         [False,  True,  True],\n         [False, False, False],\n         [ True,  True,  True],\n         [ True, False, False],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [ True,  True, False]]])\n\n\n\nobs\n\ntensor([[[0.9847, 0.0852, 0.5334],\n         [   nan, 0.2617, 0.7972],\n         [0.2088, 0.4545, 0.1455],\n         [   nan,    nan, 0.2881],\n         [   nan, 0.9087,    nan],\n         [0.5610, 0.9079, 0.2507],\n         [   nan, 0.7851, 0.0212],\n         [   nan, 0.6513, 0.3955],\n         [0.8111, 0.2558, 0.7570],\n         [   nan,    nan,    nan]],\n\n        [[   nan, 0.2511, 0.4720],\n         [0.6684,    nan, 0.1489],\n         [0.6714, 0.4719, 0.5053],\n         [   nan, 0.7793, 0.3246],\n         [   nan,    nan,    nan],\n         [0.8191, 0.7040, 0.3264],\n         [0.0842,    nan,    nan],\n         [   nan, 0.3308, 0.7610],\n         [0.3228, 0.0961, 0.3075],\n         [0.0947, 0.4745,    nan]]])\n\n\n\nobs[:, 0, :]\n\ntensor([[0.9847, 0.0852, 0.5334],\n        [   nan, 0.2511, 0.4720]])\n\n\n\n(obs[:, 0, :] - pred_obs_mean).unsqueeze(-1).shape\n\ntorch.Size([2, 3, 1])\n\n\n\nkalman_gain.shape\n\ntorch.Size([4, 3])\n\n\n\nkalman_gain @ (obs[:, 0, :] - pred_obs_mean).unsqueeze(-1)\n\ntensor([[[-0.1700],\n         [-0.1658],\n         [-0.7621],\n         [ 0.4827]],\n\n        [[    nan],\n         [    nan],\n         [    nan],\n         [    nan]]], dtype=torch.float64, grad_fn=<TransposeBackward0>)\n\n\n\npred_obs_mean = obs_matrix @ pred_state_mean + obs_offset\n\n\npred_obs_cov = obs_matrix @ pred_state_cov @ obs_matrix.T + obs_cov\n\n\npred_obs_mean, pred_obs_cov\n\n(tensor([1.1307, 1.4181, 0.4970], dtype=torch.float64, grad_fn=<AddBackward0>),\n tensor([[2.2690, 1.4801, 0.5773],\n         [1.4801, 1.3001, 0.8774],\n         [0.5773, 0.8774, 1.0938]], dtype=torch.float64, grad_fn=<AddBackward0>))\n\n\n\nobs_matrix.shape\n\ntorch.Size([3, 4])\n\n\n\n(pred_state_cov @ obs_matrix.T).shape\n\ntorch.Size([4, 3])\n\n\n\ntorch.cholesky_inverse(torch.linalg.cholesky(pred_state_cov)).shape\n\ntorch.Size([4, 4])\n\n\n\nr.shape\n\ntorch.Size([4, 3])\n\n\n\nkalman_gain = pred_state_cov @ obs_matrix.T @ torch.cholesky_inverse(torch.linalg.cholesky(pred_obs_cov))\n\n\nkalman_gain\n\ntensor([[ 0.0210,  0.1246, -0.0214],\n        [ 0.2535,  0.1004,  0.1394],\n        [ 0.3844,  0.5251, -0.1642],\n        [ 1.0030, -0.4705,  0.0583]], dtype=torch.float64,\n       grad_fn=<MmBackward0>)\n\n\n\ncorrected_state_mean = pred_state_mean + kalman_gain @ (obs - pred_obs_mean)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (6x10 and 3x4)\n\n\n\npred_obs_mean\n\ntensor([1.1307, 1.4181, 0.4970], dtype=torch.float64, grad_fn=<AddBackward0>)\n\n\n\nkalman_gain.shape\n\ntorch.Size([4, 3])\n\n\n\nkalman_gain @ (obs[0] - pred_obs_mean)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (4x3 and 10x3)\n\n\n\npred_state_mean\n\nParameter containing:\ntensor([0.8424, 0.0816, 0.8791, 0.3892], dtype=torch.float64,\n       requires_grad=True)\n\n\n\ncorrected_state_cov = pred_state_cov - kalman_gain @ obs_matrix @ pred_state_cov\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (6x10 and 3x4)\n\n\n\nk.obs_matrices @ k.init_state_mean\n\ntensor([0.6928, 0.9289, 0.2174], dtype=torch.float64, grad_fn=<MvBackward0>)\n\n\ntrying multiple random values …\n\nfor _ in range(100):\n    tst = KalmanFilterTester(nan_mask=False)\n    test_close(\n       pykalman.standard._filter_correct(\n           tst.params_pyk['observation_matrices'],\n           tst.params_pyk['observation_covariance'],\n           tst.params_pyk['observation_offsets'],\n           tst.params_pyk['initial_state_mean'],\n           tst.params_pyk['initial_state_covariance'],\n           tst.data_pyk[0]\n        ),\n\n        _filter_correct(\n           tst.params['obs_matrices'],\n           tst.params['obs_cov'],\n           tst.params['obs_offsets'],\n           tst.params['initial_state_mean'],\n           tst.params['initial_state_cov'],\n           tst.data[0],\n           tst.mask[0]\n        )\n    )\n\n\n\nFilter\n\ndef _filter(transition_matrices, obs_matrices, transition_cov,\n            obs_cov, transition_offsets, obs_offsets,\n            initial_state_mean, initial_state_cov, obs, obs_mask, check_args={}):\n    \"\"\"Apply the Kalman Filter\n\n    Calculate posterior distribution over hidden states given obss up\n    to and including the current time step.\n\n    Parameters\n    ----------\n    transition_matrices : [n_timesteps-1,n_dim_state,n_dim_state] or\n    [n_dim_state,n_dim_state] array-like\n        state transition matrices\n    obs_matrices : [n_timesteps, n_dim_obs, n_dim_state] or [n_dim_obs, \\\n    n_dim_state] array-like\n        obs matrix\n    transition_cov : [n_timesteps-1,n_dim_state,n_dim_state] or\n    [n_dim_state,n_dim_state] array-like\n        state transition cov matrix\n    obs_cov : [n_timesteps, n_dim_obs, n_dim_obs] or [n_dim_obs,\n    n_dim_obs] array-like\n        obs cov matrix\n    transition_offsets : [n_timesteps-1, n_dim_state] or [n_dim_state] \\\n    array-like\n        state offset\n    obs_offsets : [n_timesteps, n_dim_obs] or [n_dim_obs] array-like\n        obss for times [0...n_timesteps-1]\n    initial_state_mean : [n_dim_state] array-like\n        mean of initial state distribution\n    initial_state_cov : [n_dim_state, n_dim_state] array-like\n        cov of initial state distribution\n    obss : [n_timesteps, n_dim_obs] array\n        obss from times [0...n_timesteps-1].  If `obss` is a\n        masked array and any of `obss[t]` is masked, then\n        `obss[t]` will be treated as a missing obs.\n\n    Returns\n    -------\n    pred_state_means : [n_timesteps, n_dim_state] array\n        `pred_state_means[t]` = mean of hidden state at time t given\n        obss from times [0...t-1]\n    pred_state_covs : [n_timesteps, n_dim_state, n_dim_state] array\n        `pred_state_covs[t]` = cov of hidden state at time t\n        given obss from times [0...t-1]\n    kalman_gains : [n_timesteps, n_dim_state] array\n        `kalman_gains[t]` = Kalman gain matrix for time t\n    filt_state_means : [n_timesteps, n_dim_state] array\n        `filt_state_means[t]` = mean of hidden state at time t given\n        obss from times [0...t]\n    filt_state_covs : [n_timesteps, n_dim_state] array\n        `filt_state_covs[t]` = cov of hidden state at time t\n        given obss from times [0...t]\n    \"\"\"\n    n_timesteps = obs.shape[0]\n    n_dim_state = len(initial_state_mean)\n    n_dim_obs = obs.shape[1]\n    \n    # those variables need to be lists and not Tensors,\n    # otherwise pytorch tryies to compute the gradient for the whole tensor and it breaks due to the in place operations\n    \n    pred_state_means = [None for _ in range(n_timesteps)] # torch.zeros((n_timesteps, n_dim_state))\n    pred_state_covs = [None for _ in range(n_timesteps)] # torch.zeros(\n        #(n_timesteps, n_dim_state, n_dim_state)\n    #)\n    kalman_gains = [None for _ in range(n_timesteps)]\n    filt_state_means = [None for _ in range(n_timesteps)]\n    filt_state_covs = [None for _ in range(n_timesteps)]\n\n    for t in range(n_timesteps):\n        if t == 0:\n            pred_state_means[t] = initial_state_mean\n            pred_state_covs[t] = initial_state_cov\n        else:\n            transition_matrix = _last_dims(transition_matrices, t - 1)\n            transition_cov = _last_dims(transition_cov, t - 1)\n            transition_offset = _last_dims(transition_offsets, t - 1, ndims=1)\n            pred_state_means[t], pred_state_covs[t] = (\n                _filter_predict(\n                    transition_matrix,\n                    transition_cov,\n                    transition_offset,\n                    filt_state_means[t - 1],\n                    filt_state_covs[t - 1],\n                    check_args = {'t': t, **check_args} if check_args is not None else None\n                )\n            )\n\n        obs_matrix = _last_dims(obs_matrices, t)\n        obs_cov = _last_dims(obs_cov, t)\n        obs_offset = _last_dims(obs_offsets, t, ndims=1)\n        (kalman_gains[t], filt_state_means[t],\n         filt_state_covs[t]) = (\n            _filter_correct(obs_matrix,\n                obs_cov,\n                obs_offset,\n                pred_state_means[t],\n                pred_state_covs[t],\n                obs[t],\n                obs_mask[t],\n                check_args = {'t': t, **check_args} if check_args is not None else None\n            )\n        )\n\n    return (pred_state_means, pred_state_covs, filt_state_means,\n            filt_state_covs)\n\n\ndef test_filter():\n    pred_s, pred_s_cov, kal, filt_s, filt_s_cov =  pykalman.standard._filter(\n       tst.params_pyk['transition_matrices'],\n       tst.params_pyk['observation_matrices'],\n       tst.params_pyk['transition_covariance'],\n       tst.params_pyk['observation_covariance'],\n       tst.params_pyk['transition_offsets'],\n       tst.params_pyk['observation_offsets'],\n       tst.params_pyk['initial_state_mean'],\n       tst.params_pyk['initial_state_covariance'],\n       tst.data_pyk[0:1]\n    )\n    \n    pyk = (pred_s, pred_s_cov, filt_s, filt_s_cov,) # results without kalman gain\n    \n    filter_torch = tuple(map(\n        torch.vstack, # need to convert lists to tensor\n          _filter(\n           tst.params['transition_matrices'],\n           tst.params['obs_matrices'],\n           tst.params['transition_cov'],\n           tst.params['obs_cov'],\n           tst.params['transition_offsets'],\n           tst.params['obs_offsets'],\n           tst.params['initial_state_mean'],\n           tst.params['initial_state_cov'],\n           tst.data[0:1],\n           tst.mask[0:1]\n    )))\n    \n    test_close(pyk, filter_torch)\n\n\ntest_filter()\n\n\nOld testing\n\nargs_filt_np = {\n    'trans_m': np.eye(2),\n    'obs_m': np.eye(2),\n    'trans_cov': np.eye(2),\n    'obs_cov': np.eye(2),\n    'trans_off': np.zeros((2,1)),\n    'obs_off': np.zeros((3,2)),\n    'init_state_mean': np.ones(2),\n    'init_state_cov': np.eye(2),\n    'obs': np.ones((3, 2)),\n}\n\nobs_mask = np.ones(3)\n\nargs_filt_torch = {k: torch.tensor(v, dtype = torch.float32) for k,v in args_filt_np.items()}\n\n\n_filter(*args_filt_torch.values(), obs_mask=obs_mask)\n\n\nfilt_pyk = list(pykalman.standard._filter(*args_filt_np.values()))\ndel filt_pyk[2] # remove kalman gain that is not returned py _filter\n\nfilt =  list(map(lambda x: torch.stack(x), _filter(*args_filt_torch.values(), obs_mask=obs_mask)))\n\ntest_close( filt_pyk, filt)\n\nmissing data\n\nobs_mask = np.array([True, False, True])\nargs_filt_np['obs'] = np.ma.masked_array(args_filt_np['obs'], mask = np.vstack([~obs_mask] * 2).T)\n\n\nargs_filt_np['obs']\n\n\nfilt_pyk = list(pykalman.standard._filter(*args_filt_np.values()))\ndel filt_pyk[2] # remove kalman gain that is not returned py _filter\n\nfilt =  list(map(lambda x: torch.stack(x), _filter(*args_filt_torch.values(), obs_mask=torch.tensor(obs_mask))))\n\ntest_close( filt_pyk, filt)\n\n\n\n\nKalmanFilter method\n\n@patch\ndef _filter_all(self: KalmanFilter, obs, mask=None, check_args=None) -> Tuple:\n    obs, obs_mask = self._parse_obs(obs, mask)\n\n    return _filter(\n            self.transition_matrices,\n            self.obs_matrices,\n            self.transition_cov,\n            self.obs_cov,\n            self.transition_offsets,\n            self.obs_offsets,\n            self.initial_state_mean,\n            self.initial_state_cov,\n            obs,\n            obs_mask,\n            check_args\n        )\n\n@patch\ndef filter(self: KalmanFilter,\n          obs: Tensor, # [n_timesteps, n_dim_obs] obs for times [0...n_timesteps-1]\n          mask = None,\n          check_args=None\n          ) -> ListMNormal: # Filtered state\n    \"\"\"Filter observation\"\"\"\n    _, _, filt_state_means, filt_state_covs = self._filter_all(obs, mask, check_args)\n    # need to convert a list of tensors with gradients to a big tensors without gradients\n    return ListMNormal(_stack_detach(filt_state_means), _stack_detach(filt_state_covs))\n\n\n\nFinal Testing\ndue to numerical issue the error is bigger thatn 1e-5 which is default\n\n((tst.filter.filter(tst.data, tst.mask)[1] -\nnp.float32(tst.filter_pyk.filter(tst.data_pyk)[1]))).max()\n\n\ntst = KalmanFilterTester()\n\n\ntest_close(\n    tst.filter.filter(tst.data, tst.mask),\n    tst.filter_pyk.filter(tst.data_pyk), eps=4e-2) # need to increase the resolution\n\nusing float64 the error is smaller, which confirms that it is only a numerical issue\n\ntest_close(\n    tst64.filter.filter(tst.data, tst.mask),\n    tst64.filter_pyk.filter(tst.data_pyk))\n\n\n((tst64.filter.filter(tst64.data, tst64.mask)[1] -\ntst64.filter_pyk.filter(tst64.data_pyk)[1])).max()\n\n\nOld Testing\n\ntest_close(\n    pyk.filter(X.numpy()),\n    k.filter(X)\n)\n\n\ntest_close(\n    pyk2.filter(nX2),\n    k2.filter(X2)\n)\n\n\nk2.filter(X2m)\n\n\ntest_close(\n    pyk.filter(X.numpy()),\n    k.filter(X)\n)\n\n\ntest_close(\n    pyk2.filter(nX2),\n    k2.filter(X2)\n)\n\n\ntest_close(\n    pyk2.filter(nX2m),\n    k2.filter(X2m)\n)\n\n\n\n\n\nSmooth\n\nSmooth step\n\ndef _smooth_update(transition_matrix,      # [n_dim_state, n_dim_state]\n                   filt_state: Normal, # [n_dim_state] filtered state at time `t`\n                   pred_state: Normal,        # [n_dim_state] state before filtering at time `t + 1` (= using the observation until time t)\n                   next_smoothed_state: Normal, # [n_dim_state] smoothed state at time  `t+1`\n                   check_args: dict|None = None # if not None checks that the result is positive definite\n                   ) -> Normal: # mean and cov of smoothed state at time `t`\n    r\"\"\"Correct a pred state with a Kalman Smoother update\n\n    Calculates posterior distribution of the hidden state at time `t` given the the observations via Kalman Smoothing.\n    \"\"\"\n    kalman_smoothing_gain = filt_state.cov @ transition_matrix.T @ torch.cholesky_inverse(torch.linalg.cholesky(pred_state.cov))\n\n    smoothed_state_mean = filt_state.mean + kalman_smoothing_gain @ (next_smoothed_state.mean - pred_state.mean)\n    smoothed_state_cov = (filt_state.cov\n                      + kalman_smoothing_gain @ (next_smoothed_state.cov - pred_state.cov) @ kalman_smoothing_gain.T)\n\n    if check_args is not None: check_posdef(pred_state_cov, 'filter_correct', **check_args)\n    \n    return ListMNormal(smoothed_state_mean, smoothed_state_cov,)\n\n\nargs_np_sm = {\n    'pred_state_m': np.zeros(2),\n    'pred_state_cov': np.eye(2),\n    'filt_state_m': np.zeros(2),\n    'filt_state_cov': np.eye(2),\n    'next_state_m': np.zeros(2),\n    'next_state_cov': np.eye(2),\n    'trans_m': np.eye(2),\n}\n\nargs_torch_sm = {k: torch.tensor(v) for k,v in args_np_sm.items()}\n\n\npyk_mean, pyk_cov, _ = pykalman.standard._smooth_update(\n    args_np_sm['trans_m'],\n    args_np_sm['filt_state_m'],\n    args_np_sm['filt_state_cov'],\n    args_np_sm['pred_state_m'],\n    args_np_sm['pred_state_cov'],\n    args_np_sm['next_state_m'],\n    args_np_sm['next_state_cov'],\n)\n\n\ntorch_k = _smooth_update(\n    args_torch_sm['trans_m'],\n    Normal(args_torch_sm['filt_state_m'], args_torch_sm['filt_state_cov']),\n    Normal(args_torch_sm['pred_state_m'], args_torch_sm['pred_state_cov']),\n    Normal(args_torch_sm['next_state_m'], args_torch_sm['next_state_cov']),\n)\n\n\ntest_close((pyk_mean, pyk_cov, ), torch_k)\n\n\ndef _smooth(transition_matrices, # `[n_timesteps-1, n_dim_state, n_dim_state]` or `[n_dim_state, n_dim_state]`\n            filt_state: ListMNormal, # `[n_timesteps, n_dim_state]`\n                # `filt_state_means[t]` = mean state estimate for time t given obs from times `[0...t]`\n            pred_state: ListMNormal, # `[n_timesteps, n_dim_state]`\n                # `pred_state_means[t]` = mean state estimate for time t given obs from times `[0...t-1]`\n           check_args: dict|None = None # if not None checks that the result is positive definite\n           ) -> ListMNormal: # `[n_timesteps, n_dim_state]` Smoothed state \n    \"\"\"Apply the Kalman Smoother \"\"\"\n    n_timesteps, n_dim_state = len(pred_state.mean), pred_state.mean[0].shape[0]\n\n    smoothed_state = ListMNormal(torch.zeros((n_timesteps,n_dim_state), dtype=pred_state.mean[0].dtype, device=pred_state.mean[0].device), \n                                torch.zeros((n_timesteps, n_dim_state,\n                                           n_dim_state), dtype=pred_state.mean[0].dtype, device=pred_state.mean[0].device))\n\n    smoothed_state.mean[-1] = filt_state.mean[-1]\n    smoothed_state.cov[-1] = filt_state.cov[-1]\n\n    for t in reversed(range(n_timesteps - 1)):\n        transition_matrix = _last_dims(transition_matrices, t)\n        (smoothed_state.mean[t], smoothed_state.cov[t]) = (\n            _smooth_update(\n                transition_matrix,\n                filt_state.get_nth(t),\n                pred_state.get_nth(t + 1),\n                smoothed_state.get_nth(t+1),\n                check_args = {'t': t, **check_args} if check_args is not None else None\n            )\n        )\n    return smoothed_state\n\n\n(pred_state_means, pred_state_covs, filt_state_means, filt_state_covs ) = k2._filter_all(X2m)\n\n\ntorch_smooth = _smooth(k2.transition_matrices,  ListMNormal(filt_state_means, filt_state_covs), ListMNormal(pred_state_means, pred_state_covs))\n\n\npyk_sm_mean, pyk_sm_cov, _ = pykalman.standard._smooth(k2.transition_matrices.detach().numpy(),\n                          _stack_detach(filt_state_means).numpy(), _stack_detach(filt_state_covs).numpy(),\n                          _stack_detach(pred_state_means).numpy(), _stack_detach(pred_state_covs).numpy())\n\n\ntest_close((pyk_sm_mean, pyk_sm_cov,), torch_smooth)\n\n\ntorch_smooth\n\n\n\nKalmanFilter method\n\n@patch\ndef smooth(self: KalmanFilter,\n           obs: Tensor, # dataset\n           mask = None,\n           check_args=None\n          ) -> ListMNormal: # `[n_timesteps, n_dim_state]` smoothed hidden state distributions for times `[0...n_timesteps-1]`\n        \n    \"\"\"Kalman Filter Smoothing\"\"\"\n\n    (pred_state_means, pred_state_covs, filt_state_means, filt_state_covs) = self._filter_all(obs, mask, check_args)\n\n    return _smooth(\n            self.transition_matrices,\n            ListMNormal(filt_state_means, filt_state_covs),\n            ListMNormal(pred_state_means, pred_state_covs),\n            check_args\n        )\n\n\nk.smooth(X)\n\n\n\nFinal Testing\n\ntest_close(\n    tst.filter.smooth(tst.data, tst.mask),\n    tst.filter_pyk.smooth(tst.data_pyk), eps=4e-2) # need to increase the resolution\n\n\ntest_close(\n    tst64.filter.smooth(tst64.data, tst64.mask),\n    tst64.filter_pyk.smooth(tst64.data_pyk))\n\n\ntest_close(\n    pyk.smooth(nX),\n    k.smooth(X).detach()\n)\n\n\ntest_close(\n    pyk2.smooth(nX2),\n    k2.smooth(X2).detach()\n)\n\n\ntest_close(\n    pyk2.smooth(nX2m),\n    k2.smooth(X2m)\n)\n\n\n\n\nPredict\n\nfrom meteo_imp.gaussian import conditional_guassian\n\nIn order to have conditional predictions that make sense it’s not possible to return the full covariance matrix for the predictions but only the standard deviations\n\ndef _get_cond_pred(pred: ListMNormal,\n                  obs,\n                  mask\n                  ) -> ListNormal:\n    \"\"\"Conditional prediction given observations and transforms covariances into std deviations\"\"\"\n    \n    obs = obs[mask] # select only actually observed values\n    pred_cond = conditional_guassian(pred.mean, pred.cov, obs, mask)\n    \n    mean = pred.mean.clone()\n    mean[~mask] = pred_cond.mean\n    \n    std = torch.diagonal(pred.cov.clone(), dim1=-2, dim2=-1)\n    std[~mask] = torch.diagonal(pred_cond.cov, dim1=-2, dim2=-1)\n    \n    return ListMNormal(mean, std)\n\n\nobs = tst.data[1]\n\n\nmask = tst.mask[1]\n\n\nobs, mask\n\n\nconditional_guassian\n\nThe conditional gaussian returns a distributions of 2 variables, which are the un-observed ones (you can see the nan in the observation vector)\n\nconditional_guassian(tst.params['initial_state_mean'], tst.params['initial_state_cov'], obs[mask], mask)\n\nwhich are correctly merged with the predictions\n\n_get_cond_pred(ListMNormal(tst.params['initial_state_mean'], tst.params['initial_state_cov']), obs, mask)\n\n\n@patch\ndef _obs_from_state(self: KalmanFilter, state_mean, state_cov, check_args=None):\n\n    mean = self.obs_matrices @ state_mean\n    cov = self.obs_matrices @ state_cov @ self.obs_matrices.mT + self.obs_cov\n    \n    if check_args is not None: check_posdef(cov, 'predict',  **check_args)\n    \n    return ListMNormal(mean, cov)\n\n@patch\ndef predict(self: KalmanFilter, obs, mask=None, smooth=True, check_args=None):\n    \"\"\"Predicted observations at all times \"\"\"\n    state = self.smooth(obs, mask, check_args) if smooth else self.filter(obs, mask, check_args)\n    obs, mask = self._parse_obs(obs, mask)\n    \n    means = torch.empty_like(obs)\n    stds = torch.empty_like(obs)\n                             \n    for t in range(obs.shape[0]):\n        mean, std = self._obs_from_state(\n            state.mean[t],\n            state.cov[t],\n            {'t': t, **check_args} if check_args is not None else None\n        )\n        \n        means[t], stds[t] = _get_cond_pred(ListNormal(mean, std), obs[t], mask[t])\n    \n    return ListNormal(means, stds)\n\n\nk.predict(obs=X)\n\n\n@patch\ndef predict_times(self: KalmanFilter, times, obs, mask=None, smooth=True, check_args=None):\n    \"\"\"Predicted observations at specific times \"\"\"\n    state = self.smooth(obs, mask, check_args) if smooth else self.filter(obs, mask, check_args)\n    obs, mask = self._parse_obs(obs, mask)\n    times = array1d(times)\n    \n    n_timesteps = obs.shape[0]\n    n_features = obs.shape[1] if len(obs.shape) > 1 else 1\n    \n    if times.max() > n_timesteps or times.min() < 0:\n        raise ValueError(f\"provided times range from {times.min()} to {times.max()}, which is outside allowed range : 0 to {n_timesteps}\")\n\n    means = torch.empty((times.shape[0], n_features), dtype=obs.dtype, device=obs.device)\n    stds = torch.empty((times.shape[0], n_features), dtype=obs.dtype, device=obs.device) \n    for i, t in enumerate(times):\n        mean, std = self._obs_from_state(\n            state.mean[t],\n            state.cov[t],\n            {'t': t, **check_args} if check_args is not None else None\n        )\n        \n        means[i], stds[i] = _get_cond_pred(ListNormal(mean, std), obs[t], mask[t])\n    \n    return ListNormal(means, stds)\n\npykalman doesn’t support a predict method so cannot test it\n\ntst.filter.predict(obs = tst.data, mask = tst.mask)\n\n\nmean, std = tst.filter.predict_times(obs = tst.data, times = torch.tensor([0,1]), mask = tst.mask)\n\n\nmean, std\n\n\nprint_info((mean, std))\n\n\nmean, cov = tst64.filter.smooth(tst64.data, tst64.mask)\n\n\ntst64.filter._obs_from_state(mean[0], cov[0])\n\n\n\nLog Likelihood\nThis code is old now as the log likelihood is not computed here\nTODO: open issue in pykalman for error in ll missing data\n\npykalman.standard.KalmanFilter.loglikelihood??\n\n\npykalman.standard._loglikelihoods??\n\n\n@patch\ndef filter_loglikelihood(self: KalmanFilter, obs, mask=None):\n    \"Compute log likelihood using only filter step\"\n    # Those are the means and covs before the updating step,\n    # otherwise the model would have already seen the observation that we are predicting \n    pred_state_mean, pred_state_cov, _, _ = self._filter_all(obs, mask)\n    obs, obs_mask = self._parse_obs(obs, mask)\n\n    max_t = obs.shape[0]\n    lls = torch.zeros(max_t)\n    for t in range(max_t):\n        if obs_mask[t].all():\n            pred_obs_mean, pred_obs_cov = self._obs_from_state(pred_state_mean[t], pred_state_cov[t])\n            ll = MultivariateNormal(pred_obs_mean, pred_obs_cov, validate_args=False).log_prob(obs[t])\n            lls[t] = ll\n\n    return lls.sum()\n\n\nk.filter_loglikelihood(X)\n\n\ntest_close(k.filter_loglikelihood(X), pyk.loglikelihood(nX))\n\n\npred_state, pred_state_cov, _, _ = tuple(map(_stack_detach, tst.filter._filter_all(tst.data, tst.mask)))\n\n\ntst = KalmanFilterTester(nan_mask = False, p_missing=0)\n\n\npykalman.standard._loglikelihoods(\n    tst.params_pyk['observation_matrices'],\n    tst.params_pyk['observation_offsets'],\n    tst.params_pyk['observation_covariance'],\n    pred_state.numpy(),\n    pred_state_cov.numpy(),\n    np.array(tst.data_pyk)\n)\n\n\ntest_close(\n    tst.filter.filter_loglikelihood(tst.data, tst.mask),\n    tst.filter_pyk.loglikelihood(tst.data_pyk)) # need to increase the resolution\n\n\ntst64 = KalmanFilterTester(p_missing=0, dtype=torch.float64)\n\n\n(tst64.filter.filter_loglikelihood(tst64.data, tst64.mask),\ntst64.filter_pyk.loglikelihood(tst64.data_pyk))\n\n\nk2.filter_loglikelihood(X2)\n\n\ntest_close(k2.filter_loglikelihood(X2), pyk2.loglikelihood(nX2), eps=1e-4)\n\nsince the goal is to fill gaps we want the log likelihood for the whole gap and only for it\n\n@patch\ndef loglikelihood(self: KalmanFilter,\n                  obs_train: Tensor, # [n_timesteps, n_dim_obs] Observations use for the filter (can containt missing data)\n                  times: Tensor, # [n_pred_timesteps] time at which to calculate the log likelihood\n                  obs_test: Tensor, # [n_pred_timesteps, n_dim_obs] observed data to compute log likelihood\n                  mask: Tensor=None, # [n_timesteps, n_dim_obs]\n                 ) -> Tensor: # scalar that is sum of log likelihoods for all `times`\n    \"Log likelihood only for the `obs_test` at giben times\"\n    means, stds = self.predict(obs_train, mask=mask)\n    lls = torch.zeros(len(times))\n    for t in range(len(times)):\n        lls[t] = MultivariateNormal(means[t], torch.diag(stds[t]), validate_args=False).log_prob(obs_test[t:t+1])\n    return lls.sum()\n\n\ntorch.diag(std[0]).dtype\n\n\ntst.filter.loglikelihood(tst.data, tst.mask, tst.data)\n\n\nX2.dtype\n\n\nk.loglikelihood(X, [1,2], X[[1,2]])\n\n\nk2.loglikelihood(X2, [1,2], X2[[1,2]])\n\n\nk2.loglikelihood(X2m, [1,2], X2[[1,2]])\n\n\n\nGet Info\n\n@patch\ndef get_info(self: KalmanFilter, var_names=None):\n    out = {}\n    if var_names is not None: self.var_names = var_names \n    latent_names = [f\"z_{i}\" for i in range(self.transition_matrices.shape[0])]\n    out['A'] = array2df(self.transition_matrices, latent_names, latent_names, 'latent')\n    out['H'] = array2df(self.obs_matrices,        var_names,    latent_names, 'variable')\n    out['R'] = array2df(self.obs_cov,             var_names,    var_names,     'variable')\n    out['Q'] = array2df(self.transition_cov,      latent_names, latent_names, 'latent')\n    return out\n\n\ndisplay_as_row(k.get_info())\n\n\ndisplay_as_row(k2.get_info())"
  },
  {
    "objectID": "kalman/test_filter_pykalman.html#train-parameters",
    "href": "kalman/test_filter_pykalman.html#train-parameters",
    "title": "Compare PyKalman",
    "section": "Train Parameters",
    "text": "Train Parameters\nThis implementation of KalmanFilter allows to find the optimal parameters by maximising the log-likelihood using gradient descend\n\ntraining_iter = 200\nk = KalmanFilter()\nk.train()\n\noptimizer = torch.optim.Adam(k.parameters(), lr=0.005) \n\nlosses = []\nfor i in range(training_iter):\n    # Zero gradients from previous iteration\n    optimizer.zero_grad()\n    # Output from model\n    loss = - k.loglikelihood(X, range(len(X)), X)\n    losses.append(loss.item())\n    # backpropagate gradients\n    loss.backward()\n    optimizer.step()\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot(losses)\n\n\nlist(k.parameters())"
  },
  {
    "objectID": "kalman/test_filter_pykalman.html#other",
    "href": "kalman/test_filter_pykalman.html#other",
    "title": "Compare PyKalman",
    "section": "Other",
    "text": "Other\n\nTesting\n\n\nFuzzing smoother\ntrying to run the filter many times to see if some of the matrix are not symmetric\n\nfrom torch._C import _LinAlgError\n\n\ndef fuzz_symmetric(n_iter=10, n_obs=100, **kwargs):\n    tst = KalmanFilterTester(n_obs=n_obs, **kwargs)\n    _, sm_covs = tst.filter.smooth(tst.data, tst.mask)\n    i_posdef = []\n    for t, cov in enumerate(sm_covs):\n        i_posdef.append(check_posdef(cov))\n    return pd.concat(i_posdef)\n\n\ndef fuzz_smooth(n_obs=100, use_pykalman=False, **kwargs):\n    tst = KalmanFilterTester(n_obs=n_obs, **kwargs)\n    return tst.filter.smooth(tst.data, tst.mask) if not use_pykalman else tst.filter_pyk.smooth(tst.data_pyk)\n\n\ndef find_max_obs(start=100_000, end=100, steps=10, **kwargs):\n    for n in torch.logspace(torch.log10(torch.tensor(start)), torch.log10(torch.tensor(end)), 10):\n        try:\n\n            print(n, \"working\")\n            break\n        except _LinAlgError:\n            print(n, \"not working\")\n\n\nPyKalman\n\nsm_mean, sm_cov = fuzz_smooth(use_pykalman=True)\n\n\ntst.data_pyk.dtype\n\ndtype('float32')\n\n\n\ntst.filter_pyk.transition_matrices\n\narray([[0.9299797 , 0.29107332, 0.8075413 ],\n       [0.09118336, 0.87263364, 0.37048805],\n       [0.10431129, 0.9359443 , 0.9556711 ]], dtype=float32)\n\n\n\nsm_mean.dtye\n\ndtype('float64')\n\n\n\nsm_cov.shape\n\n(100, 3, 3)\n\n\n\npyk_pd = CheckPosDef(do_check=True).check(torch.tensor(sm_cov))\n\npyk_pd[['is_pd_eigv', 'is_pd_chol', 'is_sym']].all()\n\nis_pd_eigv    True\nis_pd_chol    True\nis_sym        True\ndtype: bool\n\n\n\nsm_mean, sm_cov = fuzz_smooth(use_pykalman=False)\n\n\ntst.filter\n\nKalman Filter\n        N dim obs: 3, N dim state: 3\n\n\n\nsm_cov.shape\n\ntorch.Size([1, 100, 3, 3])\n\n\n\npyk_pd = CheckPosDef(do_check=True).check(sm_cov.squeeze(0))\n\npyk_pd[['is_pd_eigv', 'is_pd_chol', 'is_sym']].all()\n\n_LinAlgError: linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 1).\n\n\n\nfind_max_obs(use_pykalman=True)\n\nCPU times: user 15.7 s, sys: 61.5 ms, total: 15.8 s\nWall time: 14.9 s\ntensor(100000.) working\n\n\n\nimport warnings\n\n\nposdef_log = pd.DataFrame()\n\n\ntotal_warn = []\nfor n in range(2, 200):\n    with warnings.catch_warnings(record=True) as w:\n        try:\n            fuzz_smooth(n_obs=n)\n        except _LinAlgError:\n            print(n)\n            break\n        finally:\n            total_warn.append((n, len(w)))\n\n\nposdef_log\n\n\nimport altair as alt\n\n\nalt.Chart(pd.DataFrame(total_warn, columns=[\"n_obs\", \"n_not_posdef\"])).mark_line().encode(alt.X(\"n_obs\"), alt.Y(\"n_not_posdef\"))\n\n\nalt.data_transformers.enable('data_server')\n\n\nalt.Chart(posdef_log).mark_line().encode(alt.X(\"t\"), alt.Y(\"average(sym_upto)\"))\n\n\nalt.Chart(posdef_log).mark_point().encode(alt.X(\"t\"), alt.Y(\"count(is_sym)\"))\n\n\nposdef_log[[\"t\", \"name\"]]\n\n\nplt.scatter(posdef_log.reset_index().index, posdef_log.sym_upto)\n\n\nfor i in range(3):\n\n\n# find_max_obs(dtype=torch.float64)\n\nThe function takes 5 min to run so this is the output saved\nwith float64 there is no problem with positive definite matrices even with 100k observations CPU times: user 30min 43s, sys: 28.6 s, total: 31min 11s Wall time: 5min 26s tensor(100000.) working\n\ntst = KalmanFilterTester(n_obs=100)\n\n\nis_posdef(tst.params['obs_cov'])\n\n\nis_posdef(tst.params['transition_cov'])\n\n\nis_posdef(tst.params['initial_state_cov'])\n\n\ntst.filter.smooth(tst.data, tst.mask);\n\n\n\nRandom Testing\nThe goal is to generate random set of data and parameters and check that meteo_imp implementation is the same of pykalman implementation\n\nn_dim_state = 3 \nn_dim_obs = 3\nn_obs = 10\np_missing = .3\n\n\nto_posdef = PosDef().transform\n\n\ndata = torch.rand(n_obs, n_dim_obs)\nmask = torch.rand(n_obs, n_dim_obs) > p_missing\nmask = mask.all(1)\n\n\nmask[:10]\n\n\nmask[:10]\n\n\nparams = {\n    'transition_matrices': torch.rand(n_dim_state, n_dim_state),\n    'transition_offsets':  torch.rand(n_dim_state),        \n    'transition_cov':      to_posdef(torch.rand(n_dim_state, n_dim_state)),        \n    'obs_matrices':        torch.rand(n_dim_obs, n_dim_state),\n    'obs_offsets':         torch.rand(n_dim_obs),          \n    'obs_cov':             to_posdef(torch.rand(n_dim_obs, n_dim_obs)),            \n    'initial_state_mean':  torch.rand(n_dim_state),        \n    'initial_state_cov':   to_posdef(torch.rand(n_dim_state, n_dim_state)),\n}\n\n\nparams2pyk = {\n    'transition_matrices': 'transition_matrices',\n    'transition_offsets':  'transition_offsets',        \n    'transition_cov':      'transition_covariance',        \n    'obs_matrices':        'observation_matrices',\n    'obs_offsets':         'observation_offsets',          \n    'obs_cov':             'observation_covariance',            \n    'initial_state_mean':  'initial_state_mean',        \n    'initial_state_cov':   'initial_state_covariance',\n}\n\n\nparams\n\n\nk = KalmanFilter(**params)\n\n\npred = k.smooth(data, mask)\n\nmake a pykalman model using the same parameters\n\ndata_pyk = np.ma.masked_array(data.numpy(), mask = mask.numpy())\n\n\npyk_k = pykalman.standard.KalmanFilter(\n\n    transition_matrices=k.transition_matrices.detach().numpy(),\n    transition_offsets=k.transition_offsets.detach().numpy(),\n    transition_covariance=k.transition_cov.detach().numpy(),\n    observation_matrices=k.obs_matrices.detach().numpy(),\n    observation_offsets=k.obs_offsets.detach().numpy(),\n    observation_covariance=k.obs_cov.detach().numpy(),\n    initial_state_mean=k.initial_state_mean.detach().numpy(),\n    initial_state_covariance=k.initial_state_cov.detach().numpy()\n)\n\n\npred_pyk = pyk_k.smooth(data_pyk)\n\n\nfor p in params.keys():\n    print(p, getattr(k, p))\n\n\nfor p in params.keys():\n    print(p, getattr(pyk_k, params2pyk[p]))\n\n\n\n\nCompare Statsmodels\n\nimport statsmodels.api as sm\nimport statsmodels\n\n\n# sm_kf = statsmodels.tsa.statespace.kalman_filter.KalmanFilter(\n#     k_endog = 3,\n#     k_states = 3,\n#     initialization = 'known',\n#     initial_state = pyk_ncov.initial_state_mean,\n#     initial_state_cov = pyk_ncov.initial_state_covariance,\n#     design = pyk_ncov.observation_matrices,\n#     obs_cov = pyk_ncov.observation_covariance,\n#     transition = pyk_ncov.transition_matrices,\n#     state_cov = pyk_ncov.transition_covariance)\n\n\n# sm_kf.bind(X_ncov.detach().numpy())\n\n\n# sm_pred = sm_kf.filter()\n\n\n# sm_pred.predicted_state.shape\n\n\n# sm_pred.predicted_state_cov.shape\n\n\n# mean = MultivariateNormal(torch.tensor(sm_pred.predicted_state[:, 0]), torch.tensor(sm_pred.predicted_state_cov[:, :, 0]))\n\n\n# sm_kf.loglikeobs()"
  },
  {
    "objectID": "kalman/old_02_pykalman_imputation.html",
    "href": "kalman/old_02_pykalman_imputation.html",
    "title": "[OLD] Imputation PyKalman Model",
    "section": "",
    "text": "@patch\ndef inverse_transform_std(self: sklearn.preprocessing.StandardScaler, \n                         x_std # standard deviations\n                        ):\n    return x_std * self.scale_"
  },
  {
    "objectID": "kalman/old_02_pykalman_imputation.html#imputation",
    "href": "kalman/old_02_pykalman_imputation.html#imputation",
    "title": "[OLD] Imputation PyKalman Model",
    "section": "Imputation",
    "text": "Imputation\n\nx = np.stack([np.eye(3)*i for i in  range(1,4)])\n\n\nx\n\narray([[[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]],\n\n       [[2., 0., 0.],\n        [0., 2., 0.],\n        [0., 0., 2.]],\n\n       [[3., 0., 0.],\n        [0., 3., 0.],\n        [0., 0., 3.]]])\n\n\n\nnp.diagonal(x, axis1=1, axis2=2)\n\narray([[1., 1., 1.],\n       [2., 2., 2.],\n       [3., 3., 3.]])\n\n\n\nsource\n\nKalmanImputation\n\n KalmanImputation (data:pandas.core.frame.DataFrame,\n                   model:meteo_imp.old.kalman.model.KalmanModel,\n                   pred_all:bool=False)\n\nImputation using a kalman model\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\n\n\n\nmodel\nKalmanModel\n\na subclass of MLEModel tto be used as model\n\n\npred_all\nbool\nFalse\nIf the dataset should be replaced by the model predictions\n\n\n\n\nfrom meteo_imp.data_preparation import MeteoDataTest\n\n\nreset_seed(1)\ndata = MeteoDataTest.generate_gpfa(2, 5).add_random_missing()\n\n\ndata.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      0.023263\n      NaN\n    \n    \n      1\n      0.219627\n      0.268028\n    \n    \n      2\n      -0.039892\n      0.063075\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      -0.645490\n      -0.144866\n    \n  \n\n\n\n\n\nk_imp = KalmanImputation(data.data, LocalLevelModel)\n\n\nk_imp.fit()\n\n<__main__.KalmanImputation>\n\n\n\nk_imp.impute()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0\n      x0\n      -0.117147\n      0.394932\n    \n    \n      1\n      1\n      x0\n      0.219627\n      0.000000\n    \n    \n      2\n      2\n      x0\n      -0.039892\n      0.000000\n    \n    \n      3\n      3\n      x0\n      -0.170874\n      0.395551\n    \n    \n      4\n      4\n      x0\n      -0.645490\n      0.000000\n    \n    \n      5\n      0\n      x1\n      0.062976\n      0.206893\n    \n    \n      6\n      1\n      x1\n      0.268028\n      0.000000\n    \n    \n      7\n      2\n      x1\n      0.063075\n      0.000000\n    \n    \n      8\n      3\n      x1\n      0.062976\n      0.206893\n    \n    \n      9\n      4\n      x1\n      -0.144866\n      0.000000\n    \n  \n\n\n\n\n\nk_imp.impute(pred_all=True)\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0\n      x0\n      -0.117147\n      0.394932\n    \n    \n      1\n      1\n      x0\n      -0.118482\n      0.383000\n    \n    \n      2\n      2\n      x0\n      -0.100529\n      0.384614\n    \n    \n      3\n      3\n      x0\n      -0.170874\n      0.395551\n    \n    \n      4\n      4\n      x0\n      -0.241218\n      0.389215\n    \n    \n      5\n      0\n      x1\n      0.062976\n      0.206893\n    \n    \n      6\n      1\n      x1\n      0.062976\n      0.206893\n    \n    \n      7\n      2\n      x1\n      0.062976\n      0.206893\n    \n    \n      8\n      3\n      x1\n      0.062976\n      0.206893\n    \n    \n      9\n      4\n      x1\n      0.062976\n      0.206893\n    \n  \n\n\n\n\n\nResult\n\nsource\n\n\n\nKalmanImputation.to_result\n\n KalmanImputation.to_result (data_compl, var_names=None, units=None,\n                             pred_all=False)\n\n\nX = np.hstack([np.arange(0,3.), np.arange(3., 0, -1)]).reshape(6, 1)\n\n\nres = k_imp.to_result(data.data_compl_tidy)\n\n\nres.display_results()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/sklearn/metrics/_regression.py:796: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n\n\n\nMetrics  r2 \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      x0\n      0.6300\n    \n    \n      x1\n      0.6336\n    \n  \n\n  RMSE \n\n  \n    \n      variable\n      rmse\n    \n  \n  \n    \n      x0\n      0.1959\n    \n    \n      x1\n      0.0975\n    \n  \n\n  r2 - Only GAP \n\n  \n    \n      variable\n      r2\n    \n  \n  \n    \n      x0\n      nan\n    \n    \n      x1\n      -0.1820\n    \n  \n\n  RMSE - Only GAP \n\n  \n    \n      variable\n      rmse\n    \n  \n  \n    \n      x0\n      0.4150\n    \n    \n      x1\n      0.1541\n    \n  \n\n \n\n\n\nModel Info  A \n\n  \n    \n      latent\n      z_0\n      z_1\n    \n  \n  \n    \n      z_0\n      1.0000\n      0.0000\n    \n    \n      z_1\n      0.0000\n      1.0000\n    \n  \n\n  H \n\n  \n    \n      z_0\n      z_1\n    \n  \n  \n    \n      1.0000\n      0.0000\n    \n    \n      0.0000\n      1.0000\n    \n  \n\n  R \n\n  \n    \n      0\n      1\n    \n  \n  \n    \n      1.1428\n      1.1225\n    \n    \n      1.1225\n      1.2565\n    \n  \n\n  Q \n\n  \n    \n      latent\n      z_0\n      z_1\n    \n  \n  \n    \n      z_0\n      0.2046\n      -0.0000\n    \n    \n      z_1\n      -0.0000\n      -0.0000"
  },
  {
    "objectID": "kalman/old_02_pykalman_imputation.html#export",
    "href": "kalman/old_02_pykalman_imputation.html#export",
    "title": "[OLD] Imputation PyKalman Model",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/learner.html",
    "href": "kalman/learner.html",
    "title": "Kalman Filter Models",
    "section": "",
    "text": "[TODO] add proper introduction here\nThe models uses a latent state variable \\(x\\) that is modelled over time, to impute gaps in \\(y\\)\n\n\nThe equations of the model are:\n\\[\\begin{align} p(x_t | x_{t-1}) & = \\mathcal{N}(x_t, Ax_{t-1}, Q) \\\\\np(y_t | x_t) & = \\mathcal{N}(Hx_t, T) \\end{align}\\]\nThe Kalman filter has 3 steps:\n\nfilter (updating the state at time t with observations till time t-1)\nupdate (update the state at time t using the observation at time t)\nsmooth (update the state using the observations at time t+1)\n\nIn case of missing data the update step is skipped.\nAfter smoothing the whole dataset the missing data (\\(y_t\\)) can be imputed from the state (\\(x_t\\)) using this formula: \\[p(y_t) = \\mathcal{N}(Hx_x, R + HP^s_tH)\\]"
  },
  {
    "objectID": "kalman/learner.html#kalman-filter-model",
    "href": "kalman/learner.html#kalman-filter-model",
    "title": "Kalman Filter Models",
    "section": "Kalman Filter Model",
    "text": "Kalman Filter Model\n\nsource\n\nKalmanModel\n\n KalmanModel (data:torch.Tensor, **kwargs)\n\nKalman Model wtih max likelihood and gradient descend to optimize paramters and support for missing observations\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nTensor\narray of observations containg NaN for missing obs\n\n\nkwargs\n\n\n\n\n\n\nX = torch.arange(6).unsqueeze(0).T\n\n\nX\n\ntensor([[0],\n        [1],\n        [2],\n        [3],\n        [4],\n        [5]])\n\n\n\nX.shape\n\ntorch.Size([6, 1])\n\n\n\nk = KalmanModel(X)\n\n\nk.train(None, None, n_iter = 10)\n\n\n\n\n<__main__.KalmanModel>\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot(k.losses)\n\n\n\n\n\nT = torch.arange(1,X.shape[0])\n\n\nT\n\ntensor([1, 2, 3, 4, 5])\n\n\n\nk.predict(T)\n\nListNormal(mean=tensor([[1.0892],\n        [1.9796],\n        [2.9386],\n        [4.0071],\n        [5.2515]], grad_fn=<CopySlices>), cov=tensor([[[0.5605]],\n\n        [[0.5763]],\n\n        [[0.5945]],\n\n        [[0.6328]],\n\n        [[0.7286]]], grad_fn=<CopySlices>))\n\n\n\n\nState\n\nsource\n\n\nKalmanModel.plot_state\n\n KalmanModel.plot_state (n_cols=2, bind_interaction=True, properties={})\n\n\n# k.plot_state()\n\n\n\nPlot Loss\n\nsource\n\n\nKalmanModel.plot_loss\n\n KalmanModel.plot_loss (size={'width': 250, 'height': 120})\n\n\nk.plot_loss()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():"
  },
  {
    "objectID": "kalman/learner.html#export",
    "href": "kalman/learner.html#export",
    "title": "Kalman Filter Models",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html",
    "href": "kalman/old_01_pykalman_models.html",
    "title": "[OLD] PyKalman Filter Models",
    "section": "",
    "text": "[TODO] add proper introduction here\nThe models uses a latent state variable \\(x\\) that is modelled over time, to impute gaps in \\(y\\)\n\n\nThe equations of the model are:\n\\[\\begin{align} p(x_t | x_{t-1}) & = \\mathcal{N}(x_t, Ax_{t-1}, Q) \\\\\np(y_t | x_t) & = \\mathcal{N}(Hx_t, T) \\end{align}\\]\nThe Kalman filter has 3 steps:\n\nfilter (updating the state at time t with observations till time t-1)\nupdate (update the state at time t using the observation at time t)\nsmooth (update the state using the observations at time t+1)\n\nIn case of missing data the update step is skipped.\nAfter smoothing the whole dataset the missing data (\\(y_t\\)) can be imputed from the state (\\(x_t\\)) using this formula: \\[p(y_t) = \\mathcal{N}(Hx_x, R + HP^s_tH)\\]"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html#kalman-filter-model",
    "href": "kalman/old_01_pykalman_models.html#kalman-filter-model",
    "title": "[OLD] PyKalman Filter Models",
    "section": "Kalman Filter Model",
    "text": "Kalman Filter Model\ngeneral kalman filter model implemented used pykalman\n\nPyKalman\nexamples from pykalman lib\n\nkf = pykalman.KalmanFilter(transition_matrices = [[1, 1], [0, 1]], observation_matrices = [[0.1, 0.5], [-0.3, 0.0]])\nmeasurements = np.asarray([[1,0], [0,0], [0,1]])\n(smoothed_state_means, smoothed_state_covariances) = kf.smooth(measurements)\n\n\nkf.em(measurements)\n\n<pykalman.standard.KalmanFilter>\n\n\n\nsmoothed_state_means\n\narray([[-0.10923868,  0.0935127 ],\n       [-0.23121289, -0.07957144],\n       [-0.5533711 , -0.0415223 ]])\n\n\n\nsmoothed_state_covariances\n\narray([[[ 0.83148067, -0.12300405],\n        [-0.12300405,  0.53081415]],\n\n       [[ 1.60960449,  0.01009906],\n        [ 0.01009906,  0.80412661]],\n\n       [[ 2.87663094,  0.45474213],\n        [ 0.45474213,  1.27365905]]])\n\n\n\nsmoothed_state_covariances.shape\n\n(3, 2, 2)\n\n\nsupport is not limited for models matrices that don’t change over time\n\nsource\n\n\nKalmanModel\n\n KalmanModel (data:numpy.ma.core.MaskedArray, transition_matrices=None,\n              observation_matrices=None, transition_covariance=None,\n              observation_covariance=None, transition_offsets=None,\n              observation_offsets=None, initial_state_mean=None,\n              initial_state_covariance=None, random_state=None,\n              em_vars=['transition_covariance', 'observation_covariance',\n              'initial_state_mean', 'initial_state_covariance'],\n              n_dim_state=None, n_dim_obs=None)\n\nBase Model for Kalman filter that wraps pykalman.KalmanFilter. Doesn’t support parameters that change over time\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nMaskedArray\n\nnumpy array of observations\n\n\ntransition_matrices\nNoneType\nNone\n\n\n\nobservation_matrices\nNoneType\nNone\n\n\n\ntransition_covariance\nNoneType\nNone\n\n\n\nobservation_covariance\nNoneType\nNone\n\n\n\ntransition_offsets\nNoneType\nNone\n\n\n\nobservation_offsets\nNoneType\nNone\n\n\n\ninitial_state_mean\nNoneType\nNone\n\n\n\ninitial_state_covariance\nNoneType\nNone\n\n\n\nrandom_state\nNoneType\nNone\n\n\n\nem_vars\nlist\n[‘transition_covariance’, ‘observation_covariance’, ‘initial_state_mean’, ‘initial_state_covariance’]\n\n\n\nn_dim_state\nNoneType\nNone\n\n\n\nn_dim_obs\nNoneType\nNone\n\n\n\n\n\nX = np.hstack([np.arange(0,3.), np.arange(3., 0, -1)]).reshape(6, 1)\n\n\nX\n\narray([[0.],\n       [1.],\n       [2.],\n       [3.],\n       [2.],\n       [1.]])\n\n\n\nX.shape\n\n(6, 1)\n\n\n\nk = KalmanModel(X)\n\n\nk.fit(10)\n\n<__main__.KalmanModel>\n\n\n\nT = np.arange(0,X.shape[0])\n\n\nT\n\narray([0, 1, 2, 3, 4, 5])\n\n\n\nk.predict(T)\n\nListNormal(mean=array([[0.21296793],\n       [1.01950281],\n       [1.98157689],\n       [2.79672264],\n       [1.99068721],\n       [1.1103803 ]]), cov=array([[[0.12474143]],\n\n       [[0.19659773]],\n\n       [[0.19733477]],\n\n       [[0.19734326]],\n\n       [[0.19743451]],\n\n       [[0.20632342]]]))\n\n\n\nGet Info\n\nsource\n\n\n\nKalmanModel.get_info\n\n KalmanModel.get_info (var_names=None)\n\n\ndisplay_as_row(k.get_info())\n\n\n  A \n\n  \n    \n      latent\n      z_0\n    \n  \n  \n    \n      z_0\n      1.0000\n    \n  \n\n  H \n\n  \n    \n      z_0\n    \n  \n  \n    \n      1.0000\n    \n  \n\n  R \n\n  \n    \n      0\n    \n  \n  \n    \n      0.1087\n    \n  \n\n  Q \n\n  \n    \n      latent\n      z_0\n    \n  \n  \n    \n      z_0\n      0.8666\n    \n  \n\n \n\n\nshit numpy arrays…\n\n_shift_1d(np.stack([np.vstack([np.arange(1, 4.)*i]*2) for i in range(4)]),2)\n\narray([[[nan, nan, nan],\n        [nan, nan, nan]],\n\n       [[nan, nan, nan],\n        [nan, nan, nan]],\n\n       [[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]],\n\n       [[ 1.,  2.,  3.],\n        [ 1.,  2.,  3.]]])\n\n\n\n_shift_1d(np.vstack([np.arange(1, 4.)*i for i in range(4)]),-1)\n\narray([[ 1.,  2.,  3.],\n       [ 2.,  4.,  6.],\n       [ 3.,  6.,  9.],\n       [nan, nan, nan]])"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html#local-level-model",
    "href": "kalman/old_01_pykalman_models.html#local-level-model",
    "title": "[OLD] PyKalman Filter Models",
    "section": "Local Level Model",
    "text": "Local Level Model\nLocal level models is a model that uses Kalman filter, where the design matrix (A) and the Transition matrix (H) are identity matrix. This means that the state of model is equal to the observations and the changes in the state are only from the process noise.\n\nsource\n\nLocalLevelModel\n\n LocalLevelModel (data)\n\nLocal level model using a kalman filter\n\nll = LocalLevelModel(X)\n\n\nll.fit()\n\n<__main__.LocalLevelModel>\n\n\n\nll.predict(np.array([5, 3]))\n\nListNormal(mean=array([[1.22847659],\n       [2.59752825]]), cov=array([[[0.45973462]],\n\n       [[0.42588627]]]))\n\n\n\nll2 = LocalLevelModel(np.hstack([X, X*2]))\n\n\nll2.fit()\n\n<__main__.LocalLevelModel>\n\n\n\nll2.predict(np.array([5, 3]))\n\nListNormal(mean=array([[1.10135575, 2.18600305],\n       [2.81259028, 5.65372346]]), cov=array([[[0.25086027, 0.35069288],\n        [0.35069288, 0.77948375]],\n\n       [[0.23700609, 0.33877014],\n        [0.33877014, 0.74905336]]]))\n\n\n\nll2.get_info()\n\n{'A':   latent  z_0  z_1\n 0    z_0  1.0  0.0\n 1    z_1  0.0  1.0,\n 'H':    z_0  z_1\n 0  1.0  0.0\n 1  0.0  1.0,\n 'R':           0         1\n 0  0.136841  0.181315\n 1  0.181315  0.408279,\n 'Q':   latent       z_0       z_1\n 0    z_0  0.960236  1.852132\n 1    z_1  1.852132  3.847645}\n\n\nWarning this implementation of the EM algorithm may actually result in matrices that aren’t correct for multivariate variables"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html#local-slope-model",
    "href": "kalman/old_01_pykalman_models.html#local-slope-model",
    "title": "[OLD] PyKalman Filter Models",
    "section": "Local slope Model",
    "text": "Local slope Model\nLocal slope models are an extentions of local level model that in the state variable keep track of also the slope\nThe transition matrix (A) is:\n\\[A = \\left[\\begin{array}{cc}1 & 1 \\\\ 0 & 1\\end{array}\\right]\\]\nthe state \\(x \\in \\mathbb{R}^(2N) \\times 1\\) where the upper half keep track of the level and the lower half of the slope. \\(A \\in \\mathbb{R}^2N \\times 2N\\)\nhence the observation matrix (H) is:\n\\[A = \\left[\\begin{array}{cc}1 & 0 end{array}\\right]\\]\nFor the multivariate case the 1 are replaced with an identiy matrix\n\nn_features = 2\nZero = np.zeros((n_features, n_features), dtype=np.float32)\nI = np.eye(n_features)\n\nA = np.vstack([np.hstack([I   , I]),\n               np.hstack([Zero, I])])\nH = np.hstack([I, Zero])\n\n\nA.shape\n\n(4, 4)\n\n\n\nI.shape\n\n(2, 2)\n\n\n\nZero.shape\n\n(2, 2)\n\n\n\nH.shape\n\n(2, 4)\n\n\n\nA, H = _init_local_slope(2)\n\n\nsource\n\nLocalSlopeModel\n\n LocalSlopeModel (data)\n\nLocal slope model using a kalman filter\n\nls = LocalSlopeModel(X)\n\n\nls.model\n\n<pykalman.standard.KalmanFilter>\n\n\n\nls.fit()\n\n<__main__.LocalSlopeModel>\n\n\n\nls.predict(np.array([3, 2]))\n\nListNormal(mean=array([[2.716141  ],\n       [2.01802129]]), cov=array([[[0.32816569]],\n\n       [[0.32739003]]]))\n\n\n\nX = np.array([[1, 2],\n              [2, 3],\n              [4, 5],\n              [6, 7]])\n\n\nX.shape\n\n(4, 2)\n\n\n\n(A @ X).shape\n\n(4, 2)\n\n\n\nH.shape\n\n(2, 4)\n\n\n\nH @ X\n\narray([[1., 2.],\n       [2., 3.]])\n\n\n\nls2 = LocalSlopeModel(np.hstack([X, X*2]))\n\n\nls2.fit()\n\n<__main__.LocalSlopeModel>\n\n\n\nls2.predict(np.array([1, 2]))\n\nListNormal(mean=array([[1.76475393, 2.53536399, 3.52950785, 5.07072797],\n       [3.88624563, 4.80683778, 7.77249127, 9.61367556]]), cov=array([[[0.62753041, 0.7826417 , 0.83204436, 1.56528339],\n        [0.7826417 , 1.707989  , 1.56528339, 2.99296154],\n        [0.83204436, 1.56528339, 1.87559695, 3.13056678],\n        [1.56528339, 2.99296154, 3.13056678, 6.19743131]],\n\n       [[0.66321396, 0.84217581, 0.90182172, 1.68435163],\n        [0.84217581, 1.81191923, 1.68435163, 3.19923227],\n        [0.90182172, 1.68435163, 2.01594655, 3.36870325],\n        [1.68435163, 3.19923227, 3.36870325, 6.61076763]]]))\n\n\n\ndisplay_as_row(ls2.get_info())\n\n\n  A \n\n  \n    \n      latent\n      level_x_0\n      level_x_1\n      level_x_2\n      level_x_3\n      slope_x_0\n      slope_x_1\n      slope_x_2\n      slope_x_3\n    \n  \n  \n    \n      level_x_0\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      level_x_1\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n    \n    \n      level_x_2\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n    \n    \n      level_x_3\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n    \n    \n      slope_x_0\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      slope_x_1\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n    \n    \n      slope_x_2\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n    \n    \n      slope_x_3\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      1.0000\n    \n  \n\n  H \n\n  \n    \n      level_x_0\n      level_x_1\n      level_x_2\n      level_x_3\n      slope_x_0\n      slope_x_1\n      slope_x_2\n      slope_x_3\n    \n  \n  \n    \n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n    \n    \n      0.0000\n      0.0000\n      0.0000\n      1.0000\n      0.0000\n      0.0000\n      0.0000\n      0.0000\n    \n  \n\n  R \n\n  \n    \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0.4218\n      0.5610\n      0.5865\n      1.1220\n    \n    \n      0.5610\n      1.2150\n      1.1220\n      2.1728\n    \n    \n      0.5865\n      1.1220\n      1.3015\n      2.2439\n    \n    \n      1.1220\n      2.1728\n      2.2439\n      4.4742\n    \n  \n\n  Q \n\n  \n    \n      latent\n      level_x_0\n      level_x_1\n      level_x_2\n      level_x_3\n      slope_x_0\n      slope_x_1\n      slope_x_2\n      slope_x_3\n    \n  \n  \n    \n      level_x_0\n      1.4877\n      1.6293\n      2.4502\n      3.2585\n      1.3881\n      1.2723\n      2.6069\n      2.5446\n    \n    \n      level_x_1\n      1.6293\n      2.4422\n      3.2585\n      4.3593\n      1.7377\n      1.7777\n      3.4754\n      3.3860\n    \n    \n      level_x_2\n      2.4502\n      3.2585\n      5.1629\n      6.5170\n      2.6069\n      2.5446\n      5.2985\n      5.0892\n    \n    \n      level_x_3\n      3.2585\n      4.3593\n      6.5170\n      8.9812\n      3.4754\n      3.3860\n      6.9508\n      6.8566\n    \n    \n      slope_x_0\n      1.3881\n      1.7377\n      2.6069\n      3.4754\n      1.8250\n      1.4320\n      2.9129\n      2.8640\n    \n    \n      slope_x_1\n      1.2723\n      1.7777\n      2.5446\n      3.3860\n      1.4320\n      1.7896\n      2.8640\n      2.8421\n    \n    \n      slope_x_2\n      2.6069\n      3.4754\n      5.2985\n      6.9508\n      2.9129\n      2.8640\n      6.1944\n      5.7279\n    \n    \n      slope_x_3\n      2.5446\n      3.3860\n      5.0892\n      6.8566\n      2.8640\n      2.8421\n      5.7279\n      6.0527\n    \n  \n\n \n\n\n\n\nState\n\nsource\n\n\nKalmanModel.plot_state\n\n KalmanModel.plot_state (n_cols=2, bind_interaction=True, properties={})\n\n\nls.plot_state()\n\n/tmp/ipykernel_84377/4282844104.py:4: RuntimeWarning: invalid value encountered in sqrt\n  return np.diagonal(np.sqrt(x), axis1=1, axis2=2)\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():"
  },
  {
    "objectID": "kalman/old_01_pykalman_models.html#export",
    "href": "kalman/old_01_pykalman_models.html#export",
    "title": "[OLD] PyKalman Filter Models",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/filter.html",
    "href": "kalman/filter.html",
    "title": "Kalman Filter",
    "section": "",
    "text": "The models uses a latent state variable \\(x\\) that is modelled over time, to impute gaps in \\(y\\)\nThe assumption of the model is that the state variable at time \\(x_t\\) depends only on the last state \\(x_{t-1}\\) and not on the previous states.\n\n\nThe equations of the model are:\n\\[\\begin{align} p(x_t | x_{t-1}) & = \\mathcal{N}(Ax_{t-1} + b, Q) \\\\\np(y_t | x_t) & = \\mathcal{N}(Hx_t + d, R) \\end{align}\\]\nwhere:\n\n\\(A\\) is the trans_matrix\n\\(b\\) is the trans_offset\n\\(Q\\) is the trans_cov\n\\(H\\) is the obs_trans\n\\(d\\) is the obs_off\n\\(R\\) is the obs_cov\n\nin addition the model has also the parameters of the initial state that are used to initialize the filter:\n\ninit_state_mean\ninit_state_cov\n\nThe Kalman filter has 3 steps:\n\nfilter (updating the state at time t with observations till time t-1)\nupdate (update the state at time t using the observation at time t)\nsmooth (update the state using the observations at time t+1)\n\nIn case of missing data the update step is skipped.\nAfter smoothing the missing data at time t (\\(y_t\\)) can be imputed from the state (\\(x_t\\)) using this formula: \\[p(y_t|x_t) = \\mathcal{N}(Hx_t + d, R + HP^s_tH^T)\\]\nThe Kalman Filter is an algorithm designed to estimate \\(P(x_t | y_{0:t})\\). As all state transitions and obss are linear with Gaussian distributed noise, these distributions can be represented exactly as Gaussian distributions with mean filt_state_means[t] and covs filt_state_covs[t]. Similarly, the Kalman Smoother is an algorithm designed to estimate \\(P(x_t | y_{0:t-1})\\)"
  },
  {
    "objectID": "kalman/filter.html#main-class",
    "href": "kalman/filter.html#main-class",
    "title": "Kalman Filter",
    "section": "Main class",
    "text": "Main class\nTODO: fill nans with 0 for all data\n\nsource\n\nKalmanFilter\n\n KalmanFilter (trans_matrix:torch.Tensor, obs_matrix:torch.Tensor,\n               trans_cov:torch.Tensor, obs_cov:torch.Tensor,\n               trans_off:torch.Tensor, obs_off:torch.Tensor,\n               init_state_mean:torch.Tensor, init_state_cov:torch.Tensor,\n               n_dim_state:int=None, n_dim_obs:int=None, cov_checker:meteo\n               _imp.gaussian.CheckPosDef=<meteo_imp.gaussian.CheckPosDef\n               object at 0x7f6cc06ad990>)\n\nBase class for Kalman Filter and Smoother using PyTorch\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrans_matrix\nTensor\n\n[n_dim_state,n_dim_state] \\(A\\), state transition matrix\n\n\nobs_matrix\nTensor\n\n[n_dim_obs, n_dim_state] \\(H\\), observation matrix\n\n\ntrans_cov\nTensor\n\n[n_dim_state, n_dim_state] \\(Q\\), state trans covariance matrix\n\n\nobs_cov\nTensor\n\n[n_dim_obs, n_dim_obs] \\(R\\), observations covariance matrix\n\n\ntrans_off\nTensor\n\n[n_dim_state] \\(b\\), state transition offset\n\n\nobs_off\nTensor\n\n[n_dim_obs] \\(d\\), observations offset\n\n\ninit_state_mean\nTensor\n\n[n_dim_state] \\(\\mu_0\\)\n\n\ninit_state_cov\nTensor\n\n[n_dim_state, n_dim_state] \\(\\Sigma_0\\)\n\n\nn_dim_state\nint\nNone\nNumber of dimensions for state - defaults to 1 if cannot be infered from parameters\n\n\nn_dim_obs\nint\nNone\nNumber of dimensions for observations - defaults to 1 if cannot be infered from parameters\n\n\ncov_checker\nCheckPosDef\n<meteo_imp.gaussian.CheckPosDef object at 0x7f6cc06ad990>"
  },
  {
    "objectID": "kalman/filter.html#constructors",
    "href": "kalman/filter.html#constructors",
    "title": "Kalman Filter",
    "section": "Constructors",
    "text": "Constructors\nGiving all the parameters manually to the KalmanFilter init method is not convenient, hence we are having some methods that help initize the class\n\nRandom parameters\n\nsource\n\n\nKalmanFilter.init_random\n\n KalmanFilter.init_random (n_dim_obs, n_dim_state, dtype=torch.float32)\n\nkalman filter with random parameters\n\nk = KalmanFilter.init_random(3,4, dtype=torch.float64)\nk\n\nKalman Filter\n        N dim obs: 3, N dim state: 4\n\n\n\nk.init_state_cov\n\ntensor([[0.8097, 0.5396, 1.1551, 0.7763],\n        [0.5396, 0.8977, 1.1206, 0.6797],\n        [1.1551, 1.1206, 2.3948, 1.5417],\n        [0.7763, 0.6797, 1.5417, 1.0826]], dtype=torch.float64,\n       grad_fn=<AddBackward0>)\n\n\ncheck that assigment works :)\n\nk.init_state_cov = to_posdef(torch.rand(4, 4, dtype=torch.float64))\n\n\nk.init_state_cov_raw\n\nParameter containing:\ntensor([[ 1.1395,  0.0000,  0.0000,  0.0000],\n        [ 1.0543,  0.6376,  0.0000,  0.0000],\n        [ 0.2703,  0.1890,  0.2075,  0.0000],\n        [ 1.1269,  0.6352, -0.1493,  0.1180]], dtype=torch.float64,\n       requires_grad=True)\n\n\n\nlist(k.named_parameters())\n\n[('trans_matrix',\n  Parameter containing:\n  tensor([[0.6840, 0.3008, 0.7896, 0.9276],\n          [0.5100, 0.4481, 0.2644, 0.9156],\n          [0.7220, 0.0185, 0.5433, 0.6530],\n          [0.0054, 0.7075, 0.9545, 0.3133]], dtype=torch.float64,\n         requires_grad=True)),\n ('trans_off',\n  Parameter containing:\n  tensor([0.1639, 0.5273, 0.2408, 0.7842], dtype=torch.float64,\n         requires_grad=True)),\n ('trans_cov_raw',\n  Parameter containing:\n  tensor([[ 1.2782,  0.0000,  0.0000,  0.0000],\n          [ 0.9392,  0.7310,  0.0000,  0.0000],\n          [ 0.9769, -0.0552,  0.1300,  0.0000],\n          [ 0.3592,  0.8284,  0.4487,  0.0705]], dtype=torch.float64,\n         requires_grad=True)),\n ('obs_matrix',\n  Parameter containing:\n  tensor([[5.9363e-01, 1.5224e-01, 1.1349e-01, 6.7027e-01],\n          [5.2014e-01, 6.6511e-01, 5.9441e-02, 3.1239e-01],\n          [4.5763e-04, 6.3865e-01, 8.8007e-01, 6.8980e-01]], dtype=torch.float64,\n         requires_grad=True)),\n ('obs_off',\n  Parameter containing:\n  tensor([0.7773, 0.2017, 0.0910], dtype=torch.float64, requires_grad=True)),\n ('obs_cov_raw',\n  Parameter containing:\n  tensor([-0.0368, -0.7748, -0.2967], dtype=torch.float64, requires_grad=True)),\n ('init_state_mean',\n  Parameter containing:\n  tensor([0.5600, 0.7580, 0.6712, 0.2729], dtype=torch.float64,\n         requires_grad=True)),\n ('init_state_cov_raw',\n  Parameter containing:\n  tensor([[ 1.1395,  0.0000,  0.0000,  0.0000],\n          [ 1.0543,  0.6376,  0.0000,  0.0000],\n          [ 0.2703,  0.1890,  0.2075,  0.0000],\n          [ 1.1269,  0.6352, -0.1493,  0.1180]], dtype=torch.float64,\n         requires_grad=True))]\n\n\n\n\nTest data\n\nreset_seed()\ndata, mask = get_test_data()\nshow_as_row(data, mask)\n\n\n data tensor([[[0.9847, 0.0852, 0.5334],\n         [0.2196, 0.2617, 0.7972],\n         [0.2088, 0.4545, 0.1455],\n         [0.2249, 0.7409, 0.2881],\n         [0.6578, 0.9087, 0.6871],\n         [0.5610, 0.9079, 0.2507],\n         [0.7647, 0.7851, 0.0212],\n         [0.2230, 0.6513, 0.3955],\n         [0.8111, 0.2558, 0.7570],\n         [0.3952, 0.8095, 0.3729]],\n\n        [[0.3451, 0.2511, 0.4720],\n         [0.6684, 0.1905, 0.1489],\n         [0.6714, 0.4719, 0.5053],\n         [0.4909, 0.7793, 0.3246],\n         [0.1249, 0.1391, 0.5222],\n         [0.8191, 0.7040, 0.3264],\n         [0.0842, 0.5963, 0.4742],\n         [0.9001, 0.3308, 0.7610],\n         [0.3228, 0.0961, 0.3075],\n         [0.0947, 0.4745, 0.8792]]]) mask tensor([[[ True,  True,  True],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [False, False,  True],\n         [False,  True, False],\n         [ True,  True,  True],\n         [False,  True,  True],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [False, False, False]],\n\n        [[False,  True,  True],\n         [ True, False,  True],\n         [ True,  True,  True],\n         [False,  True,  True],\n         [False, False, False],\n         [ True,  True,  True],\n         [ True, False, False],\n         [False,  True,  True],\n         [ True,  True,  True],\n         [ True,  True, False]]])"
  },
  {
    "objectID": "kalman/filter.html#filter",
    "href": "kalman/filter.html#filter",
    "title": "Kalman Filter",
    "section": "Filter",
    "text": "Filter\n\nFilter predict\nProbability of state at time t given state a time t-1\n\\(p(x_t) = \\mathcal{N}(x_t; m_t^-, P_t^-)\\) where:\n\npredicted state mean: \\(m_t^- = Am_{t-1} + B c_t + b\\)\npredicted state covariance: \\(P_t^- = AP_{t-1}A^T + Q\\)\n\n\ntrans_matrix, trans_cov, trans_off,curr_state_mean,curr_state_cov = (k.trans_matrix, k.trans_cov, k.trans_off,\n                                                  torch.stack([k.init_state_mean]*2).unsqueeze(-1),\n                                                  torch.stack([k.init_state_cov]*2))\n\n\npred_state_mean, pred_state_cov = _filter_predict(trans_matrix, trans_cov, trans_off,curr_state_mean,curr_state_cov)\n\n\nshow_as_row(pred_state_mean, pred_state_cov)\n\n\n pred_state_mean tensor([[[1.5581],\n         [1.5800],\n         [1.2021],\n         [2.0498]],\n\n        [[1.5581],\n         [1.5800],\n         [1.2021],\n         [2.0498]]], dtype=torch.float64, grad_fn=) pred_state_cov tensor([[[8.0592, 7.1437, 5.8126, 4.4504],\n         [7.1437, 6.9285, 5.0903, 4.6368],\n         [5.8126, 5.0903, 4.2361, 3.1597],\n         [4.4504, 4.6368, 3.1597, 3.5942]],\n\n        [[8.0592, 7.1437, 5.8126, 4.4504],\n         [7.1437, 6.9285, 5.0903, 4.6368],\n         [5.8126, 5.0903, 4.2361, 3.1597],\n         [4.4504, 4.6368, 3.1597, 3.5942]]], dtype=torch.float64,\n       grad_fn=) \n\n\n\nshow_as_row(pred_state_mean.shape, pred_state_cov.shape)\n\n\n  torch.Size([2, 4, 4]) \n\n\n\n\nFilter correct\nProbability of state at time t given the observations at time t\n\\(p(x_t|y_t) = \\mathcal{N}(x_t; m_t, P_t)\\) where:\n\npredicted obs mean: \\(z_t = Hm_t^- + d\\)\nprediced obs covariance: \\(S_t = HP_t^-H^T + R\\)\nkalman gain\\(K_t = P_t^-H^TS_t^{-1}\\)\ncorrected state mean: \\(m_t = m_t^- + K_t(y_t - z_t)\\)\ncorrected state covariance: \\(P_t = (I-K_tH)P_t^-\\)\n\nif the observation are missing this step is skipped and the corrected state is equal to the predicted state\nNeed to figure out the Nans for the gradients …\n\nMissing observations\nIf all the observations at time \\(t\\) are missing the correct step is skipped and the filtered state at time \\(t\\) () is the same of the filtered state.\nIf only some observations are missing a variation of equation can be used.\n\\(y^{ng}_t\\) is a vector containing the observations that are not missing at time \\(t\\).\nIt can be expressed as a linear transformation of \\(y_t\\)\n\\[ y^{ng}_t = My_t\\]\nwhere \\(M\\) is a mask matrix that is used to select the subset of \\(y_t\\) that is observed. \\(M \\in \\mathbb{R}^{n_{ng} \\times n}\\) and is made of columns which are made of all zeros but for an entry 1 at row corresponding to the non-missing observation. hence:\n\\[ p(y^{ng}_t) = \\mathcal{N}(M\\mu_{y_t},  M\\Sigma_{y_t}M^T)\\]\nfrom which you can derive\n\\[ p(y^{ng}_t|x_t) = p(MHx_t + Mb, MRM^T)  \\tag{1}\\]\nThen the posterior \\(p(x_t|y_t^{ng})\\) can be computed similarly of equation @filter_correct as:\n\\[ p(x_t|y^{ng}_t) = \\mathcal{N}(x_t; m_t, P_t)  \\tag{2}\\]\nwhere:\n\npredicted obs mean: \\(z_t = MHm_t^- + Md\\)\npredicted obs covariance: \\(S_t = MHP_t^-(MH)^T + MRM^T\\)\nKalman gain \\(K_t = P_t^-(MH)^TS_t^{-1}\\)\ncorrected state mean: \\(m_t = m_t^- + K_t(My_t - z_t)\\)\ncorrected state covariance: \\(P_t = (I-K_tMH)P_t^-\\)\n\n\nk.obs_off.shape\n\ntorch.Size([3])\n\n\n\nDetails implementation\nFor the implementation the matrix multiplication \\(MH\\) can be replaced with H[m] where m is the mask for the rows for H and \\(MRM^T\\) with R[m][:,m]\n\nobs_matrix, obs_cov, obs_off,obs, mm = (k.obs_matrix, k.obs_cov, k.obs_off, data[:,0,:], mask[:,0,:])\n\n\nm = torch.tensor([False,True,True]) # mask batch\nM = torch.tensor([[0,1,0], # mask matrix\n                  [0,0,1]], dtype=torch.float64)\nshow_as_row(m, M, obs_matrix, obs_cov)\n\n\n m tensor([False,  True,  True]) M tensor([[0., 1., 0.],\n        [0., 0., 1.]], dtype=torch.float64) obs_matrix Parameter containing:\ntensor([[5.9363e-01, 1.5224e-01, 1.1349e-01, 6.7027e-01],\n        [5.2014e-01, 6.6511e-01, 5.9441e-02, 3.1239e-01],\n        [4.5763e-04, 6.3865e-01, 8.8007e-01, 6.8980e-01]], dtype=torch.float64,\n       requires_grad=True) obs_cov tensor([[0.9639, 0.0000, 0.0000],\n        [0.0000, 0.4608, 0.0000],\n        [0.0000, 0.0000, 0.7433]], dtype=torch.float64,\n       grad_fn=) \n\n\n\nM @ obs_matrix, obs_matrix[m]\n\n(tensor([[5.2014e-01, 6.6511e-01, 5.9441e-02, 3.1239e-01],\n         [4.5763e-04, 6.3865e-01, 8.8007e-01, 6.8980e-01]], dtype=torch.float64,\n        grad_fn=<MmBackward0>),\n tensor([[5.2014e-01, 6.6511e-01, 5.9441e-02, 3.1239e-01],\n         [4.5763e-04, 6.3865e-01, 8.8007e-01, 6.8980e-01]], dtype=torch.float64,\n        grad_fn=<IndexBackward0>))\n\n\n\nM @ obs_cov @ M.T, obs_cov[m][:,m]\n\n(tensor([[0.4608, 0.0000],\n         [0.0000, 0.7433]], dtype=torch.float64, grad_fn=<MmBackward0>),\n tensor([[0.4608, 0.0000],\n         [0.0000, 0.7433]], dtype=torch.float64, grad_fn=<IndexBackward0>))\n\n\nBy using partially missing observations _filter_correct cannot be easily batched as the shape of the intermediate variables depends on the number of observed variables. So the idea is to divide the batch in batches where there is the same number of variables.\n\nmask_values, indices = torch.unique(mask[:,1,:], dim=0, return_inverse=True)\nmask_values, indices\n\n(tensor([[False,  True,  True],\n         [ True, False,  True]]),\n tensor([0, 1]))\n\n\n\nsource\n\n\n\n\nunsqueeze_iter\n\n unsqueeze_iter (*args, dim)\n\n\nobs_matrix, obs_cov, obs_off,obs, mm = (k.obs_matrix, k.obs_cov, k.obs_off, data[:,0,:], mask[:,0,:])\n\n\ncorr_s_mean,corr_s_cov = _filter_correct_batch(obs_matrix, obs_cov, obs_off, pred_state_mean[0:1], pred_state_cov[0:1], obs[0:1], mm[0])\n\n\ncorr_s_mean.shape, corr_s_cov.shape\n\n(torch.Size([1, 4, 1]), torch.Size([1, 4, 4]))\n\n\n\nobs_matrix, obs_cov, obs_off,obs, mm = (k.obs_matrix, k.obs_cov, k.obs_off, data[:,0,:], mask[:,0,:])\n\n\ncorr_s_mean, corr_s_cov = _filter_correct(obs_matrix, obs_cov, obs_off, pred_state_mean, pred_state_cov, obs, mm)\n\n\nshow_as_row(corr_s_mean, corr_s_cov)\n\n\n corr_s_mean tensor([[[-0.3244],\n         [-0.1878],\n         [-0.1442],\n         [ 0.8721]],\n\n        [[-0.2398],\n         [-0.1245],\n         [-0.0887],\n         [ 0.8973]]], dtype=torch.float64, grad_fn=) corr_s_cov tensor([[[ 0.5094,  0.0080,  0.3998, -0.3441],\n         [ 0.0080,  0.1799, -0.0271,  0.0955],\n         [ 0.3998, -0.0271,  0.3548, -0.2818],\n         [-0.3441,  0.0955, -0.2818,  0.5160]],\n\n        [[ 0.5272,  0.0220,  0.4126, -0.3254],\n         [ 0.0220,  0.1909, -0.0171,  0.1102],\n         [ 0.4126, -0.0171,  0.3639, -0.2685],\n         [-0.3254,  0.1102, -0.2685,  0.5356]]], dtype=torch.float64,\n       grad_fn=) \n\n\n\ncorr_s_mean.shape, corr_s_cov.shape\n\n(torch.Size([2, 4, 1]), torch.Size([2, 4, 4]))\n\n\n\ncorr_s_mean.sum().backward(retain_graph=True) # check that pytorch can compute gradients with the whole batch\n\n\n\nFilter\nThe resursive version of the kalman filter is apperently breaking pytorch gradients calculations so a workaround is needed. During the loop the states are saved in a python list and then at the end they are combined back into a tensor. The last line of the function does:\n\nconvert lists to tensors\ncorrect order dimensions\n\n\nobs, init_state_mean, init_state_cov = data, k.init_state_mean, k.init_state_cov\n\n\npred_state_means, pred_state_covs, filt_state_means, filt_state_covs = _filter(trans_matrix, obs_matrix, trans_cov, obs_cov, trans_off, obs_off, init_state_mean, init_state_cov, data, mask)\n\nPredictions at time 0 for both batches\n\nshow_as_row(list(map(Self.shape(), (pred_state_means, pred_state_covs, filt_state_means, filt_state_covs,))))\n\n\n  [torch.Size([2, 10, 4, 1]), torch.Size([2, 10, 4, 4]), torch.Size([2, 10, 4, 1]), torch.Size([2, 10, 4, 4])] \n\n\n\nshow_as_row(list(map(lambda x:x[0][0], (pred_state_means, pred_state_covs, filt_state_means, filt_state_covs,))))\n\n\n  [tensor([[0.5600],\n        [0.7580],\n        [0.6712],\n        [0.2729]], dtype=torch.float64, grad_fn=), tensor([[1.2985, 1.2013, 0.3080, 1.2841],\n        [1.2013, 1.5181, 0.4055, 1.5931],\n        [0.3080, 0.4055, 0.1519, 0.3937],\n        [1.2841, 1.5931, 0.3937, 1.7096]], dtype=torch.float64,\n       grad_fn=), tensor([[ 0.0920],\n        [ 0.2233],\n        [ 0.5305],\n        [-0.2911]], dtype=torch.float64, grad_fn=), tensor([[ 2.5826e-01,  1.6403e-02, -1.3015e-04,  2.6911e-02],\n        [ 1.6403e-02,  1.1675e-01,  3.3622e-02,  1.1311e-01],\n        [-1.3015e-04,  3.3622e-02,  5.2124e-02,  1.9131e-03],\n        [ 2.6911e-02,  1.1311e-01,  1.9131e-03,  1.4565e-01]],\n       dtype=torch.float64, grad_fn=)] \n\n\n\n\nKalmanFilter method\n\npred_mean, _, _, _ = k._filter_all(obs);\n\n\ntype(k._filter_all(obs, mask))\n\nlist\n\n\n\npred_mean.sum().backward(retain_graph=True) # it works!\n\nThe filter methods wraps _filter_all but in addition:\n\nreturns only filtered state\ndetach tensors\n\n\nsource\n\n\nKalmanFilter.filter\n\n KalmanFilter.filter (obs:torch.Tensor, mask=None)\n\nFilter observation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobs\nTensor\n\n[n_timesteps, n_dim_obs] obs for times [0…n_timesteps-1]\n\n\nmask\nNoneType\nNone\n\n\n\nReturns\nListMultiNormal\n\nFiltered state\n\n\n\n\nfilt = k.filter(obs)\nfilt.mean.shape, filt.cov.shape\n\n(torch.Size([2, 10, 4]), torch.Size([2, 10, 4, 4]))"
  },
  {
    "objectID": "kalman/filter.html#smooth",
    "href": "kalman/filter.html#smooth",
    "title": "Kalman Filter",
    "section": "Smooth",
    "text": "Smooth\n\nSmooth step\ncompute the probability of the state at time t given all the observations\n\\(p(x_t|Y) = \\mathcal{N}(x_t; m_t^s, P_t^s)\\) where:\n\nKalman smoothing gain: \\(G_t = P_tA^T(P_{t+1}^-)^{-1}\\)\nsmoothed mean: \\(m_t^s = m_t + G_t(m_{t+1}^s - m_{t+1}^-)\\)\nsmoothed covariance: \\(P_t^s = P_t + G_t(P_{t+1}^s - P_{t+1}^-)G_t^T\\)\n\n\nfilt_state, pred_state, next_smoothed_state = [MNormal(pred_state_mean, pred_state_cov)] * 3 # just for testing\n\n\nshow_as_row(*_smooth_update(trans_matrix, MNormal(pred_state_mean, pred_state_cov), MNormal(pred_state_mean, pred_state_cov), MNormal(pred_state_mean, pred_state_cov)))\n\n\n  tensor([[[8.0592, 7.1437, 5.8126, 4.4504],\n         [7.1437, 6.9285, 5.0903, 4.6368],\n         [5.8126, 5.0903, 4.2361, 3.1597],\n         [4.4504, 4.6368, 3.1597, 3.5942]],\n\n        [[8.0592, 7.1437, 5.8126, 4.4504],\n         [7.1437, 6.9285, 5.0903, 4.6368],\n         [5.8126, 5.0903, 4.2361, 3.1597],\n         [4.4504, 4.6368, 3.1597, 3.5942]]], dtype=torch.float64,\n       grad_fn=) \n\n\n\nshow_as_row(*map(Self.shape(), _smooth_update(trans_matrix, MNormal(pred_state_mean, pred_state_cov), MNormal(pred_state_mean, pred_state_cov), MNormal(pred_state_mean, pred_state_cov))))\n\n\n  torch.Size([2, 4, 4]) \n\n\n\n\nSmooth\n\n(pred_state_means, pred_state_covs, filt_state_means, filt_state_covs ) = k._filter_all(data)\nfilt_state, pred_state = ListMNormal(filt_state_means, filt_state_covs), ListMNormal(pred_state_means, pred_state_covs)\n\n\nsmooth_state = _smooth(k.trans_matrix,  filt_state, pred_state)\n\n\nshow_as_row(smooth_state.mean[0][0], smooth_state.cov[0][0])\n\n\n  tensor([[ 0.2402,  0.0004, -0.0047,  0.0098],\n        [ 0.0004,  0.0976,  0.0274,  0.0935],\n        [-0.0047,  0.0274,  0.0497, -0.0041],\n        [ 0.0098,  0.0935, -0.0041,  0.1253]], dtype=torch.float64,\n       grad_fn=) \n\n\n\nshow_as_row(smooth_state.mean.shape, smooth_state.cov.shape)\n\n\n  torch.Size([2, 10, 4, 4]) \n\n\n\n\nKalmanFilter method\n\nsource\n\n\nKalmanFilter.smooth\n\n KalmanFilter.smooth (obs:torch.Tensor, mask:torch.Tensor=None)\n\nKalman Filter Smoothing\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobs\nTensor\n\n\n\n\nmask\nTensor\nNone\n\n\n\nReturns\nListMultiNormal\n\n[n_timesteps, n_dim_state] smoothed state\n\n\n\n\nsmoothed_state = k.smooth(data)\n\n\nshow_as_row(smoothed_state.mean.shape, smoothed_state.cov.shape)\n\n\n  torch.Size([2, 10, 4, 4])"
  },
  {
    "objectID": "kalman/filter.html#predict",
    "href": "kalman/filter.html#predict",
    "title": "Kalman Filter",
    "section": "Predict",
    "text": "Predict\nThe prediction at time t (\\(y_t\\)) are computed rom the state (\\(x_t\\)) using this formula: \\[p(y_t|x_t) = \\mathcal{N}(Hx_t + d, R + HP^s_tH^T)\\]\nthis works both if the state was filtered or smoother\nThis add the supports for conditional predictions, which means that at the time (t) when we are making the predictions some of the variables have been actually observed. Since the model prediction is a normal distribution we can condition on the observed values and thus improve the predictions. See conditional_gaussian\nIn order to have conditional predictions that make sense it’s not possible to return the full covariance matrix for the predictions but only the standard deviations\n\ntest_m = torch.tensor(\n    [[True, True, True,],\n    [False, True, True],\n    [False, False, False]]\n)\n\n\ntorch.logical_xor(test_m.all(-1), test_m.any(-1))\n\ntensor([False,  True, False])\n\n\n\nA = torch.rand(2,2,3,3)\n\n\n(A @ A).shape\n\ntorch.Size([2, 2, 3, 3])\n\n\npredict can be vectorized across both the batch and the timesteps, except for timesteps that require conditional predictions\n\nsmoothed_state.mean.shape, smoothed_state.cov.shape\n\n(torch.Size([2, 10, 4]), torch.Size([2, 10, 4, 4]))\n\n\n\n(k.obs_matrix @ smoothed_state.mean.unsqueeze(-1)).shape\n\ntorch.Size([2, 10, 3, 1])\n\n\n\npred_obs0 = k._obs_from_state(smoothed_state)\npred_obs0.mean.shape\n\ntorch.Size([2, 10, 3])\n\n\n\npred_obs0.cov.shape\n\ntorch.Size([2, 10, 3, 3])\n\n\n\nsource\n\nKalmanFilter.predict\n\n KalmanFilter.predict (obs, mask=None, smooth=True)\n\nPredicted observations at all times\n\npred = k.predict(data)\n\n\npred.mean.shape, pred.std.shape\n\n(torch.Size([2, 10, 3]), torch.Size([2, 10, 3]))\n\n\nGradients …\n\ndata[~mask] = 0\n\n\ndata\n\ntensor([[[0.9847, 0.0852, 0.5334],\n         [0.0000, 0.2617, 0.7972],\n         [0.2088, 0.4545, 0.1455],\n         [0.0000, 0.0000, 0.2881],\n         [0.0000, 0.9087, 0.0000],\n         [0.5610, 0.9079, 0.2507],\n         [0.0000, 0.7851, 0.0212],\n         [0.0000, 0.6513, 0.3955],\n         [0.8111, 0.2558, 0.7570],\n         [0.0000, 0.0000, 0.0000]],\n\n        [[0.0000, 0.2511, 0.4720],\n         [0.6684, 0.0000, 0.1489],\n         [0.6714, 0.4719, 0.5053],\n         [0.0000, 0.7793, 0.3246],\n         [0.0000, 0.0000, 0.0000],\n         [0.8191, 0.7040, 0.3264],\n         [0.0842, 0.0000, 0.0000],\n         [0.0000, 0.3308, 0.7610],\n         [0.3228, 0.0961, 0.3075],\n         [0.0947, 0.4745, 0.0000]]])\n\n\n\nk.predict(data, mask).mean.sum().backward(retain_graph=True)\n\nprint(k.obs_cov_raw.grad)\n\nk.zero_grad()\n\ntensor([11.9564, -7.5735, -2.1074], dtype=torch.float64)\n\n\n\n@patch\ndef predict_times(self: KalmanFilter, times, obs, mask=None, smooth=True, check_args=None):\n    \"\"\"Predicted observations at specific times \"\"\"\n    state = self.smooth(obs, mask, check_args) if smooth else self.filter(obs, mask, check_args)\n    obs, mask = self._parse_obs(obs, mask)\n    times = array1d(times)\n    \n    n_timesteps = obs.shape[0]\n    n_features = obs.shape[1] if len(obs.shape) > 1 else 1\n    \n    if times.max() > n_timesteps or times.min() < 0:\n        raise ValueError(f\"provided times range from {times.min()} to {times.max()}, which is outside allowed range : 0 to {n_timesteps}\")\n\n    means = torch.empty((times.shape[0], n_features), dtype=obs.dtype, device=obs.device)\n    stds = torch.empty((times.shape[0], n_features), dtype=obs.dtype, device=obs.device) \n    for i, t in enumerate(times):\n        mean, std = self._obs_from_state(\n            state.mean[t],\n            state.cov[t],\n            {'t': t, **check_args} if check_args is not None else None\n        )\n        \n        means[i], stds[i] = _get_cond_pred(ListNormal(mean, std), obs[t], mask[t])\n    \n    return ListNormal(means, stds)"
  },
  {
    "objectID": "kalman/filter.html#additional",
    "href": "kalman/filter.html#additional",
    "title": "Kalman Filter",
    "section": "Additional",
    "text": "Additional\n\nGet Info\n\nk.obs_matrix\n\nParameter containing:\ntensor([[5.9363e-01, 1.5224e-01, 1.1349e-01, 6.7027e-01],\n        [5.2014e-01, 6.6511e-01, 5.9441e-02, 3.1239e-01],\n        [4.5763e-04, 6.3865e-01, 8.8007e-01, 6.8980e-01]], dtype=torch.float64,\n       requires_grad=True)\n\n\n\nsource\n\n\nKalmanFilter.get_info\n\n KalmanFilter.get_info (var_names=None)\n\n\ndisplay_as_row(k.get_info())\n\n\n  trans_matrix (A) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n      z_3\n    \n  \n  \n    \n      z_0\n      0.6840\n      0.3008\n      0.7896\n      0.9276\n    \n    \n      z_1\n      0.5100\n      0.4481\n      0.2644\n      0.9156\n    \n    \n      z_2\n      0.7220\n      0.0185\n      0.5433\n      0.6530\n    \n    \n      z_3\n      0.0054\n      0.7075\n      0.9545\n      0.3133\n    \n  \n\n  trans_cov (Q) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n      z_3\n    \n  \n  \n    \n      z_0\n      1.6338\n      1.2005\n      1.2486\n      0.4591\n    \n    \n      z_1\n      1.2005\n      1.4165\n      0.8771\n      0.9430\n    \n    \n      z_2\n      1.2486\n      0.8771\n      0.9743\n      0.3635\n    \n    \n      z_3\n      0.4591\n      0.9430\n      0.3635\n      1.0217\n    \n  \n\n  trans_off \n\n  \n    \n      latent\n      offset\n    \n  \n  \n    \n      z_0\n      0.1639\n    \n    \n      z_1\n      0.5273\n    \n    \n      z_2\n      0.2408\n    \n    \n      z_3\n      0.7842\n    \n  \n\n  obs_matrix (H) \n\n  \n    \n      variable\n      z_0\n      z_1\n      z_2\n      z_3\n    \n  \n  \n    \n      x_0\n      0.5936\n      0.1522\n      0.1135\n      0.6703\n    \n    \n      x_1\n      0.5201\n      0.6651\n      0.0594\n      0.3124\n    \n    \n      x_2\n      0.0005\n      0.6387\n      0.8801\n      0.6898\n    \n  \n\n  obs_cov (R) \n\n  \n    \n      variable\n      x_0\n      x_1\n      x_2\n    \n  \n  \n    \n      x_0\n      0.9639\n      0.0000\n      0.0000\n    \n    \n      x_1\n      0.0000\n      0.4608\n      0.0000\n    \n    \n      x_2\n      0.0000\n      0.0000\n      0.7433\n    \n  \n\n  obs_off \n\n  \n    \n      variable\n      offset\n    \n  \n  \n    \n      x_0\n      0.7773\n    \n    \n      x_1\n      0.2017\n    \n    \n      x_2\n      0.0910\n    \n  \n\n  init_state_mean \n\n  \n    \n      latent\n      mean\n    \n  \n  \n    \n      z_0\n      0.5600\n    \n    \n      z_1\n      0.7580\n    \n    \n      z_2\n      0.6712\n    \n    \n      z_3\n      0.2729\n    \n  \n\n  init_state_cov \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n      z_3\n    \n  \n  \n    \n      z_0\n      1.2985\n      1.2013\n      0.3080\n      1.2841\n    \n    \n      z_1\n      1.2013\n      1.5181\n      0.4055\n      1.5931\n    \n    \n      z_2\n      0.3080\n      0.4055\n      0.1519\n      0.3937\n    \n    \n      z_3\n      1.2841\n      1.5931\n      0.3937\n      1.7096\n    \n  \n\n \n\n\n\n\nConstructors\n\n\nSimple parameters\n\nsource\n\n\nKalmanFilter.init_simple\n\n KalmanFilter.init_simple (n_dim, dtype=torch.float32)\n\nSimplest version of kalman filter parameters\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_dim\n\n\nn_dim_obs and n_dim_state\n\n\ndtype\ndtype\ntorch.float32\n\n\n\n\n\nKalmanFilter.init_simple(2).state_dict()\n\nOrderedDict([('trans_matrix',\n              tensor([[1., 0.],\n                      [0., 1.]])),\n             ('trans_off', tensor([0., 0.])),\n             ('trans_cov_raw',\n              tensor([[1., 0.],\n                      [0., 1.]])),\n             ('obs_matrix',\n              tensor([[1., 0.],\n                      [0., 1.]])),\n             ('obs_off', tensor([0., 0.])),\n             ('obs_cov_raw', tensor([0., 0.])),\n             ('init_state_mean', tensor([0., 0.])),\n             ('init_state_cov_raw',\n              tensor([[1., 0.],\n                      [0., 1.]]))])\n\n\n\n\nLocal slope\nLocal slope models are an extentions of local level model that in the state variable keep track of also the slope\nGiven \\(n\\) as the number of dimensions of the observations\nThe transition matrix (A) is:\n\\[A = \\left[\\begin{array}{cc}I & I \\\\ 0 & I\\end{array}\\right]\\]\nwhere:\n\n\\(I \\in \\mathbb{R}^{n \\times n}\\)\n\\(A \\in \\mathbb{R}^{2n \\times 2n}\\)\n\nthe state \\(x \\in \\mathbb{R}^{2N \\times 1}\\) where the upper half keep track of the level and the lower half of the slope. \\(A \\in \\mathbb{R}^2N \\times 2N\\)\nthe observation matrix (H) is:\n\\[H = \\left[\\begin{array}{cc}I & 0 \\end{array}\\right]\\]\nFor the multivariate case the 1 are replaced with an identiy matrix\n\nsource\n\n\nKalmanFilter.init_local_slope\n\n KalmanFilter.init_local_slope (n_dim, dtype=torch.float32)\n\nSimplest version of kalman filter parameters\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_dim\n\n\nn_dim_obs and n_dim_state\n\n\ndtype\ndtype\ntorch.float32\n\n\n\n\n\nKalmanFilter.init_simple(2).state_dict()\n\nOrderedDict([('trans_matrix',\n              tensor([[1., 0.],\n                      [0., 1.]])),\n             ('trans_off', tensor([0., 0.])),\n             ('trans_cov_raw',\n              tensor([[1., 0.],\n                      [0., 1.]])),\n             ('obs_matrix',\n              tensor([[1., 0.],\n                      [0., 1.]])),\n             ('obs_off', tensor([0., 0.])),\n             ('obs_cov_raw', tensor([0., 0.])),\n             ('init_state_mean', tensor([0., 0.])),\n             ('init_state_cov_raw',\n              tensor([[1., 0.],\n                      [0., 1.]]))])"
  },
  {
    "objectID": "kalman/filter.html#export",
    "href": "kalman/filter.html#export",
    "title": "Kalman Filter",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "kalman/fastai.html",
    "href": "kalman/fastai.html",
    "title": "Implement Kalman model using FastAI",
    "section": "",
    "text": "The aim of the data preparation pipeline is to: - take the original time series and split it into time blocks - for each block generate a random gap (need to figure out the properties of the gap) - split some time blocks for testing\nthe input of the pipeline is: - a dataframe containing all observations\nthe input of the model is: - observed data (potentially containing NaN where data is missing) - missing data mask (which is telling where the data is missing) - the data needs to be standardized\n\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nreset_seed()\n\n\nimport torch\n\n\nfrom fastai.tabular.core import *\nfrom fastai.data.core import *\n\n\n@cache_disk(cache_dir / \"full_hai\")\ndef load_data(dtype=np.float32):\n    return read_fluxnet_csv(hai_path, None, num_dtype=dtype)\n\nhai = load_data()\nhai64 = load_data(np.float64)\n\n\n\nthe first step is to transfrom the original dataframe into blocks of a specified block_len\ntwo different strategies are possible:\n\ncontigous blocks\nrandom block in the dataframe\n\n\nsource\n\n\n\n\n BlockDfTransform (df, block_len=200)\n\ndivide timeseries DataFrame into blocks\n\nblk = BlockDfTransform(hai, 10)\n\n\nblk\n\nBlockDfTransform:\nencodes: (int,object) -> encodes\ndecodes: \n\n\n\nblk(1)\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 05:30:00\n      -0.23\n      0.00\n      0.138\n    \n    \n      2000-01-01 06:00:00\n      -0.23\n      0.00\n      0.122\n    \n    \n      2000-01-01 06:30:00\n      -0.22\n      0.00\n      0.098\n    \n    \n      2000-01-01 07:00:00\n      -0.24\n      0.00\n      0.066\n    \n    \n      2000-01-01 07:30:00\n      -0.23\n      0.00\n      0.044\n    \n    \n      2000-01-01 08:00:00\n      -0.22\n      0.00\n      0.026\n    \n    \n      2000-01-01 08:30:00\n      -0.19\n      0.45\n      0.016\n    \n    \n      2000-01-01 09:00:00\n      -0.14\n      3.70\n      0.010\n    \n    \n      2000-01-01 09:30:00\n      -0.03\n      7.26\n      0.006\n    \n    \n      2000-01-01 10:00:00\n      0.04\n      12.24\n      0.006\n    \n  \n\n\n\n\n\n180 * 24 * 2 / 10\n\n864.0\n\n\nwe are taking a day in the summer so there is an higher values for the variables\n\nblk(800)\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2000-06-15 16:30:00\n      14.65\n      468.190002\n      6.454\n    \n    \n      2000-06-15 17:00:00\n      14.22\n      224.800003\n      5.799\n    \n    \n      2000-06-15 17:30:00\n      14.11\n      195.279999\n      6.577\n    \n    \n      2000-06-15 18:00:00\n      14.23\n      244.169998\n      6.931\n    \n    \n      2000-06-15 18:30:00\n      14.40\n      253.919998\n      7.286\n    \n    \n      2000-06-15 19:00:00\n      14.09\n      177.309998\n      7.251\n    \n    \n      2000-06-15 19:30:00\n      13.71\n      97.070000\n      6.683\n    \n    \n      2000-06-15 20:00:00\n      13.08\n      39.709999\n      5.851\n    \n    \n      2000-06-15 20:30:00\n      12.41\n      10.650000\n      5.254\n    \n    \n      2000-06-15 21:00:00\n      12.27\n      0.320000\n      5.164\n    \n  \n\n\n\n\n\ntfms1 = TfmdLists([800,801,802,803], [BlockDfTransform(hai, 10)])\n\n\ntfms1[0]\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2000-06-15 16:30:00\n      14.65\n      468.190002\n      6.454\n    \n    \n      2000-06-15 17:00:00\n      14.22\n      224.800003\n      5.799\n    \n    \n      2000-06-15 17:30:00\n      14.11\n      195.279999\n      6.577\n    \n    \n      2000-06-15 18:00:00\n      14.23\n      244.169998\n      6.931\n    \n    \n      2000-06-15 18:30:00\n      14.40\n      253.919998\n      7.286\n    \n    \n      2000-06-15 19:00:00\n      14.09\n      177.309998\n      7.251\n    \n    \n      2000-06-15 19:30:00\n      13.71\n      97.070000\n      6.683\n    \n    \n      2000-06-15 20:00:00\n      13.08\n      39.709999\n      5.851\n    \n    \n      2000-06-15 20:30:00\n      12.41\n      10.650000\n      5.254\n    \n    \n      2000-06-15 21:00:00\n      12.27\n      0.320000\n      5.164\n    \n  \n\n\n\n\n\n\n\nadds a mask which includes a random gap\n\nsource\n\n\n\n\n MaskedDf (*args)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ndef _make_random_gap(\n    gap_length: int, # The length of the gap\n    total_length: int, # The total number of observations\n    gap_start: int = None # Optional start of gap\n): # (total_length) array of bools to indicicate if the data is missing or not\n    \"Add a continous gap of ginve length at random position\"\n    if(gap_length >= total_length):\n        return np.repeat(True, total_length)\n    gap_start = np.random.randint(total_length - gap_length) if gap_start is None else gap_start\n    return np.hstack([\n        np.repeat(False, gap_start),\n        np.repeat(True, gap_length),\n        np.repeat(False, total_length - (gap_length + gap_start))\n    ])\n\n\nsource\n\n\n\n\n AddGapTransform (variables, gap_length)\n\nAdds a random gap to a dataframe\n\na_gap = AddGapTransform(['TA', 'VPD'], 5)\na_gap\n\nAddGapTransform -- {'variables': ['TA', 'VPD'], 'gap_length': 5}:\nencodes: (DataFrame,object) -> encodes\ndecodes: \n\n\n\na_gap(blk(800))\n\n\nMasked Df  data \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      14.6500\n      468.1900\n      6.4540\n    \n    \n      14.2200\n      224.8000\n      5.7990\n    \n    \n      14.1100\n      195.2800\n      6.5770\n    \n    \n      14.2300\n      244.1700\n      6.9310\n    \n    \n      14.4000\n      253.9200\n      7.2860\n    \n    \n      14.0900\n      177.3100\n      7.2510\n    \n    \n      13.7100\n      97.0700\n      6.6830\n    \n    \n      13.0800\n      39.7100\n      5.8510\n    \n    \n      12.4100\n      10.6500\n      5.2540\n    \n    \n      12.2700\n      0.3200\n      5.1640\n    \n  \n\n  mask \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      False\n      True\n      False\n    \n    \n      False\n      True\n      False\n    \n    \n      False\n      True\n      False\n    \n    \n      False\n      True\n      False\n    \n    \n      False\n      True\n      False\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n  \n\n \n\n\n\nm_df = a_gap(blk(800))\n\n\ndisplay_as_row({'data': m_df.data, 'mask': m_df.mask})\n\n\n  data \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      14.6500\n      468.1900\n      6.4540\n    \n    \n      14.2200\n      224.8000\n      5.7990\n    \n    \n      14.1100\n      195.2800\n      6.5770\n    \n    \n      14.2300\n      244.1700\n      6.9310\n    \n    \n      14.4000\n      253.9200\n      7.2860\n    \n    \n      14.0900\n      177.3100\n      7.2510\n    \n    \n      13.7100\n      97.0700\n      6.6830\n    \n    \n      13.0800\n      39.7100\n      5.8510\n    \n    \n      12.4100\n      10.6500\n      5.2540\n    \n    \n      12.2700\n      0.3200\n      5.1640\n    \n  \n\n  mask \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      False\n      True\n      False\n    \n    \n      False\n      True\n      False\n    \n    \n      False\n      True\n      False\n    \n    \n      False\n      True\n      False\n    \n    \n      False\n      True\n      False\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n  \n\n \n\n\n\ntfms2 = TfmdLists([800,801,802,803], [BlockDfTransform(hai, 10), AddGapTransform(['TA','SW_IN'], 2)])\n\n\ntfms2[0]\n\n\nMasked Df  data \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      14.6500\n      468.1900\n      6.4540\n    \n    \n      14.2200\n      224.8000\n      5.7990\n    \n    \n      14.1100\n      195.2800\n      6.5770\n    \n    \n      14.2300\n      244.1700\n      6.9310\n    \n    \n      14.4000\n      253.9200\n      7.2860\n    \n    \n      14.0900\n      177.3100\n      7.2510\n    \n    \n      13.7100\n      97.0700\n      6.6830\n    \n    \n      13.0800\n      39.7100\n      5.8510\n    \n    \n      12.4100\n      10.6500\n      5.2540\n    \n    \n      12.2700\n      0.3200\n      5.1640\n    \n  \n\n  mask \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      False\n      False\n      True\n    \n    \n      False\n      False\n      True\n    \n    \n      True\n      True\n      True\n    \n  \n\n \n\n\n\nsource\n\n\n\n\n MaskedDf.tidy ()\n\n\nm_df.tidy()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_present\n    \n  \n  \n    \n      0\n      2000-06-15 16:30:00\n      TA\n      14.650000\n      False\n    \n    \n      1\n      2000-06-15 17:00:00\n      TA\n      14.220000\n      False\n    \n    \n      2\n      2000-06-15 17:30:00\n      TA\n      14.110000\n      False\n    \n    \n      3\n      2000-06-15 18:00:00\n      TA\n      14.230000\n      False\n    \n    \n      4\n      2000-06-15 18:30:00\n      TA\n      14.400000\n      False\n    \n    \n      5\n      2000-06-15 19:00:00\n      TA\n      14.090000\n      True\n    \n    \n      6\n      2000-06-15 19:30:00\n      TA\n      13.710000\n      True\n    \n    \n      7\n      2000-06-15 20:00:00\n      TA\n      13.080000\n      True\n    \n    \n      8\n      2000-06-15 20:30:00\n      TA\n      12.410000\n      True\n    \n    \n      9\n      2000-06-15 21:00:00\n      TA\n      12.270000\n      True\n    \n    \n      10\n      2000-06-15 16:30:00\n      SW_IN\n      468.190002\n      True\n    \n    \n      11\n      2000-06-15 17:00:00\n      SW_IN\n      224.800003\n      True\n    \n    \n      12\n      2000-06-15 17:30:00\n      SW_IN\n      195.279999\n      True\n    \n    \n      13\n      2000-06-15 18:00:00\n      SW_IN\n      244.169998\n      True\n    \n    \n      14\n      2000-06-15 18:30:00\n      SW_IN\n      253.919998\n      True\n    \n    \n      15\n      2000-06-15 19:00:00\n      SW_IN\n      177.309998\n      True\n    \n    \n      16\n      2000-06-15 19:30:00\n      SW_IN\n      97.070000\n      True\n    \n    \n      17\n      2000-06-15 20:00:00\n      SW_IN\n      39.709999\n      True\n    \n    \n      18\n      2000-06-15 20:30:00\n      SW_IN\n      10.650000\n      True\n    \n    \n      19\n      2000-06-15 21:00:00\n      SW_IN\n      0.320000\n      True\n    \n    \n      20\n      2000-06-15 16:30:00\n      VPD\n      6.454000\n      False\n    \n    \n      21\n      2000-06-15 17:00:00\n      VPD\n      5.799000\n      False\n    \n    \n      22\n      2000-06-15 17:30:00\n      VPD\n      6.577000\n      False\n    \n    \n      23\n      2000-06-15 18:00:00\n      VPD\n      6.931000\n      False\n    \n    \n      24\n      2000-06-15 18:30:00\n      VPD\n      7.286000\n      False\n    \n    \n      25\n      2000-06-15 19:00:00\n      VPD\n      7.251000\n      True\n    \n    \n      26\n      2000-06-15 19:30:00\n      VPD\n      6.683000\n      True\n    \n    \n      27\n      2000-06-15 20:00:00\n      VPD\n      5.851000\n      True\n    \n    \n      28\n      2000-06-15 20:30:00\n      VPD\n      5.254000\n      True\n    \n    \n      29\n      2000-06-15 21:00:00\n      VPD\n      5.164000\n      True\n    \n  \n\n\n\n\n\n\n\n\n\nplot_rug(m_df.tidy())\n\n\n\n\n\n\n\ndf = m_df.tidy()\n\n\ndf = df[df.variable==\"TA\"].copy()\n\n\ndf['row_number'] = df.reset_index().index\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_present\n      row_number\n    \n  \n  \n    \n      0\n      2000-06-15 16:30:00\n      TA\n      14.65\n      False\n      0\n    \n    \n      1\n      2000-06-15 17:00:00\n      TA\n      14.22\n      False\n      1\n    \n    \n      2\n      2000-06-15 17:30:00\n      TA\n      14.11\n      False\n      2\n    \n    \n      3\n      2000-06-15 18:00:00\n      TA\n      14.23\n      False\n      3\n    \n    \n      4\n      2000-06-15 18:30:00\n      TA\n      14.40\n      False\n      4\n    \n    \n      5\n      2000-06-15 19:00:00\n      TA\n      14.09\n      True\n      5\n    \n    \n      6\n      2000-06-15 19:30:00\n      TA\n      13.71\n      True\n      6\n    \n    \n      7\n      2000-06-15 20:00:00\n      TA\n      13.08\n      True\n      7\n    \n    \n      8\n      2000-06-15 20:30:00\n      TA\n      12.41\n      True\n      8\n    \n    \n      9\n      2000-06-15 21:00:00\n      TA\n      12.27\n      True\n      9\n    \n  \n\n\n\n\n\ndf.iloc[1]\n\ntime          2000-06-15 17:00:00\nvariable                       TA\nvalue                       14.22\nis_present                  False\nrow_number                      1\nName: 1, dtype: object\n\n\n\ndf.loc[2, \"is_present\"] = True\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_present\n      row_number\n    \n  \n  \n    \n      0\n      2000-06-15 16:30:00\n      TA\n      14.65\n      False\n      0\n    \n    \n      1\n      2000-06-15 17:00:00\n      TA\n      14.22\n      False\n      1\n    \n    \n      2\n      2000-06-15 17:30:00\n      TA\n      14.11\n      True\n      2\n    \n    \n      3\n      2000-06-15 18:00:00\n      TA\n      14.23\n      False\n      3\n    \n    \n      4\n      2000-06-15 18:30:00\n      TA\n      14.40\n      False\n      4\n    \n    \n      5\n      2000-06-15 19:00:00\n      TA\n      14.09\n      True\n      5\n    \n    \n      6\n      2000-06-15 19:30:00\n      TA\n      13.71\n      True\n      6\n    \n    \n      7\n      2000-06-15 20:00:00\n      TA\n      13.08\n      True\n      7\n    \n    \n      8\n      2000-06-15 20:30:00\n      TA\n      12.41\n      True\n      8\n    \n    \n      9\n      2000-06-15 21:00:00\n      TA\n      12.27\n      True\n      9\n    \n  \n\n\n\n\n\ni = 1\nprev, curr, next = df.iloc[i-1], df.iloc[i], df.iloc[i+1]\n\n\nprev, curr, next\n\n(time          2000-06-15 16:30:00\n variable                       TA\n value                       14.65\n is_present                  False\n row_number                      0\n Name: 0, dtype: object,\n time          2000-06-15 17:00:00\n variable                       TA\n value                       14.22\n is_present                  False\n row_number                      1\n Name: 1, dtype: object,\n time          2000-06-15 17:30:00\n variable                       TA\n value                       14.11\n is_present                   True\n row_number                      2\n Name: 2, dtype: object)\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_present\n      row_number\n    \n  \n  \n    \n      0\n      2000-06-15 16:30:00\n      TA\n      14.65\n      False\n      0\n    \n    \n      1\n      2000-06-15 17:00:00\n      TA\n      14.22\n      False\n      1\n    \n    \n      2\n      2000-06-15 17:30:00\n      TA\n      14.11\n      True\n      2\n    \n    \n      3\n      2000-06-15 18:00:00\n      TA\n      14.23\n      False\n      3\n    \n    \n      4\n      2000-06-15 18:30:00\n      TA\n      14.40\n      False\n      4\n    \n    \n      5\n      2000-06-15 19:00:00\n      TA\n      14.09\n      True\n      5\n    \n    \n      6\n      2000-06-15 19:30:00\n      TA\n      13.71\n      True\n      6\n    \n    \n      7\n      2000-06-15 20:00:00\n      TA\n      13.08\n      True\n      7\n    \n    \n      8\n      2000-06-15 20:30:00\n      TA\n      12.41\n      True\n      8\n    \n    \n      9\n      2000-06-15 21:00:00\n      TA\n      12.27\n      True\n      9\n    \n  \n\n\n\n\n\nfor i in range(len(df)):\n    # handle boundaries\n    prev = df.iloc[i-1].is_present if i>0 else True \n    next = df.iloc[i+1].is_present if i<(len(df)-1) else True \n    curr = df.iloc[i]\n    if not curr.is_present and prev:\n        print(\"gap start\", curr.time)\n    if not curr.is_present and next:\n        print(\"gap end\", curr.time)\n\ngap start 2000-06-15 16:30:00\ngap end 2000-06-15 17:00:00\ngap start 2000-06-15 18:00:00\ngap end 2000-06-15 18:30:00\n\n\n\nsource\n\n\n\n\n\n\n find_gap_limits (df)\n\n\nfind_gap_limits(df)\n\n\n\n\n\n  \n    \n      \n      gap_start\n      gap_end\n    \n  \n  \n    \n      0\n      2000-06-15 16:30:00\n      2000-06-15 17:00:00\n    \n    \n      1\n      2000-06-15 18:00:00\n      2000-06-15 18:30:00\n    \n  \n\n\n\n\n\nsource\n\n\n\n\n plot_missing_area (df, sel=Selection('selector002', SelectionDef({\n                    bind: 'scales',   type: 'interval' })), props={})\n\n\nplot_missing_area(df)\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n plot_points (df, y='value', y_label='', sel=Selection('selector003',\n              SelectionDef({   bind: 'scales',   type: 'interval' })),\n              props={})\n\n\nplot_points(m_df.tidy())\n\n\n\n\n\n\n\n\n\nplot_line(m_df.tidy())\n\n\n\n\n\n\n\n\n\n\nplot_error(m_df.tidy().assign(std=5))\n\n\n\n\n\n\n\n\n\n\nplot_variable(m_df.tidy(), \"TA\", title=\"title TA\")\n\n\n\n\n\n\n\nplot_variable(m_df.tidy().assign(std=.5), \"TA\", title=\"title TA\", error=True)\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n facet_variable (df, n_cols:int=3, bind_interaction:bool=True,\n                 error:bool=False, ys:list=['value', 'value'],\n                 props:dict|None=None)\n\nPlot all values of the column variable in different subplots\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\ntidy dataframe\n\n\nn_cols\nint\n3\n\n\n\nbind_interaction\nbool\nTrue\nWhether the sub-plots for each variable should be connected for zooming/panning\n\n\nerror\nbool\nFalse\nplot error bar\n\n\nys\nlist\n[‘value’, ‘value’]\n\n\n\nprops\ndict | None\nNone\nadditional properties for altair plot (eg. size)\n\n\nReturns\nChart\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n MaskedDf.show (ax=None, ctx=None, n_cols:int=3,\n                bind_interaction:bool=True, props:dict=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nax\nNoneType\nNone\n\n\n\nctx\nNoneType\nNone\n\n\n\nn_cols\nint\n3\n\n\n\nbind_interaction\nbool\nTrue\nWhether the sub-plots for each variable should be connected for zooming/panning\n\n\nprops\ndict\nNone\nadditional properties (eg. size) for altair plot\n\n\nReturns\nChart\n\n\n\n\n\n\nm_df.show(bind_interaction = False)\n\n\n\n\n\n\n\na_gap(blk(799)).show()\n\n\n\n\n\n\n\nidx = L(*blk(1).columns).argwhere(lambda x: x in ['TA','SW_IN'])\n\n\nmask = np.ones_like(blk(1), dtype=bool)\n\n\nmask\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\n\ngap = _make_random_gap(2, 10, 2)\n\n\ngap\n\narray([False, False,  True,  True, False, False, False, False, False,\n       False])\n\n\n\nnp.argwhere(gap)\n\narray([[2],\n       [3]])\n\n\n\nmask[np.argwhere(gap), idx] = False\n\n\nmask\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [False, False,  True],\n       [False, False,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\n\nmask[gap]\n\narray([[False, False,  True],\n       [False, False,  True]])\n\n\n\n\n\nthis needs to handle both the init with a list of items and when the first item is a sequence of list of items\n\nsource\n\n\n\n\n MaskedTensor (*args)\n\nAll the operations on a read-only sequence.\nConcrete subclasses must override new or init, getitem, and len.\n\nsource\n\n\n\n\n MaskedDf2Tensor (enc=None, dec=None, split_idx=None, order=None)\n\nDelegates (__call__,decode,setup) to (encodes,decodes,setups) if split_idx matches\n\nto_t = MaskedDf2Tensor()\n\n\nto_t.setup(tfms2)\n\n\nto_t(tfms2[0])\n\n__main__.MaskedTensor(data=tensor([[1.4650e+01, 4.6819e+02, 6.4540e+00],\n        [1.4220e+01, 2.2480e+02, 5.7990e+00],\n        [1.4110e+01, 1.9528e+02, 6.5770e+00],\n        [1.4230e+01, 2.4417e+02, 6.9310e+00],\n        [1.4400e+01, 2.5392e+02, 7.2860e+00],\n        [1.4090e+01, 1.7731e+02, 7.2510e+00],\n        [1.3710e+01, 9.7070e+01, 6.6830e+00],\n        [1.3080e+01, 3.9710e+01, 5.8510e+00],\n        [1.2410e+01, 1.0650e+01, 5.2540e+00],\n        [1.2270e+01, 3.2000e-01, 5.1640e+00]]), mask=tensor([[ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [False, False,  True],\n        [False, False,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True]]))\n\n\n\nto_t.decode(to_t(tfms2[0]));\n\n\ntfms2[0]\n\n\nMasked Df  data \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      14.6500\n      468.1900\n      6.4540\n    \n    \n      14.2200\n      224.8000\n      5.7990\n    \n    \n      14.1100\n      195.2800\n      6.5770\n    \n    \n      14.2300\n      244.1700\n      6.9310\n    \n    \n      14.4000\n      253.9200\n      7.2860\n    \n    \n      14.0900\n      177.3100\n      7.2510\n    \n    \n      13.7100\n      97.0700\n      6.6830\n    \n    \n      13.0800\n      39.7100\n      5.8510\n    \n    \n      12.4100\n      10.6500\n      5.2540\n    \n    \n      12.2700\n      0.3200\n      5.1640\n    \n  \n\n  mask \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      True\n      True\n      True\n    \n    \n      False\n      False\n      True\n    \n    \n      False\n      False\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n  \n\n \n\n\n\ntype(MaskedDf2Tensor())\n\n__main__.MaskedDf2Tensor\n\n\n\ntfms3 = TfmdLists([800, 801, 802], [BlockDfTransform(hai, 10), AddGapTransform(['TA','SW_IN'], 2), MaskedDf2Tensor()])\n\n\ntfms3[0]\n\n__main__.MaskedTensor(data=tensor([[1.4650e+01, 4.6819e+02, 6.4540e+00],\n        [1.4220e+01, 2.2480e+02, 5.7990e+00],\n        [1.4110e+01, 1.9528e+02, 6.5770e+00],\n        [1.4230e+01, 2.4417e+02, 6.9310e+00],\n        [1.4400e+01, 2.5392e+02, 7.2860e+00],\n        [1.4090e+01, 1.7731e+02, 7.2510e+00],\n        [1.3710e+01, 9.7070e+01, 6.6830e+00],\n        [1.3080e+01, 3.9710e+01, 5.8510e+00],\n        [1.2410e+01, 1.0650e+01, 5.2540e+00],\n        [1.2270e+01, 3.2000e-01, 5.1640e+00]]), mask=tensor([[ True,  True,  True],\n        [ True,  True,  True],\n        [False, False,  True],\n        [False, False,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True]]))\n\n\n\ntype(tfms3[0])\n\n__main__.MaskedTensor\n\n\n\ntfms3.decode(tfms3[0])\n\n\nMasked Df  data \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      14.6500\n      468.1900\n      6.4540\n    \n    \n      14.2200\n      224.8000\n      5.7990\n    \n    \n      14.1100\n      195.2800\n      6.5770\n    \n    \n      14.2300\n      244.1700\n      6.9310\n    \n    \n      14.4000\n      253.9200\n      7.2860\n    \n    \n      14.0900\n      177.3100\n      7.2510\n    \n    \n      13.7100\n      97.0700\n      6.6830\n    \n    \n      13.0800\n      39.7100\n      5.8510\n    \n    \n      12.4100\n      10.6500\n      5.2540\n    \n    \n      12.2700\n      0.3200\n      5.1640\n    \n  \n\n  mask \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      False\n      False\n      True\n    \n    \n      False\n      False\n      True\n    \n    \n      True\n      True\n      True\n    \n  \n\n \n\n\n\n\n\n\ncollections.namedtuple\n\n<function collections.namedtuple(typename, field_names, *, rename=False, defaults=None, module=None)>\n\n\n\nnt = collections.namedtuple(\"nt\", \"a\")\n\n\nisinstance(nt(1), tuple)\n\nTrue\n\n\n\nsource\n\n\n\n\n NormalsParams (*args)\n\nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list. The argument must be an iterable if specified.\n\nNormalsParams(0,1)\n\n__main__.NormalsParams(mean=0, std=1)\n\n\n\nsource\n\n\n\n\n get_stats (df, device='cpu')\n\n\nsource\n\n\n\n\n NormalizeMasked (mean=None, std=None)\n\nNormalize/denorm MaskedTensor column-wise\n\nnorm = NormalizeMasked(*get_stats(hai))\n\n\ntfms3[0]\n\n__main__.MaskedTensor(data=tensor([[1.4650e+01, 4.6819e+02, 6.4540e+00],\n        [1.4220e+01, 2.2480e+02, 5.7990e+00],\n        [1.4110e+01, 1.9528e+02, 6.5770e+00],\n        [1.4230e+01, 2.4417e+02, 6.9310e+00],\n        [1.4400e+01, 2.5392e+02, 7.2860e+00],\n        [1.4090e+01, 1.7731e+02, 7.2510e+00],\n        [1.3710e+01, 9.7070e+01, 6.6830e+00],\n        [1.3080e+01, 3.9710e+01, 5.8510e+00],\n        [1.2410e+01, 1.0650e+01, 5.2540e+00],\n        [1.2270e+01, 3.2000e-01, 5.1640e+00]]), mask=tensor([[ True,  True,  True],\n        [ True,  True,  True],\n        [False, False,  True],\n        [False, False,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True]]))\n\n\n\ntest_close(norm.decode(norm(tfms3[0]))[0], tfms3[0][0], eps=2e-5)\n\nTest that NormalsParams decode actually works\n\nNpars = NormalsParams(torch.tensor(1), torch.tensor(.1))\n\n\nnorm.decode(Npars)\n\n__main__.NormalsParams(mean=tensor([ 16.2585, 324.9604,   7.7491]), std=tensor([ 0.7925, 20.4003,  0.4368]))\n\n\n\ntfms4 = TfmdLists([800,801,803], [BlockDfTransform(hai, 10), \n                           AddGapTransform(['TA','SW_IN'], 2),\n                           MaskedDf2Tensor(),\n                           NormalizeMasked(*get_stats(hai,device='cpu'), ) ])\n\n\ntfms4[0]\n\n__main__.MaskedTensor(data=tensor([[ 0.7970,  1.7021,  0.7035],\n        [ 0.7428,  0.5090,  0.5536],\n        [ 0.7289,  0.3643,  0.7317],\n        [ 0.7440,  0.6040,  0.8127],\n        [ 0.7655,  0.6518,  0.8940],\n        [ 0.7264,  0.2762,  0.8860],\n        [ 0.6784, -0.1171,  0.7560],\n        [ 0.5989, -0.3983,  0.5655],\n        [ 0.5144, -0.5407,  0.4288],\n        [ 0.4967, -0.5914,  0.4082]]), mask=tensor([[ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [False, False,  True],\n        [False, False,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True],\n        [ True,  True,  True]]))\n\n\n\ntfms4.decode(tfms4[0])\n\n\nMasked Df  data \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      14.6500\n      468.1900\n      6.4540\n    \n    \n      14.2200\n      224.8000\n      5.7990\n    \n    \n      14.1100\n      195.2800\n      6.5770\n    \n    \n      14.2300\n      244.1700\n      6.9310\n    \n    \n      14.4000\n      253.9200\n      7.2860\n    \n    \n      14.0900\n      177.3100\n      7.2510\n    \n    \n      13.7100\n      97.0700\n      6.6830\n    \n    \n      13.0800\n      39.7100\n      5.8510\n    \n    \n      12.4100\n      10.6500\n      5.2540\n    \n    \n      12.2700\n      0.3200\n      5.1640\n    \n  \n\n  mask \n\n  \n    \n      TA\n      SW_IN\n      VPD\n    \n  \n  \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      False\n      False\n      True\n    \n    \n      False\n      False\n      True\n    \n    \n      True\n      True\n      True\n    \n    \n      True\n      True\n      True\n    \n  \n\n \n\n\nis workinggggggggggggggggg\n\n\n\nFastai likes to work with tuples … for now convert to a tuple. Should add a decode step and maybe find a way to mimic a tuple in MaskedTensor\n\nsource\n\n\n\n\n to_tuple (x)\n\n\n\n\n\nblock_len = 10\nblock_ids = list(range(0, (len(hai) // block_len) - 1))[:10]\ngap_len = 2\n\n\nsource\n\n\n\n\n imp_pipeline (df, block_len, gap_len)\n\n\npipeline, block_ids = imp_pipeline(hai, block_len, gap_len)\n\n\npipeline\n\n[BlockDfTransform:\n encodes: (int,object) -> encodes\n decodes: ,\n AddGapTransform -- {'variables': ['TA', 'SW_IN'], 'gap_length': 2}:\n encodes: (DataFrame,object) -> encodes\n decodes: ,\n __main__.MaskedDf2Tensor,\n NormalizeMasked -- {'mean': tensor([  8.3339, 120.9578,   3.3807]), 'std': tensor([  7.9246, 204.0026,   4.3684])}:\n encodes: (MaskedTensor,object) -> encodes\n decodes: (MaskedTensor,object) -> decodes\n (NormalsParams,object) -> decodes,\n <function __main__.to_tuple(x)>]\n\n\n\npp = Pipeline(pipeline)\n\n\npp\n\nPipeline: BlockDfTransform -> AddGapTransform -- {'variables': ['TA', 'SW_IN'], 'gap_length': 2} -> MaskedDf2Tensor -> NormalizeMasked -- {'mean': tensor([  8.3339, 120.9578,   3.3807]), 'std': tensor([  7.9246, 204.0026,   4.3684])} -> to_tuple\n\n\n\n\n\nrandom splitter for validation/training set\n\nreset_seed()\n\n\nsplits = RandomSplitter()(block_ids)\n\nRepeat twice the pipeline since is the same pipeline both for training data and for labels\n\nimport collections\n\n\ndef to_tuple(x):\n    return tuple(x)\n\n\nisinstance(tfms4[0], Sequence)\n\nTrue\n\n\n\nds = Datasets(block_ids, [pipeline, pipeline], splits=splits)\n\n\ndls = ds.dataloaders(bs=1)\n\n\ndls.device\n\ndevice(type='cuda', index=0)\n\n\n\ndls.one_batch()\n\n((tensor([[[ 1.1857,  2.3551,  2.1478],\n           [ 1.2122,  2.2766,  2.1885],\n           [ 1.2172,  1.8685,  2.2476],\n           [ 1.2235,  1.4807,  2.2382],\n           [ 1.2008,  0.9235,  2.1695],\n           [ 1.1769,  0.5868,  2.1063],\n           [ 1.1314,  0.2027,  2.0113],\n           [ 1.0544, -0.2726,  1.7872],\n           [ 0.9472, -0.5001,  1.5363],\n           [ 0.8525, -0.5864,  1.3749]]], device='cuda:0'),\n  tensor([[[False, False,  True],\n           [False, False,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True]]], device='cuda:0')),\n (tensor([[[ 1.1857,  2.3551,  2.1478],\n           [ 1.2122,  2.2766,  2.1885],\n           [ 1.2172,  1.8685,  2.2476],\n           [ 1.2235,  1.4807,  2.2382],\n           [ 1.2008,  0.9235,  2.1695],\n           [ 1.1769,  0.5868,  2.1063],\n           [ 1.1314,  0.2027,  2.0113],\n           [ 1.0544, -0.2726,  1.7872],\n           [ 0.9472, -0.5001,  1.5363],\n           [ 0.8525, -0.5864,  1.3749]]], device='cuda:0'),\n  tensor([[[ True,  True,  True],\n           [False, False,  True],\n           [False, False,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True],\n           [ True,  True,  True]]], device='cuda:0')))\n\n\n\n@typedispatch\ndef show_batch(x: MaskedDf, y, samples, ctxs=None, max_n=6):\n    return x\n\n\n# dls.show_batch()\n\n\ndls._types\n\n{tuple: [{tuple: [torch.Tensor, torch.Tensor]},\n  {tuple: [torch.Tensor, torch.Tensor]}]}\n\n\n\nDatasets\n\nfastai.data.core.Datasets\n\n\n\nsource\n\n\n\n\n make_dataloader (df, block_len, gap_len, bs=10)\n\n\ndls = make_dataloader(hai, 200, 10)\n\n\ndls.one_batch()[0][0].shape\n\ntorch.Size([10, 200, 3])\n\n\n\ndls = dls.cpu()"
  },
  {
    "objectID": "kalman/fastai.html#model",
    "href": "kalman/fastai.html#model",
    "title": "Implement Kalman model using FastAI",
    "section": "Model",
    "text": "Model\n\nForward Function\nin order to the a pytorch module we need a forward method to the kalman filter\n\nmodel = KalmanFilter.init_simple(n_dim = hai.shape[-1])\n\n\nmodel._predict_filter(*dls.one_batch()[0]);\n\n\nsource\n\n\nKalmanFilter.forward\n\n KalmanFilter.forward (masked_data:__main__.MaskedTensor)\n\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\n.. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class:Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.\n\ninput = dls.one_batch()[0]\ntarget = dls.one_batch()[1]\n\n\nmodel.state_dict()\n\nOrderedDict([('trans_matrix',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]])),\n             ('trans_off', tensor([0., 0., 0.])),\n             ('trans_cov_raw',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]])),\n             ('obs_matrix',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]])),\n             ('obs_off', tensor([0., 0., 0.])),\n             ('obs_cov_raw',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]])),\n             ('init_state_mean', tensor([0., 0., 0.])),\n             ('init_state_cov_raw',\n              tensor([[1., 0., 0.],\n                      [0., 1., 0.],\n                      [0., 0., 1.]]))])\n\n\n\ndata = input[0][0]\ndata.shape\n\ntorch.Size([200, 3])\n\n\n\nmask = input[1][0]\n\n\nmask.shape\n\ntorch.Size([200, 3])\n\n\n\ndata.device\n\ndevice(type='cpu')\n\n\n\ntorch.device\n\ntorch.device\n\n\n\ndata.shape, mask.shape\n\n(torch.Size([200, 3]), torch.Size([200, 3]))\n\n\n\nmodel.predict(data.unsqueeze(0), mask.unsqueeze(0));\n\n\nmodel.use_smooth = True\n\n\npred = model(input)\n\n\npred[0].shape\n\ntorch.Size([10, 200, 3])\n\n\n\npred[1].shape\n\ntorch.Size([10, 200, 3])\n\n\n\nmodel.use_smooth = False\n\n\npred_filt = model(input)\n\n\npred_filt[1].shape\n\ntorch.Size([10, 200, 3])\n\n\n\npred\n\n__main__.NormalsParams(mean=tensor([[[-0.2289, -0.3637, -0.3121],\n         [-0.3050, -0.4982, -0.4284],\n         [-0.3160, -0.5379, -0.4667],\n         ...,\n         [-0.3424, -0.5090, -0.6207],\n         [-0.3303, -0.5105, -0.6043],\n         [-0.3213, -0.4977, -0.5894]],\n\n        [[-0.4270, -0.3664, -0.4671],\n         [-0.5904, -0.5064, -0.6465],\n         [-0.6533, -0.5599, -0.7156],\n         ...,\n         [-0.3523, -0.5929, -0.7021],\n         [-0.3310, -0.5929, -0.6569],\n         [-0.3109, -0.5929, -0.6206]],\n\n        [[ 0.1127,  0.4905, -0.3426],\n         [ 0.1886,  0.5478, -0.4317],\n         [ 0.2603,  0.8844, -0.4022],\n         ...,\n         [ 1.1013,  0.0036,  0.0374],\n         [ 1.1363,  0.0922,  0.0757],\n         [ 1.1496,  0.0142,  0.0759]],\n\n        ...,\n\n        [[-0.3895, -0.3659, -0.4285],\n         [-0.5319, -0.5048, -0.5966],\n         [-0.5825, -0.5555, -0.6701],\n         ...,\n         [-0.1904,  0.2193, -0.6186],\n         [-0.0656,  0.4846, -0.5678],\n         [ 0.0168,  0.7537, -0.5344]],\n\n        [[-0.6131, -0.3664, -0.4730],\n         [-0.8432, -0.5064, -0.6527],\n         [-0.9329, -0.5599, -0.7213],\n         ...,\n         [-0.8646, -0.5929, -0.6753],\n         [-0.8524, -0.5929, -0.6702],\n         [-0.8391, -0.5929, -0.6616]],\n\n        [[-1.1085, -0.3664, -0.4620],\n         [-1.5280, -0.5064, -0.6383],\n         [-1.6896, -0.5599, -0.7056],\n         ...,\n         [-1.0616, -0.5929, -0.7273],\n         [-1.0597, -0.5929, -0.7252],\n         [-1.0620, -0.5929, -0.7250]]], grad_fn=<SqueezeBackward1>), std=tensor([[[1.1756, 1.1756, 1.1756],\n         [1.1990, 1.1990, 1.1990],\n         [1.2024, 1.2024, 1.2024],\n         ...,\n         [1.2045, 1.2045, 1.2045],\n         [1.2133, 1.2133, 1.2133],\n         [1.2720, 1.2720, 1.2720]],\n\n        [[1.1756, 1.1756, 1.1756],\n         [1.1990, 1.1990, 1.1990],\n         [1.2024, 1.2024, 1.2024],\n         ...,\n         [1.2045, 1.2045, 1.2045],\n         [1.2133, 1.2133, 1.2133],\n         [1.2720, 1.2720, 1.2720]],\n\n        [[1.1756, 1.1756, 1.1756],\n         [1.1990, 1.1990, 1.1990],\n         [1.2024, 1.2024, 1.2024],\n         ...,\n         [1.2045, 1.2045, 1.2045],\n         [1.2133, 1.2133, 1.2133],\n         [1.2720, 1.2720, 1.2720]],\n\n        ...,\n\n        [[1.1756, 1.1756, 1.1756],\n         [1.1990, 1.1990, 1.1990],\n         [1.2024, 1.2024, 1.2024],\n         ...,\n         [1.2045, 1.2045, 1.2045],\n         [1.2133, 1.2133, 1.2133],\n         [1.2720, 1.2720, 1.2720]],\n\n        [[1.1756, 1.1756, 1.1756],\n         [1.1990, 1.1990, 1.1990],\n         [1.2024, 1.2024, 1.2024],\n         ...,\n         [1.2045, 1.2045, 1.2045],\n         [1.2133, 1.2133, 1.2133],\n         [1.2720, 1.2720, 1.2720]],\n\n        [[1.1756, 1.1756, 1.1756],\n         [1.1990, 1.1990, 1.1990],\n         [1.2024, 1.2024, 1.2024],\n         ...,\n         [1.2045, 1.2045, 1.2045],\n         [1.2133, 1.2133, 1.2133],\n         [1.2720, 1.2720, 1.2720]]], grad_fn=<SqrtBackward0>))\n\n\n\ntype(pred), type(pred_filt)\n\n(__main__.NormalsParams, __main__.NormalsParams)\n\n\n\ntest_ne(pred, pred_filt)\n\n\n\nLoss Function\nadd support for complete loss (also outside gap) and for filter loss (don’t run the smooher)\nThere are two ways to compute the loss, one is to do it for all predictions the other is for doing it for only the gap - only_gap\nPlay around with flatting + diagonal\n\na = torch.diag(torch.tensor([1,2,3]))\nd = torch.stack([a, a*10])\nm = torch.stack([a.diag(), a.diag()*10])\nd\n\ntensor([[[ 1,  0,  0],\n         [ 0,  2,  0],\n         [ 0,  0,  3]],\n\n        [[10,  0,  0],\n         [ 0, 20,  0],\n         [ 0,  0, 30]]])\n\n\n\nm.flatten()\n\ntensor([ 1,  2,  3, 10, 20, 30])\n\n\n\nd\n\ntensor([[[ 1,  0,  0],\n         [ 0,  2,  0],\n         [ 0,  0,  3]],\n\n        [[10,  0,  0],\n         [ 0, 20,  0],\n         [ 0,  0, 30]]])\n\n\n\ntorch.diagonal(d, dim1=1, dim2=2).flatten()\n\ntensor([ 1,  2,  3, 10, 20, 30])\n\n\n\nmeans, stds = pred\ndata, mask = target\n\n\n# make a big matrix with all variables and observations and compute ll\nmask = mask.flatten() \nobs = data.flatten()[mask]\nmeans = data.flatten()[mask]\nstds = stds.flatten()[mask] # need to support batches\n\nMultivariateNormal(means, torch.diag(stds)).log_prob(obs)\n\ntensor(-5930.4453, grad_fn=<SubBackward0>)\n\n\n\nsource\n\n\nKalmanLoss\n\n KalmanLoss (only_gap:bool=True, reduction:str='sum')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nonly_gap\nbool\nTrue\nloss for all predictions or only gap\n\n\nreduction\nstr\nsum\none of [‘sum’, ‘mean’, ‘none’]\n\n\n\n\npred = model(input)\n\n\ndata, mask = input\n\n\ndata.shape, mask.shape\n\n(torch.Size([10, 200, 3]), torch.Size([10, 200, 3]))\n\n\n\npred.mean.shape\n\ntorch.Size([10, 200, 3])\n\n\n\nmeans, stds = pred\n\n\nstds.shape\n\ntorch.Size([10, 200, 3])\n\n\n\nmeans.shape\n\ntorch.Size([10, 200, 3])\n\n\n\ndata.isnan().any()\n\ntensor(False)\n\n\n\nmask.isnan().any()\n\ntensor(False)\n\n\n\nmeans.isnan().any()\n\ntensor(False)\n\n\n\nstds.isnan().sum()\n\ntensor(0)\n\n\n\nstds.shape\n\ntorch.Size([10, 200, 3])\n\n\n\nis_posdef_eigv(torch.diag(stds.flatten()))\n\n(tensor(True),\n tensor([1.4142, 1.4142, 1.4142,  ..., 3.5522, 3.5522, 3.5522],\n        grad_fn=<LinalgEighBackward0>))\n\n\n\nKalmanLoss(only_gap=True)(pred, target)\n\ntensor(9620.4248, grad_fn=<SumBackward0>)\n\n\n\nKalmanLoss(only_gap=False)(pred, target)\n\ntensor(9901.6641, grad_fn=<SumBackward0>)\n\n\n\npred.mean.device, target[0].device\n\n(device(type='cpu'), device(type='cpu'))\n\n\n\npred.mean.shape, target[0].shape\n\n(torch.Size([10, 200, 3]), torch.Size([10, 200, 3]))\n\n\n\ntarget[0].shape\n\ntorch.Size([10, 200, 3])\n\n\n\nKalmanLoss(only_gap=False, reduction='mean')(pred, target)\n\ntensor(990.1664, grad_fn=<MeanBackward0>)\n\n\n\n\nMetrics\nWrapper around fastai metrics to support masked tensors and normal distributions\n\nsource\n\n\nto_msk_metric\n\n to_msk_metric (metric, name)\n\n\nmsk_rmse.__name__\n\n'rmse'\n\n\n\nmsk_rmse(pred, target)\n\nTensorBase(1.2510)\n\n\n\nmsk_r2(pred, target)\n\n-0.3594884242364691\n\n\n\n\nCallback\nsave the model state\n\nsource\n\n\nSaveParams\n\n SaveParams (param_name)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\nSaveParams\n\n SaveParams (param_name)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\ndebug_preds = []\n\n\nclass DebugPredCallback(Callback):\n    order = 0\n    def after_validate(self):\n        if hasattr(self, 'gather_preds'):\n            debug_preds.append(self.gather_preds.preds)\n\n\n\nLearner\n\nobs_cov_history = SaveParams('obs_cov')\n\n\nall_data = CollectDataCallback()\n\n\nmodel = KalmanFilter.init_random(n_dim_obs = hai.shape[1], n_dim_state = hai.shape[1]).cuda()\n\n\nmodel.use_smooth = False\n\n\n# model._set_constraint('obs_cov', model.obs_cov, train=False)\n\n\npipeline, block_ids = imp_pipeline(hai[:20000], block_len, gap_len)\n    \nsplits = RandomSplitter()(block_ids)\nds = Datasets(block_ids, [pipeline, pipeline], splits=splits)\n\n\ndls = ds.dataloaders(bs=10, device='cuda')\n\n\ndls.one_batch()[0][0].device\n\ndevice(type='cuda', index=0)\n\n\n\ninput, target = dls.one_batch()\n\n\npred = model(input)\nKalmanLoss()(pred, target)\n\ntensor(2270.8555, device='cuda:0', grad_fn=<SumBackward0>)\n\n\n\nlearn = Learner(dls, model, loss_func=KalmanLoss(only_gap=False), cbs = [DebugPredCallback] , metrics = [msk_rmse, msk_r2])\n\n\nlearn.fit(1, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      rmse\n      r2\n      time\n    \n  \n  \n    \n      0\n      767.793274\n      621.327209\n      2.006266\n      -5.094438\n      00:10\n    \n  \n\n\n\n\nFloat64\n\nmodel64 = KalmanFilter.init_random(hai.shape[1], hai.shape[1], dtype=torch.float64).cuda()\n\n\nsource\n\n\n\nFloat64Callback\n\n Float64Callback (after_create=None, before_fit=None, before_epoch=None,\n                  before_train=None, before_batch=None, after_pred=None,\n                  after_loss=None, before_backward=None,\n                  after_cancel_backward=None, after_backward=None,\n                  before_step=None, after_cancel_step=None,\n                  after_step=None, after_cancel_batch=None,\n                  after_batch=None, after_cancel_train=None,\n                  after_train=None, before_validate=None,\n                  after_cancel_validate=None, after_validate=None,\n                  after_cancel_epoch=None, after_epoch=None,\n                  after_cancel_fit=None, after_fit=None)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\ndls64 = make_dataloader(hai64, 10, 2, bs=10)\n\n\ninput64 = dls64.one_batch()[0]\ntarget64 = dls64.one_batch()[1]\n\n\ndata64, mask64 = input64\n\n\ndata64.device, data64.dtype\n\n(device(type='cuda', index=0), torch.float64)\n\n\n\nmodel64.predict(data64);\n\n\npred = model64(input)\n\n\nKalmanLoss()(pred, target)\n\ntensor(340.9094, device='cuda:0', grad_fn=<SumBackward0>)\n\n\n\nmodel64.use_smooth = False\n\n\nlearn64 = Learner(dls64, model64, loss_func=KalmanLoss(), cbs = [Float64Callback] )\n\n\nlearn64.fit(1, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      95.226314\n      88.306383\n      01:41\n    \n  \n\n\n\n\n\nPredictions\nThe transformation pipeline is not working properly (there is a problem in decode_batch as the _types are more nested than the predictions, which results in an error) + the pipeline is anyway not reproducible + the test dataloaders seems that they are actually not deterministic ….. soo reimplement everything almost from scratch\nsee https://github.com/mone27/meteo_imp/blob/0335003405ec9bd3e3bd2641bc6d7924f34a0788/lib_nbs/kalman/10_fastai.ipynb for all details\n\nsource\n\n\nNormalsDf\n\n NormalsDf (mean, std)\n\nDataFrames of Normal parameters (mean and std)\n\npipe0, pipe1 = tfms4.fs[0,1], tfms4.fs[2,3]\n\n\npipe0, pipe1\n\n((#2) [BlockDfTransform:\nencodes: (int,object) -> encodes\ndecodes: ,AddGapTransform -- {'variables': ['TA', 'SW_IN'], 'gap_length': 2}:\nencodes: (DataFrame,object) -> encodes\ndecodes: ],\n (#2) [MaskedDf2Tensor:\nencodes: (MaskedDf,object) -> encodes\ndecodes: (MaskedTensor,object) -> decodes\n,NormalizeMasked -- {'mean': tensor([  8.3339, 120.9578,   3.3807]), 'std': tensor([  7.9246, 204.0026,   4.3684])}:\nencodes: (MaskedTensor,object) -> encodes\ndecodes: (MaskedTensor,object) -> decodes\n(NormalsParams,object) -> decodes\n])\n\n\n\nsource\n\n\npreds2df\n\n preds2df (preds, targs)\n\nFinal step to decode preds by getting a dataframe\n\nsource\n\n\npredict_items\n\n predict_items (items, learn, pipe0, pipe1)\n\n\npreds, targs, losses = predict_items([0,1,3], learn, pipe0, pipe1)\n\nthis is the same data!!\n\npredict_items([0], learn, pipe0, pipe1)[1][0].data == predict_items([0], learn, pipe0, pipe1)[1][0].data\n\n\n\n\n\n  \n    \n      \n      TA\n      SW_IN\n      VPD\n    \n    \n      time\n      \n      \n      \n    \n  \n  \n    \n      2000-01-01 00:30:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 01:00:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 01:30:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 02:00:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 02:30:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 03:00:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 03:30:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 04:00:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 04:30:00\n      True\n      True\n      True\n    \n    \n      2000-01-01 05:00:00\n      True\n      True\n      True\n    \n  \n\n\n\n\n\nPlot results\n\nsource\n\n\n\nplot_result\n\n plot_result (pred, targ, loss, **kwargs)\n\n\ny = \"mean\"\n\n\nplot_result(preds[0], targs[0], torch.tensor(1))\n\n\n\n\n\n\n\nsource\n\n\nplot_results\n\n plot_results (preds, targs, losses, **kwargs)\n\n\nplot_results(preds, targs, losses)\n\n\n\n\n\n\n\nShow Results\n\nrandom.choices(learn.dls.items, k=3)\n\n[503, 1855, 1365]\n\n\n\nsource\n\n\n\nget_results\n\n get_results (learn, n=3, items=None, dls=None)\n\n\nsource\n\n\nshow_results\n\n show_results (learn, n=3, items=None, **kwargs)\n\n\nlearn.model.use_smooth = False\n\n\nshow_results(learn)\n\n\n\n\n\n\n\nshow_results(learn, items=[1,2,3])\n\n\n\n\n\n\n\ndisplay_as_row(learn.model.get_info())\n\n\n  trans_matrix (A) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.7677\n      0.8855\n      0.2192\n    \n    \n      z_1\n      0.8028\n      0.1462\n      0.3398\n    \n    \n      z_2\n      0.1828\n      0.8168\n      0.1924\n    \n  \n\n  trans_cov (Q) \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      0.4421\n      0.8650\n      0.7292\n    \n    \n      z_1\n      0.8650\n      1.7721\n      1.4024\n    \n    \n      z_2\n      0.7292\n      1.4024\n      1.6913\n    \n  \n\n  trans_off \n\n  \n    \n      latent\n      offset\n    \n  \n  \n    \n      z_0\n      0.5653\n    \n    \n      z_1\n      0.1405\n    \n    \n      z_2\n      0.8181\n    \n  \n\n  obs_matrix (H) \n\n  \n    \n      variable\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      x_0\n      0.1918\n      0.3717\n      -0.0126\n    \n    \n      x_1\n      0.2597\n      0.4930\n      0.9982\n    \n    \n      x_2\n      0.1968\n      0.9201\n      0.9421\n    \n  \n\n  obs_cov (R) \n\n  \n    \n      variable\n      x_0\n      x_1\n      x_2\n    \n  \n  \n    \n      x_0\n      1.2067\n      1.1546\n      1.0598\n    \n    \n      x_1\n      1.1546\n      1.1593\n      1.1029\n    \n    \n      x_2\n      1.0598\n      1.1029\n      1.2102\n    \n  \n\n  obs_off \n\n  \n    \n      variable\n      offset\n    \n  \n  \n    \n      x_0\n      0.7426\n    \n    \n      x_1\n      0.5680\n    \n    \n      x_2\n      0.5610\n    \n  \n\n  init_state_mean \n\n  \n    \n      latent\n      mean\n    \n  \n  \n    \n      z_0\n      0.0635\n    \n    \n      z_1\n      0.6929\n    \n    \n      z_2\n      0.9313\n    \n  \n\n  init_state_cov \n\n  \n    \n      latent\n      z_0\n      z_1\n      z_2\n    \n  \n  \n    \n      z_0\n      1.1366\n      0.9955\n      0.6543\n    \n    \n      z_1\n      0.9955\n      1.1963\n      0.6032\n    \n    \n      z_2\n      0.6543\n      0.6032\n      1.3697\n    \n  \n\n \n\n\n\nInteractive\n\nsource\n\n\n\nresults_custom_gap\n\n results_custom_gap (learn, df, var_sel, gap_len, items_idx, block_len)\n\n\nplot_results(*results_custom_gap(learn64, hai64, ['TA'], 10, [800, 801], 200))\n\n\n\n\n\n\n\nsource\n\n\ninteract_results\n\n interact_results (learn, df)\n\n\ninteract_results(learn64, hai64)\n\n\n\n\n<function __main__.interact_results.<locals>._inner(gap_len, items_idx, block_len, **var_names)>"
  },
  {
    "objectID": "kalman/fastai.html#export",
    "href": "kalman/fastai.html#export",
    "title": "Implement Kalman model using FastAI",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "data_preparation.html#data-generator",
    "href": "data_preparation.html#data-generator",
    "title": "Data Preparation",
    "section": "Data generator",
    "text": "Data generator\ngenerate some fake data in order to test the imputation\nWhat is does is: - take a function to generate the “true” latent - use some random coefficient to generate all the N features - add some random noise\n\nclass GPFADataGenerator:\n    def __init__(self,\n                    n_features: int,\n                    n_obs: int,\n                    latent_func = lambda x: torch.sin(3*x), # Functions used to generate the true latent\n                    noise_std = .2,\n                    Lambda = None\n                ):\n        \n        self.n_features, self.n_obs = n_features, n_obs\n        self.time = torch.arange(0, self.n_obs, dtype=torch.float)\n        \n        self.latent = latent_func(self.time)\n        \n        self.Lambda = torch.tensor(Lambda).reshape(n_features, 1) if Lambda is not None else torch.rand(n_features, 1)\n        \n        self.exact_X = (self.Lambda * self.latent).T\n        \n        self.X =  self.exact_X + torch.normal(0., noise_std, size = (n_obs, n_features)) \n        \n        self.data = pd.DataFrame(self.X.numpy(), columns = [f\"x{i}\" for i in range(self.n_features)])\n\n\nsource\n\nGPFADataGenerator\n\n GPFADataGenerator (n_features:int, n_obs:int, latent_func=<function\n                    <lambda>>, noise_std=0.2, Lambda=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nint\n\n\n\n\nn_obs\nint\n\n\n\n\nlatent_func\nfunction\n\nFunctions used to generate the true latent\n\n\nnoise_std\nfloat\n0.2\n\n\n\nLambda\nNoneType\nNone\n\n\n\n\n\nfdg = GPFADataGenerator(3, 4)\n\n\nfd_plot = pd.DataFrame(fdg.X.numpy(), columns = [\"x1\", \"x2\", \"x3\"])\nfd_plot[\"latent\"] = pd.Series(fdg.latent.numpy())\n\n\nfd_plot.plot()\n\n<AxesSubplot: >"
  },
  {
    "objectID": "data_preparation.html#missing-data",
    "href": "data_preparation.html#missing-data",
    "title": "Data Preparation",
    "section": "Missing Data",
    "text": "Missing Data\n\nclass MeteoDataTest:\n    \"Utility class to keep track of dataset, missing data and export to right format\"\n    def __init__(self, data: pd.DataFrame):\n        \" Init with provided dataset\"\n        self.data = data.copy()\n        self.data_complete = self.data.copy()\n        self.n_features, self.n_obs = data.shape[1], data.shape[0]\n        self.time = torch.arange(0, self.n_obs, dtype=torch.float)\n    @classmethod\n    def generate_gpfa(cls, *args, **kwargs):\n        generator = GPFADataGenerator(*args, **kwargs)\n        self = MeteoDataTest(generator.data)\n        self.generator = generator\n        return self\n\n\nsource\n\nMeteoDataTest\n\n MeteoDataTest (data:pandas.core.frame.DataFrame)\n\nUtility class to keep track of dataset, missing data and export to right format\n\n\nMissing Data\ngenerate artificial gaps in the data\n\nMissing at Random\n\n@patch()\ndef add_random_missing(self: MeteoDataTest,\n                       prob_miss_row: float = .2,  #  Probability an entire row is missing\n                       prob_miss_value: float = .1  # Probability a single observation is missing\n                       ):\n    \"\"\"Make some row and same values randomly missing \"\"\"\n    # keep the original data\n        \n    self.is_miss_row = torch.rand(self.n_obs) <= prob_miss_row\n    \n    self.data[self.is_miss_row.numpy()] = np.nan\n    \n    self.is_miss_value = (torch.rand(self.n_obs * self.n_features) <= prob_miss_value).reshape(-1, self.n_features)\n    \n    self.data[self.is_miss_value.numpy()] = np.nan\n    \n    return self\n\n\nsource\n\n\n\nMeteoDataTest.add_random_missing\n\n MeteoDataTest.add_random_missing (prob_miss_row:float=0.2,\n                                   prob_miss_value:float=0.1)\n\nMake some row and same values randomly missing\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob_miss_row\nfloat\n0.2\nProbability an entire row is missing\n\n\nprob_miss_value\nfloat\n0.1\nProbability a single observation is missing\n\n\n\n\nfd = MeteoDataTest.generate_gpfa(3, 4)\n\n\nfd.add_random_missing().data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x2\n    \n  \n  \n    \n      0\n      -0.001938\n      -0.127557\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      0.137062\n      0.151774\n      -0.092625\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nMeteoDataTest.generate_gpfa(2, 10).add_random_missing(prob_miss_value = .7, prob_miss_row=.0).data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      -0.203655\n      -0.117674\n    \n    \n      1\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      -0.111792\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n    \n    \n      5\n      NaN\n      0.112875\n    \n    \n      6\n      NaN\n      NaN\n    \n    \n      7\n      NaN\n      NaN\n    \n    \n      8\n      NaN\n      NaN\n    \n    \n      9\n      NaN\n      0.409263\n    \n  \n\n\n\n\n\nContinous gap\nThe missing data is all clustered around a gap\nparameteres: - gap length - variable for gap\n\ndef _make_random_gap(\n    gap_length: int, # The length of the gap\n    total_length: int, # The total number of observations\n    gap_start: int = None # Optional start of gap\n): # (total_length) array of bools to indicicate if the data is missing or not\n    \"Add a continous gap of ginve length at random position\"\n    if(gap_length >= total_length):\n        return np.repeat(True, total_length)\n    gap_start = np.random.randint(total_length - gap_length) if gap_start is None else gap_start\n    return np.hstack([\n        np.repeat(False, gap_start),\n        np.repeat(True, gap_length),\n        np.repeat(False, total_length - (gap_length + gap_start))\n    ])\n\n\n_make_random_gap(3, 10)\n\narray([False, False, False, False, False,  True,  True,  True, False,\n       False])\n\n\n\ntest_eq(_make_random_gap(3, 10).sum(), 3) # correct gap length\n\n\n@patch\ndef add_gap(self: MeteoDataTest,\n            gap_length:int,  # length of gap\n            variables: Collection[str],  # variables that should be affected by the gap\n            gap_start: int = None  # Optional start of the gap\n            ):\n    \n    \n    self.is_gap = _make_random_gap(gap_length, self.data.shape[0], gap_start)\n    self.data.loc[self.is_gap, variables] = np.nan\n    return self\n\n\nsource\n\n\n\nMeteoDataTest.add_gap\n\n MeteoDataTest.add_gap (gap_length:int, variables:Collection[str],\n                        gap_start:int=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ngap_length\nint\n\nlength of gap\n\n\nvariables\nCollection\n\nvariables that should be affected by the gap\n\n\ngap_start\nint\nNone\nOptional start of the gap\n\n\n\n\nMeteoDataTest.generate_gpfa(5, 10).add_gap(4, [\"x1\", \"x2\"]).data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x2\n      x3\n      x4\n    \n  \n  \n    \n      0\n      0.147414\n      -0.162167\n      0.534020\n      -0.314157\n      -0.145421\n    \n    \n      1\n      0.227970\n      -0.250645\n      -0.280364\n      0.298646\n      0.023241\n    \n    \n      2\n      -0.277233\n      NaN\n      NaN\n      0.011927\n      0.350378\n    \n    \n      3\n      0.081553\n      NaN\n      NaN\n      0.091917\n      0.162622\n    \n    \n      4\n      -0.488591\n      NaN\n      NaN\n      -0.383358\n      -0.142712\n    \n    \n      5\n      0.850323\n      NaN\n      NaN\n      0.378259\n      -0.198052\n    \n    \n      6\n      -0.554473\n      -0.438560\n      -0.345700\n      -0.368945\n      -0.081258\n    \n    \n      7\n      0.904893\n      0.390038\n      0.473491\n      0.916413\n      -0.053904\n    \n    \n      8\n      -0.463149\n      -0.567804\n      -0.318116\n      -0.687993\n      0.118367\n    \n    \n      9\n      0.931138\n      0.398865\n      0.561607\n      0.701557\n      0.026598\n    \n  \n\n\n\n\n\n\nSave as DataFrame\n\n@patch\ndef tidy_df(self: MeteoDataTest,\n            complete = False,  # full dataset (False) or the one with missing data (True)\n            is_missing = False  # add flag whether value is missing\n            ):\n    \n    df = self.data if not complete else self.data_complete # no need to copy here because next lines does a copy anyway\n    df = df.assign(time = self.time.numpy())\n        \n    df = df.melt(\"time\")\n    \n    if is_missing: df = df.assign(is_missing = self.data.melt().value.isna()) #missing data is not from complete data\n        \n    return df\n\n\nsource\n\n\nMeteoDataTest.tidy_df\n\n MeteoDataTest.tidy_df (complete=False, is_missing=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncomplete\nbool\nFalse\nfull dataset (False) or the one with missing data (True)\n\n\nis_missing\nbool\nFalse\nadd flag whether value is missing\n\n\n\n\nfd.tidy_df()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n    \n    \n      1\n      1.0\n      x0\n      NaN\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n    \n    \n      3\n      3.0\n      x0\n      NaN\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n    \n    \n      5\n      1.0\n      x1\n      NaN\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n    \n    \n      7\n      3.0\n      x1\n      NaN\n    \n    \n      8\n      0.0\n      x2\n      NaN\n    \n    \n      9\n      1.0\n      x2\n      NaN\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n    \n    \n      11\n      3.0\n      x2\n      NaN\n    \n  \n\n\n\n\n\nfd.tidy_df(complete=True)\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n    \n    \n      1\n      1.0\n      x0\n      0.021966\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n    \n    \n      3\n      3.0\n      x0\n      0.031551\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n    \n    \n      5\n      1.0\n      x1\n      0.295225\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n    \n    \n      7\n      3.0\n      x1\n      0.116207\n    \n    \n      8\n      0.0\n      x2\n      0.139568\n    \n    \n      9\n      1.0\n      x2\n      0.029323\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n    \n    \n      11\n      3.0\n      x2\n      0.160619\n    \n  \n\n\n\n\n\nfd.tidy_df(complete=False, is_missing=True)\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_missing\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n      False\n    \n    \n      1\n      1.0\n      x0\n      NaN\n      True\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n      False\n    \n    \n      3\n      3.0\n      x0\n      NaN\n      True\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n      False\n    \n    \n      5\n      1.0\n      x1\n      NaN\n      True\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n      False\n    \n    \n      7\n      3.0\n      x1\n      NaN\n      True\n    \n    \n      8\n      0.0\n      x2\n      NaN\n      True\n    \n    \n      9\n      1.0\n      x2\n      NaN\n      True\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n      False\n    \n    \n      11\n      3.0\n      x2\n      NaN\n      True\n    \n  \n\n\n\n\nThe export as a dataframe is working correctly with the missing data\n\nfd_df = fd.tidy_df()\n\n\nalt.Chart(fd_df).mark_line(point=True).encode(\n    x = \"time\",\n    y = \"value\",\n    color = \"variable\"\n)\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nfd.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x2\n    \n  \n  \n    \n      0\n      -0.001938\n      -0.127557\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      0.137062\n      0.151774\n      -0.092625\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nsource\n\n\nMeteoDataTest.data_compl_tidy\n\n MeteoDataTest.data_compl_tidy ()\n\n\nfd.data_compl_tidy\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      value\n      is_missing\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.001938\n      False\n    \n    \n      1\n      1.0\n      x0\n      0.021966\n      True\n    \n    \n      2\n      2.0\n      x0\n      0.137062\n      False\n    \n    \n      3\n      3.0\n      x0\n      0.031551\n      True\n    \n    \n      4\n      0.0\n      x1\n      -0.127557\n      False\n    \n    \n      5\n      1.0\n      x1\n      0.295225\n      True\n    \n    \n      6\n      2.0\n      x1\n      0.151774\n      False\n    \n    \n      7\n      3.0\n      x1\n      0.116207\n      True\n    \n    \n      8\n      0.0\n      x2\n      0.139568\n      True\n    \n    \n      9\n      1.0\n      x2\n      0.029323\n      True\n    \n    \n      10\n      2.0\n      x2\n      -0.092625\n      False\n    \n    \n      11\n      3.0\n      x2\n      0.160619\n      True"
  },
  {
    "objectID": "data_preparation.html#standard-scaler",
    "href": "data_preparation.html#standard-scaler",
    "title": "Data Preparation",
    "section": "Standard Scaler",
    "text": "Standard Scaler\nThe different variables in the can have pretty different values so we standardize so they are more comparable. Have numbers between 0 and 1 should also help with the computation accuracy.\nOne additional complexity is the need to backtransform not only the mean but also the standard deviation.\nSo we need a but of math\n\\[x_{norm} = \\frac{x - \\mu_x}{\\sigma_x}\\] then \\[x = x_{norm}\\sigma_x + \\mu_x \\]\nusing properties of Guassian distributions 1\n\\[p(x_{norm}) = \\mathcal{N}(\\mu_{norm}, \\sigma^2_{norm})\\]\n\\[p(x) = \\mathcal{N}(\\sigma_x\\mu_{norm} + \\mu_x, \\sigma^2_x \\sigma^2_{norm})\\]\n\nsource\n\nStandardScaler\n\n StandardScaler (x:torch.Tensor)\n\nInit normalizer by storing mean and std dev\n\n\n\n\nType\nDetails\n\n\n\n\nx\nTensor\nup to 2D Tensor\n\n\n\n\nx = torch.randn(20).reshape(-1,2)\nnorm = StandardScaler(x)\ntest_close(x, norm.inverse_transform(norm.transform(x)))\ntest_close(x.std(axis=0), norm.inverse_transform_std(norm.transform(x).std(axis=0)))\n\n\nnx = x.clone()\nnx[3] = torch.nan\nStandardScaler(nx).transform(nx)\n\ntensor([[ 1.0434, -0.7018],\n        [-0.3413, -0.2264],\n        [ 0.7216, -0.9768],\n        [    nan,     nan],\n        [ 0.3866,  0.1091],\n        [-1.5779,  0.1705],\n        [ 0.1036, -1.2577],\n        [-1.6543,  1.3312],\n        [ 0.7548,  1.7514],\n        [ 0.5635, -0.1993]])"
  },
  {
    "objectID": "data_preparation.html#log-transform",
    "href": "data_preparation.html#log-transform",
    "title": "Data Preparation",
    "section": "Log transform",
    "text": "Log transform\n\nsource\n\nMeteoDataTest.log_transform\n\n MeteoDataTest.log_transform (vars:Union[str,Collection[str]])\n\nTranform the given var with log(x+1)\n\n\n\n\nType\nDetails\n\n\n\n\nvars\nUnion\nlist of variables names to log-transform\n\n\nReturns\nMeteoDataTest\n\n\n\n\n\nMeteoDataTest.generate_gpfa(3, 4).log_transform(['x1']).data\n\n\n\n\n\n  \n    \n      \n      x0\n      x2\n      log_x1\n    \n  \n  \n    \n      0\n      -0.209523\n      0.137226\n      0.221309\n    \n    \n      1\n      0.308402\n      0.304317\n      0.229732\n    \n    \n      2\n      -0.079178\n      -0.082795\n      -0.302681\n    \n    \n      3\n      0.218236\n      0.400733\n      0.326986\n    \n  \n\n\n\n\n\norig_data = MeteoDataTest.generate_gpfa(1, 200)\n\norig_data.data = np.abs(orig_data.data)\n\ndata = orig_data.log_transform(['x0'])\n\n\ndata.data.hist()\n\narray([[<AxesSubplot: title={'center': 'log_x0'}>]], dtype=object)"
  },
  {
    "objectID": "data_preparation.html#export",
    "href": "data_preparation.html#export",
    "title": "Data Preparation",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "GPFA/gpfa.html",
    "href": "GPFA/gpfa.html",
    "title": "Gaussian Processes Factor Analysis",
    "section": "",
    "text": "Derivation of the equations to solve the Gaussian Processes Factor Analysis as described in: Yu, B.M., Cunningham, J.P., Santhanam, G., Ryu, S., Shenoy, K.V., Sahani, M., 2008. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity, in: Advances in Neural Information Processing Systems. Curran Associates, Inc."
  },
  {
    "objectID": "GPFA/gpfa.html#notation",
    "href": "GPFA/gpfa.html#notation",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Notation",
    "text": "Notation\n\n\\(T\\) Number of time steps\n\\(N\\) Number of variables observed\n\\(K\\) Number of dimensions of latent variable\n\\(x_{:,t}\\) vector of all the \\(N\\) variables at time \\(t\\), \\(\\in \\mathbb{R}^N\\)\n\\(x_{n,:}\\) vector of the \\(n\\)th variable at for time steps in \\(T\\), \\(\\in \\mathbb{R}^T\\)\n\\(x_{n,t}\\) \\(n\\)th variable at time \\(t\\), \\(\\in \\mathbb{R}\\)\n\\(X_M = [x_{:,1}, ... x_{:, T}]\\) Matrix with all the \\(N\\) variables at all time steps \\(T\\), \\(\\in \\mathbb{R}^{N \\times T}\\)\n\\(X\\) is a vector obtained by “flattening” \\(X_M\\), by putting next to each other all variable at time \\(t\\), \\(\\in \\mathbb{R}^{(N \\cdot T)}\\)\n\\(t\\) time step\n\\(z_{i, t}\\) \\(i\\)th latent variable at time \\(t\\), \\(\\in \\mathbb{R}\\)\n\\(Z = [z_1 , ... z_t]\\) Vector with \\(z\\) at all time steps in \\(T\\), \\(\\in \\mathbb{R}^{K \\times T}\\)"
  },
  {
    "objectID": "GPFA/gpfa.html#gaussian-processes-factor-analysis-model",
    "href": "GPFA/gpfa.html#gaussian-processes-factor-analysis-model",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Gaussian Processes Factor Analysis model",
    "text": "Gaussian Processes Factor Analysis model\nWe model the variables in this way \\[x_{:,t} = \\Lambda z_{:,t} + \\epsilon \\] where:\n\n\\(\\Lambda\\) is a Factor loading matrix that transforms \\(z_{:,t}\\) into \\(x_{:,t}\\), \\(\\in \\mathbb{R}^{N \\times K}\\)\n\\(\\epsilon\\) Random noise. The random noise is independent between the different time steps, \\(\\in \\mathbb{R}^N\\):\n\n\\(p(\\epsilon) = \\mathcal{N}(0, \\psi)\\) distribution of noise\n\\(\\psi\\), covariance matrix of noise, it is a diagional matrix, \\(\\in \\mathbb{R}^{N \\times N}\\)\n\n\nThe model consider \\(\\langle X \\rangle = 0\\) (if \\(X\\) doesn’t have a 0 mean it can be easily transformed by substracting the mean)\nThe latent variable \\(z\\) is modelled over time using a Gaussian Process, one process for each dimension \\(k\\) for simplicity we assumed that \\(z\\) has only one dimension (\\(k = 1\\))\n\\[p(Z) = \\mathcal{GP}(0, k(t, t \\prime))\\]"
  },
  {
    "objectID": "GPFA/gpfa.html#derivation-of-px",
    "href": "GPFA/gpfa.html#derivation-of-px",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Derivation of \\(p(X)\\)",
    "text": "Derivation of \\(p(X)\\)\n\\(p(x_{:,t}|z_{:,t}) = \\mathcal{N}(\\Lambda z_{:,t}, \\psi)\\) is easy to derive and then \\(p(x_{:,t})\\) and \\(p(z_{:,t}|x_{:,t})\\) can be obtained using the rules of Gaussian inference.\nHowever, what is interesting is to have the analytical form of \\(p(X)\\), which models both the relations between \\(z\\) and \\(x\\) and the \\(z\\) and \\(t\\). The likelihood of \\(p(X)\\) can then be maximized to obtain the parameters of the latent transformation and the kernel hyperparameter.\n\\(p(X)\\) is a Guassian distribution with \\(T\\cdot N\\) dimensions.\n\\(p(X) = \\mathcal{N}(\\langle X \\rangle, \\langle X X^T \\rangle)\\)\n\nDiagonal of the covariance matrix\nLet’s start with the diagonal of the covariance matrix (\\(t = t \\prime\\))\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t} + \\epsilon_{t})^T \\rangle\\)\nby multipling the two vectors together we obtain\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t}^T + \\epsilon_t \\Lambda^T z_{:,t}^T + \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThe using the linearity of the expectation we can:\n\ntransform the expecations of a sum into a sum of expecations\nmove the \\(\\Lambda\\) out of the expecation, as it doesn’t depend on \\(z\\)\n\\(\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle\\) because \\(z_{:,t}\\) and \\(\\epsilon_t\\) are independent random variables\n\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThen considering that \\(\\langle z_{:,t} \\rangle = 0\\) and that \\(\\langle \\epsilon_t \\rangle = 0\\) the expression can be simplified as:\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThen substituting:\n\n\\(\\langle z_{:,t} z_{:,t}^T\\rangle = k(t, t)\\) as that is the covariance matrix of the Gaussian process\n\\(\\langle \\epsilon_t \\epsilon_t^T \\rangle= \\psi\\)\n\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda k(t,t) \\Lambda^T + \\psi\\)\n\n\nOff-diagonal\nsimilar to the steps of above\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle (\\Lambda z_{:,t} + \\epsilon_{t})(\\Lambda z_{:,t \\prime} + \\epsilon_{t \\prime})^T \\rangle\\)\nby multipling the two vectors together we obtain\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\langle \\Lambda z_{:,t} z_{:,t \\prime}^T \\Lambda^T + \\Lambda z_{:,t} \\epsilon_{t \\prime}^T + \\epsilon_t \\Lambda^T z_{:,t \\prime}^T + \\epsilon_t \\epsilon_{t \\prime}^T \\rangle\\)\nThen using the linearity of the expectation we can:\n\ntransform the expecations of a sum into a sum of expecations\nmove the \\(\\Lambda\\) out of the expecatios, as it doesn’t depend on t\n\\(\\langle z_{:,t} \\epsilon_t \\rangle = \\langle z_{:,t} \\rangle \\langle \\epsilon_t \\rangle\\) because \\(z_{:,t}\\) and \\(\\epsilon_t\\) are independent random variables\n\n\\(\\langle x_{:,t}x_{:,t}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t}^T\\rangle \\Lambda^T + \\Lambda \\langle z_{:,t} \\rangle \\langle \\epsilon_{t}^T \\rangle + \\langle \\epsilon_{t} \\rangle \\Lambda^T \\langle z_{:,t}^T \\rangle + \\langle \\epsilon_t \\epsilon_{t}^T \\rangle\\)\nThen considering that \\(\\langle z_{:,t} \\rangle = 0\\) and that \\(\\langle \\epsilon_t \\rangle = 0\\) the expression can be simplified as:\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda \\langle z_{:,t} z_{:,t \\prime}^T\\rangle \\Lambda^T + \\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle\\)\nThen substituting: 1) \\(\\langle z_{:,t} z_{:,t \\prime}^T\\rangle = k(t,t \\prime)\\) as that is the covariance matrix of the Gaussian process 2) \\(\\langle \\epsilon_t \\epsilon_{t \\prime}^T \\rangle= 0\\) as \\(\\epsilon_t\\) and \\(\\epsilon_{t \\prime}\\) are independent and \\(\\langle \\epsilon_t \\rangle = 0\\)\n\\(\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T\\)\n\n\nResult\nThe equation for the diagonal and off-diagonal element can be summarized as:\n\\[\\langle x_{:,t}x_{:,t \\prime}^T \\rangle = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi\\]\nwhere \\(\\delta(x) = \\begin{cases}  1 & if\\ x=0 \\\\  0 & if\\ x \\ne 0 \\\\  \\end{cases}\\)\nTherefore \\(p(X)\\) can be modelled as:\n\\[p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{cccc}\n    \\Lambda k(t_1,t_1) \\Lambda^T + \\delta(1-1)\\psi & \\Lambda k(t_{1},t_{2}) \\Lambda^T + \\delta(1-2)\\psi& \\cdots & \\Lambda k(t_1 ,t_t) \\Lambda^T + \\delta(1-t)\\psi\\\\\n    \\Lambda k(t_{2},t_{1}) \\Lambda^T+ \\delta(2-1)\\psi &  \\Lambda k(t_{2},t_{2}) \\Lambda^T + \\delta(2-2)\\psi & \\cdots & \\Lambda k(t_{2},t_{t}) \\Lambda^T + \\delta(2-t)\\psi\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    \\Lambda k(t_{t}, t_{1}) \\Lambda^T+ \\delta(t-1)\\psi & \\Lambda k(t_{t},t_{2}) \\Lambda^T + \\delta(t-2)\\psi& \\cdots & \\Lambda k(t_{t},t_{t}) \\Lambda^T + \\delta(t-t)\\psi\\\\\n    \\end{array} } \\right )\\]\nand this is also Gaussian Process with a “special” kernel. Multiplying kernel with a constant (\\(\\Lambda\\)) or adding a kernel (\\(\\delta\\)) yields another valid kernel\nIf we define a new kernel as \\[K(t,t \\prime) = \\Lambda k(t,t \\prime) \\Lambda^T + \\delta(t - t \\prime)\\psi\\]\nThen\n\\[ p(X) = \\mathcal{GP}(0, K(t, t\\prime))\\]\n\n\nLatent variable with more than one dimension\nIn order to have a latent variable with more than one-dimesions, we need to make small changes to the formula\nThe starting point is the covariance matrix of the latent at time \\(t\\), which is:\n\\[\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} \\langle z_{1:,t} z_{1:,t}^T\\rangle & \\cdots & \\langle z_{1:,t} z_{k:,t}^T\\rangle\\\\ \\vdots & \\ddots & \\vdots \\\\ \\langle z_{k:,t} z_{1:,t}^T \\rangle & \\cdots & \\langle z_{k:,t} z_{k:,t}^T\\rangle\\end{array}\\right)\\]\nsince each dimension in \\(z\\) is indipendent:\n\\(\\langle z_{k,t} z_{k\\prime,t} \\rangle = 0\\)\neach dimension in \\(z\\) is modelled using a different kernel (\\(k_{z_k}(t,t\\prime)\\)), hence:\n\\[\\langle z_{:,t} z_{:,t}^T\\rangle = \\left(\\begin{array}{ccc} k_{z_1}(t, t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t)\\end{array}\\right)\\]\nso the GPFA Kernel is:\n\\[K(t,t \\prime) = \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t, t \\prime) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t, t \\prime)\\end{array}\\right) \\Lambda^T + \\delta(t - t \\prime)\\psi\\]\nso p(X) is:\n\\[p(X) = \\mathcal{N}\\left (0 , {\\begin{array}{ccc}\n    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_1, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_1, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_1 - t_t)\\psi\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_1) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_1)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_1)\\psi & \\cdots & \\Lambda \\left(\\begin{array}{ccc} k_{z_1}(t_t, t_t) & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & k_{z_k}(t_t, t_t)\\end{array}\\right) \\Lambda^T + \\delta(t_t - t_t)\\psi\\\\\n    \\end{array} } \\right )\\]"
  },
  {
    "objectID": "GPFA/gpfa.html#next-steps",
    "href": "GPFA/gpfa.html#next-steps",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Next steps",
    "text": "Next steps\n\nThe parameters of the final GP (\\(\\Lambda, \\psi\\) and the kernel hyperparameters) can be fitted by maximizing the likelihood of \\(p(X)\\) using gradient descent"
  },
  {
    "objectID": "GPFA/gpfa.html#kernel",
    "href": "GPFA/gpfa.html#kernel",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Kernel",
    "text": "Kernel\n\nclass GPFAKernel(gpytorch.kernels.Kernel):\n    \"\"\"\n    Kernel to implement Gaussian Processes Factor Analysis\n    \"\"\"\n    def __init__(self,\n                 n_features: int, # number of variables at each time step\n                 latent_kernel: gpytorch.kernels.Kernel, # func that returns any valid GPyTorch Kernel used to model the relationship over time of the latent\n                 latent_dims:int = 1,  # Number of latent dims\n                 Lambda: torch.tensor = None, #(n_features * latent_dims) initial value for factor loading matrix. If None init to one\n                 psi: torch.tensor = None, #(n_features) initial value for random noise covariance. Note this is only the diagonal matrix\n                 **kwargs):\n        super(GPFAKernel, self).__init__(**kwargs)\n        \n        # Number of features in the X for each time step\n        self.n_features = n_features\n\n        self.latent_dims = latent_dims\n        \n        # see GPyTorch Kernels\n        self.register_parameter(\n            name = \"Lambda\",\n            parameter = torch.nn.Parameter(torch.rand(self.n_features, self.latent_dims)))\n        \n        # each dim has it's own latent kernel\n        self.latent_kernels = torch.nn.ModuleList([latent_kernel() for _ in range(self.latent_dims)])\n        \n        self.register_parameter(\n            name = \"raw_psi_diag\",\n            parameter = torch.nn.Parameter(torch.zeros(self.n_features))) \n        self.register_constraint(\"raw_psi_diag\", gpytorch.constraints.Positive())\n        if psi is not None: self.psi = psi\n    \n    # Convenient getter and setter for psi, since there is the Positive() constraint\n    @property\n    def psi(self):\n        # when accessing the parameter, apply the constraint transform\n        return self.raw_psi_diag_constraint.transform(self.raw_psi_diag)\n\n    @psi.setter\n    def psi(self, value):\n        return self._set_psi(value)\n\n    def _set_psi(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_psi_diag)\n        # when setting the paramater, transform the actual value to a raw one by applying the inverse transform\n        self.initialize(raw_psi_diag=self.raw_psi_diag_constraint.inverse_transform(value))\n    \n\n        \n    def forward(self, t1, t2, diag = False, last_dim_is_batch=False, **params):\n\n        # not implemented yet\n        assert diag is False\n        assert last_dim_is_batch is False\n\n        # take the number of observations from the input\n        n_obs = t1.shape[0]\n\n        # compute the latent kernel\n        kT = torch.stack([ kernel(t1, t2, diag, last_dim_is_batch, **params).evaluate() # this may make the whole thing slow as it breaks lazy evaluations\n                         for kernel in self.latent_kernels], dim=2)\n        return compute_gpfa_covariance(self.Lambda, kT, self.psi, self.n_features, n_obs)\n    \n    def num_outputs_per_input(self, x1,x2):\n        return self.n_features\n\n# this is a separate function, because torch script cannot take self as a parameter\n@torch.jit.script\ndef compute_gpfa_covariance(Lambda, kT, psi, n_features, n_obs):\n    # pre allocate covariance matrix\n    X_cov = torch.empty(n_features * n_obs, n_features * n_obs, device=Lambda.device)\n    for i in torch.arange(n_obs):\n        for j in torch.arange(n_obs):\n            # i:i+1 is required to keep the number of dimensions\n            cov =  Lambda @ torch.diag(kT[i,j,:]) @ Lambda.T\n            # only diagonals add the noise\n            if i == j: cov += torch.diag(psi)\n            # add a block of size n_features*n_features to the covariance matrix\n            X_cov[i*n_features:(i*n_features + n_features),j*n_features:(j*n_features+n_features)] = cov\n    return X_cov\n\n\n\nScriptFunction object at 0x7f6cc3ec8130>\n\nsource\n\n\nGPFAKernel\n\n GPFAKernel (n_features:int, latent_kernel:gpytorch.kernels.kernel.Kernel,\n             latent_dims:int=1, Lambda:<built-\n             inmethodtensoroftypeobjectat0x7f6d306dd460>=None, psi:<built-\n             inmethodtensoroftypeobjectat0x7f6d306dd460>=None, **kwargs)\n\nKernel to implement Gaussian Processes Factor Analysis\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nint\n\nnumber of variables at each time step\n\n\nlatent_kernel\nKernel\n\nfunc that returns any valid GPyTorch Kernel used to model the relationship over time of the latent\n\n\nlatent_dims\nint\n1\nNumber of latent dims\n\n\nLambda\ntensor\nNone\n(n_features * latent_dims) initial value for factor loading matrix. If None init to one\n\n\npsi\ntensor\nNone\n(n_features) initial value for random noise covariance. Note this is only the diagonal matrix\n\n\nkwargs\n\n\n\n\n\n\n\ngpfa_k = GPFAKernel(n_features=2, latent_kernel=gpytorch.kernels.RBFKernel)\n\nThe parameters are correctly registered\n\nlist(gpfa_k.named_parameters())\n\n[('Lambda',\n  Parameter containing:\n  tensor([[0.5982],\n          [0.9041]], requires_grad=True)),\n ('raw_psi_diag',\n  Parameter containing:\n  tensor([0., 0.], requires_grad=True)),\n ('latent_kernels.0.raw_lengthscale',\n  Parameter containing:\n  tensor([[0.]], requires_grad=True))]\n\n\nCheck that the Kernel is running\n\ngpfa_k(torch.tensor((1, 2, 3))).evaluate()\n\ntensor([[1.0510, 0.5409, 0.1264, 0.1910, 0.0056, 0.0084],\n        [0.5409, 1.5106, 0.1910, 0.2887, 0.0084, 0.0127],\n        [0.1264, 0.1910, 1.0510, 0.5409, 0.1264, 0.1910],\n        [0.1910, 0.2887, 0.5409, 1.5106, 0.1910, 0.2887],\n        [0.0056, 0.0084, 0.1264, 0.1910, 1.0510, 0.5409],\n        [0.0084, 0.0127, 0.1910, 0.2887, 0.5409, 1.5106]],\n       grad_fn=<CopySlices>)"
  },
  {
    "objectID": "GPFA/gpfa.html#gpfa",
    "href": "GPFA/gpfa.html#gpfa",
    "title": "Gaussian Processes Factor Analysis",
    "section": "GPFA",
    "text": "GPFA\n\nsource\n\nGPFAZeroMean\n\n GPFAZeroMean (n_features, device)\n\nZero Mean function to be used in GPFA, as it takes into account the number of features\nto change the latent_kernel you should subcall GPFA in this way the get_info function for the kernel can be changed to include the latent kernel details\n\nsource\n\n\nGPFA\n\n GPFA (train_x, train_y, likelihood, n_features, latent_dims=1)\n\nThe base class for any Gaussian process latent function to be used in conjunction with exact inference.\n:param torch.Tensor train_inputs: (size n x d) The training features :math:\\mathbf X. :param torch.Tensor train_targets: (size n) The training targets :math:\\mathbf y. :param ~gpytorch.likelihoods.GaussianLikelihood likelihood: The Gaussian likelihood that defines the observational distribution. Since we’re using exact inference, the likelihood must be Gaussian.\nThe :meth:forward function should describe how to compute the prior latent distribution on a given input. Typically, this will involve a mean and kernel function. The result must be a :obj:~gpytorch.distributions.MultivariateNormal.\nCalling this model will return the posterior of the latent Gaussian process when conditioned on the training data. The output will be a :obj:~gpytorch.distributions.MultivariateNormal.\nExample: >>> class MyGP(gpytorch.models.ExactGP): >>> def init(self, train_x, train_y, likelihood): >>> super().__init__(train_x, train_y, likelihood) >>> self.mean_module = gpytorch.means.ZeroMean() >>> self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) >>> >>> def forward(self, x): >>> mean = self.mean_module(x) >>> covar = self.covar_module(x) >>> return gpytorch.distributions.MultivariateNormal(mean, covar) >>> >>> # train_x = …; train_y = … >>> likelihood = gpytorch.likelihoods.GaussianLikelihood() >>> model = MyGP(train_x, train_y, likelihood) >>> >>> # test_x = …; >>> model(test_x) # Returns the GP latent function at test_x >>> likelihood(model(test_x)) # Returns the (approximate) predictive posterior distribution at test_x\nmake some very simple test data, to check that the model is working and can learn the parameters\n\nT = torch.arange(1,5)\n\n\nX = torch.hstack([torch.arange(0,3) + 2* i for i in T])\n\n\nX\n\ntensor([ 2,  3,  4,  4,  5,  6,  6,  7,  8,  8,  9, 10])\n\n\n\nX.shape\n\ntorch.Size([12])\n\n\n\nT\n\ntensor([1, 2, 3, 4])\n\n\n\n# initialize likelihood and model\nlikelihood = gpytorch.likelihoods.GaussianLikelihood()\nmodel = GPFA(T, X, likelihood, n_features = 3)\n\n\nmodel\n\nGPFA(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\nGetting the prior from the GP\n\nmodel(T)\n\nMultivariateNormal(loc: torch.Size([12]))\n\n\nFitting the parameters using gradient descend\n\n# this is for running the notebook in our testing framework\ntraining_iter = 10\n\n# Find optimal model hyperparameters\nmodel.train()\nlikelihood.train()\n\n# Use the adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n\n# \"Loss\" for GPs - the marginal log likelihood\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\nlosses = []\nfor i in range(training_iter):\n    # Zero gradients from previous iteration\n    optimizer.zero_grad()\n    # Output from model\n    output = model(T)\n    # Calc loss and backprop gradients\n    loss = -mll(output, X)\n    losses.append(loss.item())\n    loss.backward()\n    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f, Lambda: %.3f   noise: %.3f' % (\n        i + 1, training_iter, loss.item(),\n        model.covar_module.latent_kernels[0].lengthscale.item(),\n        model.covar_module.Lambda.mean().item(),\n        model.likelihood.noise.item()\n    ))\n    optimizer.step()\n\nIter 1/10 - Loss: 10.056   lengthscale: 0.693, Lambda: 0.591   noise: 0.693\nIter 2/10 - Loss: 8.484   lengthscale: 0.744, Lambda: 0.691   noise: 0.744\nIter 3/10 - Loss: 7.256   lengthscale: 0.798, Lambda: 0.789   noise: 0.797\nIter 4/10 - Loss: 6.304   lengthscale: 0.853, Lambda: 0.884   noise: 0.850\nIter 5/10 - Loss: 5.564   lengthscale: 0.910, Lambda: 0.975   noise: 0.904\nIter 6/10 - Loss: 4.984   lengthscale: 0.968, Lambda: 1.061   noise: 0.956\nIter 7/10 - Loss: 4.526   lengthscale: 1.027, Lambda: 1.141   noise: 1.008\nIter 8/10 - Loss: 4.159   lengthscale: 1.086, Lambda: 1.216   noise: 1.058\nIter 9/10 - Loss: 3.863   lengthscale: 1.145, Lambda: 1.286   noise: 1.106\nIter 10/10 - Loss: 3.622   lengthscale: 1.203, Lambda: 1.350   noise: 1.152\n\n\nThe model is training!"
  },
  {
    "objectID": "GPFA/gpfa.html#multi-dimensional-latent-variable",
    "href": "GPFA/gpfa.html#multi-dimensional-latent-variable",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Multi-dimensional latent variable",
    "text": "Multi-dimensional latent variable\n\n2 dimensions\n\n# initialize likelihood and model\nlikelihood_m = gpytorch.likelihoods.GaussianLikelihood()\nmodel_m = GPFA(T, X, likelihood, n_features = 3, latent_dims=2)\n\n\nmodel_m\n\nGPFA(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (1): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\ncheck GP is running\n\nmodel_m.covar_module.latent_kernels\n\nModuleList(\n  (0): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (1): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.8592, 0.4173],\n        [0.0988, 0.0617],\n        [0.4201, 0.6319]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))\n\n\n\n\n5 dimensions\n\n# initialize likelihood and model\nlikelihood_m = gpytorch.likelihoods.GaussianLikelihood()\nmodel_m = GPFA(T, X, likelihood, n_features = 3, latent_dims=5)\n\n\nmodel_m\n\nGPFA(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): GPFAZeroMean()\n  (covar_module): GPFAKernel(\n    (latent_kernels): ModuleList(\n      (0): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (1): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (2): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (3): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n      (4): RBFKernel(\n        (raw_lengthscale_constraint): Positive()\n      )\n    )\n    (raw_psi_diag_constraint): Positive()\n  )\n)\n\n\ncheck GP is running\n\nmodel_m.covar_module.latent_kernels\n\nModuleList(\n  (0): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (1): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (2): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (3): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (4): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.2383, 0.0719, 0.9205, 0.6215, 0.9508],\n        [0.5192, 0.2102, 0.9280, 0.0948, 0.4027],\n        [0.4778, 0.7141, 0.3596, 0.9803, 0.4140]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))"
  },
  {
    "objectID": "GPFA/gpfa.html#get-info",
    "href": "GPFA/gpfa.html#get-info",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Get info",
    "text": "Get info\nthis is to return the kernel info for printing, both during training and for the final results\nThe general api is:\nget_info(var_names=None) -> dict[str, pd.DataFrame] where the string the title of a kernel parameter\n\n@patch\ndef get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -> dict[str, pd.DataFrame]:\n    \"Model info for a GPFA with a RBFKernel\"\n    out = {}\n    \n    latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n    \n    out[\"Lambda\"] = pd.concat([\n        None if var_names is None else pd.Series(var_names, name='variable'),\n        pd.DataFrame(\n            self.covar_module.Lambda.detach().cpu().numpy(),\n            columns=latent_names)],\n        axis=1)\n    \n    ls = [self.covar_module.latent_kernels[i].lengthscale.detach().item() for i in range(self.covar_module.latent_dims)]\n    out[\"lengthscale\"] = pd.DataFrame({\n        'latent': latent_names,\n        'lengthscale': ls\n    })\n    \n    psi = self.covar_module.psi.detach().cpu().numpy()\n    out[\"psi\"] = pd.DataFrame({\n        'variable': var_names,\n        'psi': psi \n    })\n    \n    out[\"likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.detach().item()]})\n    \n    return out\n\n\nsource\n\nGPFA.get_info\n\n GPFA.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nmodel_m.get_info()\n\n{'Lambda':          z0        z1        z2        z3        z4\n 0  0.238300  0.071901  0.920541  0.621538  0.950799\n 1  0.519228  0.210204  0.928017  0.094791  0.402726\n 2  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0     None  0.693147\n 1     None  0.693147\n 2     None  0.693147,\n 'likelihood':       noise\n 0  1.195812}\n\n\n\nmodel_m.get_info([\"a\", \"b\", \"c\"])\n\n{'Lambda':   variable        z0        z1        z2        z3        z4\n 0        a  0.238300  0.071901  0.920541  0.621538  0.950799\n 1        b  0.519228  0.210204  0.928017  0.094791  0.402726\n 2        c  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0        a  0.693147\n 1        b  0.693147\n 2        c  0.693147,\n 'likelihood':       noise\n 0  1.195812}"
  },
  {
    "objectID": "GPFA/gpfa.html#export",
    "href": "GPFA/gpfa.html#export",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Export",
    "text": "Export\n\nmodel_m.covar_module.latent_kernels\n\nModuleList(\n  (0): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (1): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (2): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (3): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (4): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.2383, 0.0719, 0.9205, 0.6215, 0.9508],\n        [0.5192, 0.2102, 0.9280, 0.0948, 0.4027],\n        [0.4778, 0.7141, 0.3596, 0.9803, 0.4140]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))"
  },
  {
    "objectID": "GPFA/gpfa.html#get-info-1",
    "href": "GPFA/gpfa.html#get-info-1",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Get info",
    "text": "Get info\nthis is to return the kernel info for printing, both during training and for the final results\nThe general api is:\nget_info(var_names=None) -> dict[str, pd.DataFrame] where the string the title of a kernel parameter\n\n@patch\ndef get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -> dict[str, pd.DataFrame]:\n    \"Model info for a GPFA with a RBFKernel\"\n    out = {}\n    \n    latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n    \n    out[\"Lambda\"] = pd.concat([\n        None if var_names is None else pd.Series(var_names, name='variable'),\n        pd.DataFrame(\n            self.covar_module.Lambda.detach().cpu().numpy(),\n            columns=latent_names)],\n        axis=1)\n    \n    ls = [self.covar_module.latent_kernels[i].lengthscale.detach().item() for i in range(self.covar_module.latent_dims)]\n    out[\"lengthscale\"] = pd.DataFrame({\n        'latent': latent_names,\n        'lengthscale': ls\n    })\n    \n    psi = self.covar_module.psi.detach().cpu().numpy()\n    out[\"psi\"] = pd.DataFrame({\n        'variable': var_names,\n        'psi': psi \n    })\n    \n    out[\"likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.detach().item()]})\n    \n    return out\n\n\nsource\n\nGPFA.get_info\n\n GPFA.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nmodel_m.get_info()\n\n{'Lambda':          z0        z1        z2        z3        z4\n 0  0.238300  0.071901  0.920541  0.621538  0.950799\n 1  0.519228  0.210204  0.928017  0.094791  0.402726\n 2  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0     None  0.693147\n 1     None  0.693147\n 2     None  0.693147,\n 'likelihood':       noise\n 0  1.195812}\n\n\n\nmodel_m.get_info([\"a\", \"b\", \"c\"])\n\n{'Lambda':   variable        z0        z1        z2        z3        z4\n 0        a  0.238300  0.071901  0.920541  0.621538  0.950799\n 1        b  0.519228  0.210204  0.928017  0.094791  0.402726\n 2        c  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0        a  0.693147\n 1        b  0.693147\n 2        c  0.693147,\n 'likelihood':       noise\n 0  1.195812}\n\n\n\nmodel_m.covar_module.Lambda\n\nParameter containing:\ntensor([[0.2383, 0.0719, 0.9205, 0.6215, 0.9508],\n        [0.5192, 0.2102, 0.9280, 0.0948, 0.4027],\n        [0.4778, 0.7141, 0.3596, 0.9803, 0.4140]], requires_grad=True)\n\n\n\nlikelihood(model_m(T))\n\nMultivariateNormal(loc: torch.Size([12]))"
  },
  {
    "objectID": "GPFA/gpfa.html#get-info-2",
    "href": "GPFA/gpfa.html#get-info-2",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Get info",
    "text": "Get info\nthis is to return the kernel info for printing, both during training and for the final results\nThe general api is:\nget_info(var_names=None) -> dict[str, pd.DataFrame] where the string the title of a kernel parameter\n\n@patch\ndef get_info(self: GPFA,\n             var_names = None # Optional variable names for better printing\n            ) -> dict[str, pd.DataFrame]:\n    \"Model info for a GPFA with a RBFKernel\"\n    out = {}\n    \n    latent_names = [f\"z{i}\" for i in range(self.covar_module.latent_dims)]\n    \n    out[\"Lambda\"] = pd.concat([\n        None if var_names is None else pd.Series(var_names, name='variable'),\n        pd.DataFrame(\n            self.covar_module.Lambda.detach().cpu().numpy(),\n            columns=latent_names)],\n        axis=1)\n    \n    ls = [self.covar_module.latent_kernels[i].lengthscale.detach().item() for i in range(self.covar_module.latent_dims)]\n    out[\"lengthscale\"] = pd.DataFrame({\n        'latent': latent_names,\n        'lengthscale': ls\n    })\n    \n    psi = self.covar_module.psi.detach().cpu().numpy()\n    out[\"psi\"] = pd.DataFrame({\n        'variable': var_names,\n        'psi': psi \n    })\n    \n    out[\"likelihood\"] = pd.DataFrame({'noise': [self.likelihood.noise_covar.noise.detach().item()]})\n    \n    return out\n\n\nsource\n\nGPFA.get_info\n\n GPFA.get_info (var_names=None)\n\nModel info for a GPFA with a RBFKernel\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nNoneType\nNone\nOptional variable names for better printing\n\n\nReturns\ndict\n\n\n\n\n\n\nmodel_m.get_info()\n\n{'Lambda':          z0        z1        z2        z3        z4\n 0  0.238300  0.071901  0.920541  0.621538  0.950799\n 1  0.519228  0.210204  0.928017  0.094791  0.402726\n 2  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0     None  0.693147\n 1     None  0.693147\n 2     None  0.693147,\n 'likelihood':       noise\n 0  1.195812}\n\n\n\nmodel_m.get_info([\"a\", \"b\", \"c\"])\n\n{'Lambda':   variable        z0        z1        z2        z3        z4\n 0        a  0.238300  0.071901  0.920541  0.621538  0.950799\n 1        b  0.519228  0.210204  0.928017  0.094791  0.402726\n 2        c  0.477801  0.714146  0.359620  0.980278  0.414003,\n 'lengthscale':   latent  lengthscale\n 0     z0     0.693147\n 1     z1     0.693147\n 2     z2     0.693147\n 3     z3     0.693147\n 4     z4     0.693147,\n 'psi':   variable       psi\n 0        a  0.693147\n 1        b  0.693147\n 2        c  0.693147,\n 'likelihood':       noise\n 0  1.195812}"
  },
  {
    "objectID": "GPFA/gpfa.html#export-1",
    "href": "GPFA/gpfa.html#export-1",
    "title": "Gaussian Processes Factor Analysis",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "GPFA/learner.html",
    "href": "GPFA/learner.html",
    "title": "GPFA Learner",
    "section": "",
    "text": "from fastcore.test import *\nThe first thing that we need is a Learner object to keep track of:\nand that has methods to help with:\nThe first thing we need is a training loop, just wrap in a function the example one from GPyTorch"
  },
  {
    "objectID": "GPFA/learner.html#learner",
    "href": "GPFA/learner.html#learner",
    "title": "GPFA Learner",
    "section": "Learner",
    "text": "Learner\n\nsource\n\nGPFALearner\n\n GPFALearner (X:torch.Tensor, T:torch.Tensor=None, latent_dims:int=1,\n              model=<class 'meteo_imp.gpfa.gpfa.GPFA'>, var_names=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX\nTensor\n\n(n_features * n_obs) Multivariate time series\n\n\nT\nTensor\nNone\n(n_obs) Vector of time of observations.\n\n\nlatent_dims\nint\n1\nNumber of latent variables in GPFA\n\n\nmodel\ntype\nGPFA\nsub-class of GPFA\n\n\nvar_names\nNoneType\nNone\nfor model info\n\n\n\n\n# test data\nT = torch.arange(0,6)\n\nX = torch.vstack([(torch.arange(0,3, dtype=torch.float32) + 2 + i) * i for i in T])\n\n\nX\n\ntensor([[ 0.,  0.,  0.],\n        [ 3.,  4.,  5.],\n        [ 8., 10., 12.],\n        [15., 18., 21.],\n        [24., 28., 32.],\n        [35., 40., 45.]])\n\n\n\n# l for learner\nl = GPFALearner(X)\n\n\ntest_eq(T, l.T)\n\n\n# with explicit time\ntest_eq(T, GPFALearner(X, T).T)\n\n\ntest_eq(l.n_features, 3)\n\n\nl.X\n\ntensor([-1.0590, -1.0955, -1.1236, -0.8347, -0.8326, -0.8305, -0.4610, -0.4382,\n        -0.4201,  0.0623,  0.0876,  0.1075,  0.7350,  0.7449,  0.7523,  1.5573,\n         1.5337,  1.5145])\n\n\n\nl.train()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\ncheck that can train for more than once\n\nl.train()\n\n\n\n\n\nl.losses\n\ntensor([ 1.3691,  1.3265,  1.2867,  1.2495,  1.2107,  1.1702,  1.1304,  1.0929,\n         1.0566,  1.0200,  0.9822,  0.9434,  0.9039,  0.8642,  0.8246,  0.7845,\n         0.7437,  0.7019,  0.6595,  0.6170,  0.5749,  0.5328,  0.4905,  0.4481,\n         0.4058,  0.3636,  0.3214,  0.2793,  0.2370,  0.1943,  0.1520,  0.1097,\n         0.0667,  0.0243, -0.0181, -0.0608, -0.1031, -0.1452, -0.1874, -0.2293,\n        -0.2710, -0.3128, -0.3539, -0.3953, -0.4360, -0.4769, -0.5173, -0.5577,\n        -0.5974, -0.6373, -0.6766, -0.7156, -0.7544, -0.7927, -0.8304, -0.8673,\n        -0.9023, -0.9291, -0.9688, -1.0017, -1.0382, -1.0711, -1.1033, -1.1396,\n        -1.1642, -1.2035, -1.2218, -1.2612, -1.2789, -1.3111, -1.3358, -1.3537,\n        -1.3841, -1.3909, -1.3967, -1.4299, -1.4527, -1.4624, -1.4806, -1.4976,\n        -1.5096, -1.5176, -1.5356, -1.5441, -1.5443, -1.5641, -1.5724, -1.5653,\n        -1.5789, -1.5920, -1.5886, -1.5960, -1.6030, -1.5963, -1.6003, -1.6098,\n        -1.6074, -1.6175, -1.6281, -1.6264, -1.6340,  0.7754, -1.4940, -1.1752,\n        -0.3677, -0.6071, -1.2489, -1.6029, -1.5123, -1.2290, -1.0733, -1.1438,\n        -1.3421, -1.5151, -1.5719, -1.5156, -1.4143, -1.3454, -1.3485, -1.4113,\n        -1.4900, -1.5405, -1.5443, -1.5109, -1.4704, -1.4521, -1.4677, -1.5063,\n        -1.5450, -1.5630, -1.5564, -1.5371, -1.5254, -1.5339, -1.5587, -1.5839,\n        -1.5945, -1.5893, -1.5795, -1.5800, -1.5942, -1.6127, -1.6220, -1.6192,\n        -1.6136, -1.6172, -1.6295, -1.6398, -1.6410, -1.6373, -1.6382, -1.6460,\n        -1.6539, -1.6547, -1.6524, -1.6549, -1.6613, -1.6652, -1.6646, -1.6646,\n        -1.6685, -1.6730, -1.6738, -1.6737, -1.6759, -1.6800, -1.6818, -1.6816,\n        -1.6829, -1.6861, -1.6876, -1.6880, -1.6895, -1.6909, -1.6925, -1.6931,\n        -1.6938, -1.6957, -1.6964, -1.6974, -1.6982, -1.6985, -1.6993, -1.7003,\n        -1.7012, -1.7019, -1.7021, -1.7030, -1.7040, -1.7040, -1.7047, -1.7052,\n        -1.7061, -1.7067, -1.7071, -1.7073, -1.7083, -1.7082, -1.7087, -1.7090])"
  },
  {
    "objectID": "GPFA/learner.html#predictions",
    "href": "GPFA/learner.html#predictions",
    "title": "GPFA Learner",
    "section": "Predictions",
    "text": "Predictions\nadd a function to get predictions from the model\n\nsource\n\nGPFALearner.predict_raw\n\n GPFALearner.predict_raw (T)\n\n\nraw_out = l.predict_raw(T)\nraw_out\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n  warnings.warn(\n\n\nMultivariateNormal(loc: torch.Size([18]))\n\n\nthe model prediction is a distribution with len(T)*n_features dimensions\nwhich is in the in the wrong shape and need to be rescaled after the normalization\nAlso we don’t need th full distribution but only the mean and stddev for each variable at every time step\nAnd we can “fix” the shape by transforming back to a matrix\n\nraw_stddev = raw_out.stddev.reshape(-1, l.n_features)\nraw_mean = raw_out.mean.reshape(-1, l.n_features)\n\n\nraw_stddev\n\ntensor([[0.0284, 0.0148, 0.0193],\n        [0.0279, 0.0138, 0.0186],\n        [0.0277, 0.0134, 0.0183],\n        [0.0277, 0.0134, 0.0183],\n        [0.0279, 0.0138, 0.0186],\n        [0.0284, 0.0147, 0.0193]], grad_fn=<ReshapeAliasBackward0>)\n\n\nThis function transforms the raw output of the Gaussian Process (p(X)) into a prediction that can be used. need to do two things:\n\ntake mean and stddev (this is the diagonal of the covariance matrix) for each variable\nreshape so that each row is a time step and each column a variable\nreverse the normalization\n\nthe mean and the std are passed individually because in the conditional predictions is not possible to have the whole covariance matrix (and thus a MultiNormal) but the mean and stddev are enough.\n\nsource\n\n\nGPFALearner.prediction_from_raw\n\n GPFALearner.prediction_from_raw (raw_mean, raw_std)\n\nTakes a raw prediction and produces and final prediction, by reshaping and reversing normalization\n\n# TODO document this function better\n\n\n@patch\ndef predict(self: GPFALearner, T):\n    pred_raw = self.predict_raw(T)\n    return self.prediction_from_raw(pred_raw.mean, pred_raw.stddev)\n\n\nl.predict(T)\n\nNormalParameters(mean=tensor([[-0.5417, -0.0635,  0.4033],\n        [ 3.0714,  4.0467,  5.0139],\n        [ 8.2941,  9.9889, 11.6774],\n        [15.3474, 18.0108, 20.6732],\n        [24.2270, 28.1071, 31.9975],\n        [34.6030, 39.9107, 45.2332]]), std=tensor([[0.3796, 0.2245, 0.3295],\n        [0.3730, 0.2095, 0.3169],\n        [0.3705, 0.2033, 0.3116],\n        [0.3703, 0.2033, 0.3113],\n        [0.3729, 0.2092, 0.3166],\n        [0.3797, 0.2243, 0.3295]]))\n\n\n\npred = l.predict(T)\n\n\npred.mean.shape\n\ntorch.Size([6, 3])\n\n\n\npred.std.shape\n\ntorch.Size([6, 3])\n\n\n\n\nCheck learning is working\nThe idea is to use the current model to generate a dataset, that can be for sure modelled using a GPFA (because is the output of GPFA) and then train another model and see if the parameters converge\n\n# create a dummy GPFA with 3 features\nLt = GPFALearner(X)\n\n\ntest_params = {\n   \"Lambda\": torch.tensor([-1, 0.3, .8]).reshape(Lt.n_features, -1),\n   \"psi\": torch.tensor([1e-5, 5e-5, 2e-5]),\n}\n\n\nLt.model.covar_module.initialize(**test_params)\n\nGPFAKernel(\n  (latent_kernels): ModuleList(\n    (0): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n  )\n  (raw_psi_diag_constraint): Positive()\n)\n\n\n\nLt.model.covar_module.latent_kernels[0].initialize(lengthscale = torch.tensor(5))\n\nRBFKernel(\n  (raw_lengthscale_constraint): Positive()\n)\n\n\n\ntarget_X = Lt.predict(T).mean\n\n\nl2 = GPFALearner(target_X)\n\n\nl2.train()\n\n\n\n\n\nl2.predict(T).mean - target_X\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/models/exact_gp.py:273: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n  warnings.warn(\n\n\ntensor([[ 0.0058,  0.0006,  0.0034],\n        [ 0.0033,  0.0005,  0.0025],\n        [ 0.0011,  0.0002,  0.0008],\n        [-0.0011, -0.0002, -0.0010],\n        [-0.0034, -0.0005, -0.0025],\n        [-0.0056, -0.0006, -0.0033]])\n\n\nthey seems pretty small numbers, so the model is working!\n\nprint(\"Lambda:\\n\", l2.model.covar_module.Lambda.detach())\n\nprint(\"psi: \", l2.model.covar_module.psi.detach())\n\nprint(\"lengthscale:\", l2.model.covar_module.latent_kernels[0].lengthscale.item())\n\nLambda:\n tensor([[-1.6212],\n        [ 1.6023],\n        [ 1.5982]])\npsi:  tensor([4.9890e-05, 3.7080e-05, 3.7402e-05])\nlengthscale: 5.521887302398682\n\n\n\n\nConditional Predictions\nThis add the supports for conditional predictions, which means that at the time (t) when we are making the predictions some of the variables have been actually observed. Since the model prediction is a normal distribution we can condition on the observed values and thus improve the predictions.\nTherefore we need to compute the conditional distribution of a normal 1\n\\[ X = \\left[\\begin{array}{c} x \\\\ o \\end{array} \\right] \\]\n\\[ p(X) = N\\left(\\left[ \\begin{array}{c} \\mu_x \\\\ \\mu_o \\end{array} \\right], \\left[\\begin{array}{cc} \\Sigma_{xx} & \\Sigma_{xo} \\\\ \\Sigma_{ox} & \\Sigma_{oo} \\end{array} \\right]\\right)\\]\nwhere \\(X\\) is a vector of variable that need to predicted and \\(o\\) is a vector of the variables that have been observed\nThe mean is in “flat format”, where all the features from one time step are next to each other followed by the features of the next time step.\nthen\n\\[p(x|o) = N(\\mu_x + \\Sigma_{xo}\\Sigma_{oo}^{-1}(o - \\mu_o), \\Sigma_{xx} - \\Sigma_{xo}\\Sigma_{oo}^{-1}\\Sigma_{ox})\\]\n\n# example distribution with only 2 variables\nμ = torch.tensor([.5, 1.])\nΣ = torch.tensor([[1., .5], [.5 ,1.]])\n\ngauss = MultivariateNormal(μ, Σ)\n\nidx = torch.tensor([True, False]) # second variable is the observed one\n\nobs = torch.tensor(5.) # value of second variable\n\ngauss_cond = conditional_guassian(gauss, obs, idx)\n\n# hardcoded values to test that the code is working, see also for alternative implementation https://python.quantecon.org/multivariate_normal.html\ntest_close(3.25, gauss_cond.mean.item())\ntest_close(.75, gauss_cond.covariance_matrix.item())\n\nTest with multiple variables?\noverwrite the predict method to add support for conditional predictions\nNeed to have the mean and std for both the conditional predictions and the observations, with the same shape and order of the complete prediction.\n\ngauss_cond.covariance_matrix\n\ntensor([[0.7500]])\n\n\n\nmerge_pred = _merge_raw_cond_pred(gauss, gauss_cond, obs, idx)\nmerge_pred\n\nNormalParameters(mean=tensor([5.0000, 3.2500]), std=tensor([0.0000, 0.8660]))\n\n\n\n# manually calculated\ntest_close(merge_pred.mean, torch.tensor([5., 3.25]))\ntest_close(merge_pred.std, torch.tensor([0., math.sqrt(.75)]))\n\nThe problem is that the mean and the std for normalization are different for each feature, so in order to have the normalization working it is necessary to give the observations as a 2D array and not like a 1D array (like required by the model)\n\nT_pred = torch.tensor([6, 7])\n\n\nl.predict(T_pred)\n\nNormalParameters(mean=tensor([[45.8037, 52.6492, 59.5189],\n        [56.8583, 65.2222, 73.6200]]), std=tensor([[0.6374, 0.6243, 0.7313],\n        [1.5561, 1.7313, 1.9531]]))\n\n\n\nidx = torch.zeros(T_pred.shape[0] * X.shape[1], dtype=torch.bool)\n# simulate an observation using sensible numbers from the prediction\nidx[[0,2]] = torch.tensor([True, True])\nobs = torch.tensor([42., 61.])\n\n\nl._standard_obs(obs, idx)\n\ntensor([2.0806, 2.4525])\n\n\n\nsource\n\n\nGPFALearner.predict\n\n GPFALearner.predict (T:torch.Tensor, obs:torch.Tensor=None,\n                      idx:torch.Tensor=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nT\nTensor\n\n(n_pred) time where prediction is needed\n\n\nobs\nTensor\nNone\n(n_obs_pred) Optional - if at the times of the prediction there are some observationsarray with the values of observations to condition distribution\n\n\nidx\nTensor\nNone\n((n_pred*n_features)) Optional - necessary if obs are presentBoolean array that is True where an observation is present and False where a prediction is neededThis is a 1D array with the length equal to n_pred (number time steps to predict) times n_features\n\n\n\n\nl._standard_obs(obs, idx)\n\ntensor([2.0806, 2.4525])\n\n\n\nobs\n\ntensor([42., 61.])\n\n\n\nl.predict_raw(T_pred).mean\n\ntensor([2.3649, 2.3651, 2.3656, 3.1912, 3.1915, 3.1923],\n       grad_fn=<ViewBackward0>)\n\n\n\nl.predict(T_pred, obs, idx)\n\nNormalParameters(mean=tensor([[42.0000, 52.3809, 61.0000],\n        [56.2001, 64.4710, 72.7773]]), std=tensor([[0.0000, 0.2729, 0.0000],\n        [0.7405, 0.7571, 0.8751]]))\n\n\n\nl.predict(T_pred)\n\nNormalParameters(mean=tensor([[45.8037, 52.6492, 59.5189],\n        [56.8583, 65.2222, 73.6200]]), std=tensor([[0.6374, 0.6243, 0.7313],\n        [1.5561, 1.7313, 1.9531]]))\n\n\nThere is a small change in the predicted values after conditioning as you would expect"
  },
  {
    "objectID": "GPFA/learner.html#gpu-support",
    "href": "GPFA/learner.html#gpu-support",
    "title": "GPFA Learner",
    "section": "GPU Support",
    "text": "GPU Support\nadd support for CUDA to model\n\n# l for learner\nl_cuda = GPFALearner(X.cuda())\n\nAttrs of interest are:\n\nT\nX\nlikelihood\nmodel\nnorm\n\ncuda() modifies in place the tensors and the modules!\n\nl_cuda.T.cuda()\n\ntensor([0, 1, 2, 3, 4, 5], device='cuda:0')\n\n\n\nl_cuda.T.device\n\ndevice(type='cuda', index=0)\n\n\n\n@patch\ndef cuda(self: GPFALearner):\n    \"\"\"Moves all learner to gpu\"\"\"\n    for par in ['T', 'X', 'model', 'likelihood']:\n        self.__getattribute__(par).cuda()\n    self.norm.x_mean.cuda()\n    self.norm.x_std.cuda()\n\n\nsource\n\nGPFALearner.cuda\n\n GPFALearner.cuda ()\n\nMoves all learner to gpu\n\nl_cuda.cuda()\n\nparameters are on the gpu!\n\nnext(l_cuda.likelihood.parameters()).device\n\ndevice(type='cuda', index=0)\n\n\n\ngpytorch.distributions.MultivariateNormal(torch.zeros(1).cuda(), torch.ones(1, 1).cuda())\n\nMultivariateNormal(loc: tensor([0.], device='cuda:0'), covariance_matrix: tensor([[1.]], device='cuda:0'))\n\n\n\nl_cuda.model.covar_module.latent_kernels[0].lengthscale.device\n\ndevice(type='cuda', index=0)\n\n\n\nl_cuda.train()"
  },
  {
    "objectID": "GPFA/learner.html#multi-dimensional-latent",
    "href": "GPFA/learner.html#multi-dimensional-latent",
    "title": "GPFA Learner",
    "section": "Multi-dimensional latent",
    "text": "Multi-dimensional latent\n\nl2 = GPFALearner(X, T, latent_dims=2)\n\n\nl2.train()\n\n\n\n\n\nplt.plot(l2.losses)\n\n\n\n\n\nl2.model.covar_module.Lambda\n\nParameter containing:\ntensor([[1.6626, 1.0702],\n        [1.5197, 1.1574],\n        [1.3999, 1.2026]], requires_grad=True)"
  },
  {
    "objectID": "GPFA/learner.html#saving",
    "href": "GPFA/learner.html#saving",
    "title": "GPFA Learner",
    "section": "Saving",
    "text": "Saving\nload and save models parameters\n\nsource\n\nGPFALearner.load\n\n GPFALearner.load (path:pathlib.Path|str)\n\n\nsource\n\n\nGPFALearner.save\n\n GPFALearner.save (path:pathlib.Path|str)\n\n\nfrom tempfile import tempdir\n\n\np = Path(tempdir) / \"model_test.pickle\"\n\n\nl.save(p)\n\n\nnl = GPFALearner(X)\nnl.model.state_dict()\n\nOrderedDict([('likelihood.noise_covar.raw_noise', tensor([0.])),\n             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n              tensor(1.0000e-04)),\n             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.Lambda',\n              tensor([[0.6782],\n                      [0.2307],\n                      [0.7072]])),\n             ('covar_module.raw_psi_diag', tensor([0., 0., 0.])),\n             ('covar_module.latent_kernels.0.raw_lengthscale', tensor([[0.]])),\n             ('covar_module.latent_kernels.0.raw_lengthscale_constraint.lower_bound',\n              tensor(0.)),\n             ('covar_module.latent_kernels.0.raw_lengthscale_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.raw_psi_diag_constraint.lower_bound', tensor(0.)),\n             ('covar_module.raw_psi_diag_constraint.upper_bound',\n              tensor(inf))])\n\n\n\nnl.load(p)\nnl.model.state_dict()\n\nOrderedDict([('likelihood.noise_covar.raw_noise', tensor([-10.4628])),\n             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n              tensor(1.0000e-04)),\n             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.Lambda',\n              tensor([[2.2274],\n                      [2.2276],\n                      [2.2281]])),\n             ('covar_module.raw_psi_diag',\n              tensor([ -7.4228, -11.5954,  -8.7099])),\n             ('covar_module.latent_kernels.0.raw_lengthscale',\n              tensor([[5.5376]])),\n             ('covar_module.latent_kernels.0.raw_lengthscale_constraint.lower_bound',\n              tensor(0.)),\n             ('covar_module.latent_kernels.0.raw_lengthscale_constraint.upper_bound',\n              tensor(inf)),\n             ('covar_module.raw_psi_diag_constraint.lower_bound', tensor(0.)),\n             ('covar_module.raw_psi_diag_constraint.upper_bound',\n              tensor(inf))])"
  },
  {
    "objectID": "GPFA/learner.html#plot-learning-progress",
    "href": "GPFA/learner.html#plot-learning-progress",
    "title": "GPFA Learner",
    "section": "Plot learning progress",
    "text": "Plot learning progress\n\n@patch\ndef plot_progress(self: GPFALearner, size={'width': 250, 'height': 120}):\n    \n    sel = alt.selection_interval(bind=\"scales\", encodings=['x'])\n    \n    plt_losses = alt.Chart(\n        pd.DataFrame({'loss': self.losses, 'n_iter': range(self.losses.shape[0])})\n    ).mark_line().encode(\n        x = 'n_iter',\n        y = 'loss'\n    ).properties(title=\"loss\", **size).add_selection(sel)\n    \n    out_plot = [plt_losses]\n    for info_name in self.model_infos[0].keys():\n        \n        values = pd.concat([info[info_name].assign(n_iter=i) for i, info in enumerate(self.model_infos)])\n        \n        if values.shape[1] == 2:\n            # only one column so add fake facet\n            values.insert(0, 'info', info_name)\n        \n        facet = values.columns[0] # first column is either latent or variable\n        \n        values = values.melt([facet, 'n_iter'], var_name='prop')\n        \n        plt = alt.Chart(values).mark_line().encode(\n            x = 'n_iter',\n            y = 'value',\n            color = 'prop',\n            facet = facet\n        ).properties(title=info_name, **size).add_selection(sel)\n        \n        out_plot.append(plt)\n    \n    return alt.VConcatChart(vconcat=out_plot).resolve_scale(\n        color='independent'\n    )\n\n\nsource\n\nGPFALearner.plot_progress\n\n GPFALearner.plot_progress (size={'width': 250, 'height': 120})\n\n\nl.plot_progress()\n\n/home/simone/.local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for col_name, dtype in df.dtypes.iteritems():\n\n\n\n\n\n\n\n\nself = l\n\nv =[]\nfor info_name in self.model_infos[0].keys(): \n    values = pd.concat([info[info_name].assign(n_iter=i) for i, info in enumerate(self.model_infos)])\n    v.append(values)\n\n\nv\n\n[          z0  n_iter\n 0   0.746957       0\n 1   0.404298       0\n 2   0.050215       0\n 0   0.646957       1\n 1   0.504298       1\n ..       ...     ...\n 1   2.226480     198\n 2   2.226368     198\n 0   2.226924     199\n 1   2.226828     199\n 2   2.227455     199\n \n [600 rows x 2 columns],\n    latent  lengthscale  n_iter\n 0      z0     0.693147       0\n 0      z0     0.744397       1\n 0      z0     0.798158       2\n 0      z0     0.854517       3\n 0      z0     0.913540       4\n ..    ...          ...     ...\n 0      z0     5.539773     195\n 0      z0     5.541732     196\n 0      z0     5.540282     197\n 0      z0     5.537724     198\n 0      z0     5.537501     199\n \n [200 rows x 3 columns],\n    variable       psi  n_iter\n 0      None  0.693147       0\n 1      None  0.693147       0\n 2      None  0.693147       0\n 0      None  0.644397       1\n 1      None  0.644397       1\n ..      ...       ...     ...\n 1      None  0.000010     198\n 2      None  0.000164     198\n 0      None  0.000600     199\n 1      None  0.000009     199\n 2      None  0.000164     199\n \n [600 rows x 3 columns],\n        noise  n_iter\n 0   0.693247       0\n 0   0.644497       1\n 0   0.598178       2\n 0   0.554247       3\n 0   0.512663       4\n ..       ...     ...\n 0   0.000131     195\n 0   0.000131     196\n 0   0.000130     197\n 0   0.000130     198\n 0   0.000129     199\n \n [200 rows x 2 columns]]"
  },
  {
    "objectID": "GPFA/learner.html#export",
    "href": "GPFA/learner.html#export",
    "title": "GPFA Learner",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "GPFA/learner.html#other",
    "href": "GPFA/learner.html#other",
    "title": "GPFA Learner",
    "section": "Other",
    "text": "Other\n\n# #| exporti\n# def get_parameter_value(name, param, constraint):\n#     if constraint is not None:\n#         value = constraint.transform(param.data.detach())\n#         name = name.replace(\"raw_\", \"\") # parameter is not raw anymore\n#     else:\n#         value = param.data.detach()\n#     return (name, value)\n\n# name = \"covar_module.psi\"\n# test_eq(l.model.covar_module.psi.detach(), get_parameter_value(name, l.model.covar_module.raw_psi_diag, l.model.covar_module.raw_psi_diag_constraint)[1])\n\n# #| exporti\n# def tensor_to_first_item(tensor):\n#     if tensor.dim() > 0:\n#         return tensor_to_first_item(tensor[0])\n#     return tensor.item()\n\n\n# def format_parameter(name, value):\n#     value = tensor_to_first_item(value)\n#     name = name.split(\".\")[-1] # get only last part of name\n#     return f\"{name}: {value:.3f}\"\n\n# #| export\n# @patch\n# def get_formatted_params(self: GPFALearner):\n#     return \", \".join([\n#         format_parameter(*get_parameter_value(name, value, constraint))\n#         for name, value, constraint in\n#         self.model.named_parameters_and_constraints()\n#     ])\n\n# l.get_formatted_params()\n\n# # this is not really working at the moment, but it's not important\n# @patch\n# def plot_loss_printer(self: GPFALearner, i_iter):\n#     if i_iter ==0: return\n#     x = torch.arange(0, i_iter)\n#     y = self.losses[:i_iter]\n#     plot_data = [[x, y]]\n#     self.pb.update_graph(plot_data)\n    \n#     x_bounds = [x.min(), x.max()+1]\n#     y_bounds = [y.min(), y.max()]\n#     self.pb.names = [\"Training loss\"]\n\n# #|export\n# @patch\n# def printer(self: GPFALearner, i_iter):\n\n#     if i_iter%10 == 0:\n#         update_str = f\"loss: {self.losses[i_iter].item():.3f}, \" + self.get_formatted_params()\n#         #self.plot_loss(i_iter)\n    \n#     #self.pb.write(update_str)\n\n# l.train(lr = 0.01)\n\n# import matplotlib.pyplot as plt\n\n# plt.plot(l.losses)"
  },
  {
    "objectID": "GPFA/imputation.html",
    "href": "GPFA/imputation.html",
    "title": "Imputation time series",
    "section": "",
    "text": "the goal of this notebook is to be able to:"
  },
  {
    "objectID": "GPFA/imputation.html#gpfa-imputation",
    "href": "GPFA/imputation.html#gpfa-imputation",
    "title": "Imputation time series",
    "section": "GPFA Imputation",
    "text": "GPFA Imputation\nThis is the core class that does the imputation using a GPFA\nThe inputs is:\n\na dataframe containing the observed data, where the row with missing data have been removed\na vector of times where the data is missing\n\nit returns:\n\na complete dataframe with the prediction of the model\n\nThe goal is that GPFAImputation takes as imput a dataframe containing missing values and then it imputes them using GPFALearner. Therefore it needs to divide the dataframe in 3 sections:\n\ntraining data (rows with no NAs)\ntimes to be imputed (rows with some NAs)\nobservations (variables in the pred rows that are not missing) for conditional predictions\n\n\nt_df = pd.DataFrame([\n    [1., 3., 4.],\n    [2., 6., np.nan],\n    [np.nan, np.nan, np.nan],\n    [np.nan, 8., np.nan],\n    [3., 4., 5.]\n]\n)\n\n\ntrain_idx = ~t_df.isna().any(1)\n\n/tmp/ipykernel_6506/3664883938.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n  train_idx = ~t_df.isna().any(1)\n\n\n\nt_df[train_idx]\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      1.0\n      3.0\n      4.0\n    \n    \n      4\n      3.0\n      4.0\n      5.0\n    \n  \n\n\n\n\n\npred_data = t_df[~train_idx].to_numpy()\n\n\npred_data\n\narray([[ 2.,  6., nan],\n       [nan, nan, nan],\n       [nan,  8., nan]])\n\n\n\nidx_cond = ~t_df[~train_idx].isna()\n\n\nidx_cond\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      1\n      True\n      True\n      False\n    \n    \n      2\n      False\n      False\n      False\n    \n    \n      3\n      False\n      True\n      False\n    \n  \n\n\n\n\n\nidx_cond = idx_cond.to_numpy().flatten() # learner needs a 1D index\n\npred_data.flatten()[idx_cond]\n\ntrain_idx = t_df.isna().any(axis=1)\n\ntorch.tensor(~t_df[~train_idx].isna().to_numpy().flatten())\n\ntensor([True, True, True, True, True, True])\n\n\nImplement this into a function\n\nclass GPFAImputation:\n    def __init__(\n        self,\n        data: pd.DataFrame , #observed data with missing data as NA\n        latent_dims = 1,\n        cuda = False, # Use GPU?\n        model = GPFA # sub-class of `GPFA` \n    ):\n        self.data = data.copy()\n        self.latent_dims = latent_dims\n        \n        device = 'cuda' if cuda else 'cpu'\n        \n        self.T = torch.arange(0, len(data), dtype=torch.float32, device=device) # time is encoded with a increase of 1\n        \n        # Training data\n        self.train_idx = ~self.data.isna().any(axis=1)\n        self.train_data = torch.tensor(self.data[self.train_idx].to_numpy().astype(np.float32), device=device)\n        self.train_T = self.T[self.train_idx]\n        \n        self.learner = GPFALearner(X = self.train_data, T = self.train_T, latent_dims=latent_dims, model=model, var_names= self.data.columns)\n        \n\n        # Prediction data\n        self.pred_T = self.T[~self.train_idx]\n        self.cond_idx = torch.tensor(~self.data[~self.train_idx].isna().to_numpy().flatten(), device=device) # conditional obsevations\n        self.cond_obs = torch.tensor(self.data[~self.train_idx].to_numpy().astype(np.float32).flatten()[self.cond_idx.cpu()], device=device)\n        \n        if cuda: self.learner.cuda()\n        \n                                   \n    def fit(self, n_iter=100):\n        \"Fit learner to training data\"\n        self.learner.train(n_iter = n_iter)\n        return self\n\n    def impute(self,\n               add_time = True, # add column with time?\n               tidy = True, # tidy data?\n               ):\n        \n        self.pred = self.learner.predict(self.pred_T, obs = self.cond_obs, idx = self.cond_idx)\n        if not hasattr(self, \"pred\"):\n            self.fit()\n\n        \n        if tidy: return self._impute_tidy(add_time)\n        else: return self._impute_wide(add_time)\n        \n        \n    def _impute_wide(self, add_time):\n        \"\"\" Impute in wide format\"\"\"\n        \n        imp_data = self.data.copy()\n        for col_idx, col_name in enumerate(imp_data.columns):\n            imp_data.loc[~self.train_idx, col_name] = self.pred.mean[:, col_idx].cpu().numpy()\n            imp_data.loc[~self.train_idx, col_name + \"_std\"] = self.pred.std[:, col_idx].cpu().numpy()\n        \n        if add_time:\n            imp_data[\"time\"] = self.T.cpu()\n        \n        return imp_data \n    \n    def _impute_tidy(self, add_time):\n        \"\"\" transform the pred output into a tidy dataframe suitable for plotting\"\"\"\n        feature_names = self.data.columns\n\n        pred_mean = pd.DataFrame(self.pred.mean.cpu(), columns = feature_names).assign(time = self.pred_T.cpu()).melt(\"time\", value_name=\"mean\")\n        pred_std = pd.DataFrame(self.pred.std.cpu(), columns = feature_names).assign(time = self.pred_T.cpu()).melt(\"time\", value_name=\"std\")\n        \n        pred = pd.merge(pred_mean, pred_std, on=['time', 'variable'])  \n        \n        train_data = self.data[self.train_idx].assign(time = self.train_T.cpu()).melt(\"time\", value_name = \"mean\")\n               \n        imp_data = pd.concat((train_data, pred))\n        \n        self.pred_wide = imp_data\n        \n        return imp_data\n\n\nsource\n\nGPFAImputation\n\n GPFAImputation (data:pandas.core.frame.DataFrame, latent_dims=1,\n                 cuda=False, model=<class 'meteo_imp.gpfa.gpfa.GPFA'>)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nobserved data with missing data as NA\n\n\nlatent_dims\nint\n1\n\n\n\ncuda\nbool\nFalse\nUse GPU?\n\n\nmodel\ntype\nGPFA\nsub-class of GPFA\n\n\n\n\nfd = MeteoDataTest.generate_gpfa(2, 10, Lambda=[1,2.]).add_random_missing()\n\n\nfd.data\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n    \n  \n  \n    \n      0\n      -0.024085\n      0.089268\n    \n    \n      1\n      -0.133942\n      0.258532\n    \n    \n      2\n      -0.604650\n      -0.603501\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      -0.478994\n      -1.161096\n    \n    \n      5\n      NaN\n      NaN\n    \n    \n      6\n      -0.594717\n      -1.284512\n    \n    \n      7\n      NaN\n      1.251743\n    \n    \n      8\n      NaN\n      -2.001107\n    \n    \n      9\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nimp = GPFAImputation(fd.data)\n\n\nimp\n\n<__main__.GPFAImputation>\n\n\nTidy\n\nimp.impute()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.024085\n      NaN\n    \n    \n      1\n      1.0\n      x0\n      -0.133942\n      NaN\n    \n    \n      2\n      2.0\n      x0\n      -0.604650\n      NaN\n    \n    \n      3\n      4.0\n      x0\n      -0.478994\n      NaN\n    \n    \n      4\n      6.0\n      x0\n      -0.594717\n      NaN\n    \n    \n      5\n      0.0\n      x1\n      0.089268\n      NaN\n    \n    \n      6\n      1.0\n      x1\n      0.258532\n      NaN\n    \n    \n      7\n      2.0\n      x1\n      -0.603501\n      NaN\n    \n    \n      8\n      4.0\n      x1\n      -1.161096\n      NaN\n    \n    \n      9\n      6.0\n      x1\n      -1.284512\n      NaN\n    \n    \n      0\n      3.0\n      x0\n      -0.416516\n      0.386124\n    \n    \n      1\n      5.0\n      x0\n      -0.419649\n      0.386142\n    \n    \n      2\n      7.0\n      x0\n      -0.358267\n      0.387923\n    \n    \n      3\n      8.0\n      x0\n      -0.397247\n      0.390719\n    \n    \n      4\n      9.0\n      x0\n      -0.384120\n      0.391765\n    \n    \n      5\n      3.0\n      x1\n      -0.562824\n      0.833950\n    \n    \n      6\n      5.0\n      x1\n      -0.564259\n      0.833952\n    \n    \n      7\n      7.0\n      x1\n      1.251743\n      0.000000\n    \n    \n      8\n      8.0\n      x1\n      -2.001107\n      0.000000\n    \n    \n      9\n      9.0\n      x1\n      -0.547979\n      0.834502\n    \n  \n\n\n\n\nwide\n\nimp.impute(tidy=False)\n\n\n\n\n\n  \n    \n      \n      x0\n      x1\n      x0_std\n      x1_std\n      time\n    \n  \n  \n    \n      0\n      -0.024085\n      0.089268\n      NaN\n      NaN\n      0.0\n    \n    \n      1\n      -0.133942\n      0.258532\n      NaN\n      NaN\n      1.0\n    \n    \n      2\n      -0.604650\n      -0.603501\n      NaN\n      NaN\n      2.0\n    \n    \n      3\n      -0.416516\n      -0.562824\n      0.386124\n      0.833950\n      3.0\n    \n    \n      4\n      -0.478994\n      -1.161096\n      NaN\n      NaN\n      4.0\n    \n    \n      5\n      -0.419649\n      -0.564259\n      0.386142\n      0.833952\n      5.0\n    \n    \n      6\n      -0.594717\n      -1.284512\n      NaN\n      NaN\n      6.0\n    \n    \n      7\n      -0.358267\n      1.251743\n      0.387923\n      0.000000\n      7.0\n    \n    \n      8\n      -0.397247\n      -2.001107\n      0.390719\n      0.000000\n      8.0\n    \n    \n      9\n      -0.384120\n      -0.547979\n      0.391765\n      0.834502\n      9.0\n    \n  \n\n\n\n\n\n\nGPU\ncheck that the GPU support is working\n\nimp_gpu = GPFAImputation(fd.data, cuda=True)\n\n\nimp_gpu.impute()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.024085\n      NaN\n    \n    \n      1\n      1.0\n      x0\n      -0.133942\n      NaN\n    \n    \n      2\n      2.0\n      x0\n      -0.604650\n      NaN\n    \n    \n      3\n      4.0\n      x0\n      -0.478994\n      NaN\n    \n    \n      4\n      6.0\n      x0\n      -0.594717\n      NaN\n    \n    \n      5\n      0.0\n      x1\n      0.089268\n      NaN\n    \n    \n      6\n      1.0\n      x1\n      0.258532\n      NaN\n    \n    \n      7\n      2.0\n      x1\n      -0.603501\n      NaN\n    \n    \n      8\n      4.0\n      x1\n      -1.161096\n      NaN\n    \n    \n      9\n      6.0\n      x1\n      -1.284512\n      NaN\n    \n    \n      0\n      3.0\n      x0\n      -0.370183\n      0.319018\n    \n    \n      1\n      5.0\n      x0\n      -0.372927\n      0.319018\n    \n    \n      2\n      7.0\n      x0\n      -0.355584\n      0.318874\n    \n    \n      3\n      8.0\n      x0\n      -0.377191\n      0.318883\n    \n    \n      4\n      9.0\n      x0\n      -0.373072\n      0.319039\n    \n    \n      5\n      3.0\n      x1\n      -0.634743\n      0.998508\n    \n    \n      6\n      5.0\n      x1\n      -0.723967\n      0.998508\n    \n    \n      7\n      7.0\n      x1\n      1.251743\n      0.000000\n    \n    \n      8\n      8.0\n      x1\n      -2.001107\n      0.000000\n    \n    \n      9\n      9.0\n      x1\n      -0.728671\n      1.005502\n    \n  \n\n\n\n\nthe gpu and cpu version return similar results!\n\nimp.impute()[[\"mean\", \"std\"]].to_numpy() - imp_gpu.impute()[[\"mean\", \"std\"]].to_numpy()\n\narray([[ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [ 0.0000000e+00,            nan],\n       [-4.6333283e-02,  6.7106664e-02],\n       [-4.6722233e-02,  6.7123741e-02],\n       [-2.6827753e-03,  6.9049209e-02],\n       [-2.0056546e-02,  7.1836144e-02],\n       [-1.1047989e-02,  7.2726548e-02],\n       [ 7.1919739e-02, -1.6455758e-01],\n       [ 1.5970773e-01, -1.6455603e-01],\n       [ 0.0000000e+00,  0.0000000e+00],\n       [ 2.3841858e-07,  0.0000000e+00],\n       [ 1.8069166e-01, -1.7099941e-01]], dtype=float32)\n\n\n\n\nRepr\nadd __repr__ and __str__ to imputation objects\n\n@patch\ndef __repr__(self: GPFAImputation):\n    return f\"\"\"GPFA Imputation:\n    N obs: {self.data.shape[0]}\n    N features {self.data.shape[1]} ({', '.join(self.data.columns)})\n    N missing observations {(~self.cond_idx).sum()}\n    N latent: {self.learner.latent_dims}\"\"\"\n\n@patch\ndef __str__(self: GPFAImputation):\n    return self.__repr__()\n\n\nimp\n\nGPFA Imputation:\n    N obs: 10\n    N features 2 (x0, x1)\n    N missing observations 8\n    N latent: 1\n\n\n\nstr(imp)\n\n'GPFA Imputation:\\n    N obs: 10\\n    N features 2 (x0, x1)\\n    N missing observations 8\\n    N latent: 1'"
  },
  {
    "objectID": "GPFA/imputation.html#gpfa-imputation-explorer",
    "href": "GPFA/imputation.html#gpfa-imputation-explorer",
    "title": "Imputation time series",
    "section": "GPFA Imputation Explorer",
    "text": "GPFA Imputation Explorer\nThis is a class that is used for exploring the results for a GPFAImputation, the main difference is that it always return the model predictions and not only the training data\n\nclass GPFAImputationExplorer(GPFAImputation):\n    \"GPFAImputation where predictions are for all times not only missing data\"\n    \n    def predict(self):\n        \"Predict for all times, also when there is an observation, supporting cond obs, with valid std\"\n        imp_mean = pd.DataFrame({'time': self.T.cpu()})\n        imp_std = pd.DataFrame({'time': self.T.cpu()})\n        \n        # Fill using general predictions\n        \n        all_pred = self.learner.predict(self.T)\n        \n        for col_idx, col_name in enumerate(self.data.columns):\n            imp_mean.loc[:, col_name] = all_pred.mean[:, col_idx].cpu().numpy()\n            imp_std.loc[:, col_name] = all_pred.std[:, col_idx].cpu().numpy()\n        \n        # Fine tune with cond predictions\n        \n        pred_cond = self.learner.predict(self.pred_T, obs = self.cond_obs, idx = self.cond_idx)\n        obs_mask = self.cond_idx.reshape(-1, self.data.shape[1]).cpu().numpy()\n\n        for col_idx, col_name in enumerate(self.data.columns):\n            mean= pred_cond.mean[:, col_idx].cpu().numpy()\n            std = pred_cond.std[:, col_idx].cpu().numpy()\n\n            # when there is a cond obs the std is nan, which is replaced with the std without the conditional prediction\n            mask_data = ~self.train_idx.to_numpy()\n            mask_data[mask_data] = ~obs_mask[:, col_idx]\n\n            imp_mean.loc[mask_data, col_name] = mean[~obs_mask[:, col_idx]]\n            imp_std.loc[mask_data, col_name] = std[~obs_mask[:, col_idx]]\n        \n        # make tidy\n        \n        return pd.merge(\n            imp_mean.melt('time', value_name = \"mean\"),\n            imp_std.melt('time', value_name = \"std\"),\n            on = ['time', 'variable']\n        )\n\n\nsource\n\nGPFAImputationExplorer\n\n GPFAImputationExplorer (data:pandas.core.frame.DataFrame, latent_dims=1,\n                         cuda=False, model=<class\n                         'meteo_imp.gpfa.gpfa.GPFA'>)\n\nGPFAImputation where predictions are for all times not only missing data\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nDataFrame\n\nobserved data with missing data as NA\n\n\nlatent_dims\nint\n1\n\n\n\ncuda\nbool\nFalse\nUse GPU?\n\n\nmodel\ntype\nGPFA\nsub-class of GPFA\n\n\n\n\nGPFAImputationExplorer(fd.data).fit().predict()\n\n\n\n\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\ntorch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\nX = torch.triangular_solve(B, A).solution\nshould be replaced with\nX = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2183.)\n  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n/home/simone/anaconda3/envs/data-science/lib/python3.10/site-packages/torch/autograd/__init__.py:173: UserWarning: operator() profile_node %840 : int[] = prim::profile_ivalue(%838)\n does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.138583\n      0.123488\n    \n    \n      1\n      1.0\n      x0\n      -0.115288\n      0.123250\n    \n    \n      2\n      2.0\n      x0\n      -0.398590\n      0.123467\n    \n    \n      3\n      3.0\n      x0\n      -0.472666\n      0.149725\n    \n    \n      4\n      4.0\n      x0\n      -0.566287\n      0.123645\n    \n    \n      5\n      5.0\n      x0\n      -0.994120\n      0.149824\n    \n    \n      6\n      6.0\n      x0\n      -0.611462\n      0.123656\n    \n    \n      7\n      7.0\n      x0\n      0.127221\n      0.123713\n    \n    \n      8\n      8.0\n      x0\n      -0.786960\n      0.123999\n    \n    \n      9\n      9.0\n      x0\n      -1.270282\n      0.183502\n    \n    \n      10\n      0.0\n      x1\n      0.130877\n      0.154138\n    \n    \n      11\n      1.0\n      x1\n      0.199238\n      0.152487\n    \n    \n      12\n      2.0\n      x1\n      -0.632152\n      0.153993\n    \n    \n      13\n      3.0\n      x1\n      -0.849540\n      0.292393\n    \n    \n      14\n      4.0\n      x1\n      -1.124286\n      0.155218\n    \n    \n      15\n      5.0\n      x1\n      -2.379823\n      0.292828\n    \n    \n      16\n      6.0\n      x1\n      -1.256858\n      0.155297\n    \n    \n      17\n      7.0\n      x1\n      -0.990569\n      0.469497\n    \n    \n      18\n      8.0\n      x1\n      -0.675226\n      0.612087\n    \n    \n      19\n      9.0\n      x1\n      -3.190263\n      0.427115\n    \n  \n\n\n\n\n\n\nRepr\nadd __repr__ and __str__ to imputation objects\n\n@patch\ndef __repr__(self: GPFAImputationExplorer):\n    return f\"\"\"GPFA Imputation Explorer:\n    N obs: {self.data.shape[0]}\n    N features {self.data.shape[1]} ({', '.join(self.data.columns)})\n    N missing observations {self.data.isna().to_numpy().flatten().sum()}\n    N latent: {self.learner.latent_dims}\"\"\"\n\n@patch\ndef __str__(self: GPFAImputationExplorer):\n    return self.__repr__()\n\n\nimp_exp = GPFAImputationExplorer(fd.data)\n\n\nimp_exp\n\nGPFA Imputation Explorer:\n    N obs: 10\n    N features 2 (x0, x1)\n    N missing observations 8\n    N latent: 1\n\n\n\nstr(imp)\n\n'GPFA Imputation:\\n    N obs: 10\\n    N features 2 (x0, x1)\\n    N missing observations 8\\n    N latent: 1'\n\n\n\nimp_exp.predict()\n\n\n\n\n\n  \n    \n      \n      time\n      variable\n      mean\n      std\n    \n  \n  \n    \n      0\n      0.0\n      x0\n      -0.264771\n      0.343444\n    \n    \n      1\n      1.0\n      x0\n      -0.286058\n      0.342875\n    \n    \n      2\n      2.0\n      x0\n      -0.390137\n      0.343443\n    \n    \n      3\n      3.0\n      x0\n      -0.399087\n      0.349827\n    \n    \n      4\n      4.0\n      x0\n      -0.413978\n      0.344008\n    \n    \n      5\n      5.0\n      x0\n      -0.409062\n      0.349828\n    \n    \n      6\n      6.0\n      x0\n      -0.437668\n      0.344009\n    \n    \n      7\n      7.0\n      x0\n      -0.324878\n      0.347765\n    \n    \n      8\n      8.0\n      x0\n      -0.412507\n      0.348563\n    \n    \n      9\n      9.0\n      x0\n      -0.393487\n      0.351376\n    \n    \n      10\n      0.0\n      x1\n      -0.360775\n      0.857899\n    \n    \n      11\n      1.0\n      x1\n      -0.398049\n      0.857201\n    \n    \n      12\n      2.0\n      x1\n      -0.580288\n      0.857897\n    \n    \n      13\n      3.0\n      x1\n      -0.595959\n      0.865770\n    \n    \n      14\n      4.0\n      x1\n      -0.622034\n      0.858591\n    \n    \n      15\n      5.0\n      x1\n      -0.613425\n      0.865772\n    \n    \n      16\n      6.0\n      x1\n      -0.663515\n      0.858592\n    \n    \n      17\n      7.0\n      x1\n      -0.583371\n      0.866950\n    \n    \n      18\n      8.0\n      x1\n      -0.542161\n      0.868132\n    \n    \n      19\n      9.0\n      x1\n      -0.586155\n      0.867691"
  },
  {
    "objectID": "GPFA/imputation.html#export",
    "href": "GPFA/imputation.html#export",
    "title": "Imputation time series",
    "section": "Export",
    "text": "Export"
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "results.html#export",
    "href": "results.html#export",
    "title": "Results",
    "section": "Export",
    "text": "Export"
  }
]